#define CRYPTOPP_ENABLE_NAMESPACE_WEAK 1
// rapidjson configuration macros
#define RAPIDJSON_HAS_STDSTRING 1
// Default rjson policy is to use assert() - which is dangerous for two reasons:
// 1. assert() can be turned off with -DNDEBUG
// 2. assert() crashes a program
// Fortunately, the default policy can be overridden, and so rapidjson errors will
// throw an rjson::error exception instead.
#define RAPIDJSON_ASSERT(x) (void)(x)
// This macro is used for functions which are called for every json char making it
// quite costly if not inlined, by default rapidjson only enables it if NDEBUG
// is defined which isn't the case for us.
#define RAPIDJSON_FORCEINLINE __attribute__((always_inline))
#undef SEASTAR_TESTING_MAIN
#include <immintrin.h>
#include <absl/container/btree_set.h>
#include <absl/container/flat_hash_map.h>
#include <any>
#include <boost/algorithm/clamp.hpp>
#include <boost/algorithm/cxx11/any_of.hpp>
#include <boost/algorithm/string.hpp>
#include <boost/circular_buffer.hpp>
#include <boost/dynamic_bitset.hpp>
#include <boost/heap/binomial_heap.hpp>
#include <boost/icl/interval_set.hpp>
#include <boost/intrusive/set.hpp>
#include <boost/intrusive/unordered_set.hpp>
#include <boost/outcome/result.hpp>
#include <boost/program_options.hpp>
#include <boost/range/adaptors.hpp>
#include <boost/range/algorithm_ext/push_back.hpp>
#include <boost/range/algorithm.hpp>
#include <boost/range/join.hpp>
#include <boost/regex/icu.hpp>
#include <boost/signals2.hpp>
#include <cryptopp/md5.h>
#include <cryptopp/sha.h>
#include <fmt/chrono.h>
#include <link.h>
#include <rapidjson/document.h>
#include <rapidjson/writer.h>
#include <sanitizer/asan_interface.h>
#include <seastar/core/checked_ptr.hh>
#include <seastar/core/distributed.hh>
#include <seastar/core/execution_stage.hh>
#include <seastar/core/expiring_fifo.hh>
#include <seastar/core/rwlock.hh>
#include <seastar/core/shared_mutex.hh>
#include <seastar/core/sleep.hh>
#include <seastar/coroutine/maybe_yield.hh>
#include <seastar/http/client.hh>
#include <seastar/net/dns.hh>
#include <seastar/net/ipv4_address.hh>
#include <seastar/rpc/rpc.hh>
#include <seastar/testing/test_case.hh>
#include <seastar/testing/test_runner.hh>
#include <seastar/testing/thread_test_case.hh>
#include <seastar/util/alloc_failure_injector.hh>
#include <seastar/util/closeable.hh>
#include <seastar/util/later.hh>
#include <seastar/util/lazy.hh>
#include <unwind.h>
#include <xxhash.h>
#include <yaml-cpp/yaml.h>
// the database clock follows Java - 1ms granularity, 64-bit counter, 1970 epoch
extern std::atomic<int64_t> clocks_offset;
;
// Returns a time point which is earlier from t by d, or minimum time point if it cannot be represented.
template<typename Clock, typename Duration, typename Rep, typename Period>
inline
auto saturating_subtract(std::chrono::time_point<Clock, Duration> t, std::chrono::duration<Rep, Period> d) -> decltype(t) {
    return std::max(t, decltype(t)::min() + d) - d;
}
namespace seastar {
template <typename T>
class shared_ptr;
template <typename T, typename... A>
shared_ptr<T> make_shared(A&&... a);
}
using namespace seastar;
using seastar::shared_ptr;
using seastar::make_shared;
//
// This hashing differs from std::hash<> in that it decouples knowledge about
// type structure from the way the hash value is calculated:
//  * appending_hash<T> instantiation knows about what data should be included in the hash for type T.
//  * Hasher object knows how to combine the data into the final hash.
//
// The appending_hash<T> should always feed some data into the hasher, regardless of the state the object is in,
// in order for the hash to be highly sensitive for value changes. For example, vector<optional<T>> should
// ideally feed different values for empty vector and a vector with a single empty optional.
//
// appending_hash<T> is machine-independent.
//
template<typename H>
concept Hasher =
    requires(H& h, const char* ptr, size_t size) {
        { h.update(ptr, size) } noexcept -> std::same_as<void>;
    };
template<typename H, typename ValueType>
concept HasherReturning = Hasher<H> &&
    requires (H& h) {
        { h.finalize() } -> std::convertible_to<ValueType>;
    };
class hasher {
public:
    virtual ~hasher() = default;
    virtual void update(const char* ptr, size_t size) noexcept = 0;
};
template<typename T>
struct appending_hash;
template<typename H, typename T, typename... Args>
requires Hasher<H>
inline
void feed_hash(H& h, const T& value, Args&&... args) noexcept(noexcept(std::declval<appending_hash<T>>()(h, value, args...))) {
    appending_hash<T>()(h, value, std::forward<Args>(args)...);
};
template<typename T>
requires std::is_arithmetic_v<T>
struct appending_hash<T> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, T value) const noexcept {
        auto value_le = cpu_to_le(value);
        h.update(reinterpret_cast<const char*>(&value_le), sizeof(T));
    }
};
template<>
struct appending_hash<bool> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, bool value) const noexcept {
        feed_hash(h, static_cast<uint8_t>(value));
    }
};
template<typename T>
requires std::is_enum_v<T>
struct appending_hash<T> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const T& value) const noexcept {
        feed_hash(h, static_cast<std::underlying_type_t<T>>(value));
    }
};
template<typename T>
struct appending_hash<std::optional<T>>  {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const std::optional<T>& value) const noexcept {
        if (value) {
            feed_hash(h, true);
            feed_hash(h, *value);
        } else {
            feed_hash(h, false);
        }
    }
};
template<size_t N>
struct appending_hash<char[N]>  {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const char (&value) [N]) const noexcept {
        feed_hash(h, N);
        h.update(value, N);
    }
};
template<typename T>
struct appending_hash<std::vector<T>> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const std::vector<T>& value) const noexcept {
        feed_hash(h, value.size());
        for (auto&& v : value) {
            appending_hash<T>()(h, v);
        }
    }
};
template<typename K, typename V>
struct appending_hash<std::map<K, V>> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const std::map<K, V>& value) const noexcept {
        feed_hash(h, value.size());
        for (auto&& e : value) {
            appending_hash<K>()(h, e.first);
            appending_hash<V>()(h, e.second);
        }
    }
};
template<>
struct appending_hash<sstring> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const sstring& v) const noexcept {
        feed_hash(h, v.size());
        h.update(reinterpret_cast<const char*>(v.cbegin()), v.size() * sizeof(sstring::value_type));
    }
};
template<>
struct appending_hash<std::string> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const std::string& v) const noexcept {
        feed_hash(h, v.size());
        h.update(reinterpret_cast<const char*>(v.data()), v.size() * sizeof(std::string::value_type));
    }
};
template<typename T, typename R>
struct appending_hash<std::chrono::duration<T, R>> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, std::chrono::duration<T, R> v) const noexcept {
        feed_hash(h, v.count());
    }
};
template<typename Clock, typename Duration>
struct appending_hash<std::chrono::time_point<Clock, Duration>> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, std::chrono::time_point<Clock, Duration> v) const noexcept {
        feed_hash(h, v.time_since_epoch().count());
    }
};
class gc_clock final {
public:
    using base = seastar::lowres_system_clock;
    using rep = int64_t;
    using period = std::ratio<1, 1>; // seconds
    using duration = std::chrono::duration<rep, period>;
    using time_point = std::chrono::time_point<gc_clock, duration>;
    static constexpr auto is_steady = base::is_steady;
    
    
    static time_point now() noexcept ;
};
using expiry_opt = std::optional<gc_clock::time_point>;
using ttl_opt = std::optional<gc_clock::duration>;
// 20 years in seconds
static constexpr gc_clock::duration max_ttl = gc_clock::duration{20 * 365 * 24 * 60 * 60};
template<>
struct appending_hash<gc_clock::time_point> {
    template<typename Hasher>
    void operator()(Hasher& h, gc_clock::time_point t) const noexcept {
        // Remain backwards-compatible with the 32-bit duration::rep (refs #4460).
        uint64_t d64 = t.time_since_epoch().count();
        feed_hash(h, uint32_t(d64 & 0xffff'ffff));
        uint32_t msb = d64 >> 32;
        if (msb) {
            feed_hash(h, msb);
        }
    }
};
namespace ser {
// Forward-declaration - defined in serializer.hh, to avoid including it here.
;
template <typename Input>
int64_t deserialize_gc_clock_duration_value(Input& in);
template <typename T>
struct serializer;
template <>
struct serializer<gc_clock::duration> {
    template <typename Input>
    static gc_clock::duration read(Input& in) {
        return gc_clock::duration(deserialize_gc_clock_duration_value(in));
    }
    template <typename Output>
    static void write(Output& out, gc_clock::duration d) {
        serialize_gc_clock_duration_value(out, d.count());
    }
    template <typename Input>
    static void skip(Input& in) {
        read(in);
    }
};
}
class db_clock final {
public:
    using base = std::chrono::system_clock;
    using rep = int64_t;
    using period = std::ratio<1, 1000>; // milliseconds
    using duration = std::chrono::duration<rep, period>;
    using time_point = std::chrono::time_point<db_clock, duration>;
    static constexpr bool is_steady = base::is_steady;
    static constexpr std::time_t to_time_t(time_point t) {
        return std::chrono::duration_cast<std::chrono::seconds>(t.time_since_epoch()).count();
    }
};

inline
constexpr uint64_t clmul_u32_constexpr(uint32_t p1, uint32_t p2) {
    uint64_t result = 0;
    for (unsigned i = 0; i < 32; ++i) {
        result ^= (((p1 >> i) & 1) * uint64_t(p2)) << i;
    }
    return result;
}
// returns the low half of the result
inline
constexpr uint64_t clmul_u64_low_constexpr(uint64_t p1, uint64_t p2) {
    uint64_t result = 0;
    for (unsigned i = 0; i < 64; ++i) {
        result ^= (((p1 >> i) & 1) * p2) << i;
    }
    return result;
}
#if defined(__x86_64__) || defined(__i386__)
// Performs a carry-less multiplication of two integers.
uint64_t clmul_u32(uint32_t p1, uint32_t p2) ;
constexpr
inline
uint64_t clmul(uint32_t p1, uint32_t p2) {
    return std::is_constant_evaluated() ? clmul_u32_constexpr(p1, p2) : clmul_u32(p1, p2);
}
#elif defined(__aarch64__)
// Performs a carry-less multiplication of two integers.
inline
uint64_t clmul_u32(uint32_t p1, uint32_t p2) {
    return vmull_p64(p1, p2);
}
constexpr
inline
uint64_t clmul(uint32_t p1, uint32_t p2) {
    return std::is_constant_evaluated() ? clmul_u32_constexpr(p1, p2) : clmul_u32(p1, p2);
}
#endif
inline
constexpr uint64_t barrett_reduction_constants[2] = { 0x00000001F7011641, 0x00000001DB710641 };
inline constexpr uint32_t crc32_fold_barrett_u64_constexpr(uint64_t p) {
    auto x0 = p;
    auto x1 = x0;
    uint64_t mask32 = 0xffff'ffff;
    x0 = clmul_u64_low_constexpr(x0 & mask32, barrett_reduction_constants[0]);
    x0 = clmul_u64_low_constexpr(x0 & mask32, barrett_reduction_constants[1]);
    return (x0 ^ x1) >> 32;
}

uint32_t crc32_fold_barrett_u64_native(uint64_t p) ;
inline
constexpr
uint32_t crc32_fold_barrett_u64(uint64_t p) {
    return std::is_constant_evaluated() ? crc32_fold_barrett_u64_constexpr(p) : crc32_fold_barrett_u64_native(p);
}
template<typename CharT>
class basic_mutable_view {
    CharT* _begin = nullptr;
    CharT* _end = nullptr;
public:
    using value_type = CharT;
    using pointer = CharT*;
    using iterator = CharT*;
    using const_iterator = CharT*;
    basic_mutable_view() = default;
    template<typename U, U N>
    basic_mutable_view(basic_sstring<CharT, U, N>& str)
        : _begin(str.begin())
        , _end(str.end())
    { }
    basic_mutable_view(CharT* ptr, size_t length)
        : _begin(ptr)
        , _end(ptr + length)
    { }
    operator std::basic_string_view<CharT>() const noexcept {
        return std::basic_string_view<CharT>(begin(), size());
    }
    CharT& operator[](size_t idx) const { return _begin[idx]; }
    iterator begin() const { return _begin; }
    iterator end() const { return _end; }
    CharT* data() const { return _begin; }
    size_t size() const { return _end - _begin; }
    bool empty() const { return _begin == _end; }
    CharT& front() { return *_begin; }
    const CharT& front() const { return *_begin; }
    void remove_prefix(size_t n) {
        _begin += n;
    }
    void remove_suffix(size_t n) {
        _end -= n;
    }
    basic_mutable_view substr(size_t pos, size_t count) {
        size_t n = std::min(count, (_end - _begin) - pos);
        return basic_mutable_view{_begin + pos, n};
    }
};
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Warray-bounds"
#pragma GCC diagnostic pop
template<typename H>
concept SimpleHasher = HasherReturning<H, size_t>;
struct simple_xx_hasher : public hasher {
    XXH64_state_t _state;
    simple_xx_hasher(uint64_t seed = 0) noexcept {
        XXH64_reset(&_state, seed);
    }
    void update(const char* ptr, size_t length) noexcept override {
        XXH64_update(&_state, ptr, length);
    }
    size_t finalize() {
        return static_cast<size_t>(XXH64_digest(&_state));
    }
};
using bytes = basic_sstring<int8_t, uint32_t, 31, false>;
using bytes_view = std::basic_string_view<int8_t>;
using bytes_mutable_view = basic_mutable_view<bytes_view::value_type>;
using bytes_opt = std::optional<bytes>;
using sstring_view = std::string_view;
 
struct fmt_hex {
    const bytes_view& v;
    fmt_hex(const bytes_view& v) noexcept : v(v) {}
};
sstring to_hex(bytes_view b);


std::ostream& operator<<(std::ostream& os, const bytes& b);
template <>
struct fmt::formatter<fmt_hex> {
    size_t _group_size_in_bytes = 0;
    char _delimiter = ' ';
public:
    // format_spec := [group_size[delimeter]]
    // group_size := a char from '0' to '9'
    // delimeter := a char other than '{'  or '}'
    //
    // by default, the given bytes are printed without delimeter, just
    // like a string. so a string view of {0x20, 0x01, 0x0d, 0xb8} is
    // printed like:
    // "20010db8".
    //
    // but the format specifier can be used to customize how the bytes
    // are printed. for instance, to print an bytes_view like IPv6. so
    // the format specfier would be "{:2:}", where
    // - "2": bytes are printed in groups of 2 bytes
    // - ":": each group is delimeted by ":"
    // and the formatted output will look like:
    // "2001:0db8:0000"
    //
    // or we can mimic how the default format of used by hexdump using
    // "{:2 }", where
    // - "2": bytes are printed in group of 2 bytes
    // - " ": each group is delimeted by " "
    // and the formatted output will look like:
    // "2001 0db8 0000"
    //
    // or we can just print each bytes and separate them by a dash using
    // "{:1-}"
    // and the formatted output will look like:
    // "20-01-0b-b8-00-00"
    constexpr auto parse(fmt::format_parse_context& ctx) {
        // get the delimeter if any
        auto it = ctx.begin();
        auto end = ctx.end();
        if (it != end) {
            int group_size = *it++ - '0';
            if (group_size < 0 ||
                static_cast<size_t>(group_size) > sizeof(uint64_t)) {
                throw format_error("invalid group_size");
            }
            _group_size_in_bytes = group_size;
            if (it != end) {
                // optional delimiter
                _delimiter = *it++;
            }
        }
        if (it != end && *it != '}') {
            throw format_error("invalid format");
        }
        return it;
    }
    template <typename FormatContext>
    auto format(const ::fmt_hex& s, FormatContext& ctx) const {
        auto out = ctx.out();
        const auto& v = s.v;
        if (_group_size_in_bytes > 0) {
            for (size_t i = 0, size = v.size(); i < size; i++) {
                if (i != 0 && i % _group_size_in_bytes == 0) {
                    fmt::format_to(out, "{}{:02x}", _delimiter, std::byte(v[i]));
                } else {
                    fmt::format_to(out, "{:02x}", std::byte(v[i]));
                }
            }
        } else {
            for (auto b : v) {
                fmt::format_to(out, "{:02x}", std::byte(b));
            }
        }
        return out;
    }
};
template <>
struct fmt::formatter<bytes> : fmt::formatter<fmt_hex> {
    template <typename FormatContext>
    auto format(const ::bytes& s, FormatContext& ctx) const {
        return fmt::formatter<::fmt_hex>::format(::fmt_hex(bytes_view(s)), ctx);
    }
};
namespace std {
// Must be in std:: namespace, or ADL fails
}
template<>
struct appending_hash<bytes> {
    template<typename Hasher>
    void operator()(Hasher& h, const bytes& v) const {
        feed_hash(h, v.size());
        h.update(reinterpret_cast<const char*>(v.cbegin()), v.size() * sizeof(bytes::value_type));
    }
};
template<>
struct appending_hash<bytes_view> {
    template<typename Hasher>
    void operator()(Hasher& h, bytes_view v) const {
        feed_hash(h, v.size());
        h.update(reinterpret_cast<const char*>(v.begin()), v.size() * sizeof(bytes_view::value_type));
    }
};
using bytes_view_hasher = simple_xx_hasher;
namespace std {
template <>
struct hash<bytes_view> {
    size_t operator()(bytes_view v) const {
        bytes_view_hasher h;
        appending_hash<bytes_view>{}(h, v);
        return h.finalize();
    }
};
} // namespace std
 
namespace db {
/// CQL consistency levels.
///
/// Values are guaranteed to be dense and in the tight range [MIN_VALUE, MAX_VALUE].
enum class consistency_level {
    ANY, MIN_VALUE = ANY,
    ONE,
    TWO,
    THREE,
    QUORUM,
    ALL,
    LOCAL_QUORUM,
    EACH_QUORUM,
    SERIAL,
    LOCAL_SERIAL,
    LOCAL_ONE, MAX_VALUE = LOCAL_ONE
};
}
namespace db {
enum class write_type : uint8_t {
    SIMPLE,
    BATCH,
    UNLOGGED_BATCH,
    COUNTER,
    BATCH_LOG,
    CAS,
    VIEW,
};
}
namespace db {
enum class operation_type : uint8_t {
    read = 0,
    write = 1
};
}
namespace utils {
namespace bloom_calculations {
    struct bloom_specification final {
        int K; // number of hash functions.
        int buckets_per_element;
        operator sstring() {
            return format("bloom_specification(K={:d}, buckets_per_element={:d})", K, buckets_per_element);
        }
    };
    int constexpr min_buckets = 2;
    int constexpr min_k = 1;
    int constexpr EXCESS = 20;
    extern const std::vector<std::vector<double>> probs;
    extern const std::vector<int> opt_k_per_buckets;
}
}
#if 0
package org.apache.cassandra.utils;
class BloomCalculations {
    private static final int minBuckets = 2;
    private static final int minK = 1;
    private static final int EXCESS = 20;
    static final double[][] probs = new double[][]{
        {1.0}, // dummy row representing 0 buckets per element
        {1.0, 1.0}, // dummy row representing 1 buckets per element
        {1.0, 0.393,  0.400},
        {1.0, 0.283,  0.237,   0.253},
        {1.0, 0.221,  0.155,   0.147,   0.160},
        {1.0, 0.181,  0.109,   0.092,   0.092,   0.101}, // 5
        {1.0, 0.154,  0.0804,  0.0609,  0.0561,  0.0578,   0.0638},
        {1.0, 0.133,  0.0618,  0.0423,  0.0359,  0.0347,   0.0364},
        {1.0, 0.118,  0.0489,  0.0306,  0.024,   0.0217,   0.0216,   0.0229},
        {1.0, 0.105,  0.0397,  0.0228,  0.0166,  0.0141,   0.0133,   0.0135,   0.0145},
        {1.0, 0.0952, 0.0329,  0.0174,  0.0118,  0.00943,  0.00844,  0.00819,  0.00846}, // 10
        {1.0, 0.0869, 0.0276,  0.0136,  0.00864, 0.0065,   0.00552,  0.00513,  0.00509},
        {1.0, 0.08,   0.0236,  0.0108,  0.00646, 0.00459,  0.00371,  0.00329,  0.00314},
        {1.0, 0.074,  0.0203,  0.00875, 0.00492, 0.00332,  0.00255,  0.00217,  0.00199,  0.00194},
        {1.0, 0.0689, 0.0177,  0.00718, 0.00381, 0.00244,  0.00179,  0.00146,  0.00129,  0.00121,  0.0012},
        {1.0, 0.0645, 0.0156,  0.00596, 0.003,   0.00183,  0.00128,  0.001,    0.000852, 0.000775, 0.000744}, // 15
        {1.0, 0.0606, 0.0138,  0.005,   0.00239, 0.00139,  0.000935, 0.000702, 0.000574, 0.000505, 0.00047,  0.000459},
        {1.0, 0.0571, 0.0123,  0.00423, 0.00193, 0.00107,  0.000692, 0.000499, 0.000394, 0.000335, 0.000302, 0.000287, 0.000284},
        {1.0, 0.054,  0.0111,  0.00362, 0.00158, 0.000839, 0.000519, 0.00036,  0.000275, 0.000226, 0.000198, 0.000183, 0.000176},
        {1.0, 0.0513, 0.00998, 0.00312, 0.0013,  0.000663, 0.000394, 0.000264, 0.000194, 0.000155, 0.000132, 0.000118, 0.000111, 0.000109},
        {1.0, 0.0488, 0.00906, 0.0027,  0.00108, 0.00053,  0.000303, 0.000196, 0.00014,  0.000108, 8.89e-05, 7.77e-05, 7.12e-05, 6.79e-05, 6.71e-05} // 20
    };  // the first column is a dummy column representing K=0.
    private static final int[] optKPerBuckets = new int[probs.length];
    static
    {
        for (int i = 0; i < probs.length; i++)
        {
            double min = Double.MAX_VALUE;
            double[] prob = probs[i];
            for (int j = 0; j < prob.length; j++)
            {
                if (prob[j] < min)
                {
                    min = prob[j];
                    optKPerBuckets[i] = Math.max(minK, j);
                }
            }
        }
    }
    public static BloomSpecification computeBloomSpec(int bucketsPerElement)
    {
        assert bucketsPerElement >= 1;
        assert bucketsPerElement <= probs.length - 1;
        return new BloomSpecification(optKPerBuckets[bucketsPerElement], bucketsPerElement);
    }
    public static class BloomSpecification
    {
        final int K; // number of hash functions.
        final int bucketsPerElement;
        public BloomSpecification(int k, int bucketsPerElement)
        {
            K = k;
            this.bucketsPerElement = bucketsPerElement;
        }
        public String toString()
        {
            return String.format("BloomSpecification(K=%d, bucketsPerElement=%d)", K, bucketsPerElement);
        }
    }
    public static BloomSpecification computeBloomSpec(int maxBucketsPerElement, double maxFalsePosProb)
    {
        assert maxBucketsPerElement >= 1;
        assert maxBucketsPerElement <= probs.length - 1;
        int maxK = probs[maxBucketsPerElement].length - 1;
        // Handle the trivial cases
        if(maxFalsePosProb >= probs[minBuckets][minK]) {
            return new BloomSpecification(2, optKPerBuckets[2]);
        }
        if (maxFalsePosProb < probs[maxBucketsPerElement][maxK]) {
            throw new UnsupportedOperationException(String.format("Unable to satisfy %s with %s buckets per element",
                                                                  maxFalsePosProb, maxBucketsPerElement));
        }
        // First find the minimal required number of buckets:
        int bucketsPerElement = 2;
        int K = optKPerBuckets[2];
        while(probs[bucketsPerElement][K] > maxFalsePosProb){
            bucketsPerElement++;
            K = optKPerBuckets[bucketsPerElement];
        }
        // Now that the number of buckets is sufficient, see if we can relax K
        // without losing too much precision.
        while(probs[bucketsPerElement][K - 1] <= maxFalsePosProb){
            K--;
        }
        return new BloomSpecification(K, bucketsPerElement);
    }
    public static int maxBucketsPerElement(long numElements)
    {
        numElements = Math.max(1, numElements);
        double v = (Long.MAX_VALUE - EXCESS) / (double)numElements;
        if (v < 1.0)
        {
            throw new UnsupportedOperationException("Cannot compute probabilities for " + numElements + " elements.");
        }
        return Math.min(BloomCalculations.probs.length - 1, (int)v);
    }
}
#endif
namespace utils {
struct i_filter;
using filter_ptr = std::unique_ptr<i_filter>;
enum class filter_format {
    k_l_format,
    m_format,
};
class hashed_key {
private:
    std::array<uint64_t, 2> _hash;
public:
    ;
};
// FIXME: serialize() and serialized_size() not implemented. We should only be serializing to
// disk, not in the wire.
struct i_filter {
    
    virtual bool is_present(hashed_key) = 0;
};
}
namespace utils {
namespace murmur_hash {
template<typename InputIterator>
inline
uint64_t read_block(InputIterator& in) {
    typename std::iterator_traits<InputIterator>::value_type tmp[8];
    for (int i = 0; i < 8; ++i) {
        tmp[i] = *in;
        ++in;
    }
    return ((uint64_t) tmp[0] & 0xff) + (((uint64_t) tmp[1] & 0xff) << 8) +
           (((uint64_t) tmp[2] & 0xff) << 16) + (((uint64_t) tmp[3] & 0xff) << 24) +
           (((uint64_t) tmp[4] & 0xff) << 32) + (((uint64_t) tmp[5] & 0xff) << 40) +
           (((uint64_t) tmp[6] & 0xff) << 48) + (((uint64_t) tmp[7] & 0xff) << 56);
}
static inline
uint64_t rotl64(uint64_t v, uint32_t n) {
    return ((v << n) | ((uint64_t)v >> (64 - n)));
}
static inline
uint64_t fmix(uint64_t k) {
    k ^= (uint64_t)k >> 33;
    k *= 0xff51afd7ed558ccdL;
    k ^= (uint64_t)k >> 33;
    k *= 0xc4ceb9fe1a85ec53L;
    k ^= (uint64_t)k >> 33;
    return k;
}
template <typename InputIterator>
void hash3_x64_128(InputIterator in, uint32_t length, uint64_t seed, std::array<uint64_t, 2>& result) {
    const uint32_t nblocks = length >> 4; // Process as 128-bit blocks.
    uint64_t h1 = seed;
    uint64_t h2 = seed;
    uint64_t c1 = 0x87c37b91114253d5L;
    uint64_t c2 = 0x4cf5ad432745937fL;
    //----------
    // body
    for(uint32_t i = 0; i < nblocks; i++)
    {
        uint64_t k1 = read_block(in);
        uint64_t k2 = read_block(in);
        k1 *= c1; k1 = rotl64(k1,31); k1 *= c2; h1 ^= k1;
        h1 = rotl64(h1,27); h1 += h2; h1 = h1*5+0x52dce729;
        k2 *= c2; k2  = rotl64(k2,33); k2 *= c1; h2 ^= k2;
        h2 = rotl64(h2,31); h2 += h1; h2 = h2*5+0x38495ab5;
    }
    //----------
    // tail
    uint64_t k1 = 0;
    uint64_t k2 = 0;
    typename std::iterator_traits<InputIterator>::value_type tmp[15];
    std::copy_n(in, length & 15, tmp);
    switch (length & 15)
    {
        case 15: k2 ^= ((uint64_t) tmp[14]) << 48;
        case 14: k2 ^= ((uint64_t) tmp[13]) << 40;
        case 13: k2 ^= ((uint64_t) tmp[12]) << 32;
        case 12: k2 ^= ((uint64_t) tmp[11]) << 24;
        case 11: k2 ^= ((uint64_t) tmp[10]) << 16;
        case 10: k2 ^= ((uint64_t) tmp[9]) << 8;
        case  9: k2 ^= ((uint64_t) tmp[8]) << 0;
            k2 *= c2; k2  = rotl64(k2,33); k2 *= c1; h2 ^= k2;
        case  8: k1 ^= ((uint64_t) tmp[7]) << 56;
        case  7: k1 ^= ((uint64_t) tmp[6]) << 48;
        case  6: k1 ^= ((uint64_t) tmp[5]) << 40;
        case  5: k1 ^= ((uint64_t) tmp[4]) << 32;
        case  4: k1 ^= ((uint64_t) tmp[3]) << 24;
        case  3: k1 ^= ((uint64_t) tmp[2]) << 16;
        case  2: k1 ^= ((uint64_t) tmp[1]) << 8;
        case  1: k1 ^= ((uint64_t) tmp[0]);
            k1 *= c1; k1  = rotl64(k1,31); k1 *= c2; h1 ^= k1;
    };
    //----------
    // finalization
    h1 ^= length;
    h2 ^= length;
    h1 += h2;
    h2 += h1;
    h1 = fmix(h1);
    h2 = fmix(h2);
    h1 += h2;
    h2 += h1;
    result[0] = h1;
    result[1] = h2;
}

} // namespace murmur_hash
} // namespace utils
namespace utils {
template <std::ranges::range Range>
std::ostream& format_range(std::ostream& os, const Range& items, std::string_view paren = "{}") ;
namespace internal {
template<bool NeedsComma, typename Printable>
struct print_with_comma {
    const Printable& v;
};
template<bool NeedsComma, typename Printable>
std::ostream& operator<<(std::ostream& os, const print_with_comma<NeedsComma, Printable>& x) ;
} // namespace internal
} // namespace utils
namespace std {
 ;
 ;
 ;
// Vector-like ranges
template <std::ranges::range Range>
requires (
       std::same_as<Range, std::vector<std::ranges::range_value_t<Range>>>
    || std::same_as<Range, std::list<std::ranges::range_value_t<Range>>>
    || std::same_as<Range, std::initializer_list<std::ranges::range_value_t<Range>>>
    || std::same_as<Range, std::deque<std::ranges::range_value_t<Range>>>
)
std::ostream& operator<<(std::ostream& os, const Range& items) {
    return utils::format_range(os, items);
}
template <typename T, typename... Args>
std::ostream& operator<<(std::ostream& os, const std::set<T, Args...>& items) {
    return utils::format_range(os, items);
}
template <typename T, typename... Args>
std::ostream& operator<<(std::ostream& os, const std::unordered_set<T, Args...>& items) {
    return utils::format_range(os, items);
}
template <typename K, typename V, typename... Args>
std::ostream& operator<<(std::ostream& os, const std::map<K, V, Args...>& items) {
    return utils::format_range(os, items);
}
template <typename... Args>
std::ostream& operator<<(std::ostream& os, const boost::transformed_range<Args...>& items) {
    return utils::format_range(os, items);
}
template <typename T, std::size_t N>
std::ostream& operator<<(std::ostream& os, const std::array<T, N>& items) {
    return utils::format_range(os, items, "[]");
}
template <typename T>
std::ostream& operator<<(std::ostream& os, const std::optional<T>& opt) {
    if (opt) {
        os << "{" << *opt << "}";
    } else {
        os << "{}";
    }
    return os;
}
std::ostream& operator<<(std::ostream& os, const std::strong_ordering& order);
} // namespace std
template<typename T>
struct fmt::formatter<std::optional<T>> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const std::optional<T>& opt, FormatContext& ctx) const {
        if (opt) {
            return fmt::format_to(ctx.out(), "{}", *opt);
        } else {
            return fmt::format_to(ctx.out(), "{{}}");
        }
     }
};
namespace utils {
/// A vector with small buffer optimisation
///
/// small_vector is a variation of std::vector<> that reserves a configurable
/// amount of storage internally, without the need for memory allocation.
/// This can bring measurable gains if the expected number of elements is
/// small. The drawback is that moving such small_vector is more expensive
/// and invalidates iterators as well as references which disqualifies it in
/// some cases.
///
/// All member functions of small_vector provide strong exception guarantees.
///
/// It is unspecified when small_vector is going to use internal storage, except
/// for the obvious case when size() > N. In other situations user must not
/// attempt to guess if data is stored internally or externally. The same applies
/// to capacity(). Apart from the obvious fact that capacity() >= size() the user
/// must not assume anything else. In particular it may not always hold that
/// capacity() >= N.
///
/// Unless otherwise specified (e.g. move ctor and assignment) small_vector
/// provides guarantees at least as strong as those of std::vector<>.
template<typename T, size_t N>
requires std::is_nothrow_move_constructible_v<T> && std::is_nothrow_move_assignable_v<T> && std::is_nothrow_destructible_v<T> && (N > 0)
class small_vector {
private:
    T* _begin;
    T* _end;
    T* _capacity_end;
    // Use union instead of std::aligned_storage so that debuggers can see
    // the contained objects without needing any pretty printers.
    union internal {
        internal() { }
        ~internal() { }
        T storage[N];
    };
    internal _internal;
private:
    bool uses_internal_storage() const noexcept {
        return _begin == _internal.storage;
    }
    [[gnu::cold]] [[gnu::noinline]]
    void expand(size_t new_capacity) {
        auto ptr = static_cast<T*>(::aligned_alloc(alignof(T), new_capacity * sizeof(T)));
        if (!ptr) {
            throw std::bad_alloc();
        }
        auto n_end = std::uninitialized_move(begin(), end(), ptr);
        std::destroy(begin(), end());
        if (!uses_internal_storage()) {
            std::free(_begin);
        }
        _begin = ptr;
        _end = n_end;
        _capacity_end = ptr + new_capacity;
    }
    [[gnu::cold]] [[gnu::noinline]]
    void slow_copy_assignment(const small_vector& other) {
        auto ptr = static_cast<T*>(::aligned_alloc(alignof(T), other.size() * sizeof(T)));
        if (!ptr) {
            throw std::bad_alloc();
        }
        auto n_end = ptr;
        try {
            n_end = std::uninitialized_copy(other.begin(), other.end(), n_end);
        } catch (...) {
            std::free(ptr);
            throw;
        }
        std::destroy(begin(), end());
        if (!uses_internal_storage()) {
            std::free(_begin);
        }
        _begin = ptr;
        _end = n_end;
        _capacity_end = n_end;
    }
    void reserve_at_least(size_t n) {
        if (__builtin_expect(_begin + n > _capacity_end, false)) {
            expand(std::max(n, capacity() * 2));
        }
    }
    [[noreturn]] [[gnu::cold]] [[gnu::noinline]]
    void throw_out_of_range() const {
        throw std::out_of_range("out of range small vector access");
    }
public:
    using value_type = T;
    using pointer = T*;
    using const_pointer = const T*;
    using reference = T&;
    using const_reference = const T&;
    using iterator = T*;
    using const_iterator = const T*;
    using reverse_iterator = std::reverse_iterator<iterator>;
    using const_reverse_iterator = std::reverse_iterator<const_iterator>;
    small_vector() noexcept
        : _begin(_internal.storage)
        , _end(_begin)
        , _capacity_end(_begin + N)
    { }
    template<typename InputIterator>
    small_vector(InputIterator first, InputIterator last) : small_vector() {
        if constexpr (std::is_base_of_v<std::forward_iterator_tag, typename std::iterator_traits<InputIterator>::iterator_category>) {
            reserve(std::distance(first, last));
            _end = std::uninitialized_copy(first, last, _end);
        } else {
            std::copy(first, last, std::back_inserter(*this));
        }
    }
    small_vector(std::initializer_list<T> list) : small_vector(list.begin(), list.end()) { }
    // May invalidate iterators and references.
    small_vector(small_vector&& other) noexcept {
        if (other.uses_internal_storage()) {
            _begin = _internal.storage;
            _capacity_end = _begin + N;
            if constexpr (std::is_trivially_copyable_v<T>) {
                // Compilers really like loops with the number of iterations known at
                // the compile time, the usually emit less code which can be more aggressively
                // optimised. Since we can assume that N is small it is most likely better
                // to just copy everything, regardless of how many elements are actually in
                // the vector.
                std::memcpy(_internal.storage, other._internal.storage, N * sizeof(T));
                _end = _begin + other.size();
            } else {
                _end = _begin;
                // What we would really like here is std::uninintialized_move_and_destroy.
                // It is beneficial to do move and destruction in a single pass since the compiler
                // may be able to merge those operations (e.g. the destruction of a move-from
                // std::unique_ptr is a no-op).
                for (auto& e : other) {
                    new (_end++) T(std::move(e));
                    e.~T();
                }
            }
            other._end = other._internal.storage;
        } else {
            _begin = std::exchange(other._begin, other._internal.storage);
            _end = std::exchange(other._end, other._internal.storage);
            _capacity_end = std::exchange(other._capacity_end, other._internal.storage + N);
        }
    }
    small_vector(const small_vector& other) : small_vector() {
        reserve(other.size());
        _end = std::uninitialized_copy(other.begin(), other.end(), _end);
    }
    // May invalidate iterators and references.
    small_vector& operator=(small_vector&& other) noexcept {
        clear();
        if (other.uses_internal_storage()) {
            if (__builtin_expect(!uses_internal_storage(), false)) {
                std::free(_begin);
                _begin = _internal.storage;
            }
            _capacity_end = _begin + N;
            if constexpr (std::is_trivially_copyable_v<T>) {
                std::memcpy(_internal.storage, other._internal.storage, N * sizeof(T));
                _end = _begin + other.size();
            } else {
                _end = _begin;
                // Better to use single pass than std::uninitialize_move + std::destroy.
                // See comment in move ctor for details.
                for (auto& e : other) {
                    new (_end++) T(std::move(e));
                    e.~T();
                }
            }
            other._end = other._internal.storage;
        } else {
            if (__builtin_expect(!uses_internal_storage(), false)) {
                std::free(_begin);
            }
            _begin = std::exchange(other._begin, other._internal.storage);
            _end = std::exchange(other._end, other._internal.storage);
            _capacity_end = std::exchange(other._capacity_end, other._internal.storage + N);
        }
        return *this;
    }
    small_vector& operator=(const small_vector& other) {
        if constexpr (std::is_nothrow_copy_constructible_v<T>) {
            if (capacity() >= other.size()) {
                clear();
                _end = std::uninitialized_copy(other.begin(), other.end(), _end);
                return *this;
            }
        }
        slow_copy_assignment(other);
        return *this;
    }
    ~small_vector() {
        clear();
        if (__builtin_expect(!uses_internal_storage(), false)) {
            std::free(_begin);
        }
    }
    size_t external_memory_usage() const {
        if (uses_internal_storage()) {
            return 0;
        }
        return ::malloc_usable_size(_begin);
    }
    void reserve(size_t n) {
        if (__builtin_expect(_begin + n > _capacity_end, false)) {
            expand(n);
        }
    }
    void clear() noexcept {
        std::destroy(_begin, _end);
        _end = _begin;
    }
    iterator begin() noexcept { return _begin; }
    const_iterator begin() const noexcept { return _begin; }
    const_iterator cbegin() const noexcept { return _begin; }
    iterator end() noexcept { return _end; }
    const_iterator end() const noexcept { return _end; }
    const_iterator cend() const noexcept { return _end; }
    reverse_iterator rbegin() noexcept { return reverse_iterator(end()); }
    const_reverse_iterator rbegin() const noexcept { return const_reverse_iterator(end()); }
    const_reverse_iterator crbegin() const noexcept { return const_reverse_iterator(end()); }
    reverse_iterator rend() noexcept { return reverse_iterator(begin()); }
    const_reverse_iterator rend() const noexcept { return const_reverse_iterator(begin()); }
    const_reverse_iterator crend() const noexcept { return const_reverse_iterator(begin()); }
    T* data() noexcept { return _begin; }
    const T* data() const noexcept { return _begin; }
    T& front() noexcept { return *begin(); }
    const T& front() const noexcept { return *begin(); }
    T& back() noexcept { return end()[-1]; }
    const T& back() const noexcept { return end()[-1]; }
    T& operator[](size_t idx) noexcept { return data()[idx]; }
    const T& operator[](size_t idx) const noexcept { return data()[idx]; }
    T& at(size_t idx) {
        if (__builtin_expect(idx >= size(), false)) {
            throw_out_of_range();
        }
        return operator[](idx);
    }
    const T& at(size_t idx) const {
        if (__builtin_expect(idx >= size(), false)) {
            throw_out_of_range();
        }
        return operator[](idx);
    }
    bool empty() const noexcept { return _begin == _end; }
    size_t size() const noexcept { return _end - _begin; }
    size_t capacity() const noexcept { return _capacity_end - _begin; }
    template<typename... Args>
    T& emplace_back(Args&&... args) {
        if (__builtin_expect(_end == _capacity_end, false)) {
            expand(std::max<size_t>(capacity() * 2, 1));
        }
        auto& ref = *new (_end) T(std::forward<Args>(args)...);
        ++_end;
        return ref;
    }
    T& push_back(const T& value) {
        return emplace_back(value);
    }
    T& push_back(T&& value) {
        return emplace_back(std::move(value));
    }
    template<typename InputIterator>
    iterator insert(const_iterator cpos, InputIterator first, InputIterator last) {
        if constexpr (std::is_base_of_v<std::forward_iterator_tag, typename std::iterator_traits<InputIterator>::iterator_category>) {
            if (first == last) {
                return const_cast<iterator>(cpos);
            }
            auto idx = cpos - _begin;
            auto new_count = std::distance(first, last);
            reserve_at_least(size() + new_count);
            auto pos = _begin + idx;
            auto after = std::distance(pos, end());
            if (__builtin_expect(pos == end(), true)) {
                _end = std::uninitialized_copy(first, last, end());
                return pos;
            } else if (after > new_count) {
                std::uninitialized_move(end() - new_count, end(), end());
                std::move_backward(pos, end() - new_count, end());
                try {
                    std::copy(first, last, pos);
                } catch (...) {
                    std::move(pos + new_count, end() + new_count, pos);
                    std::destroy(end(), end() + new_count);
                    throw;
                }
            } else {
                std::uninitialized_move(pos, end(), pos + new_count);
                auto mid = std::next(first, after);
                try {
                    std::uninitialized_copy(mid, last, end());
                    try {
                        std::copy(first, mid, pos);
                    } catch (...) {
                        std::destroy(end(), pos + new_count);
                        throw;
                    }
                } catch (...) {
                    std::move(pos + new_count, end() + new_count, pos);
                    std::destroy(pos + new_count, end() + new_count);
                    throw;
                }
            }
            _end += new_count;
            return pos;
        } else {
            auto start = cpos - _begin;
            auto idx = start;
            while (first != last) {
                try {
                    insert(begin() + idx, *first);
                    ++first;
                    ++idx;
                } catch (...) {
                    erase(begin() + start, begin() + idx);
                    throw;
                }
            }
            return begin() + idx;
        }
    }
    template<typename... Args>
    iterator emplace(const_iterator cpos, Args&&... args) {
        auto idx = cpos - _begin;
        reserve_at_least(size() + 1);
        auto pos = _begin + idx;
        if (pos != _end) {
            new (_end) T(std::move(_end[-1]));
            std::move_backward(pos, _end - 1, _end);
            pos->~T();
        }
        try {
            new (pos) T(std::forward<Args>(args)...);
        } catch (...) {
            if (pos != _end) {
                new (pos) T(std::move(pos[1]));
                std::move(pos + 2, _end + 1, pos + 1);
                _end->~T();
            }
            throw;
        }
        _end++;
        return pos;
    }
    iterator insert(const_iterator cpos, const T& obj) {
        return emplace(cpos, obj);
    }
    iterator insert(const_iterator cpos, T&& obj) {
        return emplace(cpos, std::move(obj));
    }
    void resize(size_t n) {
        if (n < size()) {
            erase(end() - (size() - n), end());
        } else if (n > size()) {
            reserve_at_least(n);
            _end = std::uninitialized_value_construct_n(_end, n - size());
        }
    }
    void resize(size_t n, const T& value) {
        if (n < size()) {
            erase(end() - (size() - n), end());
        } else if (n > size()) {
            reserve_at_least(n);
            auto nend = _begin + n;
            std::uninitialized_fill(_end, nend, value);
            _end = nend;
        }
    }
    void pop_back() noexcept {
        (--_end)->~T();
    }
    iterator erase(const_iterator cit) noexcept {
        return erase(cit, cit + 1);
    }
    iterator erase(const_iterator cfirst, const_iterator clast) noexcept {
        auto first = const_cast<iterator>(cfirst);
        auto last = const_cast<iterator>(clast);
        std::move(last, end(), first);
        auto nend = _end - (clast - cfirst);
        std::destroy(nend, _end);
        _end = nend;
        return first;
    }
    void swap(small_vector& other) noexcept {
        std::swap(*this, other);
    }
    auto operator<=>(const small_vector& other) const noexcept requires std::three_way_comparable<T> {
        return std::lexicographical_compare_three_way(this->begin(), this->end(),
                                                      other.begin(), other.end());
    }
    bool operator==(const small_vector& other) const noexcept {
        return size() == other.size() && std::equal(_begin, _end, other.begin());
    }
};
 ;
}
// chunked_vector is a vector-like container that uses discontiguous storage.
// It provides fast random access, the ability to append at the end, and aims
// to avoid large contiguous allocations - unlike std::vector which allocates
// all the data in one contiguous allocation.
//
// std::deque aims to achieve the same goals, but its implementation in
// libstdc++ still results in large contiguous allocations: std::deque
// keeps the items in small (512-byte) chunks, and then keeps a contiguous
// vector listing these chunks. This chunk vector can grow pretty big if the
// std::deque grows big: When an std::deque contains just 8 MB of data, it
// needs 16384 chunks, and the vector listing those needs 128 KB.
//
// Therefore, in chunked_vector we use much larger 128 KB chunks (this is
// configurable, with the max_contiguous_allocation template parameter).
// With 128 KB chunks, the contiguous vector listing them is 256 times
// smaller than it would be in std::dequeue with its 512-byte chunks.
//
// In particular, when a chunked_vector stores up to 2 GB of data, the
// largest contiguous allocation is guaranteed to be 128 KB: 2 GB of data
// fits in 16384 chunks of 128 KB each, and the vector of 16384 8-byte
// pointers requires another 128 KB allocation.
//
// Remember, however, that when the chunked_vector grows beyond 2 GB, its
// largest contiguous allocation (used to store the chunk list) continues to
// grow as O(N). This is not a problem for current real-world uses of
// chunked_vector which never reach 2 GB.
//
// Always allocating large 128 KB chunks can be wasteful for small vectors;
// This is why std::deque chose small 512-byte chunks. chunked_vector solves
// this problem differently: It makes the last chunk variable in size,
// possibly smaller than a full 128 KB.
namespace utils {
struct chunked_vector_free_deleter {
    void operator()(void* x) const { ::free(x); }
};
template <typename T, size_t max_contiguous_allocation = 128*1024>
class chunked_vector {
    static_assert(std::is_nothrow_move_constructible<T>::value, "T must be nothrow move constructible");
    using chunk_ptr = std::unique_ptr<T[], chunked_vector_free_deleter>;
    // Each chunk holds max_chunk_capacity() items, except possibly the last
    utils::small_vector<chunk_ptr, 1> _chunks;
    size_t _size = 0;
    size_t _capacity = 0;
public:
    // Maximum number of T elements fitting in a single chunk.
    static size_t max_chunk_capacity() {
        return std::max(max_contiguous_allocation / sizeof(T), size_t(1));
    }
private:
    void reserve_for_push_back() {
        if (_size == _capacity) {
            do_reserve_for_push_back();
        }
    }
    void do_reserve_for_push_back();
    size_t make_room(size_t n, bool stop_after_one);
    chunk_ptr new_chunk(size_t n);
    T* addr(size_t i) const {
        return &_chunks[i / max_chunk_capacity()][i % max_chunk_capacity()];
    }
    
    static void migrate(T* begin, T* end, T* result);
public:
    using value_type = T;
    using size_type = size_t;
    using difference_type = ssize_t;
    using reference = T&;
    using const_reference = const T&;
    using pointer = T*;
    using const_pointer = const T*;
public:
    chunked_vector() = default;
    
    chunked_vector(chunked_vector&& x) noexcept;
    ;
    
    
    ~chunked_vector();
    chunked_vector& operator=(const chunked_vector& x);
    
    bool empty() const ;
    size_t size() const {
        return _size;
    }
    
    void push_back(const T& x) {
        reserve_for_push_back();
        new (addr(_size)) T(x);
        ++_size;
    }
    
    template <typename... Args>
    T& emplace_back(Args&&... args) {
        reserve_for_push_back();
        auto& ret = *new (addr(_size)) T(std::forward<Args>(args)...);
        ++_size;
        return ret;
    }
    void pop_back() {
        --_size;
        addr(_size)->~T();
    }
    
    
    void clear();
    void shrink_to_fit();
    
    void reserve(size_t n) {
        if (n > _capacity) {
            make_room(n, false);
        }
    }
    /// Reserve some of the memory.
    ///
    /// Allows reserving the memory chunk-by-chunk, avoiding stalls when a lot of
    /// chunks are needed. To drive the reservation to completion, call this
    /// repeatedly with the value returned from the previous call until it
    /// returns 0, yielding between calls when necessary. Example usage:
    ///
    ///     return do_until([&size] { return !size; }, [&my_vector, &size] () mutable {
    ///         size = my_vector.reserve_partial(size);
    ///     });
    ///
    /// Here, `do_until()` takes care of yielding between iterations when
    /// necessary.
    ///
    /// \returns the memory that remains to be reserved
    
public:
    template <class ValueType>
    class iterator_type {
        const chunk_ptr* _chunks;
        size_t _i;
    public:
        using iterator_category = std::random_access_iterator_tag;
        using value_type = ValueType;
        using difference_type = ssize_t;
        using pointer = ValueType*;
        using reference = ValueType&;
    private:
        pointer addr() const {
            return &_chunks[_i / max_chunk_capacity()][_i % max_chunk_capacity()];
        }
        iterator_type(const chunk_ptr* chunks, size_t i) : _chunks(chunks), _i(i) {}
    public:
        iterator_type() = default;
         // needed for iterator->const_iterator conversion
        reference operator*() const {
            return *addr();
        }
        pointer operator->() const ;
        
        iterator_type& operator++() {
            ++_i;
            return *this;
        }
        iterator_type operator++(int) ;
        iterator_type& operator--() {
            --_i;
            return *this;
        }
        iterator_type operator--(int) ;
        
        friend ssize_t operator-(iterator_type a, iterator_type b) {
            return a._i - b._i;
        }
        bool operator==(iterator_type x) const {
            return _i == x._i;
        }
        friend class chunked_vector;
    };
    using iterator = iterator_type<T>;
    using const_iterator = iterator_type<const T>;
public:
    iterator begin() { return iterator(_chunks.data(), 0); }
    iterator end() { return iterator(_chunks.data(), _size); }
    const_iterator begin() const { return const_iterator(_chunks.data(), 0); }
    const_iterator end() const { return const_iterator(_chunks.data(), _size); }
public:
};
template <typename T, size_t max_contiguous_allocation>
chunked_vector<T, max_contiguous_allocation>::chunked_vector(chunked_vector&& x) noexcept
        : _chunks(std::exchange(x._chunks, {}))
        , _size(std::exchange(x._size, 0))
        , _capacity(std::exchange(x._capacity, 0)) {
}
template <typename T, size_t max_contiguous_allocation>
chunked_vector<T, max_contiguous_allocation>::~chunked_vector() {
    if constexpr (!std::is_trivially_destructible_v<T>) {
        for (auto i = size_t(0); i != _size; ++i) {
            addr(i)->~T();
        }
    }
}
template <typename T, size_t max_contiguous_allocation>
typename chunked_vector<T, max_contiguous_allocation>::chunk_ptr
chunked_vector<T, max_contiguous_allocation>::new_chunk(size_t n) {
    auto p = malloc(n * sizeof(T));
    if (!p) {
        throw std::bad_alloc();
    }
    return chunk_ptr(reinterpret_cast<T*>(p));
}
template <typename T, size_t max_contiguous_allocation>
void
chunked_vector<T, max_contiguous_allocation>::migrate(T* begin, T* end, T* result) {
    while (begin != end) {
        new (result) T(std::move(*begin));
        begin->~T();
        ++begin;
        ++result;
    }
}
template <typename T, size_t max_contiguous_allocation>
size_t
chunked_vector<T, max_contiguous_allocation>::make_room(size_t n, bool stop_after_one) {
    // First, if the last chunk is below max_chunk_capacity(), enlarge it
    auto last_chunk_capacity_deficit = _chunks.size() * max_chunk_capacity() - _capacity;
    if (last_chunk_capacity_deficit) {
        auto last_chunk_capacity = max_chunk_capacity() - last_chunk_capacity_deficit;
        auto capacity_increase = std::min(last_chunk_capacity_deficit, n - _capacity);
        auto new_last_chunk_capacity = last_chunk_capacity + capacity_increase;
        // FIXME: realloc? maybe not worth the complication; only works for PODs
        auto new_last_chunk = new_chunk(new_last_chunk_capacity);
        if (_size > _capacity - last_chunk_capacity) {
            migrate(addr(_capacity - last_chunk_capacity), addr(_size), new_last_chunk.get());
        }
        _chunks.back() = std::move(new_last_chunk);
        _capacity += capacity_increase;
    }
    // Reduce reallocations in the _chunks vector
    auto nr_chunks = (n + max_chunk_capacity() - 1) / max_chunk_capacity();
    _chunks.reserve(nr_chunks);
    // Add more chunks as needed
    bool stop = false;
    while (_capacity < n && !stop) {
        auto now = std::min(n - _capacity, max_chunk_capacity());
        _chunks.push_back(new_chunk(now));
        _capacity += now;
        stop = stop_after_one;
    }
    return (n - _capacity);
}
template <typename T, size_t max_contiguous_allocation>
void
chunked_vector<T, max_contiguous_allocation>::do_reserve_for_push_back() {
    if (_capacity == 0) {
        // allocate a bit of room in case utilization will be low
        reserve(boost::algorithm::clamp(512 / sizeof(T), 1, max_chunk_capacity()));
    } else if (_capacity < max_chunk_capacity() / 2) {
        // exponential increase when only one chunk to reduce copying
        reserve(_capacity * 2);
    } else {
        // add a chunk at a time later, since no copying will take place
        reserve((_capacity / max_chunk_capacity() + 1) * max_chunk_capacity());
    }
}
template <typename T, size_t max_contiguous_allocation>
void
chunked_vector<T, max_contiguous_allocation>::shrink_to_fit() {
    if (_chunks.empty()) {
        return;
    }
    while (!_chunks.empty() && _size <= (_chunks.size() - 1) * max_chunk_capacity()) {
        _chunks.pop_back();
        _capacity = _chunks.size() * max_chunk_capacity();
    }
    auto overcapacity = _size - _capacity;
    if (overcapacity) {
        auto new_last_chunk_capacity = _size - (_chunks.size() - 1) * max_chunk_capacity();
        // FIXME: realloc? maybe not worth the complication; only works for PODs
        auto new_last_chunk = new_chunk(new_last_chunk_capacity);
        migrate(addr((_chunks.size() - 1) * max_chunk_capacity()), addr(_size), new_last_chunk.get());
        _chunks.back() = std::move(new_last_chunk);
        _capacity = _size;
    }
}
template <typename T, size_t max_contiguous_allocation>
void
chunked_vector<T, max_contiguous_allocation>::clear() {
    while (_size > 0) {
        pop_back();
    }
    shrink_to_fit();
}
 ;
}
using namespace seastar;
class large_bitset {
    using int_type = uint64_t;
    size_t _nr_bits = 0;
    utils::chunked_vector<int_type> _storage;
public:
};
namespace utils {
namespace filter {
class bloom_filter: public i_filter {
public:
    using bitmap = large_bitset;
private:
    bitmap _bitset;
    int _hash_count;
    filter_format _format;
    static thread_local struct stats {
        uint64_t memory_size = 0;
    } _shard_stats;
    stats& _stats = _shard_stats;
public:
    
};
struct murmur3_bloom_filter: public bloom_filter {
};
struct always_present_filter: public i_filter {
};
}
}
template<typename EnumType, EnumType... Items>
struct super_enum {
    using enum_type = EnumType;
    template<enum_type... values>
    struct max {
        static constexpr enum_type max_of(enum_type a, enum_type b) {
            return a > b ? a : b;
        }
        template<enum_type first, enum_type second, enum_type... rest>
        static constexpr enum_type get() {
            return max_of(first, get<second, rest...>());
        }
        template<enum_type first>
        static constexpr enum_type get() { return first; }
        static constexpr enum_type value = get<values...>();
    };
    template<enum_type... values>
    struct min {
        static constexpr enum_type min_of(enum_type a, enum_type b) {
            return a < b ? a : b;
        }
        template<enum_type first, enum_type second, enum_type... rest>
        static constexpr enum_type get() {
            return min_of(first, get<second, rest...>());
        }
        template<enum_type first>
        static constexpr enum_type get() { return first; }
        static constexpr enum_type value = get<values...>();
    };
    using sequence_type = typename std::underlying_type<enum_type>::type;
    template <enum_type first, enum_type... rest>
    struct valid_sequence {
        
    };
    template <enum_type first>
    struct valid_sequence<first> {
    };
    template<enum_type Elem>
    static constexpr sequence_type sequence_for() {
        return static_cast<sequence_type>(Elem);
    }
    
    static constexpr sequence_type max_sequence = sequence_for<max<Items...>::value>();
    static constexpr sequence_type min_sequence = sequence_for<min<Items...>::value>();
    static_assert(min_sequence >= 0, "negative enum values unsupported");
};
class bad_enum_set_mask : public std::invalid_argument {
public:
    
};
template<typename Enum>
class enum_set {
public:
    using mask_type = size_t; // TODO: use the smallest sufficient type
    using enum_type = typename Enum::enum_type;
private:
    static constexpr int mask_digits = std::numeric_limits<mask_type>::digits;
    using mask_iterator = seastar::bitsets::set_iterator<mask_digits>;
    mask_type _mask;
    constexpr enum_set(mask_type mask) : _mask(mask) {}
    template<enum_type Elem>
    static constexpr unsigned shift_for() {
        return Enum::template sequence_for<Elem>();
    }
    static auto make_iterator(mask_iterator iter) {
        return boost::make_transform_iterator(std::move(iter), [](typename Enum::sequence_type s) {
            return enum_type(s);
        });
    }
public:
    using iterator = std::invoke_result_t<decltype(&enum_set::make_iterator), mask_iterator>;
    constexpr enum_set() : _mask(0) {}
    template<enum_type Elem>
    static constexpr mask_type mask_for() {
        return mask_type(1) << shift_for<Elem>();
    }
    struct prepared {
        mask_type mask;
        
    };
     ;
    static_assert(std::numeric_limits<mask_type>::max() >= ((size_t)1 << Enum::max_sequence), "mask type too small");
    template<enum_type e>
    bool contains() const ;
    bool contains(enum_type e) const ;
    template<enum_type e>
    void remove() ;
    
    template<enum_type e>
    void set() {
        _mask |= mask_for<e>();
    }
     ;
     ;
    template<enum_type... items>
    struct frozen {
        template<enum_type first>
        static constexpr mask_type make_mask() {
            return mask_for<first>();
        }
        template<enum_type first, enum_type second, enum_type... rest>
        static constexpr mask_type make_mask() {
            return mask_for<first>() | make_mask<second, rest...>();
        }
        static constexpr mask_type mask = make_mask<items...>();
         ;
        
        
        static constexpr enum_set<Enum> unfreeze() {
            return enum_set<Enum>(mask);
        }
    };
    template<enum_type... items>
    static constexpr enum_set<Enum> of() {
        return frozen<items...>::unfreeze();
    }
};
// A function used by compacting collectors to migrate objects during
// compaction. The function should reconstruct the object located at src
// in the location pointed by dst. The object at old location should be
// destroyed. See standard_migrator() above for example. Both src and dst
// are aligned as requested during alloc()/construct().
class migrate_fn_type {
    // Migrators may be registered by thread-local objects. The table of all
    // registered migrators is also thread-local which may cause problems with
    // the order of object destruction and lead to use-after-free.
    // This can be worked around by making migrators keep a shared pointer
    // to the table of migrators. std::any is used so that its type doesn't
    // have to be made public.
    std::any _migrators;
    uint32_t _align = 0;
    uint32_t _index;
private:
    static uint32_t register_migrator(migrate_fn_type* m);
    static void unregister_migrator(uint32_t index);
protected:
    explicit migrate_fn_type(size_t align) : _align(align), _index(register_migrator(this)) {}
public:
    virtual ~migrate_fn_type() { unregister_migrator(_index); }
    virtual void migrate(void* src, void* dsts, size_t size) const noexcept = 0;
    virtual size_t size(const void* obj) const = 0;
    size_t align() const { return _align; }
    uint32_t index() const ;
};
// Non-constant-size classes (ending with `char data[0]`) must provide
// the method telling the underlying storage size
template <typename T>
concept DynamicObject = requires (const T& obj) {
    { obj.storage_size() } noexcept -> std::same_as<size_t>;
};
template <typename T>
inline
size_t
size_for_allocation_strategy(const T& obj) noexcept {
    if constexpr (DynamicObject<T>) {
        return obj.storage_size();
    } else {
        return sizeof(T);
    }
}
template <typename T>
requires std::is_nothrow_move_constructible_v<T> && std::is_nothrow_destructible_v<T>
class standard_migrator final : public migrate_fn_type {
    friend class allocation_strategy;
    standard_migrator() : migrate_fn_type(alignof(T)) {}
public:
    virtual void migrate(void* src, void* dst, size_t size) const noexcept override {
        T* src_t = static_cast<T*>(src);
        new (static_cast<T*>(dst)) T(std::move(*src_t));
        src_t->~T();
    }
    virtual size_t size(const void* obj) const override {
        return size_for_allocation_strategy(*static_cast<const T*>(obj));
    }
};
//
// Abstracts allocation strategy for managed objects.
//
// Managed objects may be moved by the allocator during compaction, which
// invalidates any references to those objects. Compaction may be started
// synchronously with allocations. To ensure that references remain valid, use
// logalloc::compaction_lock.
//
// Because references may get invalidated, managing allocators can't be used
// with standard containers, because they assume the reference is valid until freed.
//
// For example containers compatible with compacting allocators see:
//   - managed_ref - managed version of std::unique_ptr<>
//   - managed_bytes - managed version of "bytes"
//
// Note: When object is used as an element inside intrusive containers,
// typically no extra measures need to be taken for reference tracking, if the
// link member is movable. When object is moved, the member hook will be moved
// too and it should take care of updating any back-references. The user must
// be aware though that any iterators into such container may be invalidated
// across deferring points.
//
class allocation_strategy {
    template <typename T>
    standard_migrator<T>& get_standard_migrator()
    {
        seastar::memory::scoped_critical_alloc_section dfg;
        static thread_local standard_migrator<T> instance;
        return instance;
    }
protected:
    size_t _preferred_max_contiguous_allocation = std::numeric_limits<size_t>::max();
    uint64_t _invalidate_counter = 1;
public:
    using migrate_fn = const migrate_fn_type*;
    virtual ~allocation_strategy() {}
    virtual void* alloc(migrate_fn, size_t size, size_t alignment) = 0;
    //
    // Allocates space for a new ManagedObject. The caller must construct the
    // object before compaction runs. "size" is the amount of space to reserve
    // in bytes. It can be larger than MangedObjects's size.
    //
    // Throws std::bad_alloc on allocation failure.
    //
    // Doesn't invalidate references to objects allocated with this strategy.
    //
    template <typename T>
    requires DynamicObject<T>
    void* alloc(size_t size) {
        return alloc(&get_standard_migrator<T>(), size, alignof(T));
    }
    // Releases storage for the object. Doesn't invoke object's destructor.
    // Doesn't invalidate references to objects allocated with this strategy.
    virtual void free(void* object, size_t size) = 0;
    virtual void free(void* object) = 0;
    // Returns the total immutable memory size used by the allocator to host
    // this object.  This will be at least the size of the object itself, plus
    // any immutable overhead needed to represent the object (if any).
    //
    // The immutable overhead is the overhead that cannot change over the
    // lifetime of the object (such as padding, etc).
    virtual size_t object_memory_size_in_allocator(const void* obj) const noexcept = 0;
    // Like alloc() but also constructs the object with a migrator using
    // standard move semantics. Allocates respecting object's alignment
    // requirement.
    template<typename T, typename... Args>
    T* construct(Args&&... args) {
        void* storage = alloc(&get_standard_migrator<T>(), sizeof(T), alignof(T));
        try {
            return new (storage) T(std::forward<Args>(args)...);
        } catch (...) {
            free(storage, sizeof(T));
            throw;
        }
    }
    // Destroys T and releases its storage.
    // Doesn't invalidate references to allocated objects.
    template<typename T>
    void destroy(T* obj) {
        size_t size = size_for_allocation_strategy(*obj);
        obj->~T();
        free(obj, size);
    }
    size_t preferred_max_contiguous_allocation() const noexcept {
        return _preferred_max_contiguous_allocation;
    }
    // Returns a number which is increased when references to objects managed by this allocator
    // are invalidated, e.g. due to internal events like compaction or eviction.
    // When the value returned by this method doesn't change, references obtained
    // between invocations remain valid.
    
    
};
class standard_allocation_strategy : public allocation_strategy {
public:
    constexpr standard_allocation_strategy() {
        _preferred_max_contiguous_allocation = 128 * 1024;
    }
    virtual void* alloc(migrate_fn, size_t size, size_t alignment) override {
        seastar::memory::on_alloc_point();
        // ASAN doesn't intercept aligned_alloc() and complains on free().
        void* ret;
        // The system posix_memalign will return EINVAL if alignment is not
        // a multiple of pointer size.
        if (alignment < sizeof(void*)) {
            alignment = sizeof(void*);
        }
        if (posix_memalign(&ret, alignment, size) != 0) {
            throw std::bad_alloc();
        }
        return ret;
    }
    virtual void free(void* obj, size_t size) override {
        ::free(obj);
    }
    virtual void free(void* obj) override {
        ::free(obj);
    }
    virtual size_t object_memory_size_in_allocator(const void* obj) const noexcept override {
        return ::malloc_usable_size(const_cast<void *>(obj));
    }
};
extern standard_allocation_strategy standard_allocation_strategy_instance;

inline
allocation_strategy*& current_allocation_strategy_ptr() {
    static thread_local allocation_strategy* current = &standard_allocation_strategy_instance;
    return current;
}
inline
allocation_strategy& current_allocator() {
    return *current_allocation_strategy_ptr();
}
template<typename T>
inline
auto current_deleter() {
    auto& alloc = current_allocator();
    return [&alloc] (T* obj) noexcept {
        alloc.destroy(obj);
    };
}
template<typename T>
struct alloc_strategy_deleter {
    void operator()(T* ptr) const noexcept {
        current_allocator().destroy(ptr);
    }
};
// std::unique_ptr which can be used for owning an object allocated using allocation_strategy.
// Must be destroyed before the pointer is invalidated. For compacting allocators, that
// means it must not escape outside allocating_section or reclaim lock.
// Must be destroyed in the same allocating context in which T was allocated.
template<typename T>
using alloc_strategy_unique_ptr = std::unique_ptr<T, alloc_strategy_deleter<T>>;
//
// Passing allocators to objects.
//
// The same object type can be allocated using different allocators, for
// example standard allocator (for temporary data), or log-structured
// allocator for long-lived data. In case of LSA, objects may be allocated
// inside different LSA regions. Objects should be freed only from the region
// which owns it.
//
// There's a problem of how to ensure correct usage of allocators. Storing the
// reference to the allocator used for construction of some object inside that
// object is a possible solution. This has a disadvantage of extra space
// overhead per-object though. We could avoid that if the code which decides
// about which allocator to use is also the code which controls object's life
// time. That seems to be the case in current uses, so a simplified scheme of
// passing allocators will do. Allocation strategy is set in a thread-local
// context, as shown below. From there, aware objects pick up the allocation
// strategy. The code controling the objects must ensure that object allocated
// in one regime is also freed in the same regime.
//
// with_allocator() provides a way to set the current allocation strategy used
// within given block of code. with_allocator() can be nested, which will
// temporarily shadow enclosing strategy. Use current_allocator() to obtain
// currently active allocation strategy. Use current_deleter() to obtain a
// Deleter object using current allocation strategy to destroy objects.
//
// Example:
//
//   logalloc::region r;
//   with_allocator(r.allocator(), [] {
//       auto obj = make_managed<int>();
//   });
//
class allocator_lock {
    allocation_strategy* _prev;
public:
    
    
};
 ;
class marshal_exception : public std::exception {
    sstring _why;
public:
    
    marshal_exception(sstring why) : _why(sstring("marshaling error: ") + why) {}
    
};
// Speed up compilation of code using throw_with_backtrace<marshal_exception,
// sstring> by compiling it only once (in types.cc), and elsewhere say that
// it is extern and not compile it again.
namespace seastar {
template <class Exc, typename... Args> [[noreturn]] void throw_with_backtrace(Args&&... args);
extern template void throw_with_backtrace<marshal_exception, sstring>(sstring&&);
}
template <class T> concept Trivial = std::is_trivial_v<T>;
template <class T> concept TriviallyCopyable = std::is_trivially_copyable_v<T>;
template <TriviallyCopyable To>
To read_unaligned(const void* src) {
    To dst;
    std::memcpy(&dst, src, sizeof(To));
    return dst;
}
template <TriviallyCopyable From>
void write_unaligned(void* dst, const From& src) ;
enum class mutable_view { no, yes, };
/// Fragmented buffer
///
/// Concept `FragmentedBuffer` is satisfied by any class that is a range of
/// fragments and provides a method `size_bytes()` which returns the total
/// size of the buffer. The interfaces accepting `FragmentedBuffer` will attempt
/// to avoid unnecessary linearisation.
template<typename T>
concept FragmentRange = requires (T range) {
    typename T::fragment_type;
    requires std::is_same_v<typename T::fragment_type, bytes_view>
        || std::is_same_v<typename T::fragment_type, bytes_mutable_view>;
    { *range.begin() } -> std::convertible_to<const typename T::fragment_type&>;
    { *range.end() } -> std::convertible_to<const typename T::fragment_type&>;
    { range.size_bytes() } -> std::convertible_to<size_t>;
    { range.empty() } -> std::same_as<bool>; // returns true iff size_bytes() == 0.
};
template<typename T, typename = void>
struct is_fragment_range : std::false_type { };
template<typename T>
struct is_fragment_range<T, std::void_t<typename T::fragment_type>> : std::true_type { };
template<typename T>
static constexpr bool is_fragment_range_v = is_fragment_range<T>::value;
/// A non-mutable view of a FragmentRange
///
/// Provide a trivially copyable and movable, non-mutable view on a
/// fragment range. This allows uniform ownership semantics across
/// multi-fragment ranges and the single fragment and empty fragment
/// adaptors below, i.e. it allows treating all fragment ranges
/// uniformly as views.
template <typename T>
requires FragmentRange<T>
class fragment_range_view {
    const T* _range;
public:
    using fragment_type = typename T::fragment_type;
    using iterator = typename T::const_iterator;
    using const_iterator = typename T::const_iterator;
public:
};
/// Single-element fragment range
///
/// This is a helper that allows converting a bytes_view into a FragmentRange.
template<mutable_view is_mutable>
class single_fragment_range {
public:
    using fragment_type = std::conditional_t<is_mutable == mutable_view::no,
                                             bytes_view, bytes_mutable_view>;
private:
    fragment_type _view;
public:
    using iterator = const fragment_type*;
    using const_iterator = const fragment_type*;
    explicit single_fragment_range(fragment_type f) : _view { f } { }
    const_iterator begin() const { return &_view; }
    const_iterator end() const { return &_view + 1; }
    size_t size_bytes() const { return _view.size(); }
    bool empty() const ;
};
single_fragment_range(bytes_view) -> single_fragment_range<mutable_view::no>;

/// Empty fragment range.
struct empty_fragment_range {
    using fragment_type = bytes_view;
    using iterator = bytes_view*;
    using const_iterator = bytes_view*;
    iterator begin() const ;
    iterator end() const ;
    size_t size_bytes() const ;
    bool empty() const ;
};
static_assert(FragmentRange<empty_fragment_range>);
static_assert(FragmentRange<single_fragment_range<mutable_view::no>>);
static_assert(FragmentRange<single_fragment_range<mutable_view::yes>>);
template<typename FragmentedBuffer>
requires FragmentRange<FragmentedBuffer>
bytes linearized(const FragmentedBuffer& buffer)
;
template<typename FragmentedBuffer, typename Function>
requires FragmentRange<FragmentedBuffer> && requires (Function fn, bytes_view bv) {
    fn(bv);
}
decltype(auto) with_linearized(const FragmentedBuffer& buffer, Function&& fn)
{
    bytes b;
    bytes_view bv;
    if (__builtin_expect(!buffer.empty() && std::next(buffer.begin()) == buffer.end(), true)) {
        bv = *buffer.begin();
    } else if (!buffer.empty()) {
        b = linearized(buffer);
        bv = b;
    }
    return fn(bv);
}
template<typename T>
concept FragmentedView = requires (T view, size_t n) {
    typename T::fragment_type;
    requires std::is_same_v<typename T::fragment_type, bytes_view>
            || std::is_same_v<typename T::fragment_type, bytes_mutable_view>;
    // No preconditions.
    { view.current_fragment() } -> std::convertible_to<const typename T::fragment_type&>;
    // No preconditions.
    { view.empty() } -> std::same_as<bool>;
    // No preconditions.
    { view.size_bytes() } -> std::convertible_to<size_t>;
    // Precondition: n <= size_bytes()
    { view.prefix(n) } -> std::same_as<T>;
    // Precondition: n <= size_bytes()
    view.remove_prefix(n);
    // Precondition: size_bytes() > 0
    view.remove_current();
};
template<typename T>
concept FragmentedMutableView = requires (T view) {
    requires FragmentedView<T>;
    requires std::is_same_v<typename T::fragment_type, bytes_mutable_view>;
};
template<FragmentedView View>
struct fragment_range {
    using fragment_type = typename View::fragment_type;
    View view;
    class fragment_iterator {
        using iterator_category = std::input_iterator_tag;
        using value_type = typename View::fragment_type;
        using difference_type = std::ptrdiff_t;
        using pointer = const value_type*;
        using reference = const value_type&;
        View _view;
        value_type _current;
    public:
        fragment_iterator() : _view(value_type()) {}
        
        fragment_iterator& operator++() ;
        
        reference operator*() const ;
        pointer operator->() const ;
        bool operator==(const fragment_iterator& i) const ;
    };
    using iterator = fragment_iterator;
    fragment_range(const View& v) : view(v) {}
    fragment_iterator begin() const ;
    fragment_iterator end() const ;
    size_t size_bytes() const ;
    bool empty() const ;
};
template<FragmentedView View>
requires (!FragmentRange<View>)
bytes linearized(View v)
{
    bytes b(bytes::initialized_later(), v.size_bytes());
    auto out = b.begin();
    while (v.size_bytes()) {
        out = std::copy(v.current_fragment().begin(), v.current_fragment().end(), out);
        v.remove_current();
    }
    return b;
}
template<FragmentedView View, typename Function>
requires (!FragmentRange<View>) && std::invocable<Function, bytes_view>
decltype(auto) with_linearized(const View& v, Function&& fn)
{
    if (v.size_bytes() == v.current_fragment().size()) [[likely]] {
        return fn(v.current_fragment());
    } else {
        return fn(linearized(v));
    }
}
template <mutable_view is_mutable>
class basic_single_fragmented_view {
public:
    using fragment_type = std::conditional_t<is_mutable == mutable_view::yes, bytes_mutable_view, bytes_view>;
private:
    fragment_type _view;
public:
    explicit basic_single_fragmented_view(fragment_type bv) : _view(bv) {}
    size_t size_bytes() const { return _view.size(); }
    bool empty() const { return _view.empty(); }
    void remove_prefix(size_t n) { _view.remove_prefix(n); }
    void remove_current() { _view = fragment_type(); }
    fragment_type current_fragment() const { return _view; }
    basic_single_fragmented_view prefix(size_t n) { return basic_single_fragmented_view(_view.substr(0, n)); }
};
using single_fragmented_view = basic_single_fragmented_view<mutable_view::no>;
using single_fragmented_mutable_view = basic_single_fragmented_view<mutable_view::yes>;
static_assert(FragmentedView<single_fragmented_view>);
static_assert(FragmentedMutableView<single_fragmented_mutable_view>);
static_assert(FragmentRange<fragment_range<single_fragmented_view>>);
static_assert(FragmentRange<fragment_range<single_fragmented_mutable_view>>);
template<FragmentedView View, typename Function>
requires std::invocable<Function, View> && std::invocable<Function, single_fragmented_view>
decltype(auto) with_simplified(const View& v, Function&& fn)
{
    if (v.size_bytes() == v.current_fragment().size()) [[likely]] {
        return fn(single_fragmented_view(v.current_fragment()));
    } else {
        return fn(v);
    }
}
template<FragmentedView View>
void skip_empty_fragments(View& v) {
    while (!v.empty() && v.current_fragment().empty()) {
        v.remove_current();
    }
}
template<FragmentedView V1, FragmentedView V2>
std::strong_ordering compare_unsigned(V1 v1, V2 v2) {
    while (!v1.empty() && !v2.empty()) {
        size_t n = std::min(v1.current_fragment().size(), v2.current_fragment().size());
        if (int d = memcmp(v1.current_fragment().data(), v2.current_fragment().data(), n)) {
            return d <=> 0;
        }
        v1.remove_prefix(n);
        v2.remove_prefix(n);
        skip_empty_fragments(v1);
        skip_empty_fragments(v2);
    }
    return v1.size_bytes() <=> v2.size_bytes();
}
template<FragmentedView V1, FragmentedView V2>
int equal_unsigned(V1 v1, V2 v2) {
    return v1.size_bytes() == v2.size_bytes() && compare_unsigned(v1, v2) == 0;
}
template<FragmentedMutableView Dest, FragmentedView Src>
void write_fragmented(Dest& dest, Src src) {
    if (dest.size_bytes() < src.size_bytes()) [[unlikely]] {
        throw std::out_of_range(format("tried to copy a buffer of size {} to a buffer of smaller size {}", src.size_bytes(), dest.size_bytes()));
    }
    while (!src.empty()) {
        size_t n = std::min(dest.current_fragment().size(), src.current_fragment().size());
        memcpy(dest.current_fragment().data(), src.current_fragment().data(), n);
        dest.remove_prefix(n);
        src.remove_prefix(n);
        skip_empty_fragments(dest);
        skip_empty_fragments(src);
    }
}
template<FragmentedMutableView Dest, FragmentedView Src>
void copy_fragmented_view(Dest dest, Src src) {
    if (dest.size_bytes() < src.size_bytes()) [[unlikely]] {
        throw std::out_of_range(format("tried to copy a buffer of size {} to a buffer of smaller size {}", src.size_bytes(), dest.size_bytes()));
    }
    while (!src.empty()) {
        size_t n = std::min(dest.current_fragment().size(), src.current_fragment().size());
        memcpy(dest.current_fragment().data(), src.current_fragment().data(), n);
        dest.remove_prefix(n);
        src.remove_prefix(n);
        skip_empty_fragments(dest);
        skip_empty_fragments(src);
    }
}
// Does not check bounds. Must be called only after size is already checked.
template<FragmentedView View>
void read_fragmented(View& v, size_t n, bytes::value_type* out) {
    while (n) {
        if (n <= v.current_fragment().size()) {
            std::copy_n(v.current_fragment().data(), n, out);
            v.remove_prefix(n);
            n = 0;
        } else {
            out = std::copy_n(v.current_fragment().data(), v.current_fragment().size(), out);
            n -= v.current_fragment().size();
            v.remove_current();
        }
    }
}
template<> void inline read_fragmented(single_fragmented_view& v, size_t n, bytes::value_type* out) {
    std::copy_n(v.current_fragment().data(), n, out);
    v.remove_prefix(n);
}
template<typename T, FragmentedView View>
T read_simple_native(View& v) {
    if (v.current_fragment().size() >= sizeof(T)) [[likely]] {
        auto p = v.current_fragment().data();
        v.remove_prefix(sizeof(T));
        return read_unaligned<T>(p);
    } else if (v.size_bytes() >= sizeof(T)) {
        T buf;
        read_fragmented(v, sizeof(T), reinterpret_cast<bytes::value_type*>(&buf));
        return buf;
    } else {
        throw_with_backtrace<marshal_exception>(format("read_simple - not enough bytes (expected {:d}, got {:d})", sizeof(T), v.size_bytes()));
    }
}
template<typename T, FragmentedView View>
T read_simple(View& v) {
    if (v.current_fragment().size() >= sizeof(T)) [[likely]] {
        auto p = v.current_fragment().data();
        v.remove_prefix(sizeof(T));
        return net::ntoh(read_unaligned<T>(p));
    } else if (v.size_bytes() >= sizeof(T)) {
        T buf;
        read_fragmented(v, sizeof(T), reinterpret_cast<bytes::value_type*>(&buf));
        return net::ntoh(buf);
    } else {
        throw_with_backtrace<marshal_exception>(format("read_simple - not enough bytes (expected {:d}, got {:d})", sizeof(T), v.size_bytes()));
    }
}
template<typename T, FragmentedView View>
T read_simple_exactly(View v) {
    if (v.current_fragment().size() == sizeof(T)) [[likely]] {
        auto p = v.current_fragment().data();
        return net::ntoh(read_unaligned<T>(p));
    } else if (v.size_bytes() == sizeof(T)) {
        T buf;
        read_fragmented(v, sizeof(T), reinterpret_cast<bytes::value_type*>(&buf));
        return net::ntoh(buf);
    } else {
        throw_with_backtrace<marshal_exception>(format("read_simple_exactly - size mismatch (expected {:d}, got {:d})", sizeof(T), v.size_bytes()));
    }
}
template<typename T, FragmentedMutableView Out>
static inline
void write(Out& out, std::type_identity_t<T> val) {
    auto v = net::ntoh(val);
    auto p = reinterpret_cast<const bytes_view::value_type*>(&v);
    if (out.current_fragment().size() >= sizeof(v)) [[likely]] {
        std::copy_n(p, sizeof(v), out.current_fragment().data());
        out.remove_prefix(sizeof(v));
    } else {
        write_fragmented(out, single_fragmented_view(bytes_view(p, sizeof(v))));
    }
}
template<typename T, FragmentedMutableView Out>
static inline
void write_native(Out& out, std::type_identity_t<T> v) {
    auto p = reinterpret_cast<const bytes_view::value_type*>(&v);
    if (out.current_fragment().size() >= sizeof(v)) [[likely]] {
        std::copy_n(p, sizeof(v), out.current_fragment().data());
        out.remove_prefix(sizeof(v));
    } else {
        write_fragmented(out, single_fragmented_view(bytes_view(p, sizeof(v))));
    }
}
template <FragmentedView View>
struct fmt::formatter<View> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const View& b, FormatContext& ctx) const {
        auto out = ctx.out();
        for (auto frag : fragment_range(b)) {
            fmt::format_to(out, "{}", fmt_hex(frag));
        }
        return out;
    }
};
class bytes_ostream;
template <mutable_view is_mutable_view>
class managed_bytes_basic_view;
using managed_bytes_view = managed_bytes_basic_view<mutable_view::no>;
using managed_bytes_mutable_view = managed_bytes_basic_view<mutable_view::yes>;
struct blob_storage {
    struct [[gnu::packed]] ref_type {
        blob_storage* ptr = nullptr;
        ref_type() {}
        ref_type(blob_storage* ptr) : ptr(ptr) {}
        operator blob_storage*() const { return ptr; }
        blob_storage* operator->() const { return ptr; }
        blob_storage& operator*() const { return *ptr; }
    };
    using size_type = uint32_t;
    using char_type = bytes_view::value_type;
    ref_type* backref;
    size_type size;
    size_type frag_size;
    ref_type next;
    char_type data[];
    blob_storage(ref_type* backref, size_type size, size_type frag_size) noexcept
        : backref(backref)
        , size(size)
        , frag_size(frag_size)
        , next(nullptr)
    {
        *backref = this;
    }
    
    size_t storage_size() const noexcept {
        return sizeof(*this) + frag_size;
    }
} __attribute__((packed));
// A managed version of "bytes" (can be used with LSA).
class managed_bytes {
    friend class bytes_ostream;
    static constexpr size_t max_inline_size = 15;
    struct small_blob {
        bytes_view::value_type data[max_inline_size];
        int8_t size; // -1 -> use blob_storage
    };
    union u {
        u() {}
        
        blob_storage::ref_type ptr;
        small_blob small;
    } _u;
    static_assert(sizeof(small_blob) > sizeof(blob_storage*), "inline size too small");
private:
    bool external() const noexcept {
        return _u.small.size < 0;
    }
    size_t max_seg(allocation_strategy& alctr) {
        return alctr.preferred_max_contiguous_allocation() - sizeof(blob_storage);
    }
    void free_chain(blob_storage* p) noexcept {
        auto& alctr = current_allocator();
        while (p) {
            auto n = p->next;
            alctr.destroy(p);
            p = n;
        }
    }
    bytes_view::value_type& value_at_index(blob_storage::size_type index) {
        if (!external()) {
            return _u.small.data[index];
        }
        blob_storage* a = _u.ptr;
        while (index >= a->frag_size) {
            index -= a->frag_size;
            a = a->next;
        }
        return a->data[index];
    }
    std::unique_ptr<bytes_view::value_type[]> do_linearize_pure() const;
    explicit managed_bytes(blob_storage* data) ;
public:
    using size_type = blob_storage::size_type;
    struct initialized_later {};
    managed_bytes() ;
    managed_bytes(const blob_storage::char_type* ptr, size_type size)
        : managed_bytes(bytes_view(ptr, size)) {}
    
    template <FragmentedView View>
    explicit managed_bytes(View v);
    managed_bytes(initialized_later, size_type size) {
        memory::on_alloc_point();
        if (size <= max_inline_size) {
            _u.small.size = size;
        } else {
            _u.small.size = -1;
            auto& alctr = current_allocator();
            auto maxseg = max_seg(alctr);
            auto now = std::min(size_t(size), maxseg);
            void* p = alctr.alloc<blob_storage>(sizeof(blob_storage) + now);
            auto first = new (p) blob_storage(&_u.ptr, size, now);
            auto last = first;
            size -= now;
            try {
                while (size) {
                    auto now = std::min(size_t(size), maxseg);
                    void* p = alctr.alloc<blob_storage>(sizeof(blob_storage) + now);
                    last = new (p) blob_storage(&last->next, 0, now);
                    size -= now;
                }
            } catch (...) {
                free_chain(first);
                throw;
            }
        }
    }
    explicit managed_bytes(bytes_view v) : managed_bytes(initialized_later(), v.size()) {
        if (!external()) {
            // Workaround for https://github.com/scylladb/scylla/issues/4086
            #pragma GCC diagnostic push
            #pragma GCC diagnostic ignored "-Warray-bounds"
            std::copy(v.begin(), v.end(), _u.small.data);
            #pragma GCC diagnostic pop
            return;
        }
        auto p = v.data();
        auto s = v.size();
        auto b = _u.ptr;
        while (s) {
            memcpy(b->data, p, b->frag_size);
            p += b->frag_size;
            s -= b->frag_size;
            b = b->next;
        }
        assert(!b);
    }
    managed_bytes(std::initializer_list<bytes::value_type> b) : managed_bytes(b.begin(), b.size()) {}
    ~managed_bytes() noexcept {
        if (external()) {
            free_chain(_u.ptr);
        }
    }
    managed_bytes(const managed_bytes& o) : managed_bytes(initialized_later(), o.size()) {
        if (!o.external()) {
            _u.small = o._u.small;
            return;
        }
        auto s = size();
        const blob_storage::ref_type* next_src = &o._u.ptr;
        blob_storage* blob_src = nullptr;
        size_type size_src = 0;
        size_type offs_src = 0;
        blob_storage::ref_type* next_dst = &_u.ptr;
        blob_storage* blob_dst = nullptr;
        size_type size_dst = 0;
        size_type offs_dst = 0;
        while (s) {
            if (!size_src) {
                blob_src = *next_src;
                next_src = &blob_src->next;
                size_src = blob_src->frag_size;
                offs_src = 0;
            }
            if (!size_dst) {
                blob_dst = *next_dst;
                next_dst = &blob_dst->next;
                size_dst = blob_dst->frag_size;
                offs_dst = 0;
            }
            auto now = std::min(size_src, size_dst);
            memcpy(blob_dst->data + offs_dst, blob_src->data + offs_src, now);
            s -= now;
            offs_src += now; size_src -= now;
            offs_dst += now; size_dst -= now;
        }
        assert(size_src == 0 && size_dst == 0);
    }
    managed_bytes(managed_bytes&& o) noexcept
        : _u(o._u)
    {
        if (external()) {
            // _u.ptr cannot be null
            _u.ptr->backref = &_u.ptr;
        }
        o._u.small.size = 0;
    }
    managed_bytes& operator=(managed_bytes&& o) noexcept {
        if (this != &o) {
            this->~managed_bytes();
            new (this) managed_bytes(std::move(o));
        }
        return *this;
    }
    managed_bytes& operator=(const managed_bytes& o) {
        if (this != &o) {
            managed_bytes tmp(o);
            this->~managed_bytes();
            new (this) managed_bytes(std::move(tmp));
        }
        return *this;
    }
    
    bytes_view::value_type& operator[](size_type index) {
        return value_at_index(index);
    }
    
    size_type size() const {
        if (external()) {
            return _u.ptr->size;
        } else {
            return _u.small.size;
        }
    }
    bool empty() const {
        return _u.small.size == 0;
    }
    // Returns the amount of external memory used.
    // Returns the minimum possible amount of external memory used by a managed_bytes
    // of the same size as us.
    // In other words, it returns the amount of external memory that would used by this
    // managed_bytes if all data was allocated in one big fragment.
     ;
    template <mutable_view is_mutable_view>
    friend class managed_bytes_basic_view;
};
template <mutable_view is_mutable>
class managed_bytes_basic_view {
public:
    using fragment_type = std::conditional_t<is_mutable == mutable_view::yes, bytes_mutable_view, bytes_view>;
    using owning_type = std::conditional_t<is_mutable == mutable_view::yes, managed_bytes, const managed_bytes>;
    using value_type = typename fragment_type::value_type;
private:
    fragment_type _current_fragment = {};
    blob_storage* _next_fragments = nullptr;
    size_t _size = 0;
private:
public:
    managed_bytes_basic_view() = default;
    managed_bytes_basic_view(const managed_bytes_basic_view&) = default;
    managed_bytes_basic_view(owning_type& mb) {
        if (mb._u.small.size != -1) {
            _current_fragment = fragment_type(mb._u.small.data, mb._u.small.size);
            _size = mb._u.small.size;
        } else {
            auto p = mb._u.ptr;
            _current_fragment = fragment_type(p->data, p->frag_size);
            _next_fragments = p->next;
            _size = p->size;
        }
    }
    managed_bytes_basic_view(fragment_type bv)
        : _current_fragment(bv)
        , _size(bv.size()) {
    }
    size_t size() const { return _size; }
    size_t size_bytes() const { return _size; }
    bool empty() const { return _size == 0; }
    fragment_type current_fragment() const { return _current_fragment; }
    void remove_prefix(size_t n) {
        while (n >= _current_fragment.size() && n > 0) {
            n -= _current_fragment.size();
            remove_current();
        }
        _size -= n;
        _current_fragment.remove_prefix(n);
    }
    void remove_current() {
        _size -= _current_fragment.size();
        if (_size) {
            _current_fragment = fragment_type(_next_fragments->data, _next_fragments->frag_size);
            _next_fragments = _next_fragments->next;
            _current_fragment = _current_fragment.substr(0, _size);
        } else {
            _current_fragment = fragment_type();
        }
    }
    managed_bytes_basic_view prefix(size_t len) const {
        managed_bytes_basic_view v = *this;
        v._size = len;
        v._current_fragment = v._current_fragment.substr(0, len);
        return v;
    }
    managed_bytes_basic_view substr(size_t offset, size_t len) const {
        size_t end = std::min(offset + len, _size);
        managed_bytes_basic_view v = prefix(end);
        v.remove_prefix(offset);
        return v;
    }
    const auto& front() const ;
    auto& front() { return _current_fragment.front(); }
    const value_type& operator[](size_t index) const {
        auto v = *this;
        v.remove_prefix(index);
        return v.current_fragment().front();
    }
    
    bool is_linearized() const {
        return _current_fragment.size() == _size;
    }
    // Allow casting mutable views to immutable views.
    template <mutable_view Other>
    friend class managed_bytes_basic_view;
    template <mutable_view Other>
    managed_bytes_basic_view(const managed_bytes_basic_view<Other>& other)
    requires (is_mutable == mutable_view::no) && (Other == mutable_view::yes)
        : _current_fragment(other._current_fragment.data(), other._current_fragment.size())
        , _next_fragments(other._next_fragments)
        , _size(other._size)
    {}
     ;
    
};
static_assert(FragmentedView<managed_bytes_view>);
static_assert(FragmentedMutableView<managed_bytes_mutable_view>);
using managed_bytes_opt = std::optional<managed_bytes>;
using managed_bytes_view_opt = std::optional<managed_bytes_view>;
 
inline bytes to_bytes(managed_bytes_view v) {
    return linearized(v);
}
/// Converts a possibly fragmented managed_bytes_opt to a
/// linear bytes_opt.
///
/// \note copies data
/// Converts a linear bytes_opt to a possibly fragmented
/// managed_bytes_opt.
///
/// \note copies data

template<>
struct appending_hash<managed_bytes_view> {
    template<Hasher Hasher>
    void operator()(Hasher& h, managed_bytes_view v) const ;
};
namespace std {
template <>
struct hash<managed_bytes_view> {
    size_t operator()(managed_bytes_view v) const {
        bytes_view_hasher h;
        appending_hash<managed_bytes_view>{}(h, v);
        return h.finalize();
    }
};
template <>
struct hash<managed_bytes> {
    size_t operator()(const managed_bytes& v) const {
        return hash<managed_bytes_view>{}(v);
    }
};
} // namespace std
// The operators below are used only by tests.
inline bool operator==(const managed_bytes_view& a, const managed_bytes_view& b) {
    return a.size_bytes() == b.size_bytes() && compare_unsigned(a, b) == 0;
}
 std::ostream& operator<<(std::ostream& os, const managed_bytes_view& v) ;
 

class bytes_ostream {
public:
    using size_type = bytes::size_type;
    using value_type = bytes::value_type;
    using fragment_type = bytes_view;
    static constexpr size_type max_chunk_size() ;
private:
    static_assert(sizeof(value_type) == 1, "value_type is assumed to be one byte long");
    // Note: while appending data, chunk::size refers to the allocated space in the chunk,
    //       and chunk::frag_size refers to the currently occupied space in the chunk.
    //       After building, the first chunk::size is the whole object size, and chunk::frag_size
    //       doesn't change. This fits with managed_bytes interpretation.
    using chunk = blob_storage;
    static constexpr size_type default_chunk_size{512};
private:
    blob_storage::ref_type _begin;
    chunk* _current;
    size_type _size;
    size_type _initial_chunk_size = default_chunk_size;
public:
    class fragment_iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = bytes_view;
        using difference_type = std::ptrdiff_t;
        using pointer = bytes_view*;
        using reference = bytes_view&;
        struct implementation {
            blob_storage* current_chunk;
        };
    private:
        chunk* _current = nullptr;
    public:
        bytes_view operator*() const ;
        
        fragment_iterator& operator++() ;
        fragment_iterator operator++(int) ;
        bool operator==(const fragment_iterator&) const = default;
    };
    using const_iterator = fragment_iterator;
    class output_iterator {
    public:
        using iterator_category = std::output_iterator_tag;
        using difference_type = std::ptrdiff_t;
        using value_type = bytes_ostream::value_type;
        using pointer = bytes_ostream::value_type*;
        using reference = bytes_ostream::value_type&;
        friend class bytes_ostream;
    private:
        bytes_ostream* _ostream = nullptr;
    private:
    public:
    };
private:
     size_type current_space_left() const ;
    // Figure out next chunk size.
    //   - must be enough for data_size + sizeof(chunk)
    //   - must be at least _initial_chunk_size
    //   - try to double each time to prevent too many allocations
    //   - should not exceed max_alloc_size, unless data_size requires so
    //   - will be power-of-two so the allocated memory can be fully utilized.
    size_type next_alloc_size(size_t data_size) const ;
    // Makes room for a contiguous region of given size.
    // The region is accounted for as already written.
    // size must not be zero.
    [[gnu::always_inline]]
    value_type* alloc(size_type size) {
        if (__builtin_expect(size <= current_space_left(), true)) {
            auto ret = _current->data + _current->frag_size;
            _current->frag_size += size;
            _size += size;
            return ret;
        } else {
            return alloc_new(size);
        }
    }
    [[gnu::noinline]]
    value_type* alloc_new(size_type size) {
            auto alloc_size = next_alloc_size(size);
            auto space = malloc(alloc_size);
            if (!space) {
                throw std::bad_alloc();
            }
            auto backref = _current ? &_current->next : &_begin;
            auto new_chunk = new (space) chunk(backref, alloc_size - sizeof(chunk), size);
            _current = new_chunk;
            _size += size;
            return _current->data;
    }
    [[gnu::noinline]]
    void free_chain(chunk* c) noexcept {
        while (c) {
            auto n = c->next;
            c->~chunk();
            ::free(c);
            c = n;
        }
    }
public:
    explicit bytes_ostream(size_t initial_chunk_size) noexcept
        : _begin()
        , _current(nullptr)
        , _size(0)
        , _initial_chunk_size(initial_chunk_size)
    { }
    bytes_ostream() noexcept : bytes_ostream(default_chunk_size) {}
    bytes_ostream(bytes_ostream&& o) noexcept
        : _begin(std::exchange(o._begin, {}))
        , _current(o._current)
        , _size(o._size)
        , _initial_chunk_size(o._initial_chunk_size)
    {
        o._current = nullptr;
        o._size = 0;
    }
    bytes_ostream(const bytes_ostream& o)
        : _begin()
        , _current(nullptr)
        , _size(0)
        , _initial_chunk_size(o._initial_chunk_size)
    {
        append(o);
    }
    ~bytes_ostream() {
        free_chain(_begin.ptr);
    }
    bytes_ostream& operator=(const bytes_ostream& o) {
        if (this != &o) {
            auto x = bytes_ostream(o);
            *this = std::move(x);
        }
        return *this;
    }
    bytes_ostream& operator=(bytes_ostream&& o) noexcept {
        if (this != &o) {
            this->~bytes_ostream();
            new (this) bytes_ostream(std::move(o));
        }
        return *this;
    }
    template <typename T>
    struct place_holder {
        value_type* ptr;
        // makes the place_holder looks like a stream
        seastar::simple_output_stream get_stream() {
            return seastar::simple_output_stream(reinterpret_cast<char*>(ptr), sizeof(T));
        }
    };
    // Returns a place holder for a value to be written later.
     ;
    [[gnu::always_inline]]
    value_type* write_place_holder(size_type size) {
        return alloc(size);
    }
    // Writes given sequence of bytes
    [[gnu::always_inline]]
    inline void write(bytes_view v) {
        if (v.empty()) {
            return;
        }
        auto this_size = std::min(v.size(), size_t(current_space_left()));
        if (__builtin_expect(this_size, true)) {
            memcpy(_current->data + _current->frag_size, v.begin(), this_size);
            _current->frag_size += this_size;
            _size += this_size;
            v.remove_prefix(this_size);
        }
        while (!v.empty()) {
            auto this_size = std::min(v.size(), size_t(max_chunk_size()));
            std::copy_n(v.begin(), this_size, alloc_new(this_size));
            v.remove_prefix(this_size);
        }
    }
    [[gnu::always_inline]]
    void write(const char* ptr, size_t size) {
        write(bytes_view(reinterpret_cast<const signed char*>(ptr), size));
    }
    // Call only when is_linearized()
    // Makes the underlying storage contiguous and returns a view to it.
    // Invalidates all previously created placeholders.
    // Returns the amount of bytes written so far
    size_type size() const ;
    // For the FragmentRange concept
    
    void append(const bytes_ostream& o) ;
    // Removes n bytes from the end of the bytes_ostream.
    // Beware of O(n) algorithm.
    
    // begin() and end() form an input range to bytes_view representing fragments.
    // Any modification of this instance invalidates iterators.
    
    boost::iterator_range<fragment_iterator> fragments() const ;
    struct position {
        chunk* _chunk;
        size_type _offset;
    };
    // Returns the amount of bytes written since given position.
    // "pos" must be valid.
    // Rollbacks all data written after "pos".
    // Invalidates all placeholders and positions created after "pos".
    // Makes this instance empty.
    //
    // The first buffer is not deallocated, so callers may rely on the
    // fact that if they write less than the initial chunk size between
    // the clear() calls then writes will not involve any memory allocations,
    // except for the first write made on this instance.
    // Makes this instance empty using async continuations, while allowing yielding.
    //
    // The first buffer is not deallocated, so callers may rely on the
    // fact that if they write less than the initial chunk size between
    // the clear() calls then writes will not involve any memory allocations,
    // except for the first write made on this instance.
};
namespace utils {
using input_stream = seastar::memory_input_stream<bytes_ostream::fragment_iterator>;
}
namespace ser {
/// A fragmented view of an opaque buffer in a stream of serialised data
///
/// This class allows reading large, fragmented blobs serialised by the IDL
/// infrastructure without linearising or copying them. The view remains valid
/// as long as the underlying IDL-serialised buffer is alive.
///
/// Satisfies FragmentRange concept.
template<typename FragmentIterator>
class buffer_view {
    bytes_view _first;
    size_t _total_size;
    FragmentIterator _next;
public:
    using fragment_type = bytes_view;
    struct implementation {
        bytes_view current;
        FragmentIterator next;
        size_t size;
    };
    class iterator {
        bytes_view _current;
        size_t _left = 0;
        FragmentIterator _next;
    public:
        using iterator_category	= std::input_iterator_tag;
        using value_type = bytes_view;
        using pointer = const bytes_view*;
        using reference = const bytes_view&;
        using difference_type = std::ptrdiff_t;
        
        bytes_view operator*() const ;
        
        iterator& operator++() ;
        
        bool operator==(const iterator& other) const ;
    };
    using const_iterator = iterator;
    iterator begin() const ;
    iterator end() const ;
    size_t size_bytes() const ;
    bool empty() const ;
    // FragmentedView implementation
    void remove_prefix(size_t n) ;
    void remove_current() ;
    buffer_view prefix(size_t n) const ;
    bytes_view current_fragment() ;
    
    
    ;
    implementation extract_implementation() const ;
};
static_assert(FragmentedView<buffer_view<bytes_ostream::fragment_iterator>>);
using size_type = uint32_t;
template<typename T, typename Input>
requires std::is_integral_v<T>
inline T deserialize_integral(Input& input) {
    T data;
    input.read(reinterpret_cast<char*>(&data), sizeof(T));
    return le_to_cpu(data);
}
template<typename T, typename Output>
requires std::is_integral_v<T>
inline void serialize_integral(Output& output, T data) {
    data = cpu_to_le(data);
    output.write(reinterpret_cast<const char*>(&data), sizeof(T));
}
template<typename T>
struct serializer;
template<typename T>
struct integral_serializer {
    template<typename Input>
    static T read(Input& v) ;
     ;
     ;
};
template<> struct serializer<bool> {
     ;
     ;
     ;
};
template<> struct serializer<int8_t> : public integral_serializer<int8_t> {};
template<> struct serializer<uint8_t> : public integral_serializer<uint8_t> {};
template<> struct serializer<int16_t> : public integral_serializer<int16_t> {};
template<> struct serializer<uint16_t> : public integral_serializer<uint16_t> {};
template<> struct serializer<int32_t> : public integral_serializer<int32_t> {};
template<> struct serializer<uint32_t> : public integral_serializer<uint32_t> {};
template<> struct serializer<int64_t> : public integral_serializer<int64_t> {};
template<> struct serializer<uint64_t> : public integral_serializer<uint64_t> {};
;
template<typename T, typename Output>
 void serialize(Output& out, const T& v) ;;
 ;
template<typename T, typename Input>
inline auto deserialize(Input& in, boost::type<T> t) {
    return serializer<T>::read(in);
}
template<typename T, typename Input>
 void skip(Input& v, boost::type<T>) ;
;
;
;
template<typename Buffer, typename T>
Buffer serialize_to_buffer(const T& v, size_t head_space = 0);
template<typename T, typename Buffer>
T deserialize_from_buffer(const Buffer&, boost::type<T>, size_t head_space = 0);
;
;
;
;
struct unknown_variant_type {
    size_type index;
    sstring data;
};
;
;
template <typename T>
struct normalize {
    using type = T;
};
template <>
struct normalize<bytes_view> {
     using type = bytes;
};
template <>
struct normalize<managed_bytes> {
     using type = bytes;
};
template <>
struct normalize<bytes_ostream> {
    using type = bytes;
};
template <typename T, typename U>
struct is_equivalent : std::is_same<typename normalize<std::remove_const_t<std::remove_reference_t<T>>>::type, typename normalize<std::remove_const_t <std::remove_reference_t<U>>>::type> {
};
template <typename T, typename U>
struct is_equivalent<std::reference_wrapper<T>, U> : is_equivalent<T, U> {
};
template <typename T, typename U>
struct is_equivalent<T, std::reference_wrapper<U>> : is_equivalent<T, U> {
};
template <typename T, typename U>
struct is_equivalent<std::optional<T>, std::optional<U>> : is_equivalent<T, U> {
};
template <typename T, typename U, bool>
struct is_equivalent_arity;
template <typename ...T, typename ...U>
struct is_equivalent_arity<std::tuple<T...>, std::tuple<U...>, false> : std::false_type {
};
template <typename ...T, typename ...U>
struct is_equivalent_arity<std::tuple<T...>, std::tuple<U...>, true> {
    static constexpr bool value = (is_equivalent<T, U>::value && ...);
};
template <typename ...T, typename ...U>
struct is_equivalent<std::tuple<T...>, std::tuple<U...>> : is_equivalent_arity<std::tuple<T...>, std::tuple<U...>, sizeof...(T) == sizeof...(U)> {
};
template <typename ...T, typename ...U>
struct is_equivalent<std::variant<T...>, std::variant<U...>> : is_equivalent<std::tuple<T...>, std::tuple<U...>> {
};
// gc_clock duration values were serialized as 32-bit prior to 3.1, and
// are serialized as 64-bit in 3.1.0.
//
// TTL values are capped to 20 years, which fits into 32 bits, so
// truncation is not a concern.
inline bool gc_clock_using_3_1_0_serialization = false;
 ;
 ;
}
// The following is a redesigned subset of Java's DataOutput,
// DataOutputStream, DataInput, DataInputStream, etc. It allows serializing
// several primitive types (e.g., integer, string, etc.) to an object which
// is only capable of write()ing a single byte (write(char)) or an array of
// bytes (write(char *, int)), and deserializing the same data from an object
// with a char read() interface.
//
// The format of this serialization is identical to the format used by
// Java's DataOutputStream class. This is important to allow us communicate
// with nodes running Java version of the code.
//
// We only support the subset actually used in Cassandra, and the subset
// that is reversible, i.e., can be read back by data_input. For example,
// we only support DataOutput.writeUTF(string) and not
// DataOutput.writeChars(string) - because the latter does not include
// the length, which is necessary for reading the string back.
class UTFDataFormatException { };
class EOFException { };
static constexpr size_t serialize_int8_size = 1;
static constexpr size_t serialize_bool_size = 1;
static constexpr size_t serialize_int16_size = 2;
static constexpr size_t serialize_int32_size = 4;
static constexpr size_t serialize_int64_size = 8;
namespace internal_impl {
template <typename ExplicitIntegerType, typename CharOutputIterator, typename IntegerType>
requires std::is_integral<ExplicitIntegerType>::value && std::is_integral<IntegerType>::value && requires (CharOutputIterator it) {
    *it++ = 'a';
}
inline
void serialize_int(CharOutputIterator& out, IntegerType val) {
    ExplicitIntegerType nval = net::hton(ExplicitIntegerType(val));
    out = std::copy_n(reinterpret_cast<const char*>(&nval), sizeof(nval), out);
}
}
 ;
 ;
template <typename CharOutputIterator>
inline
void serialize_int32(CharOutputIterator& out, uint32_t val) {
    internal_impl::serialize_int<uint32_t>(out, val);
}
template <typename CharOutputIterator>
inline
void serialize_int64(CharOutputIterator& out, uint64_t val) {
    internal_impl::serialize_int<uint64_t>(out, val);
}
 ;
// The following serializer is compatible with Java's writeUTF().
// In our C++ implementation, we assume the string is already UTF-8
// encoded. Unfortunately, Java's implementation is a bit different from
// UTF-8 for encoding characters above 16 bits in unicode (see
// http://docs.oracle.com/javase/7/docs/api/java/io/DataInput.html#modified-utf-8)
// For now we'll just assume those aren't in the string...
// TODO: fix the compatibility with Java even in this case.
 ;
 ;
template<typename T, typename CharOutputIterator>
static inline
void write(CharOutputIterator& out, const T& val) {
    auto v = net::ntoh(val);
    out = std::copy_n(reinterpret_cast<char*>(&v), sizeof(v), out);
}
// This class is the parts of java.util.UUID that we need
namespace utils {
class UUID {
private:
    int64_t most_sig_bits;
    int64_t least_sig_bits;
public:
    constexpr UUID() noexcept : most_sig_bits(0), least_sig_bits(0) {}
    constexpr UUID(int64_t most_sig_bits, int64_t least_sig_bits) noexcept
        : most_sig_bits(most_sig_bits), least_sig_bits(least_sig_bits) {}
    // May throw marshal_exception is failed to parse uuid string.
    
    explicit UUID(const char * s)  ;
    
    int64_t get_most_significant_bits() const noexcept ;
    int64_t get_least_significant_bits() const noexcept ;
    int version() const noexcept {
        return (most_sig_bits >> 12) & 0xf;
    }
    bool is_timestamp() const noexcept {
        return version() == 1;
    }
    
    friend ::fmt::formatter<UUID>;
    
    
    bool operator==(const UUID& v) const noexcept = default;
    // Please note that this comparator does not preserve timeuuid
    // monotonicity. For this reason you should avoid using it for
    // UUIDs that could store timeuuids, otherwise bugs like
    // https://github.com/scylladb/scylla/issues/7729 may happen.
    std::strong_ordering operator<=>(const UUID& v) const noexcept {
        auto cmp = uint64_t(most_sig_bits) <=> uint64_t(v.most_sig_bits);
        if (cmp != 0) {
            return cmp;
        }
        return uint64_t(least_sig_bits) <=> uint64_t(v.least_sig_bits);
    }
    // nibble set to a non-zero value
    template <typename CharOutputIterator>
    void serialize(CharOutputIterator& out) const {
        serialize_int64(out, most_sig_bits);
        serialize_int64(out, least_sig_bits);
    }
};
 
UUID make_random_uuid() noexcept;
// Read 8 most significant bytes of timeuuid from serialized bytes
inline uint64_t timeuuid_read_msb(const int8_t *b) noexcept {
    // cast to unsigned to avoid sign-compliment during shift.
    auto u64 = [](uint8_t i) -> uint64_t { return i; };
    // Scylla and Cassandra use a standard UUID memory layout for MSB:
    // 4 bytes    2 bytes    2 bytes
    // time_low - time_mid - time_hi_and_version
    //
    // The storage format uses network byte order.
    // Reorder bytes to allow for an integer compare.
    return u64(b[6] & 0xf) << 56 | u64(b[7]) << 48 |
           u64(b[4]) << 40 | u64(b[5]) << 32 |
           u64(b[0]) << 24 | u64(b[1]) << 16 |
           u64(b[2]) << 8  | u64(b[3]);
}
inline uint64_t uuid_read_lsb(const int8_t *b) noexcept {
    auto u64 = [](uint8_t i) -> uint64_t { return i; };
    return u64(b[8]) << 56 | u64(b[9]) << 48 |
           u64(b[10]) << 40 | u64(b[11]) << 32 |
           u64(b[12]) << 24 | u64(b[13]) << 16 |
           u64(b[14]) << 8  | u64(b[15]);
}
// Compare two values of timeuuid type.
// Cassandra legacy requires:
// - using signed compare for least significant bits.
// - masking off UUID version during compare, to
// treat possible non-version-1 UUID the same way as UUID.
//
// To avoid breaking ordering in existing sstables, Scylla preserves
// Cassandra compare order.
//
inline std::strong_ordering timeuuid_tri_compare(bytes_view o1, bytes_view o2) noexcept {
    auto timeuuid_read_lsb = [](bytes_view o) -> uint64_t {
        return uuid_read_lsb(o.begin()) ^ 0x8080808080808080;
    };
    auto res = timeuuid_read_msb(o1.begin()) <=> timeuuid_read_msb(o2.begin());
    if (res == 0) {
        res = timeuuid_read_lsb(o1) <=> timeuuid_read_lsb(o2);
    }
    return res;
}
// Compare two values of UUID type, if they happen to be
// both of Version 1 (timeuuids).
//
// This function uses memory order for least significant bits,
// which is both faster and monotonic, so should be preferred
// to @timeuuid_tri_compare() used for all new features.
//
inline std::strong_ordering uuid_tri_compare_timeuuid(bytes_view o1, bytes_view o2) noexcept {
    auto res = timeuuid_read_msb(o1.begin()) <=> timeuuid_read_msb(o2.begin());
    if (res == 0) {
        res = uuid_read_lsb(o1.begin()) <=> uuid_read_lsb(o2.begin());
    }
    return res;
}
template<typename Tag>
struct tagged_uuid {
    utils::UUID id;
    std::strong_ordering operator<=>(const tagged_uuid&) const noexcept = default;
    explicit operator bool() const noexcept ;
    
    static tagged_uuid create_null_id() noexcept ;
    explicit tagged_uuid(const utils::UUID& uuid) noexcept : id(uuid) {}
    tagged_uuid() = default;
    
};
} // namespace utils
template<>
struct appending_hash<utils::UUID> {
     ;
};
template<typename Tag>
struct appending_hash<utils::tagged_uuid<Tag>> {
     ;
};
namespace std {
template<>
struct hash<utils::UUID> {
    size_t operator()(const utils::UUID& id) const noexcept {
        auto hilo = id.get_most_significant_bits()
                ^ id.get_least_significant_bits();
        return size_t((hilo >> 32) ^ hilo);
    }
};
template<typename Tag>
struct hash<utils::tagged_uuid<Tag>> {
    size_t operator()(const utils::tagged_uuid<Tag>& id) const noexcept {
        return hash<utils::UUID>()(id.id);
    }
};
template<typename Tag>
std::ostream& operator<<(std::ostream& os, const utils::tagged_uuid<Tag>& id) ;
} // namespace std
template <>
struct fmt::formatter<utils::UUID> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const utils::UUID& id, FormatContext& ctx) const {
        // This matches Java's UUID.toString() actual implementation. Note that
        // that method's documentation suggest something completely different!
        return fmt::format_to(ctx.out(),
                "{:08x}-{:04x}-{:04x}-{:04x}-{:012x}",
                ((uint64_t)id.most_sig_bits >> 32),
                ((uint64_t)id.most_sig_bits >> 16 & 0xffff),
                ((uint64_t)id.most_sig_bits & 0xffff),
                ((uint64_t)id.least_sig_bits >> 48 & 0xffff),
                ((uint64_t)id.least_sig_bits & 0xffffffffffffLL));
    }
};
template <typename Tag>
struct fmt::formatter<utils::tagged_uuid<Tag>> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const utils::tagged_uuid<Tag>& id, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", id.id);
    }
};
namespace utils {
// Scylla uses specialized timeuuids for list keys. They use
// limited space of timeuuid clockseq component to store
// sub-microsecond time. This exception is thrown when an attempt
// is made to construct such a UUID with a sub-microsecond argument
// which is outside the available bit range.
struct timeuuid_submicro_out_of_range: public std::out_of_range {
    using out_of_range::out_of_range;
};
class UUID_gen
{
public:
    // UUID timestamp time component is represented in intervals
    // of 1/10 of a microsecond since the beginning of GMT epoch.
    using decimicroseconds = std::chrono::duration<int64_t, std::ratio<1, 10'000'000>>;
    using milliseconds = std::chrono::milliseconds;
private:
    // A grand day! millis at 00:00:00.000 15 Oct 1582.
    static constexpr decimicroseconds START_EPOCH = decimicroseconds{-122192928000000000L};
    // UUID time must fit in 60 bits
    static constexpr milliseconds UUID_UNIXTIME_MAX = duration_cast<milliseconds>(
        decimicroseconds{0x0fffffffffffffffL} + START_EPOCH);
    // A random mac address for use in timeuuids
    // where we can not use clockseq to randomize the physical
    // node, and prefer using a random address to a physical one
    // to avoid duplicate timeuuids when system time goes back
    // while scylla is restarting. Using a spoof node also helps
    // avoid timeuuid duplicates when multiple nodes run on the
    // same host and share the physical MAC address.
    static thread_local const int64_t spoof_node;
    static thread_local const int64_t clock_seq_and_node;
    static constexpr int64_t MIN_CLOCK_SEQ_AND_NODE = 0x8080808080808080L;
    static constexpr int64_t MAX_CLOCK_SEQ_AND_NODE = 0x7f7f7f7f7f7f7f7fL;
    // An instance of UUID_gen uses clock_seq_and_node so should
    // be constructed after it.
    static thread_local UUID_gen _instance;
    decimicroseconds _last_used_time = decimicroseconds{0};
    UUID_gen()
    {
        // make sure someone didn't whack the clockSeqAndNode by changing the order of instantiation.
        assert(clock_seq_and_node != 0);
    }
    // Return decimicrosecond time based on the system time,
    // in milliseconds. If the current millisecond hasn't change
    // from the previous call, increment the previously used
    // value by one decimicrosecond.
    // NOTE: In the original Java code this function was
    // "synchronized". This isn't needed since in Scylla we do not
    // need monotonicity between time UUIDs created at different
    // shards and UUID code uses thread local state on each shard.
    int64_t create_time_safe() {
        using std::chrono::system_clock;
        auto millis = duration_cast<milliseconds>(system_clock::now().time_since_epoch());
        decimicroseconds when = from_unix_timestamp(millis);
        if (when > _last_used_time) {
            _last_used_time = when;
        } else {
            when = ++_last_used_time;
        }
        return create_time(when);
    }
public:
    // We have only 17 timeuuid bits available to store this
    // value.
    static constexpr int SUBMICRO_LIMIT = (1<<17);
    static UUID get_time_UUID()
    {
        auto uuid = UUID(_instance.create_time_safe(), clock_seq_and_node);
        assert(uuid.is_timestamp());
        return uuid;
    }
    
    
    // Generate a time-based (Version 1) UUID using
    // a microsecond-precision Unix time and a unique number in
    // range [0, 131072).
    // Used to generate many unique, monotonic UUIDs
    // sharing the same microsecond part. In lightweight
    // transactions we must ensure monotonicity between all UUIDs
    // which belong to one lightweight transaction and UUIDs of
    // another transaction, but still need multiple distinct and
    // monotonic UUIDs within the same transaction.
    // \throws timeuuid_submicro_out_of_range
    //
    static UUID get_name_UUID(sstring_view str);
    
    
     ;
    template <std::intmax_t N, std::intmax_t D>
    static decimicroseconds from_unix_timestamp(std::chrono::duration<int64_t, std::ratio<N, D>> d) {
        // Avoid 64-bit representation overflow when adding
        // timeuuid epoch to nanosecond resolution time.
        auto dmc = duration_cast<decimicroseconds>(d);
        return dmc - START_EPOCH;
    }
    // std::chrono typeaware wrapper around create_time().
    // Creates a timeuuid compatible time (decimicroseconds since
    // the start of GMT epoch).
    template <std::intmax_t N, std::intmax_t D>
    static int64_t create_time(std::chrono::duration<int64_t, std::ratio<N, D>> d) {
        auto dmc = duration_cast<decimicroseconds>(d);
        uint64_t msb = dmc.count();
        // timeuuid time must fit in 60 bits
        assert(!(0xf000000000000000UL & msb));
        return ((0x00000000ffffffffL & msb) << 32 |
               (0x0000ffff00000000UL & msb) >> 16 |
               (0x0fff000000000000UL & msb) >> 48 |
                0x0000000000001000L); // sets the version to 1.
    }
    // Produce an UUID which is derived from this UUID in a reversible manner
    //
    // Such that:
    //
    //      auto original_uuid = UUID_gen::get_time_UUID();
    //      auto negated_uuid = UUID_gen::negate(original_uuid);
    //      assert(original_uuid != negated_uuid);
    //      assert(original_uuid == UUID_gen::negate(negated_uuid));
    
};
// for the curious, here is how I generated START_EPOCH
//        Calendar c = Calendar.getInstance(TimeZone.getTimeZone("GMT-0"));
//        c.set(Calendar.YEAR, 1582);
//        c.set(Calendar.MONTH, Calendar.OCTOBER);
//        c.set(Calendar.DAY_OF_MONTH, 15);
//        c.set(Calendar.HOUR_OF_DAY, 0);
//        c.set(Calendar.MINUTE, 0);
//        c.set(Calendar.SECOND, 0);
//        c.set(Calendar.MILLISECOND, 0);
//        long START_EPOCH = c.getTimeInMillis();
} // namespace utils
using column_count_type = uint32_t;
// Column ID, unique within column_kind
using column_id = column_count_type;
class schema;
class schema_extension;
using schema_ptr = seastar::lw_shared_ptr<const schema>;
using table_id = utils::tagged_uuid<struct table_id_tag>;
// Cluster-wide identifier of schema version of particular table.
//
// The version changes the value not only on structural changes but also
// temporal. For example, schemas with the same set of columns but created at
// different times should have different versions. This allows nodes to detect
// if the version they see was already synchronized with or not even if it has
// the same structure as the past versions.
//
// Schema changes merged in any order should result in the same final version.
//
// When table_schema_version changes, schema_tables::calculate_schema_digest() should
// also change when schema mutations are applied.
using table_schema_version = utils::tagged_uuid<struct table_schema_version_tag>;
namespace sstables {
class file_io_extension;
}
namespace db {
class commitlog_file_extension;
class extensions {
public:
    using map_type = std::map<sstring, sstring>;
    using schema_ext_config = std::variant<sstring, map_type, bytes>;
    using schema_ext_create_func = std::function<seastar::shared_ptr<schema_extension>(schema_ext_config)>;
    using sstable_file_io_extension = std::unique_ptr<sstables::file_io_extension>;
    using commitlog_file_extension_ptr = std::unique_ptr<db::commitlog_file_extension>;
     ;
private:
    std::map<sstring, schema_ext_create_func> _schema_extensions;
    std::map<sstring, sstable_file_io_extension> _sstable_file_io_extensions;
    std::map<sstring, commitlog_file_extension_ptr> _commitlog_file_extensions;
    std::unordered_set<std::string> _extension_internal_keyspaces;
};
}
namespace cdc {
enum class delta_mode : uint8_t {
    keys,
    full,
};
enum class image_mode : uint8_t {
    off, 
    on,
    full,
};
class options final {
    std::optional<bool> _enabled;
    image_mode _preimage = image_mode::off;
    bool _postimage = false;
    delta_mode _delta_mode = delta_mode::full;
    int _ttl = 86400; // 24h in seconds
public:
    
    
    sstring to_sstring() const;
    
    bool operator==(const options& o) const;
};
} // namespace cdc
// Wrapper for a value with a type-tag for differentiating instances.
template <class Value, class Tag>
class cql_duration_counter final {
public:
    using value_type = Value;
    explicit constexpr cql_duration_counter(value_type count) noexcept : _count(count) {}
    constexpr operator value_type() const noexcept { return _count; }
private:
    value_type _count;
};
using months_counter = cql_duration_counter<int32_t, struct month_tag>;
using days_counter = cql_duration_counter<int32_t, struct day_tag>;
using nanoseconds_counter = cql_duration_counter<int64_t, struct nanosecond_tag>;
class cql_duration_error : public std::invalid_argument {
public:
    
};
//
// A duration of time.
//
// Three counters represent the time: the number of months, of days, and of nanoseconds. This is necessary because
// the number hours in a day can vary during daylight savings and because the number of days in a month vary.
//
// As a consequence of this representation, there can exist no total ordering relation on durations. To see why,
// consider a duration `1mo5s` (1 month and 5 seconds). In a month with 30 days, this represents a smaller duration of
// time than in a month with 31 days.
//
// The primary use of this type is to manipulate absolute time-stamps with relative offsets. For example,
// `"Jan. 31 2005 at 23:15" + 3mo5d`.
//
class cql_duration final {
public:
    using common_counter_type = int64_t;
    static_assert(
            (sizeof(common_counter_type) >= sizeof(months_counter::value_type)) &&
            (sizeof(common_counter_type) >= sizeof(days_counter::value_type)) &&
            (sizeof(common_counter_type) >= sizeof(nanoseconds_counter::value_type)),
            "The common counter type is smaller than one of the component counter types.");
    // A zero-valued duration.
    // Construct a duration with explicit values for its three counters.
    //
    // Parse a duration string.
    //
    // Three formats for durations are supported:
    //
    // 1. "Standard" format. This consists of one or more pairs of a count and a unit specifier. Examples are "23d1mo"
    //    and "5h23m10s". Components of the total duration must be written in decreasing order. That is, "5h2y" is
    //    an invalid duration string.
    //
    //    The allowed units are:
    //      - "y": years
    //      - "mo": months
    //      - "w": weeks
    //      - "d": days
    //      - "h": hours
    //      - "m": minutes
    //      - "s": seconds
    //      - "ms": milliseconds
    //      - "us" or "µs": microseconds
    //      - "ns": nanoseconds
    //
    //    Units are case-insensitive.
    //
    // 2. ISO-8601 format. "P[n]Y[n]M[n]DT[n]H[n]M[n]S" or "P[n]W". All specifiers are optional. Examples are
    //    "P23Y1M" or "P10W".
    //
    // 3. ISO-8601 alternate format. "P[YYYY]-[MM]-[DD]T[hh]:[mm]:[ss]". All specifiers are mandatory. An example is
    //    "P2000-10-14T07:22:30".
    //
    // For all formats, a negative duration is indicated by beginning the string with the '-' symbol. For example,
    // "-2y10ns".
    //
    // Throws `cql_duration_error` in the event of a parsing error.
    //
    months_counter::value_type months{0};
    days_counter::value_type days{0};
    nanoseconds_counter::value_type nanoseconds{0};
    //
    // Note that equality comparison is based on exact counter matches. It is not valid to expect equivalency across
    // counters like months and days. See the documentation for `duration` for more.
    //
};
//
// Pretty-print a duration using the standard format.
//
// Durations are simplified during printing so that `duration(24, 0, 0)` is printed as "2y".
//
// See above.
/// Fragmented buffer consisting of multiple temporary_buffer<char>
class fragmented_temporary_buffer {
    using vector_type = std::vector<seastar::temporary_buffer<char>>;
    vector_type _fragments;
    size_t _size_bytes = 0;
public:
    static constexpr size_t default_fragment_size = 128 * 1024;
    class view;
    class istream;
    class reader;
    using ostream = seastar::memory_output_stream<vector_type::iterator>;
    // Linear complexity, invalidates views and istreams
    // Linear complexity, invalidates views and istreams
    // Creates a fragmented temporary buffer of a specified size, supplied as a parameter.
    // Max chunk size is limited to 128kb (the same limit as `bytes_stream` has).
};
class fragmented_temporary_buffer::view {
    vector_type::const_iterator _current;
    const char* _current_position = nullptr;
    size_t _current_size = 0;
    size_t _total_size = 0;
public:
    view(vector_type::const_iterator it, size_t position, size_t total_size)
        : _current(it)
        , _current_position(it->get() + position)
        , _current_size(std::min(it->size() - position, total_size))
        , _total_size(total_size)
    { }
    
    using fragment_type = bytes_view;
    class iterator {
        vector_type::const_iterator _it;
        size_t _left = 0;
        bytes_view _current;
    public:
        using iterator_category = std::forward_iterator_tag;
        using value_type = bytes_view;
        using difference_type = ptrdiff_t;
        using pointer = const bytes_view*;
        using reference = const bytes_view&;
        
        
        reference operator*() const noexcept ;
        
        iterator& operator++() noexcept ;
        iterator operator++(int) noexcept ;
        bool operator==(const iterator& other) const noexcept ;
    };
    using const_iterator = iterator;
    iterator begin() const noexcept ;
    iterator end() const noexcept ;
    bool empty() const noexcept ;
    size_t size_bytes() const noexcept ;
    void remove_prefix(size_t n) noexcept ;
    void remove_current() noexcept ;
    view prefix(size_t n) const ;
    bytes_view current_fragment() const noexcept ;
    // Invalidates iterators
    void remove_suffix(size_t n) noexcept ;
    
};
static_assert(FragmentRange<fragmented_temporary_buffer::view>);
static_assert(FragmentedView<fragmented_temporary_buffer::view>);
namespace fragmented_temporary_buffer_concepts {
template<typename T>
concept ExceptionThrower = requires(T obj, size_t n) {
    obj.throw_out_of_range(n, n);
};
}
class fragmented_temporary_buffer::istream {
    vector_type::const_iterator _current;
    const char* _current_position;
    const char* _current_end;
    size_t _bytes_left = 0;
private:
    size_t contig_remain() const ;
    void next_fragment() ;
    template<typename ExceptionThrower>
    requires fragmented_temporary_buffer_concepts::ExceptionThrower<ExceptionThrower>
    void check_out_of_range(ExceptionThrower& exceptions, size_t n) {
        if (__builtin_expect(bytes_left() < n, false)) {
            exceptions.throw_out_of_range(n, bytes_left());
            // Let's allow skipping this check if the user trusts its input
            // data.
        }
    }
    template<typename T, typename ExceptionThrower>
    [[gnu::noinline]] [[gnu::cold]]
    T read_slow(ExceptionThrower&& exceptions) {
        check_out_of_range(exceptions, sizeof(T));
        T obj;
        size_t left = sizeof(T);
        while (left) {
            auto this_length = std::min<size_t>(left, _current_end - _current_position);
            std::copy_n(_current_position, this_length, reinterpret_cast<char*>(&obj) + sizeof(T) - left);
            left -= this_length;
            if (left) {
                next_fragment();
            } else {
                _current_position += this_length;
            }
        }
        return obj;
    }
    [[gnu::noinline]] [[gnu::cold]]
    void skip_slow(size_t n) noexcept {
        auto left = std::min<size_t>(n, bytes_left());
        while (left) {
            auto this_length = std::min<size_t>(left, _current_end - _current_position);
            left -= this_length;
            if (left) {
                next_fragment();
            } else {
                _current_position += this_length;
            }
        }
    }
public:
    struct default_exception_thrower {
        [[noreturn]] [[gnu::cold]]
        static void throw_out_of_range(size_t attempted_read, size_t actual_left) {
            throw std::out_of_range(format("attempted to read {:d} bytes from a {:d} byte buffer", attempted_read, actual_left));
        }
    };
    static_assert(fragmented_temporary_buffer_concepts::ExceptionThrower<default_exception_thrower>);
    
    size_t bytes_left() const noexcept ;
    
    template<typename T, typename ExceptionThrower = default_exception_thrower>
    requires fragmented_temporary_buffer_concepts::ExceptionThrower<ExceptionThrower>
    T read(ExceptionThrower&& exceptions = default_exception_thrower()) {
        if (__builtin_expect(contig_remain() < sizeof(T), false)) {
            return read_slow<T>(std::forward<ExceptionThrower>(exceptions));
        }
        T obj;
        std::copy_n(_current_position, sizeof(T), reinterpret_cast<char*>(&obj));
        _current_position += sizeof(T);
        return obj;
    }
    template<typename Output, typename ExceptionThrower = default_exception_thrower>
    requires fragmented_temporary_buffer_concepts::ExceptionThrower<ExceptionThrower>
    Output read_to(size_t n, Output out, ExceptionThrower&& exceptions = default_exception_thrower()) {
        if (__builtin_expect(contig_remain() >= n, true)) {
            out = std::copy_n(_current_position, n, out);
            _current_position += n;
            return out;
        }
        check_out_of_range(exceptions, n);
        out = std::copy(_current_position, _current_end, out);
        n -= _current_end - _current_position;
        next_fragment();
        while (n > _current->size()) {
            out = std::copy(_current_position, _current_end, out);
            n -= _current->size();
            next_fragment();
        }
        out = std::copy_n(_current_position, n, out);
        _current_position += n;
        return out;
    }
    template<typename ExceptionThrower = default_exception_thrower>
    requires fragmented_temporary_buffer_concepts::ExceptionThrower<ExceptionThrower>
    view read_view(size_t n, ExceptionThrower&& exceptions = default_exception_thrower()) {
        if (__builtin_expect(contig_remain() >= n, true)) {
            auto v = view(_current, _current_position - _current->get(), n);
            _current_position += n;
            return v;
        }
        check_out_of_range(exceptions, n);
        auto v = view(_current, _current_position - _current->get(), n);
        n -= _current_end - _current_position;
        next_fragment();
        while (n > _current->size()) {
            n -= _current->size();
            next_fragment();
        }
        _current_position += n;
        return v;
    }
    template<typename ExceptionThrower = default_exception_thrower>
    requires fragmented_temporary_buffer_concepts::ExceptionThrower<ExceptionThrower>
    bytes_view read_bytes_view(size_t n, bytes_ostream& linearization_buffer, ExceptionThrower&& exceptions = default_exception_thrower()) {
        if (__builtin_expect(contig_remain() >= n, true)) {
            auto v = bytes_view(reinterpret_cast<const bytes::value_type*>(_current_position), n);
            _current_position += n;
            return v;
        }
        check_out_of_range(exceptions, n);
        auto ptr = linearization_buffer.write_place_holder(n);
        read_to(n, ptr, std::forward<ExceptionThrower>(exceptions));
        return bytes_view(reinterpret_cast<const bytes::value_type*>(ptr), n);
    }
};
class fragmented_temporary_buffer::reader {
    std::vector<temporary_buffer<char>> _fragments;
    size_t _left = 0;
public:
};
// The operator below is used only for logging
namespace exceptions {
enum class exception_code : int32_t {
    SERVER_ERROR    = 0x0000,
    PROTOCOL_ERROR  = 0x000A,
    BAD_CREDENTIALS = 0x0100,
    // 1xx: problem during request execution
    UNAVAILABLE     = 0x1000,
    OVERLOADED      = 0x1001,
    IS_BOOTSTRAPPING= 0x1002,
    TRUNCATE_ERROR  = 0x1003,
    WRITE_TIMEOUT   = 0x1100,
    READ_TIMEOUT    = 0x1200,
    READ_FAILURE    = 0x1300,
    FUNCTION_FAILURE= 0x1400,
    WRITE_FAILURE   = 0x1500,
    CDC_WRITE_FAILURE = 0x1600,
    // 2xx: problem validating the request
    SYNTAX_ERROR    = 0x2000,
    UNAUTHORIZED    = 0x2100,
    INVALID         = 0x2200,
    CONFIG_ERROR    = 0x2300,
    ALREADY_EXISTS  = 0x2400,
    UNPREPARED      = 0x2500,
    // Scylla-specific error codes
    // The error codes below are advertised to the drivers during connection
    // handshake using the protocol extension negotiation, and are only
    // enabled if the drivers explicitly enable them. Therefore it's perfectly
    // fine to change them in case some new error codes are introduced
    // in Cassandra.
    // NOTE TO DRIVER DEVELOPERS: These constants must not be relied upon,
    // they must be learned from protocol extensions instead.
    RATE_LIMIT_ERROR = 0xF000
};
class cassandra_exception : public std::exception {
private:
    exception_code _code;
    sstring _msg;
public:
    cassandra_exception(exception_code code, sstring msg) noexcept
        : _code(code)
        , _msg(std::move(msg))
    { }
    virtual const char* what() const noexcept override { return _msg.c_str(); }
    
};
class server_exception : public cassandra_exception {
public:
};
class protocol_exception : public cassandra_exception {
public:
};
struct unavailable_exception : cassandra_exception {
    db::consistency_level consistency;
    int32_t required;
    int32_t alive;
};
class request_execution_exception : public cassandra_exception {
public:
};
class truncate_exception : public request_execution_exception
{
public:
};
class request_timeout_exception : public cassandra_exception {
public:
    db::consistency_level consistency;
    int32_t received;
    int32_t block_for;
};
class read_timeout_exception : public request_timeout_exception {
public:
    bool data_present;
};
struct mutation_write_timeout_exception : public request_timeout_exception {
    db::write_type type;
};
class request_failure_exception : public cassandra_exception {
public:
    db::consistency_level consistency;
    int32_t received;
    int32_t failures;
    int32_t block_for;
protected:
};
struct mutation_write_failure_exception : public request_failure_exception {
    db::write_type type;
};
struct read_failure_exception : public request_failure_exception {
    bool data_present;
};
struct overloaded_exception : public cassandra_exception {
};
struct rate_limit_exception : public cassandra_exception {
    db::operation_type op_type;
    bool rejected_by_coordinator;
};
class request_validation_exception : public cassandra_exception {
public:
    using cassandra_exception::cassandra_exception;
};
class invalidated_prepared_usage_attempt_exception : public exceptions::request_validation_exception {
public:
};
class unauthorized_exception: public request_validation_exception {
public:
};
class authentication_exception: public request_validation_exception {
public:
};
class invalid_request_exception : public request_validation_exception {
public:
    invalid_request_exception(sstring cause) noexcept
        : request_validation_exception(exception_code::INVALID, std::move(cause))
    { }
};
class keyspace_not_defined_exception : public invalid_request_exception {
public:
    
};
class overflow_error_exception : public invalid_request_exception {
public:
};
class prepared_query_not_found_exception : public request_validation_exception {
public:
    bytes id;
};
class syntax_exception : public request_validation_exception {
public:
    
};
class configuration_exception : public request_validation_exception {
public:
    configuration_exception(sstring msg) noexcept
        : request_validation_exception{exception_code::CONFIG_ERROR, std::move(msg)}
    { }
};
class already_exists_exception : public configuration_exception {
public:
    const sstring ks_name;
    const sstring cf_name;
private:
public:
};
class recognition_exception : public std::runtime_error {
public:
    ;
};
class unsupported_operation_exception : public std::runtime_error {
public:
};
class function_execution_exception : public cassandra_exception {
public:
    const sstring ks_name;
    const sstring func_name;
    const std::vector<sstring> args;
};
}
// Specifies position in a lexicographically ordered sequence
// relative to some value.
//
// For example, if used with a value "bc" with lexicographical ordering on strings,
// each enum value represents the following positions in an example sequence:
//
//   aa
//   aaa
//   b
//   ba
// --> before_all_prefixed
//   bc
// --> before_all_strictly_prefixed
//   bca
//   bcd
// --> after_all_prefixed
//   bd
//   bda
//   c
//   ca
//
enum class lexicographical_relation : int8_t {
    before_all_prefixed,
    before_all_strictly_prefixed,
    after_all_prefixed
};
// Like std::lexicographical_compare but injects values from shared sequence (types) to the comparator
// Compare is an abstract_type-aware less comparator, which takes the type as first argument.
 ;
// Like std::lexicographical_compare but injects values from shared sequence
// (types) to the comparator. Compare is an abstract_type-aware trichotomic
// comparator, which takes the type as first argument.
template <std::input_iterator TypesIterator, std::input_iterator InputIt1, std::input_iterator InputIt2, typename Compare>
requires requires (TypesIterator types, InputIt1 i1, InputIt2 i2, Compare cmp) {
    { cmp(*types, *i1, *i2) } -> std::same_as<std::strong_ordering>;
}
std::strong_ordering lexicographical_tri_compare(TypesIterator types_first, TypesIterator types_last,
        InputIt1 first1, InputIt1 last1,
        InputIt2 first2, InputIt2 last2,
        Compare comp,
        lexicographical_relation relation1 = lexicographical_relation::before_all_strictly_prefixed,
        lexicographical_relation relation2 = lexicographical_relation::before_all_strictly_prefixed) {
    while (types_first != types_last && first1 != last1 && first2 != last2) {
        auto c = comp(*types_first, *first1, *first2);
        if (c != 0) {
            return c;
        }
        ++first1;
        ++first2;
        ++types_first;
    }
    bool e1 = first1 == last1;
    bool e2 = first2 == last2;
    if (e1 && e2) {
        return static_cast<int>(relation1) <=> static_cast<int>(relation2);
    }
    if (e2) {
        return relation2 == lexicographical_relation::after_all_prefixed ? std::strong_ordering::less : std::strong_ordering::greater;
    } else if (e1) {
        return relation1 == lexicographical_relation::after_all_prefixed ? std::strong_ordering::greater : std::strong_ordering::less;
    } else {
        return std::strong_ordering::equal;
    }
}
// A trichotomic comparator for prefix equality total ordering.
// In this ordering, two sequences are equal iff any of them is a prefix
// of the another. Otherwise, lexicographical ordering determines the order.
//
// 'comp' is an abstract_type-aware trichotomic comparator, which takes the
// type as first argument.
//
template <typename TypesIterator, typename InputIt1, typename InputIt2, typename Compare>
requires requires (TypesIterator ti, InputIt1 i1, InputIt2 i2, Compare c) {
    { c(*ti, *i1, *i2) } -> std::same_as<std::strong_ordering>;
}
std::strong_ordering prefix_equality_tri_compare(TypesIterator types, InputIt1 first1, InputIt1 last1,
        InputIt2 first2, InputIt2 last2, Compare comp) {
    while (first1 != last1 && first2 != last2) {
        auto c = comp(*types, *first1, *first2);
        if (c != 0) {
            return c;
        }
        ++first1;
        ++first2;
        ++types;
    }
    return std::strong_ordering::equal;
}
// Returns true iff the second sequence is a prefix of the first sequence
// Equality is an abstract_type-aware equality checker which takes the type as first argument.
template <typename TypesIterator, typename InputIt1, typename InputIt2, typename Equality>
bool is_prefixed_by(TypesIterator types, InputIt1 first1, InputIt1 last1,
        InputIt2 first2, InputIt2 last2, Equality equality) {
    while (first1 != last1 && first2 != last2) {
        if (!equality(*types, *first1, *first2)) {
            return false;
        }
        ++first1;
        ++first2;
        ++types;
    }
    return first2 == last2;
}
namespace tasks {
using task_id = utils::tagged_uuid<struct task_id_tag>;
struct task_info {
    task_id id;
    unsigned shard;
    task_info() noexcept : id(task_id::create_null_id()) {}
    task_info(task_id id, unsigned parent_shard) noexcept : id(id), shard(parent_shard) {}
    operator bool() const noexcept {
        return bool(id);
    }
};
}
class tuple_type_impl;
namespace cql3 {
class cql3_type;
}
struct runtime_exception : public std::exception {
    sstring _why;
public:
};
struct empty_t {};
class empty_value_exception : public std::exception {
public:
};
[[noreturn]] void on_types_internal_error(std::exception_ptr ex);
// Cassandra has a notion of empty values even for scalars (i.e. int).  This is
// distinct from NULL which means deleted or never set.  It is serialized
// as a zero-length byte array (whereas NULL is serialized as a negative-length
// byte array).
template <typename T>
requires std::is_default_constructible_v<T>
class emptyable {
    // We don't use optional<>, to avoid lots of ifs during the copy and move constructors
    bool _is_empty = false;
    T _value;
public:
    // default-constructor defaults to a non-empty value, since empty is the
    // exception rather than the rule
    emptyable() : _value{} {}
    emptyable(const T& x) : _value(x) {}
    emptyable(T&& x) : _value(std::move(x)) {}
    emptyable(empty_t) : _is_empty(true) {}
    template <typename... U>
    emptyable(U&&... args) : _value(std::forward<U>(args)...) {}
    bool empty() const { return _is_empty; }
    operator const T& () const { verify(); return _value; }
    operator T&& () && { verify(); return std::move(_value); }
    const T& get() const & { verify(); return _value; }
    T&& get() && { verify(); return std::move(_value); }
private:
    void verify() const {
        if (_is_empty) {
            throw empty_value_exception();
        }
    }
};
 ;
template <typename T>
bool
operator<(const emptyable<T>& me1, const emptyable<T>& me2) ;
// Checks whether T::empty() const exists and returns bool
template <typename T>
concept has_empty = requires (T obj) {
    { obj.empty() } -> std::same_as<bool>;
};
template <typename T>
using maybe_empty =
        std::conditional_t<has_empty<T>, T, emptyable<T>>;
class abstract_type;
class data_value;
struct ascii_native_type {
    using primary_type = sstring;
    primary_type string;
};
struct simple_date_native_type {
    using primary_type = uint32_t;
    primary_type days;
};
struct date_type_native_type {
    using primary_type = db_clock::time_point;
    primary_type tp;
};
struct time_native_type {
    using primary_type = int64_t;
    primary_type nanoseconds;
};
struct timeuuid_native_type {
    using primary_type = utils::UUID;
    primary_type uuid;
};
using data_type = shared_ptr<const abstract_type>;
;
;
struct empty_type_representation {
};
class data_value {
    void* _value;  // FIXME: use "small value optimization" for small types
    data_type _type;
private:
    data_value(void* value, data_type type) : _value(value), _type(std::move(type)) {}
    template <typename T>
    static data_value make_new(data_type type, T&& value);
public:
    ;
    data_value(const data_value&);
    
    // common conversions from C++ types to database types
    // note: somewhat dangerous, consider a factory function instead
    explicit data_value(bytes);
    data_value(sstring&&);
    data_value(std::string_view);
    // We need the following overloads just to avoid ambiguity because
    // seastar::net::inet_address is implicitly constructible from a
    // const sstring&.
    
    
    data_value(const sstring&);
    // Do not allow construction of a data_value from nullptr. The reason is
    // that this is error prone, for example: it conflicts with `const char*` overload
    // which tries to allocate a value from it and will cause UB.
    //
    // We want the null value semantics here instead. So the user will be forced
    // to explicitly call `make_null()` instead.
    data_value(std::nullptr_t) = delete;
    data_value(ascii_native_type);
    data_value(bool);
    data_value(int8_t);
    data_value(int16_t);
    data_value(int32_t);
    data_value(int64_t);
    data_value(utils::UUID);
    ;
    data_value(double);
    data_value(net::ipv4_address);
    
    data_value(seastar::net::inet_address);
    data_value(simple_date_native_type);
    data_value(db_clock::time_point);
    data_value(time_native_type);
    data_value(timeuuid_native_type);
    data_value(date_type_native_type);
    data_value(cql_duration);
    data_value(empty_type_representation);
    
    ;
    ;
    
    ;
    const data_type& type() const {
        return _type;
    }
    bool is_null() const {   // may return false negatives for strings etc.
        return !_value;
    }
    size_t serialized_size() const;
    void serialize(bytes::iterator& out) const;
    bytes_opt serialize() const;
    bytes serialize_nonnull() const;
    
    friend class abstract_type;
    static data_value make_null(data_type type) {
        return data_value(nullptr, std::move(type));
    }
    template <typename T>
    static data_value make(data_type type, std::unique_ptr<T> value) {
        return data_value(value.release(), std::move(type));
    }
    friend class empty_type_impl;
    ;
    ;
    
    friend data_value make_tuple_value(data_type, maybe_empty<std::vector<data_value>>);
    friend data_value make_set_value(data_type, maybe_empty<std::vector<data_value>>);
    friend data_value make_list_value(data_type, maybe_empty<std::vector<data_value>>);
    friend data_value make_map_value(data_type, maybe_empty<std::vector<std::pair<data_value, data_value>>>);
    friend data_value make_user_value(data_type, std::vector<data_value>);
    template <typename Func>
    friend auto visit(const data_value& v, Func&& f);
    // Prints a value of this type in a way which is parsable back from CQL.
    // Differs from operator<< for collections.
};
 ;
class serialized_compare;
class serialized_tri_compare;
class user_type_impl;
// Unsafe to access across shards unless otherwise noted.
class abstract_type : public enable_shared_from_this<abstract_type> {
    sstring _name;
    std::optional<uint32_t> _value_length_if_fixed;
public:
    enum class kind : int8_t {
        ascii,
        boolean,
        byte,
        bytes,
        counter,
        date,
        double_kind,
        duration,
        empty,
        float_kind,
        inet,
        int32,
        list,
        long_kind,
        map,
        reversed,
        set,
        short_kind,
        simple_date,
        time,
        timestamp,
        timeuuid,
        tuple,
        user,
        utf8,
        uuid,
        last = uuid,
    };
private:
    kind _kind;
public:
    kind get_kind() const { return _kind; }
    abstract_type(kind k, sstring name, std::optional<uint32_t> value_length_if_fixed)
        : _name(name), _value_length_if_fixed(std::move(value_length_if_fixed)), _kind(k) {}
    virtual ~abstract_type() {}
    bool less(bytes_view v1, bytes_view v2) const { return compare(v1, v2) < 0; }
    // returns a callable that can be called with two byte_views, and calls this->less() on them.
    serialized_compare as_less_comparator() const ;
    
    
    
    
    
    bool equal(bytes_view v1, managed_bytes_view v2) const;
    std::strong_ordering compare(bytes_view v1, bytes_view v2) const;
    std::strong_ordering compare(managed_bytes_view v1, managed_bytes_view v2) const;
    
    
private:
    // Explicitly instantiated in .cc
    template <FragmentedView View> data_value deserialize_impl(View v) const;
public:
    template <FragmentedView View> data_value deserialize(View v) const {
        if (v.size_bytes() == v.current_fragment().size()) [[likely]] {
            return deserialize_impl(single_fragmented_view(v.current_fragment()));
        } else {
            return deserialize_impl(v);
        }
    }
    data_value deserialize(bytes_view v) const {
        return deserialize_impl(single_fragmented_view(v));
    }
    data_value deserialize(const managed_bytes& v) const ;
     ;
    ;
    // Explicitly instantiated in .cc
    template <FragmentedView View> void validate(const View& v) const;
    void validate(bytes_view view) const;
    
    shared_ptr<const abstract_type> underlying_type() const;
    
    bool references_user_type(const sstring& keyspace, const bytes& name) const;
    // For types that contain (or are equal to) the given user type (e.g., a set of elements of this type),
    // updates them with the new version of the type ('updated'). For other types does nothing.
    std::optional<data_type> update_user_type(const shared_ptr<const user_type_impl> updated) const;
    bool references_duration() const;
    std::optional<uint32_t> value_length_if_fixed() const ;
public:
    bytes decompose(const data_value& value) const;
    // Safe to call across shards
    const sstring& name() const {
        return _name;
    }
    bool is_byte_order_equal() const;
    sstring get_string(const bytes& b) const;
    sstring to_string(bytes_view bv) const {
        return to_string_impl(deserialize(bv));
    }
    
    sstring to_string_impl(const data_value& v) const;
    
    bool is_counter() const;
    bool is_string() const;
    bool is_collection() const;
    
    // Lists and sets are similar: they are both represented as std::vector<data_value>
    // @sa listlike_collection_type_impl
    
    bool is_multi_cell() const;
    bool is_atomic() const { return !is_multi_cell(); }
    bool is_reversed() const { return _kind == kind::reversed; }
    bool is_tuple() const;
    
    bool is_native() const;
    cql3::cql3_type as_cql3_type() const;
    const sstring& cql3_type_name() const;
    virtual shared_ptr<const abstract_type> freeze() const { return shared_from_this(); }
    const abstract_type& without_reversed() const {
        return is_reversed() ? *underlying_type() : *this;
    }
    // Checks whether there can be a set or map somewhere inside a value of this type.
    bool contains_set_or_map() const;
    // Checks whether there can be a collection somewhere inside a value of this type.
    bool contains_collection() const;
    // Checks whether a bound value of this type has to be reserialized.
    // This can be for example because there is a set inside that needs to be sorted.
    
    friend class list_type_impl;
private:
    mutable sstring _cql3_type_name;
protected:
    bool _contains_set_or_map = false;
    bool _contains_collection = false;
    // native_value_* methods are virualized versions of native_type's
    // sizeof/alignof/copy-ctor/move-ctor etc.
    void* native_value_clone(const void* from) const;
    // abstract_type is a friend of data_value, but derived classes are not.
    static const void* get_value_ptr(const data_value& v) {
        return v._value;
    }
    friend class tuple_type_impl;
    friend class data_value;
    friend class reversed_type_impl;
    ;
    ;
    friend bool operator==(const abstract_type& x, const abstract_type& y);
};
inline bool operator==(const abstract_type& x, const abstract_type& y)
{
     return &x == &y;
}
template <typename T>
inline
data_value
data_value::make_new(data_type type, T&& v) {
    maybe_empty<std::remove_reference_t<T>> value(std::forward<T>(v));
    return data_value(type->native_value_clone(&value), type);
}
 ;
 ;
/// Special case: sometimes we cast uuid to timeuuid so we can correctly compare timestamps.  See #7729.
// CRTP: implements translation between a native_type (C++ type) to abstract_type
// AbstractType is parametrized because we want a
//    abstract_type -> collection_type_impl -> map_type
// type hierarchy, and native_type is only known at the last step.
template <typename NativeType, typename AbstractType = abstract_type>
class concrete_type : public AbstractType {
public:
    using native_type = maybe_empty<NativeType>;
    using AbstractType::AbstractType;
public:
    data_value make_value(std::unique_ptr<native_type> value) const {
        return data_value::make(this->shared_from_this(), std::move(value));
    }
    data_value make_value(native_type value) const {
        return make_value(std::make_unique<native_type>(std::move(value)));
    }
    
    data_value make_empty() const {
        return make_value(native_type(empty_t()));
    }
    const native_type& from_value(const void* v) const {
        return *reinterpret_cast<const native_type*>(v);
    }
    friend class abstract_type;
};

using bytes_view_opt = std::optional<bytes_view>;
using managed_bytes_view_opt = std::optional<managed_bytes_view>;
static inline
std::strong_ordering tri_compare(data_type t, managed_bytes_view e1, managed_bytes_view e2) {
    return t->compare(e1, e2);
}
inline
std::strong_ordering
tri_compare_opt(data_type t, managed_bytes_view_opt v1, managed_bytes_view_opt v2) {
    if (!v1 || !v2) {
        return bool(v1) <=> bool(v2);
    } else {
        return tri_compare(std::move(t), *v1, *v2);
    }
}
static
bool equal(data_type t, managed_bytes_view e1, managed_bytes_view e2) ;
class row_tombstone;
class collection_type_impl;
using collection_type = shared_ptr<const collection_type_impl>;
template <typename... T>
struct simple_tuple_hash;
template <>
struct simple_tuple_hash<> {
    
};
template <typename Arg0, typename... Args >
struct simple_tuple_hash<std::vector<Arg0>, Args...> {
    size_t operator()(const std::vector<Arg0>& vec, const Args&... args) const {
        size_t h0 = 0;
        size_t h1;
        for (auto&& i : vec) {
            h1 = std::hash<Arg0>()(i);
            h0 = h0 ^ ((h1 << 7) | (h1 >> (std::numeric_limits<size_t>::digits - 7)));
        }
        h1 = simple_tuple_hash<Args...>()(args...);
        return h0 ^ ((h1 << 7) | (h1 >> (std::numeric_limits<size_t>::digits - 7)));
    }
};
template <typename Arg0, typename... Args>
struct simple_tuple_hash<Arg0, Args...> {
    
};
template <typename InternedType, typename... BaseTypes>
class type_interning_helper {
    using key_type = std::tuple<BaseTypes...>;
    using value_type = shared_ptr<const InternedType>;
    struct hash_type {
        size_t operator()(const key_type& k) const ;
    };
    using map_type = std::unordered_map<key_type, value_type, hash_type>;
    static thread_local map_type _instances;
public:
    static shared_ptr<const InternedType> get_instance(BaseTypes... keys) ;
};
template <typename InternedType, typename... BaseTypes>
thread_local typename type_interning_helper<InternedType, BaseTypes...>::map_type
    type_interning_helper<InternedType, BaseTypes...>::_instances;
class reversed_type_impl : public abstract_type {
    using intern = type_interning_helper<reversed_type_impl, data_type>;
    friend struct shared_ptr_make_helper<reversed_type_impl, true>;
    data_type _underlying_type;
    
public:
    const data_type& underlying_type() const {
        return _underlying_type;
    }
};
using reversed_type = shared_ptr<const reversed_type_impl>;
// Reverse the sort order of the type by wrapping in or stripping reversed_type,
// as needed.
class map_type_impl;
using map_type = shared_ptr<const map_type_impl>;
class set_type_impl;
using set_type = shared_ptr<const set_type_impl>;
class list_type_impl;
using list_type = shared_ptr<const list_type_impl>;
struct no_match_for_native_data_type {};
template <typename T>
inline constexpr auto data_type_for_v = no_match_for_native_data_type();
 ;
class serialized_compare {
    data_type _type;
public:
    serialized_compare(data_type type) : _type(type) {}
    bool operator()(const bytes& v1, const bytes& v2) const {
        return _type->less(v1, v2);
    }
};
inline
serialized_compare
abstract_type::as_less_comparator() const {
    return serialized_compare(shared_from_this());
}
class serialized_tri_compare {
    data_type _type;
public:
};
using key_compare = serialized_compare;
// Remember to update type_codec in transport/server.cc and cql3/cql3_type.cc
extern thread_local const shared_ptr<const abstract_type> byte_type;
extern thread_local const shared_ptr<const abstract_type> short_type;
extern thread_local const shared_ptr<const abstract_type> int32_type;
extern thread_local const shared_ptr<const abstract_type> long_type;
extern thread_local const shared_ptr<const abstract_type> ascii_type;
extern thread_local const shared_ptr<const abstract_type> bytes_type;
extern thread_local const shared_ptr<const abstract_type> utf8_type;
extern thread_local const shared_ptr<const abstract_type> boolean_type;
extern thread_local const shared_ptr<const abstract_type> date_type;
extern thread_local const shared_ptr<const abstract_type> timeuuid_type;
extern thread_local const shared_ptr<const abstract_type> timestamp_type;
extern thread_local const shared_ptr<const abstract_type> simple_date_type;
extern thread_local const shared_ptr<const abstract_type> time_type;
extern thread_local const shared_ptr<const abstract_type> uuid_type;
extern thread_local const shared_ptr<const abstract_type> inet_addr_type;
extern thread_local const shared_ptr<const abstract_type> float_type;
extern thread_local const shared_ptr<const abstract_type> double_type;
extern thread_local const shared_ptr<const abstract_type> counter_type;
extern thread_local const shared_ptr<const abstract_type> duration_type;
extern thread_local const data_type empty_type;
template <> inline thread_local const data_type& data_type_for_v<int8_t> = byte_type;
template <> inline thread_local const data_type& data_type_for_v<int16_t> = short_type;
template <> inline thread_local const data_type& data_type_for_v<int32_t> = int32_type;
template <> inline thread_local const data_type& data_type_for_v<int64_t> = long_type;
template <> inline thread_local const data_type& data_type_for_v<sstring> = utf8_type;
template <> inline thread_local const data_type& data_type_for_v<bytes> = bytes_type;
template <> inline thread_local const data_type& data_type_for_v<utils::UUID> = uuid_type;
template <> inline thread_local const data_type& data_type_for_v<date_type_native_type> = date_type;
template <> inline thread_local const data_type& data_type_for_v<simple_date_native_type> = simple_date_type;
template <> inline thread_local const data_type& data_type_for_v<db_clock::time_point> = timestamp_type;
template <> inline thread_local const data_type& data_type_for_v<ascii_native_type> = ascii_type;
template <> inline thread_local const data_type& data_type_for_v<time_native_type> = time_type;
template <> inline thread_local const data_type& data_type_for_v<timeuuid_native_type> = timeuuid_type;
template <> inline thread_local const data_type& data_type_for_v<net::inet_address> = inet_addr_type;
template <> inline thread_local const data_type& data_type_for_v<bool> = boolean_type;
template <> inline thread_local const data_type& data_type_for_v<float> = float_type;
template <> inline thread_local const data_type& data_type_for_v<double> = double_type;
template <> inline thread_local const data_type& data_type_for_v<cql_duration> = duration_type;
namespace std {
template <>
struct hash<shared_ptr<const abstract_type>> : boost::hash<shared_ptr<abstract_type>> {
};
}
// FIXME: make more explicit
// FIXME: make more explicit



// FIXME: make more explicit
inline
bytes
to_bytes(const sstring& x) {
    return bytes(reinterpret_cast<const int8_t*>(x.c_str()), x.size());
}
// FIXME: make more explicit
 ;
template<typename T>
T read_simple(bytes_view& v) ;
 ;
template<FragmentedView View>
View read_simple_bytes(View& v, size_t n) {
    if (v.size_bytes() < n) {
        throw_with_backtrace<marshal_exception>(format("read_simple_bytes - not enough bytes (requested {:d}, got {:d})", n, v.size_bytes()));
    }
    auto prefix = v.prefix(n);
    v.remove_prefix(n);
    return prefix;
}
 
size_t collection_size_len();
size_t collection_value_len();
void write_collection_size(bytes::iterator& out, int size);
void write_collection_size(managed_bytes_mutable_view&, int size);
void write_collection_value(bytes::iterator& out, bytes_view_opt val_bytes);

void write_collection_value(managed_bytes_mutable_view&, const managed_bytes_view_opt& val_bytes);

// Splits a serialized collection into a vector of elements, but does not recursively deserialize the elements.
// Does not perform validation.
template <FragmentedView View>
utils::chunked_vector<managed_bytes_opt> partially_deserialize_listlike(View in);
template <FragmentedView View>
std::vector<std::pair<managed_bytes, managed_bytes>> partially_deserialize_map(View in);
using user_type = shared_ptr<const user_type_impl>;
using tuple_type = shared_ptr<const tuple_type_impl>;
template<>
struct appending_hash<data_type> {
     ;
};
enum class allow_prefixes { no, yes };
template<allow_prefixes AllowPrefixes = allow_prefixes::no>
class compound_type final {
private:
    const std::vector<data_type> _types;
    const bool _byte_order_equal;
    const bool _byte_order_comparable;
    const bool _is_reversed;
public:
    static constexpr bool is_prefixable = AllowPrefixes == allow_prefixes::yes;
    using prefix_type = compound_type<allow_prefixes::yes>;
    using value_type = std::vector<bytes>;
    using size_type = uint16_t;
    compound_type(std::vector<data_type> types)
        : _types(std::move(types))
        , _byte_order_equal(std::all_of(_types.begin(), _types.end(), [] (const auto& t) {
                return t->is_byte_order_equal();
            }))
        , _byte_order_comparable(false)
        , _is_reversed(_types.size() == 1 && _types[0]->is_reversed())
    { }
    
    auto const& types() const {
        return _types;
    }
    bool is_singular() const {
        return _types.size() == 1;
    }
    prefix_type as_prefix() ;
private:
    template<typename RangeOfSerializedComponents, FragmentedMutableView Out>
    static void serialize_value(RangeOfSerializedComponents&& values, Out out) {
        for (auto&& val : values) {
            using val_type = std::remove_cvref_t<decltype(val)>;
            if constexpr (FragmentedView<val_type>) {
                assert(val.size_bytes() <= std::numeric_limits<size_type>::max());
                write<size_type>(out, size_type(val.size_bytes()));
                write_fragmented(out, val);
            } else if constexpr (std::same_as<val_type, managed_bytes>) {
                assert(val.size() <= std::numeric_limits<size_type>::max());
                write<size_type>(out, size_type(val.size()));
                write_fragmented(out, managed_bytes_view(val));
            } else {
                assert(val.size() <= std::numeric_limits<size_type>::max());
                write<size_type>(out, size_type(val.size()));
                write_fragmented(out, single_fragmented_view(val));
            }
        }
    }
    template <typename RangeOfSerializedComponents>
    static size_t serialized_size(RangeOfSerializedComponents&& values) {
        size_t len = 0;
        for (auto&& val : values) {
            using val_type = std::remove_cvref_t<decltype(val)>;
            if constexpr (FragmentedView<val_type>) {
                len += sizeof(size_type) + val.size_bytes();
            } else {
                len += sizeof(size_type) + val.size();
            }
        }
        return len;
    }
public:
    template<typename RangeOfSerializedComponents>
    static managed_bytes serialize_value(RangeOfSerializedComponents&& values) {
        auto size = serialized_size(values);
        if (size > std::numeric_limits<size_type>::max()) {
            throw std::runtime_error(format("Key size too large: {:d} > {:d}", size, std::numeric_limits<size_type>::max()));
        }
        managed_bytes b(managed_bytes::initialized_later(), size);
        serialize_value(values, managed_bytes_mutable_view(b));
        return b;
    }
     ;
    
    
    class iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = const managed_bytes_view;
        using difference_type = std::ptrdiff_t;
        using pointer = const value_type*;
        using reference = const value_type&;
    private:
        managed_bytes_view _v;
        managed_bytes_view _current;
        size_t _remaining = 0;
    private:
        void read_current() {
            _remaining = _v.size_bytes();
            size_type len;
            {
                if (_v.empty()) {
                    return;
                }
                len = read_simple<size_type>(_v);
                if (_v.size() < len) {
                    throw_with_backtrace<marshal_exception>(format("compound_type iterator - not enough bytes, expected {:d}, got {:d}", len, _v.size()));
                }
            }
            _current = _v.prefix(len);
            _v.remove_prefix(_current.size_bytes());
        }
    public:
        struct end_iterator_tag {};
        iterator(const managed_bytes_view& v) : _v(v) {
            read_current();
        }
        iterator(end_iterator_tag, const managed_bytes_view& v) : _v() {}
        
        iterator& operator++() {
            read_current();
            return *this;
        }
        
        const value_type& operator*() const { return _current; }
        const value_type* operator->() const { return &_current; }
        bool operator==(const iterator& i) const { return _remaining == i._remaining; }
    };
    static iterator begin(managed_bytes_view v) {
        return iterator(v);
    }
    static iterator end(managed_bytes_view v) {
        return iterator(typename iterator::end_iterator_tag(), v);
    }
    static boost::iterator_range<iterator> components(managed_bytes_view v) {
        return { begin(v), end(v) };
    }
    bool less(managed_bytes_view b1, managed_bytes_view b2) const {
        return with_linearized(b1, [&] (bytes_view bv1) {
            return with_linearized(b2, [&] (bytes_view bv2) {
                return less(bv1, bv2);
            });
        });
    }
    
    // Retruns true iff given prefix has no missing components
    bool is_full(managed_bytes_view v) const {
        assert(AllowPrefixes == allow_prefixes::yes);
        return std::distance(begin(v), end(v)) == (ssize_t)_types.size();
    }
    
    
    bool equal(managed_bytes_view v1, managed_bytes_view v2) const {
        return with_linearized(v1, [&] (bytes_view bv1) {
            return with_linearized(v2, [&] (bytes_view bv2) {
                return equal(bv1, bv2);
            });
        });
    }
};
using compound_prefix = compound_type<allow_prefixes::yes>;
class compressor {
    sstring _name;
public:
    // to cheaply bridge sstable compression options / maps
    using opt_string = std::optional<sstring>;
    using opt_getter = std::function<opt_string(const sstring&)>;
    using ptr_type = shared_ptr<compressor>;
    static thread_local const ptr_type lz4;
    static thread_local const ptr_type snappy;
    static thread_local const ptr_type deflate;
    static const sstring namespace_prefix;
};
template<typename BaseType, typename... Args>
class class_registry;
using compressor_ptr = compressor::ptr_type;
using compressor_registry = class_registry<compressor, const typename compressor::opt_getter&>;
class compression_parameters {
public:
    static constexpr int32_t DEFAULT_CHUNK_LENGTH = 4 * 1024;
    static constexpr double DEFAULT_CRC_CHECK_CHANCE = 1.0;
    static const sstring SSTABLE_COMPRESSION;
    static const sstring CHUNK_LENGTH_KB;
    static const sstring CHUNK_LENGTH_KB_ERR;
    static const sstring CRC_CHECK_CHANCE;
private:
    compressor_ptr _compressor;
    std::optional<int> _chunk_length;
    std::optional<double> _crc_check_chance;
public:
    std::map<sstring, sstring> get_options() const ;
    bool operator==(const compression_parameters& other) const;
    
private:
    
};
namespace sstables {
enum class compaction_strategy_type {
    null,
    size_tiered,
    leveled,
    date_tiered,
    time_window,
};
enum class reshape_mode { strict, relaxed };
}
class schema;
class caching_options {
    // For Origin, the default value for the row is "NONE". However, since our
    // row_cache will cache both keys and rows, we will default to ALL.
    //
    // FIXME: We don't yet make any changes to our caching policies based on
    // this (and maybe we shouldn't)
    static constexpr auto default_key = "ALL";
    static constexpr auto default_row = "ALL";
    sstring _key_cache;
    sstring _row_cache;
    bool _enabled = true;
    caching_options(sstring k, sstring r, bool enabled);
    friend class schema;
    caching_options();
public:
    
    std::map<sstring, sstring> to_map() const;
    sstring to_sstring() const;
    static caching_options get_disabled_caching_options();
    static caching_options from_map(const std::map<sstring, sstring>& map);
    static caching_options from_sstring(const sstring& str);
    bool operator==(const caching_options& other) const = default;
};
class schema;
class partition_key;
struct atomic_cell_view;
struct tombstone;
namespace db::view {
struct clustering_or_static_row;
struct view_key_and_action;
}
class column_computation;
using column_computation_ptr = std::unique_ptr<column_computation>;
class column_computation {
public:
    virtual ~column_computation() = default;
    
    virtual column_computation_ptr clone() const = 0;
    virtual bytes serialize() const = 0;
    virtual bytes compute_value(const schema& schema, const partition_key& key) const = 0;
    virtual bool depends_on_non_primary_key_column() const ;
};
class legacy_token_column_computation : public column_computation {
public:
    
};
class token_column_computation : public column_computation {
public:
    
};
class collection_column_computation final : public column_computation {
    enum class kind {
        keys,
        values,
        entries,
    };
    const bytes _collection_name;
    const kind _kind;
    
    using collection_kv = std::pair<bytes_view, atomic_cell_view>;
    void operate_on_collection_entries(
            std::invocable<collection_kv*, collection_kv*, tombstone> auto&& old_and_new_row_func, const schema& schema,
            const partition_key& key, const db::view::clustering_or_static_row& update, const std::optional<db::view::clustering_or_static_row>& existing) const;
public:
    
    
    
    
    
    
    virtual column_computation_ptr clone() const override ;
    
    
};
namespace api {
using timestamp_type = int64_t;
timestamp_type constexpr missing_timestamp = std::numeric_limits<timestamp_type>::min();
timestamp_type constexpr min_timestamp = std::numeric_limits<timestamp_type>::min() + 1;
timestamp_type constexpr max_timestamp = std::numeric_limits<timestamp_type>::max();
// Used for generating server-side mutation timestamps.
// Same epoch as Java's System.currentTimeMillis() for compatibility.
// Satisfies requirements of Clock.
class timestamp_clock final {
    using base = std::chrono::system_clock;
public:
    using rep = timestamp_type;
    using duration = std::chrono::microseconds;
    using period = typename duration::period;
    using time_point = std::chrono::time_point<timestamp_clock, duration>;
    static constexpr bool is_steady = base::is_steady;
    
};
static
timestamp_type new_timestamp() ;
}
std::string format_timestamp(api::timestamp_type);
enum class tombstone_gc_mode : uint8_t { timeout, disabled, immediate, repair };
class tombstone_gc_options {
private:
    tombstone_gc_mode _mode = tombstone_gc_mode::timeout;
    std::chrono::seconds _propagation_delay_in_seconds = std::chrono::seconds(3600);
public:
    tombstone_gc_options() = default;
    const tombstone_gc_mode& mode() const ;
    explicit tombstone_gc_options(const std::map<seastar::sstring, seastar::sstring>& map);
    
    std::map<seastar::sstring, seastar::sstring> to_map() const;
    seastar::sstring to_sstring() const;
    bool operator==(const tombstone_gc_options&) const = default;
};

using namespace seastar;
namespace db {
class per_partition_rate_limit_options final {
private:
    static const char* max_writes_per_second_key;
    static const char* max_reads_per_second_key;
private:
    std::optional<uint32_t> _max_writes_per_second;
    std::optional<uint32_t> _max_reads_per_second;
public:
    per_partition_rate_limit_options() = default;
    
    
     
     
     
     
     
};
}
namespace replica {
class database;
}
namespace data_dictionary {
class keyspace_element {
public:
    virtual seastar::sstring keypace_name() const = 0;
    virtual seastar::sstring element_name() const = 0;
    // Override one of these element_type() overloads.
    virtual seastar::sstring element_type() const { return ""; }
    virtual seastar::sstring element_type(replica::database& db) const { return element_type(); }
    // Override one of these describe() overloads.
    virtual std::ostream& describe(std::ostream& os) const { return os; }
    virtual std::ostream& describe(replica::database& db, std::ostream& os, bool with_internals) const { return describe(os); }
};
}
namespace cql3 {
class column_identifier;
class column_specification final {
public:
    const sstring ks_name;
    const sstring cf_name;
    const ::shared_ptr<column_identifier> name;
    const data_type type;
    column_specification(std::string_view ks_name_, std::string_view cf_name_, ::shared_ptr<column_identifier> name_, data_type type_);
    
};
}
namespace dht {
class i_partitioner;
class sharder;
}
namespace cdc {
class options;
}
namespace replica {
class database;
}
using column_count_type = uint32_t;
// Column ID, unique within column_kind
using column_id = column_count_type;
// Column ID unique within a schema. Enum class to avoid
// mixing wtih column id.
enum class ordinal_column_id: column_count_type {};
// Maintains a set of columns used in a query. The columns are
// identified by ordinal_id.
//
// @sa column_definition::ordinal_id.
class column_set {
public:
    using bitset = boost::dynamic_bitset<uint64_t>;
    using size_type = bitset::size_type;
    // column_count_type is more narrow than size_type, but truncating a size_type max value does
    // give column_count_type max value. This is used to avoid extra branching in
    // find_first()/find_next().
    static_assert(static_cast<column_count_type>(boost::dynamic_bitset<uint64_t>::npos) == ~static_cast<column_count_type>(0));
    static constexpr ordinal_column_id npos = static_cast<ordinal_column_id>(bitset::npos);
    // Set the appropriate bit for column id.
    // Test the mask for use of a given column id.
    // @sa boost::dynamic_bistet docs
    // Logical or
    
private:
    bitset _mask;
};
class schema_registry_entry;
class schema_builder;
// Useful functions to manipulate the schema's comparator field
namespace cell_comparator {
sstring to_sstring(const schema& s);


}
namespace db {
class extensions;
}
// make sure these match the order we like columns back from schema
enum class column_kind { partition_key, clustering_key, static_column, regular_column };
enum class column_view_virtual { no, yes };
sstring to_sstring(column_kind k);

enum class cf_type : uint8_t {
    standard,
    super,
};
 sstring cf_type_to_sstring(cf_type t) ;
 
struct speculative_retry {
    enum class type {
        NONE, CUSTOM, PERCENTILE, ALWAYS
    };
private:
    type _t;
    double _v;
public:
    speculative_retry(type t, double v) : _t(t), _v(v) {}
    sstring to_sstring() const ;
    
    
    
    bool operator==(const speculative_retry& other) const = default;
};
typedef std::unordered_map<sstring, sstring> index_options_map;
enum class index_metadata_kind {
    keys,
    custom,
    composites,
};
class index_metadata final {
public:
    struct is_local_index_tag {};
    using is_local_index = bool_class<is_local_index_tag>;
private:
    table_id _id;
    sstring _name;
    index_metadata_kind _kind;
    index_options_map _options;
    bool _local;
public:
    index_metadata(const sstring& name, const index_options_map& options, index_metadata_kind kind, is_local_index local);
    bool operator==(const index_metadata& other) const;
    bool equals_noname(const index_metadata& other) const;
    const table_id& id() const;
    const sstring& name() const;
    const index_metadata_kind kind() const;
    const index_options_map& options() const;
    bool local() const;
    static sstring get_default_index_name(const sstring& cf_name, std::optional<sstring> root);
};
class column_definition final {
public:
    struct name_comparator {
        data_type type;
        
        bool operator()(const column_definition& cd1, const column_definition& cd2) const {
            return type->less(cd1.name(), cd2.name());
        }
    };
private:
    bytes _name;
    api::timestamp_type _dropped_at;
    bool _is_atomic;
    bool _is_counter;
    column_view_virtual _is_view_virtual;
    column_computation_ptr _computation;
    struct thrift_bits {
        thrift_bits()
            : is_on_all_components(0)
        {}
        uint8_t is_on_all_components : 1;
        // more...?
    };
    thrift_bits _thrift_bits;
    friend class schema;
public:
    column_definition(bytes name, data_type type, column_kind kind,
        column_id component_index = 0,
        column_view_virtual view_virtual = column_view_virtual::no,
        column_computation_ptr = nullptr,
        api::timestamp_type dropped_at = api::missing_timestamp);
    data_type type;
    // Unique within (kind, schema instance).
    // schema::position() and component_index() depend on the fact that for PK columns this is
    // equivalent to component index.
    column_id id;
    // Unique within schema instance
    ordinal_column_id ordinal_id;
    column_kind kind;
    lw_shared_ptr<cql3::column_specification> column_specification;
    // NOTICE(sarna): This copy constructor is hand-written instead of default,
    // because it involves deep copying of the computation object.
    // Computation has a strict ownership policy provided by
    // unique_ptr, and as such cannot rely on default copying.
    column_definition(const column_definition& other)
            : _name(other._name)
            , _dropped_at(other._dropped_at)
            , _is_atomic(other._is_atomic)
            , _is_counter(other._is_counter)
            , _is_view_virtual(other._is_view_virtual)
            , _computation(other.get_computation_ptr())
            , _thrift_bits(other._thrift_bits)
            , type(other.type)
            , id(other.id)
            , ordinal_id(other.ordinal_id)
            , kind(other.kind)
            , column_specification(other.column_specification)
        {}
    column_definition& operator=(const column_definition& other) {
        if (this == &other) {
            return *this;
        }
        column_definition tmp(other);
        *this = std::move(tmp);
        return *this;
    }
    column_definition& operator=(column_definition&& other) = default;
    bool is_static() const { return kind == column_kind::static_column; }
    bool is_regular() const ;
    
    
    bool is_primary_key() const ;
    bool is_atomic() const { return _is_atomic; }
    
    bool is_counter() const { return _is_counter; }
    // "virtual columns" appear in a materialized view as placeholders for
    // unselected columns, with liveness information but without data, and
    // allow view rows to remain alive despite having no data (issue #3362).
    // These columns should be hidden from the user's SELECT queries.
    bool is_view_virtual() const ;
    column_view_virtual view_virtual() const ;
    // Computed column values are generated from other columns (and possibly other sources) during updates.
    // Their values are still stored on disk, same as a regular columns.
    bool is_computed() const ;
    const column_computation& get_computation() const ;
    column_computation_ptr get_computation_ptr() const {
        return _computation ? _computation->clone() : nullptr;
    }
    void set_computed(column_computation_ptr computation) ;
    // Columns hidden from CQL cannot be in any way retrieved by the user,
    // either explicitly or via the '*' operator, or functions, aggregates, etc.
    
    const sstring& name_as_text() const;
    const bytes& name() const;
    sstring name_as_cql_string() const;
    friend std::ostream& operator<<(std::ostream& os, const column_definition& cd);
    bool has_component_index() const ;
    uint32_t component_index() const ;
    uint32_t position() const ;
    bool is_on_all_components() const;
    
    
    friend bool operator==(const column_definition&, const column_definition&);
};
class schema_builder;
class thrift_schema {
    bool _compound = true;
    bool _is_dynamic = false;
public:
    bool has_compound_comparator() const;
    bool is_dynamic() const;
    friend class schema;
};
bool operator==(const column_definition&, const column_definition&);
static constexpr int DEFAULT_MIN_COMPACTION_THRESHOLD = 4;
static constexpr int DEFAULT_MAX_COMPACTION_THRESHOLD = 32;
static constexpr int DEFAULT_MIN_INDEX_INTERVAL = 128;
static constexpr int DEFAULT_GC_GRACE_SECONDS = 864000;
// Unsafe to access across shards.
// Safe to copy across shards.
class column_mapping_entry {
    bytes _name;
    data_type _type;
    bool _is_atomic;
public:
    column_mapping_entry(bytes name, data_type type)
        : _name(std::move(name)), _type(std::move(type)), _is_atomic(_type->is_atomic()) { }
    
    const bytes& name() const ;
    
    const sstring& type_name() const ;
    
};

// Encapsulates information needed for converting mutations between different schema versions.
//
// Unsafe to access across shards.
// Safe to copy across shards.
class column_mapping {
private:
    // Contains _n_static definitions for static columns followed by definitions for regular columns,
    // both ordered by consecutive column_ids.
    // Primary key column sets are not mutable so we don't need to map them.
    std::vector<column_mapping_entry> _columns;
    column_count_type _n_static = 0;
public:
    column_mapping() {}
    column_mapping(std::vector<column_mapping_entry> columns, column_count_type n_static)
            : _columns(std::move(columns))
            , _n_static(n_static)
    { }
    const std::vector<column_mapping_entry>& columns() const ;
    column_count_type n_static() const ;
    
    
    
};
class raw_view_info final {
    table_id _base_id;
    sstring _base_name;
    bool _include_all_columns;
    sstring _where_clause;
public:
};

class view_info;
// Represents a column set which is compactible with Cassandra 3.x.
//
// This layout differs from the layout Scylla uses in schema/schema_builder for static compact tables.
// For such tables, Scylla expects all columns to be of regular type and no clustering columns,
// whereas in v3 those columns are static and there is a clustering column with type matching the
// cell name comparator and a regular column with type matching the default validator.
// See issues #2555 and #1474.
class v3_columns {
    bool _is_dense = false;
    bool _is_compound = false;
    std::vector<column_definition> _columns;
    std::unordered_map<bytes, const column_definition*> _columns_by_name;
public:
    v3_columns(std::vector<column_definition> columns, bool is_dense, bool is_compound);
    v3_columns() = default;
    
    
    
    static v3_columns from_v2_schema(const schema&);
public:
    
    
    
    
    
};
namespace query {
class partition_slice;
}
class schema_extension {
public:
    virtual ~schema_extension() {};
    virtual bytes serialize() const = 0;
    virtual bool is_placeholder() const {
        return false;
    }
};
// To break some cyclic data dependencies in the early stages of a node boot process,
// the system tables are loaded in two phases. This allows to use the tables
// from the first phase to load the tables from the second phase.
// For example, we need system.scylla_local table to load raft tables, since it
// stores the enabled features, and SCHEMA_COMMITLOG feature is used to choose
// what commitlog (regular or schema) will be used for raft tables.
enum class system_table_load_phase {
    phase1,
    phase2
};
constexpr system_table_load_phase all_system_table_load_phases[] = {
    system_table_load_phase::phase1,
    system_table_load_phase::phase2
};
struct schema_static_props {
    bool use_null_sharder = false; // use a sharder that puts everything on shard 0
    bool wait_for_sync_to_commitlog = false; // true if all writes using this schema have to be synced immediately by commitlog
    bool use_schema_commitlog = false;
    system_table_load_phase load_phase = system_table_load_phase::phase1;
};
class schema final : public enable_lw_shared_from_this<schema>, public data_dictionary::keyspace_element {
    friend class v3_columns;
public:
    struct dropped_column {
        data_type type;
        api::timestamp_type timestamp;
        bool operator==(const dropped_column& rhs) const ;
    };
    using extensions_map = std::map<sstring, ::shared_ptr<schema_extension>>;
private:
    // More complex fields are derived from these inside rebuild().
    // Contains only fields which can be safely default-copied.
    struct raw_schema {
        raw_schema(table_id id);
        table_id _id;
        sstring _ks_name;
        sstring _cf_name;
        // regular columns are sorted by name
        // static columns are sorted by name, but present only when there's any clustering column
        std::vector<column_definition> _columns;
        sstring _comment;
        gc_clock::duration _default_time_to_live = gc_clock::duration::zero();
        data_type _regular_column_name_type;
        data_type _default_validation_class = bytes_type;
        double _bloom_filter_fp_chance = 0.01;
        compression_parameters _compressor_params;
        extensions_map _extensions;
        bool _is_dense = false;
        bool _is_compound = true;
        bool _is_counter = false;
        cf_type _type = cf_type::standard;
        int32_t _gc_grace_seconds = DEFAULT_GC_GRACE_SECONDS;
        std::optional<int32_t> _paxos_grace_seconds;
        double _dc_local_read_repair_chance = 0.0;
        double _read_repair_chance = 0.0;
        double _crc_check_chance = 1;
        db::per_partition_rate_limit_options _per_partition_rate_limit_options;
        int32_t _min_compaction_threshold = DEFAULT_MIN_COMPACTION_THRESHOLD;
        int32_t _max_compaction_threshold = DEFAULT_MAX_COMPACTION_THRESHOLD;
        int32_t _min_index_interval = DEFAULT_MIN_INDEX_INTERVAL;
        int32_t _max_index_interval = 2048;
        int32_t _memtable_flush_period = 0;
        ::speculative_retry _speculative_retry = ::speculative_retry(speculative_retry::type::PERCENTILE, 0.99);
        // This is the compaction strategy that will be used by default on tables which don't have one explicitly specified.
        sstables::compaction_strategy_type _compaction_strategy = sstables::compaction_strategy_type::size_tiered;
        std::map<sstring, sstring> _compaction_strategy_options;
        bool _compaction_enabled = true;
        ::caching_options _caching_options;
        table_schema_version _version;
        std::unordered_map<sstring, dropped_column> _dropped_columns;
        std::map<bytes, data_type> _collections;
        std::unordered_map<sstring, index_metadata> _indices_by_name;
        std::reference_wrapper<const dht::i_partitioner> _partitioner;
        // Sharding info is not stored in the schema mutation and does not affect
        // schema digest. It is also not set locally on a schema tables.
        std::reference_wrapper<const dht::sharder> _sharder;
    };
    raw_schema _raw;
    schema_static_props _static_props;
    thrift_schema _thrift;
    v3_columns _v3_columns;
    mutable schema_registry_entry* _registry_entry = nullptr;
    std::unique_ptr<::view_info> _view_info;
    const std::array<column_count_type, 3> _offsets;
    inline column_count_type column_offset(column_kind k) const {
        return k == column_kind::partition_key ? 0 : _offsets[column_count_type(k) - 1];
    }
    std::unordered_map<bytes, const column_definition*> _columns_by_name;
    lw_shared_ptr<compound_type<allow_prefixes::no>> _partition_key_type;
    lw_shared_ptr<compound_type<allow_prefixes::yes>> _clustering_key_type;
    column_mapping _column_mapping;
    shared_ptr<query::partition_slice> _full_slice;
    column_count_type _clustering_key_size;
    column_count_type _regular_column_count;
    column_count_type _static_column_count;
    
    friend class db::extensions;
    friend class schema_builder;
public:
    using row_column_ids_are_ordered_by_name = std::true_type;
    typedef std::vector<column_definition> columns_type;
    typedef typename columns_type::iterator iterator;
    typedef typename columns_type::const_iterator const_iterator;
    typedef boost::iterator_range<iterator> iterator_range_type;
    typedef boost::iterator_range<const_iterator> const_iterator_range_type;
    static constexpr int32_t NAME_LENGTH = 48;
    struct column {
        bytes name;
        data_type type;
    };
private:
    struct reversed_tag { };
    lw_shared_ptr<cql3::column_specification> make_column_specification(const column_definition& def) const;
    void rebuild();
    
    class private_tag{};
public:
    schema(private_tag, const raw_schema&, std::optional<raw_view_info>, const schema_static_props& props);
    schema(const schema&);
    // See \ref make_reversed().
    
    ~schema();
    
    table_schema_version version() const ;
    double bloom_filter_fp_chance() const ;
    sstring thrift_key_validator() const;
    
    bool is_dense() const {
        return _raw._is_dense;
    }
    bool is_compound() const {
        return _raw._is_compound;
    }
    bool is_static_compact_table() const {
        return !is_super() && !is_dense() && !is_compound();
    }
    thrift_schema& thrift() {
        return _thrift;
    }
    
    bool is_counter() const {
        return _raw._is_counter;
    }
    
    bool is_super() const {
        return _raw._type == cf_type::super;
    }
    
    gc_clock::duration paxos_grace_seconds() const;
    
    
    const cdc::options& cdc_options() const;
    const ::tombstone_gc_options& tombstone_gc_options() const;
    
    const dht::i_partitioner& get_partitioner() const;
    
    const column_definition* get_column_definition(const bytes& name) const;
    const column_definition& column_at(column_kind, column_id) const;
    // Find a column definition given column ordinal id in the schema
    const column_definition& column_at(ordinal_column_id ordinal_id) const;
    
    
    
    
    static data_type column_name_type(const column_definition& def, const data_type& regular_column_name_type);
    data_type column_name_type(const column_definition& def) const;
    
    const column_definition& regular_column_at(column_id id) const;
    const column_definition& static_column_at(column_id id) const;
    
    bool has_multi_cell_collections() const;
    bool has_static_columns() const;
    column_count_type columns_count(column_kind kind) const;
    column_count_type partition_key_size() const;
    column_count_type clustering_key_size() const { return _clustering_key_size; }
    column_count_type static_columns_count() const { return _static_column_count; }
    
    // Returns a range of column definitions
    const_iterator_range_type partition_key_columns() const;
    // Returns a range of column definitions
    const_iterator_range_type clustering_key_columns() const;
    // Returns a range of column definitions
    const_iterator_range_type static_columns() const;
    // Returns a range of column definitions
    const_iterator_range_type regular_columns() const;
    // Returns a range of column definitions
    // Returns a range of column definitions
    typedef boost::range::joined_range<const_iterator_range_type, const_iterator_range_type>
        select_order_range;
    const columns_type& all_columns() const {
        return _raw._columns;
    }
    
    
    
    data_type make_legacy_default_validator() const;
    const sstring& ks_name() const {
        return _raw._ks_name;
    }
    const sstring& cf_name() const {
        return _raw._cf_name;
    }
    const lw_shared_ptr<compound_type<allow_prefixes::no>>& partition_key_type() const {
        return _partition_key_type;
    }
    const lw_shared_ptr<compound_type<allow_prefixes::yes>>& clustering_key_type() const ;
    const lw_shared_ptr<compound_type<allow_prefixes::yes>>& clustering_key_prefix_type() const {
        return _clustering_key_type;
    }
    const data_type& regular_column_name_type() const {
        return _raw._regular_column_name_type;
    }
    const data_type& static_column_name_type() const {
        return utf8_type;
    }
    const std::unique_ptr<::view_info>& view_info() const ;
    bool is_view() const ;
    const query::partition_slice& full_slice() const ;
    // Returns all index names of this schema.
    // Returns all indices of this schema.
    // Search for an index with a given name.
    // Search for an existing index with same kind and options.
    friend std::ostream& operator<<(std::ostream& os, const schema& s);
    virtual sstring keypace_name() const override { return ks_name(); }
    virtual sstring element_name() const override { return cf_name(); }
    virtual sstring element_type(replica::database& db) const override;
    virtual std::ostream& describe(replica::database& db, std::ostream& os, bool with_internals) const override;
    friend bool operator==(const schema&, const schema&);
    const column_mapping& get_column_mapping() const;
    friend class schema_registry_entry;
    // May be called from different shard
    schema_registry_entry* registry_entry() const noexcept;
    // Returns true iff this schema version was synced with on current node.
    // Schema version is said to be synced with when its mutations were merged
    // into current node's schema, so that current node's schema is at least as
    // recent as this version.
public:
    // Make a copy of the schema with reversed clustering order.
    //
    // The reversing is revertible, so that:
    //
    //      s->make_reversed()->make_reversed()->version() == s->version()
    //
    // But note that: `s != s->make_reversed()->make_reversed()` (they are two
    // different C++ objects).
    // The schema's version is also reversed using UUID_gen::negate().
    schema_ptr make_reversed() const;
    // Get the reversed counterpart of this schema from the schema registry.
    //
    // If not present in the registry, create one (via \ref make_reversed()) and
    // load it. Unlike \ref make_reversed(), this method guarantees that double
    // reversing will return the very same C++ object:
    //
    //      auto schema = make_schema();
    //      auto reverse_schema = schema->get_reversed();
    //      assert(reverse_schema->get_reversed().get() == schema.get());
    //      assert(schema->get_reversed().get() == reverse_schema.get());
    //
    
};
lw_shared_ptr<const schema> make_shared_schema(std::optional<table_id> id, std::string_view ks_name, std::string_view cf_name,
    std::vector<schema::column> partition_key, std::vector<schema::column> clustering_key, std::vector<schema::column> regular_columns,
    std::vector<schema::column> static_columns, data_type regular_column_name_type, sstring comment = "");
bool operator==(const schema&, const schema&);
class view_ptr final {
    schema_ptr _schema;
public:
    
    operator schema_ptr() const noexcept {
        return _schema;
    }
    explicit operator bool() const noexcept {
        return bool(_schema);
    }
    friend std::ostream& operator<<(std::ostream& os, const view_ptr& s);
};
// Thrown when attempted to access a schema-dependent object using
// an incompatible version of the schema object.
class schema_mismatch_error : public std::runtime_error {
public:
};
// Throws schema_mismatch_error when a schema-dependent object of "expected" version
// cannot be accessed using "access" schema.
namespace logging {
//
// Seastar changed the names of some of these types. Maintain the old names here to avoid too much churn.
//
using log_level = seastar::log_level;
using logger = seastar::logger;
using registry = seastar::logger_registry;
using settings = seastar::logging_settings;
using seastar::pretty_type_name;
using seastar::level_name;
}
namespace seastar {
extern logger seastar_logger;
}
namespace ser {
 ;
 ;
 ;
 ;
template<bool Fast, typename T>
struct serialize_array_helper;
template<typename T>
struct serialize_array_helper<true, T> {
     ;
};
template<typename T>
struct serialize_array_helper<false, T> {
     ;
};
 ;
template<typename Container>
struct container_traits;
template<typename T>
struct container_traits<absl::btree_set<T>> {
    struct back_emplacer {
        absl::btree_set<T>& c;
        back_emplacer(absl::btree_set<T>& c_) : c(c_) {}
        void operator()(T&& v) {
            c.emplace(std::move(v));
        }
    };
};
template<typename T, typename... Args>
struct container_traits<std::unordered_set<T, Args...>> {
    struct back_emplacer {
        std::unordered_set<T, Args...>& c;
        back_emplacer(std::unordered_set<T, Args...>& c_) : c(c_) {}
        void operator()(T&& v) {
            c.emplace(std::move(v));
        }
    };
};
template<typename T>
struct container_traits<std::list<T>> {
    struct back_emplacer {
        std::list<T>& c;
        back_emplacer(std::list<T>& c_) : c(c_) {}
        void operator()(T&& v) {
            c.emplace_back(std::move(v));
        }
    };
    void resize(std::list<T>& c, size_t size) {
        c.resize(size);
    }
};
template<typename T>
struct container_traits<std::vector<T>> {
    struct back_emplacer {
        std::vector<T>& c;
        back_emplacer(std::vector<T>& c_) : c(c_) {}
        void operator()(T&& v) {
            c.emplace_back(std::move(v));
        }
    };
    void resize(std::vector<T>& c, size_t size) {
        c.resize(size);
    }
};
template<typename T, size_t N>
struct container_traits<utils::small_vector<T, N>> {
    struct back_emplacer {
        utils::small_vector<T, N>& c;
        back_emplacer(utils::small_vector<T, N>& c_) : c(c_) {}
        void operator()(T&& v) {
            c.emplace_back(std::move(v));
        }
    };
    void resize(utils::small_vector<T, N>& c, size_t size) {
        c.resize(size);
    }
};
template<typename T>
struct container_traits<utils::chunked_vector<T>> {
    struct back_emplacer {
        utils::chunked_vector<T>& c;
    };
};
template<typename T, size_t N>
struct container_traits<std::array<T, N>> {
    struct back_emplacer {
        std::array<T, N>& c;
        size_t idx = 0;
        back_emplacer(std::array<T, N>& c_) : c(c_) {}
        void operator()(T&& v) {
            c[idx++] = std::move(v);
        }
    };
    void resize(std::array<T, N>& c, size_t size) {}
};
template<bool Fast, typename T>
struct deserialize_array_helper;
template<typename T>
struct deserialize_array_helper<true, T> {
     ;
     ;
};
template<typename T>
struct deserialize_array_helper<false, T> {
     ;
     ;
};
 ;
 ;
namespace idl::serializers::internal {
template<typename Vector>
struct vector_serializer {
    using value_type = typename Vector::value_type;
     ;
     ;
     ;
};
}
template<typename T>
struct serializer<std::list<T>> {
    template<typename Input>
    static std::list<T> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        std::list<T> v;
        deserialize_array_helper<false, T>::doit(in, v, sz);
        return v;
    }
    template<typename Output>
    static void write(Output& out, const std::list<T>& v) {
        safe_serialize_as_uint32(out, v.size());
        serialize_array_helper<false, T>::doit(out, v);
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        skip_array<T>(in, sz);
    }
};
template<typename T>
struct serializer<absl::btree_set<T>> {
    template<typename Input>
    static absl::btree_set<T> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        absl::btree_set<T> v;
        deserialize_array_helper<false, T>::doit(in, v, sz);
        return v;
    }
    template<typename Output>
    static void write(Output& out, const absl::btree_set<T>& v) {
        safe_serialize_as_uint32(out, v.size());
        serialize_array_helper<false, T>::doit(out, v);
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        skip_array<T>(in, sz);
    }
};
template<typename T, typename... Args>
struct serializer<std::unordered_set<T, Args...>> {
    template<typename Input>
    static std::unordered_set<T, Args...> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        std::unordered_set<T, Args...> v;
        v.reserve(sz);
        deserialize_array_helper<false, T>::doit(in, v, sz);
        return v;
    }
    template<typename Output>
    static void write(Output& out, const std::unordered_set<T, Args...>& v) {
        safe_serialize_as_uint32(out, v.size());
        serialize_array_helper<false, T>::doit(out, v);
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        skip_array<T>(in, sz);
    }
};
template<typename T>
struct serializer<std::vector<T>>
    : idl::serializers::internal::vector_serializer<std::vector<T>>
{ };
template<typename T>
struct serializer<utils::chunked_vector<T>>
    : idl::serializers::internal::vector_serializer<utils::chunked_vector<T>>
{ };
template<typename T, size_t N>
struct serializer<utils::small_vector<T, N>>
    : idl::serializers::internal::vector_serializer<utils::small_vector<T, N>>
{ };
template<typename T, typename Ratio>
struct serializer<std::chrono::duration<T, Ratio>> {
    template<typename Input>
    static std::chrono::duration<T, Ratio> read(Input& in) {
        return std::chrono::duration<T, Ratio>(deserialize(in, boost::type<T>()));
    }
    template<typename Output>
    static void write(Output& out, const std::chrono::duration<T, Ratio>& d) {
        serialize(out, d.count());
    }
    template<typename Input>
    static void skip(Input& in) {
        read(in);
    }
};
template<typename Clock, typename Duration>
struct serializer<std::chrono::time_point<Clock, Duration>> {
    using value_type = std::chrono::time_point<Clock, Duration>;
    template<typename Input>
    static value_type read(Input& in) {
        return typename Clock::time_point(Duration(deserialize(in, boost::type<uint64_t>())));
    }
    template<typename Output>
    static void write(Output& out, const value_type& v) {
        serialize(out, uint64_t(v.time_since_epoch().count()));
    }
    template<typename Input>
    static void skip(Input& in) {
        read(in);
    }
};
template<size_t N, typename T>
struct serializer<std::array<T, N>> {
    template<typename Input>
    static std::array<T, N> read(Input& in) {
        std::array<T, N> v;
        deserialize_array<T>(in, v, N);
        return v;
    }
    template<typename Output>
    static void write(Output& out, const std::array<T, N>& v) {
        serialize_array<T>(out, v);
    }
    template<typename Input>
    static void skip(Input& in) {
        skip_array<T>(in, N);
    }
};
template<typename K, typename V>
struct serializer<std::map<K, V>> {
    template<typename Input>
    static std::map<K, V> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        std::map<K, V> m;
        while (sz--) {
            K k = deserialize(in, boost::type<K>());
            V v = deserialize(in, boost::type<V>());
            m[k] = v;
        }
        return m;
    }
    template<typename Output>
    static void write(Output& out, const std::map<K, V>& v) {
        safe_serialize_as_uint32(out, v.size());
        for (auto&& e : v) {
            serialize(out, e.first);
            serialize(out, e.second);
        }
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        while (sz--) {
            serializer<K>::skip(in);
            serializer<V>::skip(in);
        }
    }
};
template<typename K, typename V>
struct serializer<std::unordered_map<K, V>> {
    template<typename Input>
    static std::unordered_map<K, V> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        std::unordered_map<K, V> m;
        m.reserve(sz);
        while (sz--) {
            auto k = deserialize(in, boost::type<K>());
            auto v = deserialize(in, boost::type<V>());
            m.emplace(std::move(k), std::move(v));
        }
        return m;
    }
    template<typename Output>
    static void write(Output& out, const std::unordered_map<K, V>& v) {
        safe_serialize_as_uint32(out, v.size());
        for (auto&& e : v) {
            serialize(out, e.first);
            serialize(out, e.second);
        }
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        while (sz--) {
            serializer<K>::skip(in);
            serializer<V>::skip(in);
        }
    }
};
template<typename Tag>
struct serializer<bool_class<Tag>> {
    template<typename Input>
    static bool_class<Tag> read(Input& in) {
        return bool_class<Tag>(deserialize(in, boost::type<bool>()));
    }
    template<typename Output>
    static void write(Output& out, bool_class<Tag> v) {
        serialize(out, bool(v));
    }
    template<typename Input>
    static void skip(Input& in) {
        read(in);
    }
};
template<typename Stream>
class deserialized_bytes_proxy {
    Stream _stream;
    template<typename OtherStream>
    friend class deserialized_bytes_proxy;
public:
    template<typename OtherStream>
    requires std::convertible_to<OtherStream, Stream>
    deserialized_bytes_proxy(deserialized_bytes_proxy<OtherStream> proxy)
        : _stream(std::move(proxy._stream)) { }
    auto view() const {
      if constexpr (std::is_same_v<Stream, simple_input_stream>) {
        return bytes_view(reinterpret_cast<const int8_t*>(_stream.begin()), _stream.size());
      } else {
        using iterator_type = typename Stream::iterator_type ;
        static_assert(FragmentRange<buffer_view<iterator_type>>);
        return seastar::with_serialized_stream(_stream, seastar::make_visitor(
            [&] (typename seastar::memory_input_stream<iterator_type >::simple stream) {
                return buffer_view<iterator_type>(bytes_view(reinterpret_cast<const int8_t*>(stream.begin()),
                                                        stream.size()));
            },
            [&] (typename seastar::memory_input_stream<iterator_type >::fragmented stream) {
                return buffer_view<iterator_type>(bytes_view(reinterpret_cast<const int8_t*>(stream.first_fragment_data()),
                                                        stream.first_fragment_size()),
                                             stream.size(), stream.fragment_iterator());
            }
        ));
      }
    }
    [[gnu::always_inline]]
    operator bytes() && {
        bytes v(bytes::initialized_later(), _stream.size());
        _stream.read(reinterpret_cast<char*>(v.begin()), _stream.size());
        return v;
    }
    [[gnu::always_inline]]
    operator managed_bytes() && {
        managed_bytes mb(managed_bytes::initialized_later(), _stream.size());
        for (bytes_mutable_view frag : fragment_range(managed_bytes_mutable_view(mb))) {
            _stream.read(reinterpret_cast<char*>(frag.data()), frag.size());
        }
        return mb;
    }
    [[gnu::always_inline]]
    operator bytes_ostream() && {
        bytes_ostream v;
        _stream.copy_to(v);
        return v;
    }
};
template<>
struct serializer<bytes> {
    template<typename Input>
    static deserialized_bytes_proxy<Input> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        return deserialized_bytes_proxy<Input>(in.read_substream(sz));
    }
    template<typename Output>
    static void write(Output& out, bytes_view v) {
        safe_serialize_as_uint32(out, uint32_t(v.size()));
        out.write(reinterpret_cast<const char*>(v.begin()), v.size());
    }
    template<typename Output>
    static void write(Output& out, const bytes& v) {
        write(out, static_cast<bytes_view>(v));
    }
    template<typename Output>
    static void write(Output& out, const managed_bytes& mb) {
        safe_serialize_as_uint32(out, uint32_t(mb.size()));
        for (bytes_view frag : fragment_range(managed_bytes_view(mb))) {
            out.write(reinterpret_cast<const char*>(frag.data()), frag.size());
        }
    }
    template<typename Output>
    static void write(Output& out, const bytes_ostream& v) {
        safe_serialize_as_uint32(out, uint32_t(v.size()));
        for (bytes_view frag : v.fragments()) {
            out.write(reinterpret_cast<const char*>(frag.begin()), frag.size());
        }
    }
    template<typename Output, typename FragmentedBuffer>
    requires FragmentRange<FragmentedBuffer>
    static void write_fragmented(Output& out, FragmentedBuffer&& fragments) {
        safe_serialize_as_uint32(out, uint32_t(fragments.size_bytes()));
        for (bytes_view frag : fragments) {
            out.write(reinterpret_cast<const char*>(frag.begin()), frag.size());
        }
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        in.skip(sz);
    }
};
 ;
 ;
 ;
 ;
 ;
template<typename T>
struct serializer<std::optional<T>> {
    template<typename Input>
    static std::optional<T> read(Input& in) {
        std::optional<T> v;
        auto b = deserialize(in, boost::type<bool>());
        if (b) {
            v.emplace(deserialize(in, boost::type<T>()));
        }
        return v;
    }
    template<typename Output>
    static void write(Output& out, const std::optional<T>& v) {
        serialize(out, bool(v));
        if (v) {
            serialize(out, v.value());
        }
    }
    template<typename Input>
    static void skip(Input& in) {
        auto present = deserialize(in, boost::type<bool>());
        if (present) {
            serializer<T>::skip(in);
        }
    }
};
extern logging::logger serlog;
// Warning: assumes that pointer is never null
template<typename T>
struct serializer<seastar::lw_shared_ptr<T>> {
    template<typename Input>
    static seastar::lw_shared_ptr<T> read(Input& in) {
        return seastar::make_lw_shared<T>(deserialize(in, boost::type<T>()));
    }
    template<typename Output>
    static void write(Output& out, const seastar::lw_shared_ptr<T>& v) {
        if (!v) {
            on_internal_error(serlog, "Unexpected nullptr while serializing a pointer");
        }
        serialize(out, *v);
    }
    template<typename Input>
    static void skip(Input& in) {
        serializer<T>::skip(in);
    }
};
template<>
struct serializer<sstring> {
    template<typename Input>
    static sstring read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        sstring v = uninitialized_string(sz);
        in.read(v.data(), sz);
        return v;
    }
    template<typename Output>
    static void write(Output& out, const sstring& v) {
        safe_serialize_as_uint32(out, uint32_t(v.size()));
        out.write(v.data(), v.size());
    }
    template<typename Input>
    static void skip(Input& in) {
        in.skip(deserialize(in, boost::type<size_type>()));
    }
};
template<typename T>
struct serializer<std::unique_ptr<T>> {
    template<typename Input>
    static std::unique_ptr<T> read(Input& in) {
        std::unique_ptr<T> v;
        auto b = deserialize(in, boost::type<bool>());
        if (b) {
            v = std::make_unique<T>(deserialize(in, boost::type<T>()));
        }
        return v;
    }
    template<typename Output>
    static void write(Output& out, const std::unique_ptr<T>& v) {
        serialize(out, bool(v));
        if (v) {
            serialize(out, *v);
        }
    }
    template<typename Input>
    static void skip(Input& in) {
        auto present = deserialize(in, boost::type<bool>());
        if (present) {
            serializer<T>::skip(in);
        }
    }
};
template<typename Enum>
struct serializer<enum_set<Enum>> {
     ;
     ;
     ;
};
template<>
struct serializer<std::monostate> {
    template<typename Input>
    static std::monostate read(Input& in) {
        return std::monostate{};
    }
    template<typename Output>
    static void write(Output& out, std::monostate v) {}
    template<typename Input>
    static void skip(Input& in) {
    }
};
 ;
 ;
 ;
utils::input_stream as_input_stream(const bytes_ostream& b) ;
 ;
 ;
 ;
 ;
template<typename Input, typename ...T>
std::variant<T...> deserialize(Input& in, boost::type<std::variant<T...>> v) ;
 ;
template<typename Input>
unknown_variant_type deserialize(Input& in, boost::type<unknown_variant_type>) ;
// Class for iteratively deserializing a frozen vector
// using a range.
// Use begin() and end() to iterate through the frozen vector,
// deserializing (or skipping) one element at a time.
template <typename T, bool IsForward=true>
class vector_deserializer {
public:
    using value_type = T;
    using input_stream = utils::input_stream;
private:
    input_stream _in;
    size_t _size;
    utils::chunked_vector<input_stream> _substreams;
    void fill_substreams() requires (!IsForward) ;
    struct forward_iterator_data {
        input_stream _in = simple_input_stream();
        
        
    };
    struct reverse_iterator_data {
        std::reverse_iterator<utils::chunked_vector<input_stream>::const_iterator> _substream_it;
        
        
    };
public:
    
    explicit vector_deserializer(input_stream in)
        : _in(std::move(in))
        , _size(deserialize(_in, boost::type<uint32_t>()))
    {
        if constexpr (!IsForward) {
            fill_substreams();
        }
    }
    // Get the number of items in the vector
    // Input iterator
    class iterator {
        // _idx is the distance from .begin(). It is used only for comparing iterators.
        size_t _idx = 0;
        bool _consumed = false;
        std::conditional_t<IsForward, forward_iterator_data, reverse_iterator_data> _data;
        friend class vector_deserializer;
   public:
        using iterator_category = std::input_iterator_tag;
        using value_type = T;
        using pointer = value_type*;
        using reference = value_type&;
        using difference_type = ssize_t;
        
        bool operator==(const iterator& it) const noexcept ;
        // Deserializes and returns the item, effectively incrementing the iterator..
        value_type operator*() const ;
        iterator& operator++() ;
        iterator operator++(int) ;
        
    };
    using const_iterator = iterator;
    static_assert(std::input_iterator<iterator>);
    static_assert(std::sentinel_for<iterator, iterator>);
    
    const_iterator begin() const noexcept requires(IsForward) ;
    
    
    const_iterator end() const noexcept requires(IsForward) ;
    
};
static_assert(std::ranges::range<vector_deserializer<int>>);
}
namespace cdc {
class cdc_extension : public schema_extension {
    cdc::options _cdc_options;
public:
    static constexpr auto NAME = "cdc";
};
}
namespace db {
namespace functions {
class function_name final {
public:
    sstring keyspace;
    sstring name;
     // for ANTLR
    bool has_keyspace() const ;
    bool operator==(const function_name& x) const ;
};
}
}
template <>
struct fmt::formatter<db::functions::function_name> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const db::functions::function_name& fn, FormatContext& ctx) const {
        auto out = ctx.out();
        if (fn.has_keyspace()) {
            out = fmt::format_to(out, "{}.", fn.keyspace);
        }
        return fmt::format_to(out, "{}", fn.name);
    }
};
namespace std {
template <>
struct hash<db::functions::function_name> {
    size_t operator()(const db::functions::function_name& x) const {
        return std::hash<sstring>()(x.keyspace) ^ std::hash<sstring>()(x.name);
    }
};
}
namespace db {
namespace functions {
class function_name;
class function {
public:
    using opt_bytes = std::optional<bytes>;
    
    
    
    virtual void print(std::ostream& os) const = 0;
    virtual sstring column_name(const std::vector<sstring>& column_names) const = 0;
    friend class function_call;
};
}
}
namespace db::functions {
class scalar_function : public virtual function {
public:
};
}
namespace db::functions {
struct stateless_aggregate_function final {
    function_name name;
    std::optional<sstring> column_name_override; // if unset, column name is synthesized from name and argument names
    data_type state_type;
    data_type result_type;
    std::vector<data_type> argument_types;
    bytes_opt initial_state;
    // aggregates another input
    // signature: (state_type, argument_types...) -> state_type
    shared_ptr<scalar_function> aggregation_function;
    // converts the state type to a result
    // signature: (state_type) -> result_type
    shared_ptr<scalar_function> state_to_result_function;
    // optional: reduces states computed in parallel
    // signature: (state_type, state_type) -> state_type
    shared_ptr<scalar_function> state_reduction_function;
};
}
namespace db {
namespace functions {
class aggregate_function : public virtual function {
protected:
    stateless_aggregate_function _agg;
private:
    shared_ptr<aggregate_function> _reducible;
private:
public:
    
    
};
}
}
namespace sstables {
enum class sstable_version_types { ka, la, mc, md, me };
enum class sstable_format_types { big };
constexpr std::array<sstable_version_types, 5> all_sstable_versions = {
    sstable_version_types::ka,
    sstable_version_types::la,
    sstable_version_types::mc,
    sstable_version_types::md,
    sstable_version_types::me,
};
constexpr std::array<sstable_version_types, 3> writable_sstable_versions = {
    sstable_version_types::mc,
    sstable_version_types::md,
    sstable_version_types::me,
};
constexpr sstable_version_types oldest_writable_sstable_format = sstable_version_types::mc;
template <size_t S1, size_t S2>
constexpr bool check_sstable_versions(const std::array<sstable_version_types, S1>& all_sstable_versions,
        const std::array<sstable_version_types, S2>& writable_sstable_versions, sstable_version_types oldest_writable_sstable_format) {
    for (auto v : writable_sstable_versions) {
        if (v < oldest_writable_sstable_format) {
            return false;
        }
    }
    size_t expected = 0;
    for (auto v : all_sstable_versions) {
        if (v >= oldest_writable_sstable_format) {
            ++expected;
        }
    }
    return expected == S2;
}
static_assert(check_sstable_versions(all_sstable_versions, writable_sstable_versions, oldest_writable_sstable_format));
 


extern const std::unordered_map<sstable_version_types, seastar::sstring, seastar::enum_hash<sstable_version_types>> version_string;
extern const std::unordered_map<sstable_format_types, seastar::sstring, seastar::enum_hash<sstable_format_types>> format_string;
 
}
template <>
struct fmt::formatter<sstables::sstable_version_types> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const sstables::sstable_version_types& version, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", sstables::version_string.at(version));
    }
};
template <>
struct fmt::formatter<sstables::sstable_format_types> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const sstables::sstable_format_types& format, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", sstables::format_string.at(format));
    }
};
//FIXME: de-inline methods and define this as static in a .cc file.
extern logging::logger compound_logger;
//
// This header provides adaptors between the representation used by our compound_type<>
// and representation used by Origin.
//
// For single-component keys the legacy representation is equivalent
// to the only component's serialized form. For composite keys it the following
// (See org.apache.cassandra.db.marshal.CompositeType):
//
//   <representation> ::= ( <component> )+
//   <component>      ::= <length> <value> <EOC>
//   <length>         ::= <uint16_t>
//   <EOC>            ::= <uint8_t>
//
//  <value> is component's value in serialized form. <EOC> is always 0 for partition key.
//
// Given a representation serialized using @CompoundType, provides a view on the
// representation of the same components as they would be serialized by Origin.
//
// The view is exposed in a form of a byte range. For example of use see to_legacy() function.
template <typename CompoundType>
class legacy_compound_view {
    static_assert(!CompoundType::is_prefixable, "Legacy view not defined for prefixes");
    CompoundType& _type;
    managed_bytes_view _packed;
public:
    legacy_compound_view(CompoundType& c, managed_bytes_view packed)
        : _type(c)
        , _packed(packed)
    { }
    class iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = bytes::value_type;
        using difference_type = std::ptrdiff_t;
        using pointer = bytes::value_type*;
        using reference = bytes::value_type&;
    private:
        bool _singular;
        // Offset within virtual output space of a component.
        //
        // Offset: -2             -1             0  ...  LEN-1 LEN
        // Field:  [ length MSB ] [ length LSB ] [   VALUE   ] [ EOC ]
        //
        int32_t _offset;
        typename CompoundType::iterator _i;
    public:
        struct end_tag {};
        iterator(const legacy_compound_view& v)
            : _singular(v._type.is_singular())
            , _offset(_singular ? 0 : -2)
            , _i(_singular && !v._type.begin(v._packed)->size() ?
                    v._type.end(v._packed) : v._type.begin(v._packed))
        { }
        
        // Default constructor is incorrectly needed for c++20
        // weakly_incrementable concept requires for ranges.
        // Will be fixed by https://wg21.link/P2325R3 but still
        // needed for now.
        
        value_type operator*() const {
            int32_t component_size = _i->size();
            if (_offset == -2) {
                return (component_size >> 8) & 0xff;
            } else if (_offset == -1) {
                return component_size & 0xff;
            } else if (_offset < component_size) {
                return (*_i)[_offset];
            } else { // _offset == component_size
                return 0; // EOC field
            }
        }
        iterator& operator++() {
            auto component_size = (int32_t) _i->size();
            if (_offset < component_size
                // When _singular, we skip the EOC byte.
                && (!_singular || _offset != (component_size - 1)))
            {
                ++_offset;
            } else {
                ++_i;
                _offset = -2;
            }
            return *this;
        }
    };
    // A trichotomic comparator defined on @CompoundType representations which
    // orders them according to lexicographical ordering of their corresponding
    // legacy representations.
    //
    //   tri_comparator(t)(k1, k2)
    //
    // ...is equivalent to:
    //
    //   compare_unsigned(to_legacy(t, k1), to_legacy(t, k2))
    //
    // ...but more efficient.
    //
    struct tri_comparator {
        const CompoundType& _type;
        // @k1 and @k2 must be serialized using @type, which was passed to the constructor.
    };
    // Equivalent to std::distance(begin(), end()), but computes faster
    size_t size() const {
        if (_type.is_singular()) {
            return _type.begin(_packed)->size();
        }
        size_t s = 0;
        for (auto&& component : _type.components(_packed)) {
            s += 2  + component.size() + 1 ;
        }
        return s;
    }
    iterator begin() const {
        return iterator(*this);
    }
    
};
// Converts compound_type<> representation to legacy representation
// @packed is assumed to be serialized using supplied @type.
 ;
class composite_view;
// Represents a value serialized according to Origin's CompositeType.
// If is_compound is true, then the value is one or more components encoded as:
//
//   <representation> ::= ( <component> )+
//   <component>      ::= <length> <value> <EOC>
//   <length>         ::= <uint16_t>
//   <EOC>            ::= <uint8_t>
//
// If false, then it encodes a single value, without a prefix length or a suffix EOC.
class composite final {
    bytes _bytes;
    bool _is_compound;
public:
    composite(bytes&& b, bool is_compound) 
    ;
    explicit composite(bytes&& b) 
    ;
    
    
    using size_type = uint16_t;
    using eoc_type = int8_t;
    enum class eoc : eoc_type {
        start = -1,
        none = 0,
        end = 1
    };
    using component = std::pair<bytes, eoc>;
    using component_view = std::pair<bytes_view, eoc>;
private:
    template<typename Value>
    requires (!std::same_as<const data_value, std::decay_t<Value>>)
    static size_t size(const Value& val) {
        return val.size();
    }
    static size_t size(const data_value& val) {
        return val.serialized_size();
    }
    template<typename Value, typename CharOutputIterator>
    requires (!std::same_as<const data_value, std::decay_t<Value>>)
    static void write_value(Value&& val, CharOutputIterator& out) {
        out = std::copy(val.begin(), val.end(), out);
    }
    template<typename CharOutputIterator>
    static void write_value(managed_bytes_view val, CharOutputIterator& out) {
        for (bytes_view frag : fragment_range(val)) {
            out = std::copy(frag.begin(), frag.end(), out);
        }
    }
    template <typename CharOutputIterator>
    static void write_value(const data_value& val, CharOutputIterator& out) {
        val.serialize(out);
    }
    template<typename RangeOfSerializedComponents, typename CharOutputIterator>
    static void serialize_value(RangeOfSerializedComponents&& values, CharOutputIterator& out, bool is_compound) {
        if (!is_compound) {
            auto it = values.begin();
            write_value(std::forward<decltype(*it)>(*it), out);
            return;
        }
        for (auto&& val : values) {
            write<size_type>(out, static_cast<size_type>(size(val)));
            write_value(std::forward<decltype(val)>(val), out);
            // Range tombstones are not keys. For collections, only frozen
            // values can be keys. Therefore, for as long as it is safe to
            // assume that this code will be used to create keys, it is safe
            // to assume the trailing byte is always zero.
            write<eoc_type>(out, eoc_type(eoc::none));
        }
    }
    template <typename RangeOfSerializedComponents>
    static size_t serialized_size(RangeOfSerializedComponents&& values, bool is_compound) {
        size_t len = 0;
        auto it = values.begin();
        if (it != values.end()) {
            // CQL3 uses a specific prefix (0xFFFF) to encode "static columns"
            // (CASSANDRA-6561). This does mean the maximum size of the first component of a
            // composite is 65534, not 65535 (or we wouldn't be able to detect if the first 2
            // bytes is the static prefix or not).
            auto value_size = size(*it);
            if (value_size > static_cast<size_type>(std::numeric_limits<size_type>::max() - uint8_t(is_compound))) {
                throw std::runtime_error(format("First component size too large: {:d} > {:d}", value_size, std::numeric_limits<size_type>::max() - is_compound));
            }
            if (!is_compound) {
                return value_size;
            }
            len += sizeof(size_type) + value_size + sizeof(eoc_type);
            ++it;
        }
        for ( ; it != values.end(); ++it) {
            auto value_size = size(*it);
            if (value_size > std::numeric_limits<size_type>::max()) {
                throw std::runtime_error(format("Component size too large: {:d} > {:d}", value_size, std::numeric_limits<size_type>::max()));
            }
            len += sizeof(size_type) + value_size + sizeof(eoc_type);
        }
        return len;
    }
public:
    template <typename Describer>
    auto describe_type(sstables::sstable_version_types v, Describer f) const {
        return f(const_cast<bytes&>(_bytes));
    }
    // marker is ignored if !is_compound
    template<typename RangeOfSerializedComponents>
    static composite serialize_value(RangeOfSerializedComponents&& values, bool is_compound = true, eoc marker = eoc::none) {
        auto size = serialized_size(values, is_compound);
        bytes b(bytes::initialized_later(), size);
        auto i = b.begin();
        serialize_value(std::forward<decltype(values)>(values), i, is_compound);
        if (is_compound && !b.empty()) {
            b.back() = eoc_type(marker);
        }
        return composite(std::move(b), is_compound);
    }
    template<typename RangeOfSerializedComponents>
    static composite serialize_static(const schema& s, RangeOfSerializedComponents&& values) {
        // FIXME: Optimize
        auto b = bytes(size_t(2), bytes::value_type(0xff));
        std::vector<bytes_view> sv(s.clustering_key_size());
        b += composite::serialize_value(boost::range::join(sv, std::forward<RangeOfSerializedComponents>(values)), true).release_bytes();
        return composite(std::move(b));
    }
    static eoc to_eoc(int8_t eoc_byte) {
        return eoc_byte == 0 ? eoc::none : (eoc_byte < 0 ? eoc::start : eoc::end);
    }
    class iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = const component_view;
        using difference_type = std::ptrdiff_t;
        using pointer = const component_view*;
        using reference = const component_view&;
    private:
        bytes_view _v;
        component_view _current;
        bool _strict_mode = true;
    private:
        void do_read_current() {
            size_type len;
            {
                if (_v.empty()) {
                    _v = bytes_view(nullptr, 0);
                    return;
                }
                len = read_simple<size_type>(_v);
                if (_v.size() < len) {
                    throw_with_backtrace<marshal_exception>(format("composite iterator - not enough bytes, expected {:d}, got {:d}", len, _v.size()));
                }
            }
            auto value = bytes_view(_v.begin(), len);
            _v.remove_prefix(len);
            _current = component_view(std::move(value), to_eoc(read_simple<eoc_type>(_v)));
        }
        void read_current() {
            try {
                do_read_current();
            } catch (marshal_exception&) {
                if (_strict_mode) {
                    on_internal_error(compound_logger, std::current_exception());
                } else {
                    throw;
                }
            }
        }
        struct end_iterator_tag {};
        // In strict-mode de-serialization errors will invoke `on_internal_error()`.
        iterator(const bytes_view& v, bool is_compound, bool is_static, bool strict_mode = true)
                : _v(v), _strict_mode(strict_mode) {
            if (is_static) {
                _v.remove_prefix(2);
            }
            if (is_compound) {
                read_current();
            } else {
                _current = component_view(_v, eoc::none);
                _v.remove_prefix(_v.size());
            }
        }
        iterator(end_iterator_tag) : _v(nullptr, 0) {}
    public:
        iterator() : iterator(end_iterator_tag()) {}
        iterator& operator++() {
            read_current();
            return *this;
        }
        iterator operator++(int) {
            iterator i(*this);
            ++(*this);
            return i;
        }
        const value_type& operator*() const { return _current; }
        const value_type* operator->() const { return &_current; }
        bool operator==(const iterator& i) const { return _v.begin() == i._v.begin(); }
        friend class composite;
        friend class composite_view;
    };
     ;
    
    explicit operator bytes_view() const {
        return _bytes;
    }
    template <typename Component>
    friend inline std::ostream& operator<<(std::ostream& os, const std::pair<Component, eoc>& c) {
        fmt::print(os, "{}", c);
        return os;
    }
    struct tri_compare {
        const std::vector<data_type>& _types;
        tri_compare(const std::vector<data_type>& types) : _types(types) {}
        std::strong_ordering operator()(const composite&, const composite&) const;
        std::strong_ordering operator()(composite_view, composite_view) const;
    };
};
template <typename Component>
struct fmt::formatter<std::pair<Component, composite::eoc>> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const std::pair<Component, composite::eoc>& c, FormatContext& ctx) const {
        if constexpr (std::same_as<Component, bytes_view>) {
            return fmt::format_to(ctx.out(), "{{value={}; eoc={:#02x}}}",
                                  fmt_hex(c.first), composite::eoc_type(c.second) & 0xff);
        } else {
            return fmt::format_to(ctx.out(), "{{value={}; eoc={:#02x}}}",
                                  c.first, composite::eoc_type(c.second) & 0xff);
        }
    }
};
class composite_view final {
    friend class composite;
    bytes_view _bytes;
    bool _is_compound;
public:
    
    composite_view(const composite& c) 
    ;
    
    
    
    
    boost::iterator_range<composite::iterator> components() const ;
    
    
    
    bool empty() const ;
    bool is_static() const ;
    
    explicit operator bytes_view() const {
        return _bytes;
    }
    bool operator==(const composite_view& k) const { return k._bytes == _bytes && k._is_compound == _is_compound; }
    friend fmt::formatter<composite_view>;
};
template <>
struct fmt::formatter<composite_view> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const composite_view& v, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{{{}, compound={}, static={}}}",
                              fmt::join(v.components(), ", "), v._is_compound, v.is_static());
    }
};
template <>
struct fmt::formatter<composite> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const composite& v, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", composite_view(v));
    }
};
inline
std::strong_ordering composite::tri_compare::operator()(const composite& v1, const composite& v2) const {
    return (*this)(composite_view(v1), composite_view(v2));
}
inline
std::strong_ordering composite::tri_compare::operator()(composite_view v1, composite_view v2) const {
    // See org.apache.cassandra.db.composites.AbstractCType#compare
    if (v1.empty()) {
        return v2.empty() ? std::strong_ordering::equal : std::strong_ordering::less;
    }
    if (v2.empty()) {
        return std::strong_ordering::greater;
    }
    if (v1.is_static() != v2.is_static()) {
        return v1.is_static() ? std::strong_ordering::less : std::strong_ordering::greater;
    }
    auto a_values = v1.components();
    auto b_values = v2.components();
    auto cmp = [&](const data_type& t, component_view c1, component_view c2) {
        // First by value, then by EOC
        auto r = t->compare(c1.first, c2.first);
        if (r != 0) {
            return r;
        }
        return (static_cast<int>(c1.second) - static_cast<int>(c2.second)) <=> 0;
    };
    return lexicographical_tri_compare(_types.begin(), _types.end(),
        a_values.begin(), a_values.end(),
        b_values.begin(), b_values.end(),
        cmp);
}
namespace utils {
namespace utf8 {
namespace internal {
struct partial_validation_results {
    bool error;
    size_t unvalidated_tail;
    size_t bytes_needed_for_tail;
};

}
// If data represents a correct UTF-8 string, return std::nullopt,
// otherwise return a position of first error byte.

} // namespace utf8
} // namespace utils
namespace replica {
// replica/database.hh
class database;
class keyspace;
class table;
using column_family = table;
class memtable_list;
}
// mutation.hh
class mutation;
class mutation_partition;
// schema/schema.hh
class schema;
class column_definition;
class column_mapping;
// schema_mutations.hh
class schema_mutations;
// keys.hh
class exploded_clustering_prefix;
class partition_key;
class partition_key_view;
class clustering_key_prefix;
class clustering_key_prefix_view;
using clustering_key = clustering_key_prefix;
using clustering_key_view = clustering_key_prefix_view;
// memtable.hh
namespace replica {
class memtable;
}
//
// This header defines type system for primary key holders.
//
// We distinguish partition keys and clustering keys. API-wise they are almost
// the same, but they're separate type hierarchies.
//
// Clustering keys are further divided into prefixed and non-prefixed (full).
// Non-prefixed keys always have full component set, as defined by schema.
// Prefixed ones can have any number of trailing components missing. They may
// differ in underlying representation.
//
// The main classes are:
//
//   partition_key           - full partition key
//   clustering_key          - full clustering key
//   clustering_key_prefix   - clustering key prefix
//
// These classes wrap only the minimum information required to store the key
// (the key value itself). Any information which can be inferred from schema
// is not stored. Therefore accessors need to be provided with a pointer to
// schema, from which information about structure is extracted.
// Abstracts a view to serialized compound.
template <typename TopLevelView>
class compound_view_wrapper {
protected:
    managed_bytes_view _bytes;
protected:
    compound_view_wrapper(managed_bytes_view v)
        : _bytes(v)
    { }
    
public:
    
    managed_bytes_view representation() const ;
    struct less_compare {
        typename TopLevelView::compound _t;
    };
    struct tri_compare {
        typename TopLevelView::compound _t;
    };
    struct hashing {
        typename TopLevelView::compound _t;
    };
    struct equality {
        typename TopLevelView::compound _t;
    };
    // begin() and end() return iterators over components of this compound. The iterator yields a managed_bytes_view to the component.
    // The iterators satisfy InputIterator concept.
    // See begin()
    // begin() and end() return iterators over components of this compound. The iterator yields a managed_bytes_view to the component.
    // The iterators satisfy InputIterator concept.
    // See begin()
    // Returns a range of managed_bytes_view
    // Returns a range of managed_bytes_view
    
    
    // For backward compatibility with existing code.
    
};
template <typename TopLevel, typename TopLevelView>
class compound_wrapper {
protected:
    managed_bytes _bytes;
protected:
    compound_wrapper(managed_bytes&& b) : _bytes(std::move(b)) {}
    static inline const auto& get_compound_type(const schema& s) {
        return TopLevel::get_compound_type(s);
    }
private:
    
public:
    struct with_schema_wrapper {
        
        const schema& s;
        const TopLevel& key;
    };
    with_schema_wrapper with_schema(const schema& s) const ;
    static TopLevel make_empty() {
        return from_exploded(std::vector<bytes>());
    }
    
    template<typename RangeOfSerializedComponents>
    static TopLevel from_exploded(RangeOfSerializedComponents&& v) {
        return TopLevel::from_range(std::forward<RangeOfSerializedComponents>(v));
    }
    static TopLevel from_exploded(const schema& s, const std::vector<bytes>& v) {
        return from_exploded(v);
    }
    
    
    // We don't allow optional values, but provide this method as an efficient adaptor
    
    
    static TopLevel from_deeply_exploded(const schema& s, const std::vector<data_value>& v) ;
     ;
    TopLevelView view() const {
        return TopLevelView::from_bytes(_bytes);
    }
    operator TopLevelView() const {
        return view();
    }
    // FIXME: return views
    
    std::vector<bytes> explode() const {
        std::vector<bytes> result;
        for (managed_bytes_view c : components()) {
            result.emplace_back(to_bytes(c));
        }
        return result;
    }
    
    struct tri_compare {
        typename TopLevel::compound _t;
    };
    struct less_compare {
        typename TopLevel::compound _t;
        less_compare(const schema& s) : _t(get_compound_type(s)) {}
        bool operator()(const TopLevel& k1, const TopLevel& k2) const {
            return _t->less(k1.representation(), k2.representation());
        }
    };
    struct hashing {
        typename TopLevel::compound _t;
    };
    struct equality {
        typename TopLevel::compound _t;
        
        
        
        
    };
    bool equal(const schema& s, const TopLevel& other) const {
        return get_compound_type(s)->equal(representation(), other.representation());
    }
    
    operator managed_bytes_view() const
    ;
    const managed_bytes& representation() const {
        return _bytes;
    }
    // begin() and end() return iterators over components of this compound. The iterator yields a managed_bytes_view to the component.
    // The iterators satisfy InputIterator concept.
    auto begin(const schema& s) const {
        return get_compound_type(s)->begin(_bytes);
    }
    // See begin()
    auto end(const schema& s) const {
        return get_compound_type(s)->end(_bytes);
    }
    bool is_empty() const {
        return _bytes.empty();
    }
    
    // For backward compatibility with existing code.
    bool is_empty(const schema& s) const {
        return is_empty();
    }
    // Returns a range of managed_bytes_view
    auto components() const {
        return TopLevelView::compound::element_type::components(representation());
    }
    // Returns a range of managed_bytes_view
    auto components(const schema& s) const {
        return components();
    }
    
    // Returns the number of components of this compound.
    size_t size(const schema& s) const {
        return std::distance(begin(s), end(s));
    }
    
};
template <typename TopLevel, typename PrefixTopLevel>
class prefix_view_on_full_compound {
public:
    using iterator = typename compound_type<allow_prefixes::no>::iterator;
private:
    bytes_view _b;
    unsigned _prefix_len;
    iterator _begin;
    iterator _end;
public:
    struct less_compare_with_prefix {
        typename PrefixTopLevel::compound prefix_type;
    };
};
template <typename TopLevel>
class prefix_view_on_prefix_compound {
public:
    using iterator = typename compound_type<allow_prefixes::yes>::iterator;
private:
    bytes_view _b;
    unsigned _prefix_len;
    iterator _begin;
    iterator _end;
public:
    struct less_compare_with_prefix {
        typename TopLevel::compound prefix_type;
    };
};
template <typename TopLevel, typename TopLevelView, typename PrefixTopLevel>
class prefixable_full_compound : public compound_wrapper<TopLevel, TopLevelView> {
    using base = compound_wrapper<TopLevel, TopLevelView>;
protected:
public:
    using prefix_view_type = prefix_view_on_full_compound<TopLevel, PrefixTopLevel>;
    struct less_compare_with_prefix {
        typename PrefixTopLevel::compound prefix_type;
        typename TopLevel::compound full_type;
    };
    // In prefix equality two sequences are equal if any of them is a prefix
    // of the other. Otherwise lexicographical ordering is applied.
    // Note: full compounds sorted according to lexicographical ordering are also
    // sorted according to prefix equality ordering.
    struct prefix_equality_less_compare {
        typename PrefixTopLevel::compound prefix_type;
        typename TopLevel::compound full_type;
    };
};
template <typename TopLevel, typename FullTopLevel>
class prefix_compound_view_wrapper : public compound_view_wrapper<TopLevel> {
    using base = compound_view_wrapper<TopLevel>;
protected:
public:
};
template <typename TopLevel, typename TopLevelView, typename FullTopLevel>
class prefix_compound_wrapper : public compound_wrapper<TopLevel, TopLevelView> {
    using base = compound_wrapper<TopLevel, TopLevelView>;
protected:
    prefix_compound_wrapper(managed_bytes&& b) : base(std::move(b)) {}
public:
    using prefix_view_type = prefix_view_on_prefix_compound<TopLevel>;
    
    bool is_full(const schema& s) const {
        return TopLevel::get_compound_type(s)->is_full(base::_bytes);
    }
    // Can be called only if is_full()
    // In prefix equality two sequences are equal if any of them is a prefix
    // of the other. Otherwise lexicographical ordering is applied.
    // Note: full compounds sorted according to lexicographical ordering are also
    // sorted according to prefix equality ordering.
    struct prefix_equality_less_compare {
        typename TopLevel::compound prefix_type;
    };
    // See prefix_equality_less_compare.
    struct prefix_equal_tri_compare {
        typename TopLevel::compound prefix_type;
        
        
    };
};
class partition_key_view : public compound_view_wrapper<partition_key_view> {
public:
    using c_type = compound_type<allow_prefixes::no>;
private:
    partition_key_view(managed_bytes_view v)
        : compound_view_wrapper<partition_key_view>(v)
    { }
public:
    using compound = lw_shared_ptr<c_type>;
    static partition_key_view from_bytes(managed_bytes_view v) {
        return { v };
    }
    static const compound& get_compound_type(const schema& s) {
        return s.partition_key_type();
    }
    // Returns key's representation which is compatible with Origin.
    // The result is valid as long as the schema is live.
    const legacy_compound_view<c_type> legacy_form(const schema& s) const;
    // A trichotomic comparator for ordering compatible with Origin.
    
    // Checks if keys are equal in a way which is compatible with Origin.
    
    
    // A trichotomic comparator which orders keys according to their ordering on the ring.
    
};
template <>
struct fmt::formatter<partition_key_view> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const partition_key_view& pk, FormatContext& ctx) const {
        return with_linearized(pk.representation(), [&] (bytes_view v) {
            return fmt::format_to(ctx.out(), "pk{{{}}}", fmt_hex(v));
        });
    }
};
class partition_key : public compound_wrapper<partition_key, partition_key_view> {
    explicit partition_key(managed_bytes&& b)
        : compound_wrapper<partition_key, partition_key_view>(std::move(b))
    { }
public:
    using c_type = compound_type<allow_prefixes::no>;
    template<typename RangeOfSerializedComponents>
    static partition_key from_range(RangeOfSerializedComponents&& v) {
        return partition_key(managed_bytes(c_type::serialize_value(std::forward<RangeOfSerializedComponents>(v))));
    }
    
    
    using compound = lw_shared_ptr<c_type>;
    
    static const compound& get_compound_type(const schema& s) ;
    // Returns key's representation which is compatible with Origin.
    // The result is valid as long as the schema is live.
    
    // A trichotomic comparator for ordering compatible with Origin.
    
    // Checks if keys are equal in a way which is compatible with Origin.
    
    
};
template <>
struct fmt::formatter<partition_key> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const partition_key& pk, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "pk{{{}}}", managed_bytes_view(pk.representation()));
    }
};
namespace detail {
template <typename WithSchemaWrapper, typename FormatContext>
auto format_pk(const WithSchemaWrapper& pk, FormatContext& ctx) ;
} // namespace detail
template <>
struct fmt::formatter<partition_key::with_schema_wrapper> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const partition_key::with_schema_wrapper& pk, FormatContext& ctx) const {
        return ::detail::format_pk(pk, ctx);
    }
};
class exploded_clustering_prefix {
    std::vector<bytes> _v;
public:
    
    
    
};
class clustering_key_prefix_view : public prefix_compound_view_wrapper<clustering_key_prefix_view, clustering_key> {
public:
    using compound = lw_shared_ptr<compound_type<allow_prefixes::yes>>;
};
class clustering_key_prefix : public prefix_compound_wrapper<clustering_key_prefix, clustering_key_prefix_view, clustering_key> {
    explicit clustering_key_prefix(managed_bytes&& b)
            : prefix_compound_wrapper<clustering_key_prefix, clustering_key_prefix_view, clustering_key>(std::move(b))
    { }
public:
    template<typename RangeOfSerializedComponents>
    static clustering_key_prefix from_range(RangeOfSerializedComponents&& v) {
        return clustering_key_prefix(compound::element_type::serialize_value(std::forward<RangeOfSerializedComponents>(v)));
    }
    clustering_key_prefix(std::vector<bytes> v)
        : prefix_compound_wrapper(compound::element_type::serialize_value(std::move(v)))
    { }
    
    
    clustering_key_prefix(clustering_key_prefix&& v) = default;
    clustering_key_prefix(const clustering_key_prefix& v) = default;
    clustering_key_prefix(clustering_key_prefix& v) = default;
    
    clustering_key_prefix& operator=(clustering_key_prefix&) = default;
    clustering_key_prefix& operator=(clustering_key_prefix&&) = default;
    using compound = lw_shared_ptr<compound_type<allow_prefixes::yes>>;
    static const compound& get_compound_type(const schema& s) {
        return s.clustering_key_prefix_type();
    }
    
    
};
template <>
struct fmt::formatter<clustering_key_prefix> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const clustering_key_prefix& ckp, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "ckp{{{}}}", managed_bytes_view(ckp.representation()));
    }
};
template <>
struct fmt::formatter<clustering_key_prefix::with_schema_wrapper> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const clustering_key_prefix::with_schema_wrapper& pk, FormatContext& ctx) const {
        return ::detail::format_pk(pk, ctx);
    }
};
template<>
struct appending_hash<partition_key_view> {
     ;
};
template<>
struct appending_hash<partition_key> {
    template<typename Hasher>
    void operator()(Hasher& h, const partition_key& pk, const schema& s) const ;
};
template<>
struct appending_hash<clustering_key_prefix_view> {
     ;
};
template<>
struct appending_hash<clustering_key_prefix> {
    template<typename Hasher>
    void operator()(Hasher& h, const clustering_key_prefix& ck, const schema& s) const ;
};
template <typename Comparator, typename T>
concept IntervalComparatorFor = requires (T a, T b, Comparator& cmp) {
    { cmp(a, b) } -> std::same_as<std::strong_ordering>;
};
template <typename LessComparator, typename T>
concept IntervalLessComparatorFor = requires (T a, T b, LessComparator& cmp) {
    { cmp(a, b) } -> std::same_as<bool>;
};

template<typename T>
class interval_bound {
    T _value;
    bool _inclusive;
public:
    interval_bound(T value, bool inclusive = true)
              : _value(std::move(value))
              , _inclusive(inclusive)
    { }
    const T& value() const & { return _value; }
    
    bool is_inclusive() const { return _inclusive; }
    
    
};
template<typename T>
class nonwrapping_interval;
template <typename T>
using interval = nonwrapping_interval<T>;
// An interval which can have inclusive, exclusive or open-ended bounds on each end.
// The end bound can be smaller than the start bound.
template<typename T>
class wrapping_interval {
    template <typename U>
    using optional = std::optional<U>;
public:
    using bound = interval_bound<T>;
    template <typename Transformer>
    using transformed_type = typename std::remove_cv_t<std::remove_reference_t<std::result_of_t<Transformer(T)>>>;
private:
    optional<bound> _start;
    optional<bound> _end;
    bool _singular;
public:
    wrapping_interval(optional<bound> start, optional<bound> end, bool singular = false)
        : _start(std::move(start))
        , _singular(singular) {
        if (!_singular) {
            _end = std::move(end);
        }
    }
    wrapping_interval(T value)
        : _start(bound(std::move(value), true))
        , _end()
        , _singular(true)
    { }
    wrapping_interval() : wrapping_interval({}, {}) { }
private:
    // Bound wrappers for compile-time dispatch and safety.
    struct start_bound_ref { const optional<bound>& b; };
    struct end_bound_ref { const optional<bound>& b; };
    
    
    
public:
    // the point is before the interval (works only for non wrapped intervals)
    // Comparator must define a total ordering on T.
    // the point is after the interval (works only for non wrapped intervals)
    // Comparator must define a total ordering on T.
    // check if two intervals overlap.
    // Comparator must define a total ordering on T.
    
    
    bool is_singular() const {
        return _singular;
    }
    
    
    const optional<bound>& start() const {
        return _start;
    }
    const optional<bound>& end() const {
        return _singular ? _start : _end;
    }
    // Range is a wrap around if end value is smaller than the start value
    // or they're equal and at least one bound is not inclusive.
    // Comparator must define a total ordering on T.
    bool is_wrap_around(IntervalComparatorFor<T> auto&& cmp) const ;
    // Converts a wrap-around interval to two non-wrap-around intervals.
    // The returned intervals are not overlapping and ordered.
    // Call only when is_wrap_around().
    std::pair<wrapping_interval, wrapping_interval> unwrap() const ;
    // the point is inside the interval
    // Comparator must define a total ordering on T.
    
    // Returns true iff all values contained by other are also contained by this.
    // Comparator must define a total ordering on T.
    bool contains(const wrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const ;
    // Returns intervals which cover all values covered by this interval but not covered by the other interval.
    // Ranges are not overlapping and ordered.
    // Comparator must define a total ordering on T.
    
    // split interval in two around a split_point. split_point has to be inside the interval
    // split_point will belong to first interval
    // Comparator must define a total ordering on T.
    
    // Create a sub-interval including values greater than the split_point. Returns std::nullopt if
    // split_point is after the end (but not included in the interval, in case of wraparound intervals)
    // Comparator must define a total ordering on T.
    
    template<typename Bound, typename Transformer, typename U = transformed_type<Transformer>>
    static std::optional<typename wrapping_interval<U>::bound> transform_bound(Bound&& b, Transformer&& transformer) {
        if (b) {
            return { { transformer(std::forward<Bound>(b).value().value()), b->is_inclusive() } };
        };
        return {};
    }
    // Transforms this interval into a new interval of a different value type
    // Supplied transformer should transform value of type T (the old type) into value of type U (the new type).
     ;
    template<typename Transformer, typename U = transformed_type<Transformer>>
    wrapping_interval<U> transform(Transformer&& transformer) const & {
        return wrapping_interval<U>(transform_bound(_start, transformer), transform_bound(_end, transformer), _singular);
    }
    
    bool operator==(const wrapping_interval& other) const ;
    ;
private:
    friend class nonwrapping_interval<T>;
};
 ;
// An interval which can have inclusive, exclusive or open-ended bounds on each end.
// The end bound can never be smaller than the start bound.
template<typename T>
class nonwrapping_interval {
    template <typename U>
    using optional = std::optional<U>;
public:
    using bound = interval_bound<T>;
    template <typename Transformer>
    using transformed_type = typename wrapping_interval<T>::template transformed_type<Transformer>;
private:
    wrapping_interval<T> _interval;
public:
    // Can only be called if start <= end. IDL ctor.
    nonwrapping_interval(optional<bound> start, optional<bound> end, bool singular = false)
        : _interval(std::move(start), std::move(end), singular)
    { }
    // Can only be called if !r.is_wrap_around().
    explicit nonwrapping_interval(wrapping_interval<T>&& r)
        : _interval(std::move(r))
    { }
    // Can only be called if !r.is_wrap_around().
    explicit nonwrapping_interval(const wrapping_interval<T>& r)
        : _interval(r)
    { }
    operator wrapping_interval<T>() const & {
        return _interval;
    }
    operator wrapping_interval<T>() && {
        return std::move(_interval);
    }
    // the point is before the interval.
    // Comparator must define a total ordering on T.
    bool before(const T& point, IntervalComparatorFor<T> auto&& cmp) const {
        return _interval.before(point, std::forward<decltype(cmp)>(cmp));
    }
    // the point is after the interval.
    // Comparator must define a total ordering on T.
    bool after(const T& point, IntervalComparatorFor<T> auto&& cmp) const {
        return _interval.after(point, std::forward<decltype(cmp)>(cmp));
    }
    // check if two intervals overlap.
    // Comparator must define a total ordering on T.
    bool overlaps(const nonwrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        // if both this and other have an open start, the two intervals will overlap.
        if (!start() && !other.start()) {
            return true;
        }
        return wrapping_interval<T>::greater_than_or_equal(_interval.end_bound(), other._interval.start_bound(), cmp)
            && wrapping_interval<T>::greater_than_or_equal(other._interval.end_bound(), _interval.start_bound(), cmp);
    }
    static nonwrapping_interval make(bound start, bound end) {
        return nonwrapping_interval({std::move(start)}, {std::move(end)});
    }
    static nonwrapping_interval make_open_ended_both_sides() {
        return {{}, {}};
    }
    static nonwrapping_interval make_singular(T value) {
        return {std::move(value)};
    }
    static nonwrapping_interval make_starting_with(bound b) {
        return {{std::move(b)}, {}};
    }
    static nonwrapping_interval make_ending_with(bound b) {
        return {{}, {std::move(b)}};
    }
    bool is_singular() const {
        return _interval.is_singular();
    }
    bool is_full() const {
        return _interval.is_full();
    }
    const optional<bound>& start() const {
        return _interval.start();
    }
    const optional<bound>& end() const {
        return _interval.end();
    }
    // the point is inside the interval
    // Comparator must define a total ordering on T.
    bool contains(const T& point, IntervalComparatorFor<T> auto&& cmp) const {
        return !before(point, cmp) && !after(point, cmp);
    }
    // Returns true iff all values contained by other are also contained by this.
    // Comparator must define a total ordering on T.
    bool contains(const nonwrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        return wrapping_interval<T>::less_than_or_equal(_interval.start_bound(), other._interval.start_bound(), cmp)
                && wrapping_interval<T>::greater_than_or_equal(_interval.end_bound(), other._interval.end_bound(), cmp);
    }
    // Returns intervals which cover all values covered by this interval but not covered by the other interval.
    // Ranges are not overlapping and ordered.
    // Comparator must define a total ordering on T.
    std::vector<nonwrapping_interval> subtract(const nonwrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        auto subtracted = _interval.subtract(other._interval, std::forward<decltype(cmp)>(cmp));
        return boost::copy_range<std::vector<nonwrapping_interval>>(subtracted | boost::adaptors::transformed([](auto&& r) {
            return nonwrapping_interval(std::move(r));
        }));
    }
    // split interval in two around a split_point. split_point has to be inside the interval
    // split_point will belong to first interval
    // Comparator must define a total ordering on T.
    std::pair<nonwrapping_interval<T>, nonwrapping_interval<T>> split(const T& split_point, IntervalComparatorFor<T> auto&& cmp) const {
        assert(contains(split_point, std::forward<decltype(cmp)>(cmp)));
        nonwrapping_interval left(start(), bound(split_point));
        nonwrapping_interval right(bound(split_point, false), end());
        return std::make_pair(std::move(left), std::move(right));
    }
    // Create a sub-interval including values greater than the split_point. If split_point is after
    // the end, returns std::nullopt.
    std::optional<nonwrapping_interval> split_after(const T& split_point, IntervalComparatorFor<T> auto&& cmp) const {
        if (end() && cmp(split_point, end()->value()) >= 0) {
            return std::nullopt;
        } else if (start() && cmp(split_point, start()->value()) < 0) {
            return *this;
        } else {
            return nonwrapping_interval(interval_bound<T>(split_point, false), end());
        }
    }
    // Creates a new sub-interval which is the intersection of this interval and an interval starting with "start".
    // If there is no overlap, returns std::nullopt.
    std::optional<nonwrapping_interval> trim_front(std::optional<bound>&& start, IntervalComparatorFor<T> auto&& cmp) const {
        return intersection(nonwrapping_interval(std::move(start), {}), cmp);
    }
    // Transforms this interval into a new interval of a different value type
    // Supplied transformer should transform value of type T (the old type) into value of type U (the new type).
    template<typename Transformer, typename U = transformed_type<Transformer>>
    nonwrapping_interval<U> transform(Transformer&& transformer) && {
        return nonwrapping_interval<U>(std::move(_interval).transform(std::forward<Transformer>(transformer)));
    }
    template<typename Transformer, typename U = transformed_type<Transformer>>
    nonwrapping_interval<U> transform(Transformer&& transformer) const & {
        return nonwrapping_interval<U>(_interval.transform(std::forward<Transformer>(transformer)));
    }
    bool equal(const nonwrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        return _interval.equal(other._interval, std::forward<decltype(cmp)>(cmp));
    }
    bool operator==(const nonwrapping_interval& other) const {
        return _interval == other._interval;
    }
    // Takes a vector of possibly overlapping intervals and returns a vector containing
    // a set of non-overlapping intervals covering the same values.
    template<IntervalComparatorFor<T> Comparator, typename IntervalVec>
    requires requires (IntervalVec vec) {
        { vec.begin() } -> std::random_access_iterator;
        { vec.end() } -> std::random_access_iterator;
        { vec.reserve(1) };
        { vec.front() } -> std::same_as<nonwrapping_interval&>;
    }
    static IntervalVec deoverlap(IntervalVec intervals, Comparator&& cmp) {
        auto size = intervals.size();
        if (size <= 1) {
            return intervals;
        }
        std::sort(intervals.begin(), intervals.end(), [&](auto&& r1, auto&& r2) {
            return wrapping_interval<T>::less_than(r1._interval.start_bound(), r2._interval.start_bound(), cmp);
        });
        IntervalVec deoverlapped_intervals;
        deoverlapped_intervals.reserve(size);
        auto&& current = intervals[0];
        for (auto&& r : intervals | boost::adaptors::sliced(1, intervals.size())) {
            bool includes_end = wrapping_interval<T>::greater_than_or_equal(r._interval.end_bound(), current._interval.start_bound(), cmp)
                                && wrapping_interval<T>::greater_than_or_equal(current._interval.end_bound(), r._interval.end_bound(), cmp);
            if (includes_end) {
                continue; // last.start <= r.start <= r.end <= last.end
            }
            bool includes_start = wrapping_interval<T>::greater_than_or_equal(current._interval.end_bound(), r._interval.start_bound(), cmp);
            if (includes_start) {
                current = nonwrapping_interval(std::move(current.start()), std::move(r.end()));
            } else {
                deoverlapped_intervals.emplace_back(std::move(current));
                current = std::move(r);
            }
        }
        deoverlapped_intervals.emplace_back(std::move(current));
        return deoverlapped_intervals;
    }
private:
    // These private functions optimize the case where a sequence supports the
    // lower and upper bound operations more efficiently, as is the case with
    // some boost containers.
    struct std_ {};
    struct built_in_ : std_ {};
    template<typename Range, IntervalLessComparatorFor<T> LessComparator,
             typename = decltype(std::declval<Range>().lower_bound(std::declval<T>(), std::declval<LessComparator>()))>
    typename std::remove_reference<Range>::type::const_iterator do_lower_bound(const T& value, Range&& r, LessComparator&& cmp, built_in_) const {
        return r.lower_bound(value, std::forward<LessComparator>(cmp));
    }
    template<typename Range, IntervalLessComparatorFor<T> LessComparator,
             typename = decltype(std::declval<Range>().upper_bound(std::declval<T>(), std::declval<LessComparator>()))>
    typename std::remove_reference<Range>::type::const_iterator do_upper_bound(const T& value, Range&& r, LessComparator&& cmp, built_in_) const {
        return r.upper_bound(value, std::forward<LessComparator>(cmp));
    }
    template<typename Range, IntervalLessComparatorFor<T> LessComparator>
    typename std::remove_reference<Range>::type::const_iterator do_lower_bound(const T& value, Range&& r, LessComparator&& cmp, std_) const {
        return std::lower_bound(r.begin(), r.end(), value, std::forward<LessComparator>(cmp));
    }
    template<typename Range, IntervalLessComparatorFor<T> LessComparator>
    typename std::remove_reference<Range>::type::const_iterator do_upper_bound(const T& value, Range&& r, LessComparator&& cmp, std_) const {
        return std::upper_bound(r.begin(), r.end(), value, std::forward<LessComparator>(cmp));
    }
public:
    // Return the lower bound of the specified sequence according to these bounds.
    template<typename Range, IntervalLessComparatorFor<T> LessComparator>
    typename std::remove_reference<Range>::type::const_iterator lower_bound(Range&& r, LessComparator&& cmp) const {
        return start()
            ? (start()->is_inclusive()
                ? do_lower_bound(start()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_())
                : do_upper_bound(start()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_()))
            : std::cbegin(r);
    }
    // Return the upper bound of the specified sequence according to these bounds.
    template<typename Range, IntervalLessComparatorFor<T> LessComparator>
    typename std::remove_reference<Range>::type::const_iterator upper_bound(Range&& r, LessComparator&& cmp) const {
        return end()
             ? (end()->is_inclusive()
                ? do_upper_bound(end()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_())
                : do_lower_bound(end()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_()))
             : (is_singular()
                ? do_upper_bound(start()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_())
                : std::cend(r));
    }
    // Returns a subset of the range that is within these bounds.
    template<typename Range, IntervalLessComparatorFor<T> LessComparator>
    boost::iterator_range<typename std::remove_reference<Range>::type::const_iterator>
    slice(Range&& range, LessComparator&& cmp) const {
        return boost::make_iterator_range(lower_bound(range, cmp), upper_bound(range, cmp));
    }
    // Returns the intersection between this interval and other.
    std::optional<nonwrapping_interval> intersection(const nonwrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        auto p = std::minmax(_interval, other._interval, [&cmp] (auto&& a, auto&& b) {
            return wrapping_interval<T>::less_than(a.start_bound(), b.start_bound(), cmp);
        });
        if (wrapping_interval<T>::greater_than_or_equal(p.first.end_bound(), p.second.start_bound(), cmp)) {
            auto end = std::min(p.first.end_bound(), p.second.end_bound(), [&cmp] (auto&& a, auto&& b) {
                return !wrapping_interval<T>::greater_than_or_equal(a, b, cmp);
            });
            return nonwrapping_interval(p.second.start(), end.b);
        }
        return {};
    }
    template<typename U>
    friend std::ostream& operator<<(std::ostream& out, const nonwrapping_interval<U>& r);
};
 ;
template<template<typename> typename T, typename U>
concept Interval = std::is_same<T<U>, wrapping_interval<U>>::value || std::is_same<T<U>, nonwrapping_interval<U>>::value;
// Allow using interval<T> in a hash table. The hash function 31 * left +
// right is the same one used by Cassandra's AbstractBounds.hashCode().
namespace std {
template<typename T>
struct hash<wrapping_interval<T>> {
    using argument_type = wrapping_interval<T>;
    using result_type = decltype(std::hash<T>()(std::declval<T>()));
    result_type operator()(argument_type const& s) const {
        auto hash = std::hash<T>();
        auto left = s.start() ? hash(s.start()->value()) : 0;
        auto right = s.end() ? hash(s.end()->value()) : 0;
        return 31 * left + right;
    }
};
template<typename T>
struct hash<nonwrapping_interval<T>> {
    using argument_type = nonwrapping_interval<T>;
    using result_type = decltype(std::hash<T>()(std::declval<T>()));
    result_type operator()(argument_type const& s) const {
        return hash<wrapping_interval<T>>()(s);
    }
};
}
// range.hh is deprecated and should be replaced with interval.hh
template <typename T>
using range_bound = interval_bound<T>;
template <typename T>
using nonwrapping_range = interval<T>;
template <typename T>
using wrapping_range = wrapping_interval<T>;
template <typename T>
using range = wrapping_interval<T>;
template <template<typename> typename T, typename U>
concept Range = Interval<T, U>;
namespace dht {
class token;
enum class token_kind {
    before_all_keys,
    key,
    after_all_keys,
};
class token {
    // INT64_MIN is not a legal token, but a special value used to represent
    // infinity in token intervals.
    // If a token with value INT64_MIN is generated by the hashing algorithm,
    // the result is coerced into INT64_MAX.
    // (So INT64_MAX is twice as likely as every other token.)
    static inline int64_t normalize(int64_t t) {
        return t == std::numeric_limits<int64_t>::min() ? std::numeric_limits<int64_t>::max() : t;
    }
public:
    using kind = token_kind;
    kind _kind;
    int64_t _data;
    token()  ;
    token(kind k, int64_t d)
        : _kind(std::move(k))
        , _data(normalize(d)) { }
    // This constructor seems redundant with the bytes_view constructor, but
    // it's necessary for IDL, which passes a deserialized_bytes_proxy here.
    // (deserialized_bytes_proxy is convertible to bytes&&, but not bytes_view.)
    bool is_minimum() const noexcept {
        return _kind == kind::before_all_keys;
    }
    bool is_maximum() const noexcept {
        return _kind == kind::after_all_keys;
    }
    static int64_t to_int64(token);
    static unsigned shard_of_minimum_token() {
        return 0;  // hardcoded for now; unlikely to change
    }
};
static std::strong_ordering tri_compare_raw(const int64_t l1, const int64_t l2) noexcept ;
template <typename T>
concept TokenCarrier = requires (const T& v) {
    { v.token() } noexcept -> std::same_as<const token&>;
};
struct raw_token_less_comparator {
    bool operator()(const int64_t k1, const int64_t k2) const noexcept ;
    template <typename Key>
    requires TokenCarrier<Key>
    bool operator()(const Key& k1, const int64_t k2) const noexcept {
        return dht::tri_compare_raw(k1.token().raw(), k2) < 0;
    }
    template <typename Key>
    requires TokenCarrier<Key>
    bool operator()(const int64_t k1, const Key& k2) const noexcept {
        return dht::tri_compare_raw(k1, k2.token().raw()) < 0;
    }
    template <typename Key>
    requires TokenCarrier<Key>
    int64_t simplify_key(const Key& k) const noexcept {
        return k.token().raw();
    }
    int64_t simplify_key(int64_t k) const noexcept {
        return k;
    }
};

const token& maximum_token() noexcept;
std::strong_ordering operator<=>(const token& t1, const token& t2);
// Returns a successor for token t.
// The caller must ensure there is a next token, otherwise
// the result is unspecified.
//
// Precondition: t.kind() == dht::token::kind::key
// Returns the smallest token in the ring which can be associated with a partition key.
} // namespace dht
template <>
struct fmt::formatter<dht::token> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const dht::token& t, FormatContext& ctx) const {
        if (t.is_maximum()) {
            return fmt::format_to(ctx.out(), "maximum token");
        } else if (t.is_minimum()) {
            return fmt::format_to(ctx.out(), "minimum token");
        } else {
            return fmt::format_to(ctx.out(), "{}", dht::token::to_int64(t));
        }
    }
};
namespace dht {
 
std::vector<uint64_t> init_zero_based_shard_start(unsigned shards, unsigned sharding_ignore_msb_bits);
unsigned shard_of(unsigned shard_count, unsigned sharding_ignore_msb_bits, const token& t);
token token_for_next_shard(const std::vector<uint64_t>& shard_start, unsigned shard_count, unsigned sharding_ignore_msb_bits, const token& t, shard_id shard, unsigned spans);
class sharder {
protected:
    unsigned _shard_count;
    unsigned _sharding_ignore_msb_bits;
    std::vector<uint64_t> _shard_start;
public:
    sharder(unsigned shard_count = smp::count, unsigned sharding_ignore_msb_bits = 0);
    virtual ~sharder() = default;
    virtual unsigned shard_of(const token& t) const;
    virtual token token_for_next_shard(const token& t, shard_id shard, unsigned spans = 1) const;
    
    
    
};
} //namespace dht
namespace utils {
class can_yield_tag;
using can_yield = seastar::bool_class<can_yield_tag>;
} // namespace utils
namespace sstables {
class key_view;
class decorated_key_view;
}
namespace dht {
//
// Origin uses a complex class hierarchy where Token is an abstract class,
// and various subclasses use different implementations (LongToken vs.
// BigIntegerToken vs. StringToken), plus other variants to to signify the
// the beginning of the token space etc.
//
// We'll fold all of that into the token class and push all of the variations
// into its users.
class decorated_key;
class ring_position;
using partition_range = nonwrapping_range<ring_position>;
using token_range = nonwrapping_range<token>;
using partition_range_vector = std::vector<partition_range>;
using token_range_vector = std::vector<token_range>;
// Wraps partition_key with its corresponding token.
//
// Total ordering defined by comparators is compatible with Origin's ordering.
class decorated_key {
public:
    dht::token _token;
    partition_key _key;
    struct less_comparator {
        schema_ptr s;
        bool operator()(const decorated_key& k1, const decorated_key& k2) const;
        
        
    };
    bool equal(const schema& s, const decorated_key& other) const;
    bool less_compare(const schema& s, const decorated_key& other) const;
    bool less_compare(const schema& s, const ring_position& other) const;
    // Trichotomic comparators defining total ordering on the union of
    // decorated_key and ring_position objects.
    std::strong_ordering tri_compare(const schema& s, const decorated_key& other) const;
    std::strong_ordering tri_compare(const schema& s, const ring_position& other) const;
    const dht::token& token() const noexcept ;
    const partition_key& key() const ;
    size_t external_memory_usage() const ;
    
};
class decorated_key_equals_comparator {
    const schema& _schema;
public:
    
    bool operator()(const dht::decorated_key& k1, const dht::decorated_key& k2) const ;
};
using decorated_key_opt = std::optional<decorated_key>;
class i_partitioner {
public:
    using ptr_type = std::unique_ptr<i_partitioner>;
    i_partitioner() = default;
    virtual ~i_partitioner() {}
    decorated_key decorate_key(const schema& s, const partition_key& key) const ;
    decorated_key decorate_key(const schema& s, partition_key&& key) const {
        auto token = get_token(s, key);
        return { std::move(token), std::move(key) };
    }
    virtual token get_token(const schema& s, partition_key_view key) const = 0;
    virtual token get_token(const sstables::key_view& key) const = 0;
    // FIXME: token.tokenFactory
    //virtual token.tokenFactory gettokenFactory() = 0;
    virtual const sstring name() const = 0;
    bool operator==(const i_partitioner& o) const ;
};
//
// Represents position in the ring of partitions, where partitions are ordered
// according to decorated_key ordering (first by token, then by key value).
// Intended to be used for defining partition ranges.
//
// The 'key' part is optional. When it's absent, this object represents a position
// which is either before or after all keys sharing given token. That's determined
// by relation_to_keys().
//
// For example for the following data:
//
//   tokens: |    t1   | t2 |
//           +----+----+----+
//   keys:   | k1 | k2 | k3 |
//
// The ordering is:
//
//   ring_position(t1, token_bound::start) < ring_position(k1)
//   ring_position(k1)                     < ring_position(k2)
//   ring_position(k1)                     == decorated_key(k1)
//   ring_position(k2)                     == decorated_key(k2)
//   ring_position(k2)                     < ring_position(t1, token_bound::end)
//   ring_position(k2)                     < ring_position(k3)
//   ring_position(t1, token_bound::end)   < ring_position(t2, token_bound::start)
//
// Maps to org.apache.cassandra.db.RowPosition and its derivatives in Origin.
//
class ring_position {
public:
    enum class token_bound : int8_t { start = -1, end = 1 };
private:
    friend class ring_position_comparator;
    friend class ring_position_ext;
    dht::token _token;
    token_bound _token_bound{}; // valid when !_key
    std::optional<partition_key> _key;
public:
    
    
    const dht::token& token() const noexcept ;
    // Valid when !has_key()
    // Returns -1 if smaller than keys with the same token, +1 if greater.
    int relation_to_keys() const ;
    const std::optional<partition_key>& key() const ;
    bool has_key() const ;
    // Call only when has_key()
    dht::decorated_key as_decorated_key() const ;
    // Trichotomic comparator defining a total ordering on ring_position objects
    // "less" comparator corresponding to tri_compare()
};
// Non-owning version of ring_position and ring_position_ext.
//
// Unlike ring_position, it can express positions which are right after and right before the keys.
// ring_position still can not because it is sent between nodes and such a position
// would not be (yet) properly interpreted by old nodes. That's why any ring_position
// can be converted to ring_position_view, but not the other way.
//
// It is possible to express a partition_range using a pair of two ring_position_views v1 and v2,
// where v1 = ring_position_view::for_range_start(r) and v2 = ring_position_view::for_range_end(r).
// Such range includes all keys k such that v1 <= k < v2, with order defined by ring_position_comparator.
//
class ring_position_view {
    friend std::strong_ordering ring_position_tri_compare(const schema& s, ring_position_view lh, ring_position_view rh);
    friend class ring_position_comparator;
    friend class ring_position_comparator_for_sstables;
    friend class ring_position_ext;
    // Order is lexicographical on (_token, _key) tuples, where _key part may be missing, and
    // _weight affecting order between tuples if one is a prefix of the other (including being equal).
    // A positive weight puts the position after all strictly prefixed by it, while a non-positive
    // weight puts it before them. If tuples are equal, the order is further determined by _weight.
    //
    // For example {_token=t1, _key=nullptr, _weight=1} is ordered after {_token=t1, _key=k1, _weight=0},
    // but {_token=t1, _key=nullptr, _weight=-1} is ordered before it.
    //
    const dht::token* _token; // always not nullptr
    const partition_key* _key; // Can be nullptr
    int8_t _weight;
private:
    ring_position_view() noexcept : _token(nullptr), _key(nullptr), _weight(0) { }
    explicit operator bool() const noexcept ;
public:
    using token_bound = ring_position::token_bound;
    struct after_key_tag {};
    using after_key = bool_class<after_key_tag>;
    static ring_position_view min() noexcept ;
    // Only when key() == nullptr
    // Only when key() != nullptr
    friend class optimized_optional<ring_position_view>;
};
using ring_position_ext_view = ring_position_view;
using ring_position_view_opt = optimized_optional<ring_position_view>;
//
// Represents position in the ring of partitions, where partitions are ordered
// according to decorated_key ordering (first by token, then by key value).
// Intended to be used for defining partition ranges.
//
// Unlike ring_position, it can express positions which are right after and right before the keys.
// ring_position still can not because it is sent between nodes and such a position
// would not be (yet) properly interpreted by old nodes. That's why any ring_position
// can be converted to ring_position_ext, but not the other way.
//
// It is possible to express a partition_range using a pair of two ring_position_exts v1 and v2,
// where v1 = ring_position_ext::for_range_start(r) and v2 = ring_position_ext::for_range_end(r).
// Such range includes all keys k such that v1 <= k < v2, with order defined by ring_position_comparator.
//
class ring_position_ext {
    // Order is lexicographical on (_token, _key) tuples, where _key part may be missing, and
    // _weight affecting order between tuples if one is a prefix of the other (including being equal).
    // A positive weight puts the position after all strictly prefixed by it, while a non-positive
    // weight puts it before them. If tuples are equal, the order is further determined by _weight.
    //
    // For example {_token=t1, _key=nullptr, _weight=1} is ordered after {_token=t1, _key=k1, _weight=0},
    // but {_token=t1, _key=nullptr, _weight=-1} is ordered before it.
    //
    dht::token _token;
    std::optional<partition_key> _key;
    int8_t _weight;
public:
    using token_bound = ring_position::token_bound;
    struct after_key_tag {};
    using after_key = bool_class<after_key_tag>;
    ring_position_ext(const dht::ring_position& pos, after_key after = after_key::no)
        : _token(pos.token())
        , _key(pos.key())
        , _weight(pos.has_key() ? bool(after) : pos.relation_to_keys())
    { }
    ring_position_ext(const ring_position_ext& pos) = default;
    ring_position_ext& operator=(const ring_position_ext& other) = default;
    ring_position_ext(ring_position_view v)
        : _token(*v._token)
        , _key(v._key ? std::make_optional(*v._key) : std::nullopt)
        , _weight(v._weight)
    { }
    
    // Only when key() == std::nullopt
    // Only when key() != std::nullopt
};
template <typename T>
requires std::is_convertible<T, ring_position_view>::value
ring_position_view ring_position_view_to_compare(const T& val) {
    return val;
}
// Trichotomic comparator for ring order
struct ring_position_comparator {
    const schema& s;
    ring_position_comparator(const schema& s_) : s(s_) {}
    std::strong_ordering operator()(ring_position_view lh, ring_position_view rh) const {
        return ring_position_tri_compare(s, lh, rh);
    }
    template <typename T>
    std::strong_ordering operator()(const T& lh, ring_position_view rh) const {
        return ring_position_tri_compare(s, ring_position_view_to_compare(lh), rh);
    }
    template <typename T>
    std::strong_ordering operator()(ring_position_view lh, const T& rh) const {
        return ring_position_tri_compare(s, lh, ring_position_view_to_compare(rh));
    }
    template <typename T1, typename T2>
    std::strong_ordering operator()(const T1& lh, const T2& rh) const {
        return ring_position_tri_compare(s, ring_position_view_to_compare(lh), ring_position_view_to_compare(rh));
    }
};
struct ring_position_comparator_for_sstables {
    const schema& s;
};
// "less" comparator giving the same order as ring_position_comparator
struct ring_position_less_comparator {
    ring_position_comparator tri;
    
    template<typename T, typename U>
    bool operator()(const T& lh, const U& rh) const ;
};
struct token_comparator {
    // Return values are those of a trichotomic comparison.
    std::strong_ordering operator()(const token& t1, const token& t2) const;
};
std::ostream& operator<<(std::ostream& out, const decorated_key& t);
std::ostream& operator<<(std::ostream& out, const i_partitioner& p);
class partition_ranges_view {
    const dht::partition_range* _data = nullptr;
    size_t _size = 0;
public:
};
std::ostream& operator<<(std::ostream& out, partition_ranges_view v);
unsigned shard_of(const schema&, const token&);
 decorated_key decorate_key(const schema& s, const partition_key& key) ;
inline decorated_key decorate_key(const schema& s, partition_key&& key) {
    return s.get_partitioner().decorate_key(s, std::move(key));
}
 


// Each shard gets a sorted, disjoint vector of ranges

// Intersect a partition_range with a shard and return the the resulting sub-ranges, in sorted order
future<utils::chunked_vector<partition_range>> split_range_to_single_shard(const schema& s, const dht::partition_range& pr, shard_id shard);
std::unique_ptr<dht::i_partitioner> make_partitioner(sstring name);
// Returns a sorted and deoverlapped list of ranges that are
// the result of subtracting all ranges from ranges_to_subtract.
// ranges_to_subtract must be sorted and deoverlapped.
// Returns a token_range vector split based on the given number of most-significant bits
} // dht
namespace std {
template<>
struct hash<dht::token> {
    size_t operator()(const dht::token& t) const {
        // We have to reverse the bytes here to keep compatibility with
        // the behaviour that was here when tokens were represented as
        // sequence of bytes.
        return bswap_64(t._data);
    }
};
template <>
struct hash<dht::decorated_key> {
    size_t operator()(const dht::decorated_key& k) const {
        auto h_token = hash<dht::token>();
        return h_token(k.token());
    }
};
}
namespace gms {
class inet_address {
private:
    net::inet_address _addr;
public:
    explicit inet_address(uint32_t ip) noexcept
        : _addr(net::ipv4_address(ip)) {
    }
    inet_address(const net::inet_address& addr) noexcept : _addr(addr) {}
    inet_address(const socket_address& sa) noexcept
        : inet_address(sa.addr())
    {}
    const net::inet_address& addr() const noexcept ;
    inet_address(const inet_address&) = default;
    operator const seastar::net::inet_address&() const noexcept {
        return _addr;
    }
    // throws std::invalid_argument if sstring is invalid
    inet_address(const sstring& addr) {
        // FIXME: We need a real DNS resolver
        if (addr == "localhost") {
            _addr = net::ipv4_address("127.0.0.1");
        } else {
            _addr = net::inet_address(addr);
        }
    }
    bytes_view bytes() const noexcept {
        return bytes_view(reinterpret_cast<const int8_t*>(_addr.data()), _addr.size());
    }
    // TODO remove
    uint32_t raw_addr() const {
        return addr().as_ipv4_address().ip;
    }
    sstring to_sstring() const;
    friend inline bool operator==(const inet_address& x, const inet_address& y) noexcept = default;
    
    friend struct std::hash<inet_address>;
    using opt_family = std::optional<net::inet_address::family>;
    
};
std::ostream& operator<<(std::ostream& os, const inet_address& x);
}
namespace std {
template<>
struct hash<gms::inet_address> {
    size_t operator()(gms::inet_address a) const noexcept { return std::hash<net::inet_address>()(a._addr); }
};
}
template <>
struct fmt::formatter<gms::inet_address> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const ::gms::inet_address& x, FormatContext& ctx) const {
        if (x.addr().is_ipv4()) {
            return fmt::format_to(ctx.out(), "{}", x.addr());
        }
        // print 2 bytes in a group, and use ':' as the delimeter
        fmt::format_to(ctx.out(), "{:2:}", fmt_hex(x.bytes()));
        if (x.addr().scope() != seastar::net::inet_address::invalid_scope) {
            return fmt::format_to(ctx.out(), "%{}", x.addr().scope());
        }
        return ctx.out();
    }
};
namespace cql3 { class query_processor; }
namespace tracing {
using elapsed_clock = std::chrono::steady_clock;
class trace_state_ptr;
class tracing;
enum class trace_type : uint8_t {
    NONE,
    QUERY,
    REPAIR,
};
extern std::vector<sstring> trace_type_names;
class span_id {
private:
    uint64_t _id = illegal_id;
public:
    static constexpr uint64_t illegal_id = 0;
public:
};
// !!!!IMPORTANT!!!!
//
// The enum_set based on this enum is serialized using IDL, therefore new items
// should always be added to the end of this enum - never before the existing
// ones.
//
// Otherwise this may break IDL's backward compatibility.
enum class trace_state_props {
    write_on_close, primary, log_slow_query, full_tracing, ignore_events
};
using trace_state_props_set = enum_set<super_enum<trace_state_props,
    trace_state_props::write_on_close,
    trace_state_props::primary,
    trace_state_props::log_slow_query,
    trace_state_props::full_tracing,
    trace_state_props::ignore_events>>;
class trace_info {
public:
    utils::UUID session_id;
    trace_type type;
    bool write_on_close;
    trace_state_props_set state_props;
    uint32_t slow_query_threshold_us; // in microseconds
    uint32_t slow_query_ttl_sec; // in seconds
    span_id parent_id;
    uint64_t start_ts_us = 0u; // sentinel value (== "unset")
public:
};
struct one_session_records;
using records_bulk = std::deque<lw_shared_ptr<one_session_records>>;
struct backend_session_state_base {
    ;
};
struct i_tracing_backend_helper {
    using wall_clock = std::chrono::system_clock;
protected:
    tracing& _local_tracing;
public:
    using ptr_type = std::unique_ptr<i_tracing_backend_helper>;
private:
    friend class tracing;
};
struct event_record {
    sstring message;
    elapsed_clock::duration elapsed;
    i_tracing_backend_helper::wall_clock::time_point event_time_point;
};
struct session_record {
    gms::inet_address client;
    // Keep the containers below sorted since some backends require that and
    // it's very cheap to always do that because the amount of elements in a
    // container is very small.
    std::map<sstring, sstring> parameters;
    std::set<sstring> tables;
    sstring username;
    sstring request;
    size_t request_size = 0;
    size_t response_size = 0;
    std::chrono::system_clock::time_point started_at;
    trace_type command = trace_type::NONE;
    elapsed_clock::duration elapsed;
    std::chrono::seconds slow_query_record_ttl;
private:
    bool _consumed = false;
public:
};
class one_session_records {
private:
    shared_ptr<tracing> _local_tracing_ptr;
public:
    utils::UUID session_id;
    session_record session_rec;
    std::chrono::seconds ttl;
    std::deque<event_record> events_recs;
    std::unique_ptr<backend_session_state_base> backend_state_ptr;
    bool do_log_slow_query = false;
    // A pointer to the records counter of the corresponding state new records
    // of this tracing session should consume from (e.g. "cached" or "pending
    // for write").
    uint64_t* budget_ptr;
    // Each tracing session object represents a single tracing span.
    //
    // Each span has a span ID. In order to be able to build a full tree of all
    // spans of the same query we need a parent span ID as well.
    span_id parent_id;
    span_id my_span_id;
private:
    bool _is_pending_for_write = false;
};
class tracing : public seastar::async_sharded_service<tracing> {
public:
    static const gc_clock::duration write_period;
    // maximum number of sessions pending for write per shard
    static constexpr int max_pending_sessions = 1000;
    // expectation of an average number of trace records per session
    static constexpr int exp_trace_events_per_session = 10;
    // maximum allowed pending records per-shard
    static constexpr int max_pending_trace_records = max_pending_sessions * exp_trace_events_per_session;
    // number of pending sessions that would trigger a write event
    static constexpr int write_event_sessions_threshold = 100;
    // number of pending records that would trigger a write event
    static constexpr int write_event_records_threshold = write_event_sessions_threshold * exp_trace_events_per_session;
    // Number of events when an info message is printed
    static constexpr int log_warning_period = 10000;
    static const std::chrono::microseconds default_slow_query_duraion_threshold;
    static const std::chrono::seconds default_slow_query_record_ttl;
    struct stats {
        uint64_t dropped_sessions = 0;
        uint64_t dropped_records = 0;
        uint64_t trace_records_count = 0;
        uint64_t trace_errors = 0;
    } stats;
private:
    // A number of currently active tracing sessions
    uint64_t _active_sessions = 0;
    // Below are 3 counters that describe the total amount of tracing records on
    // this shard. Each counter describes a state in which a record may be.
    //
    // Each record may only be in a specific state at every point of time and
    // thereby it must be accounted only in one and only one of the three
    // counters below at any given time.
    //
    // The sum of all three counters should not be greater than
    // (max_pending_trace_records + write_event_records_threshold) at any time
    // (actually it can get as high as a value above plus (max_pending_sessions)
    // if all sessions are primary but we won't take this into an account for
    // simplicity).
    //
    // The same is about the number of outstanding sessions: it may not be
    // greater than (max_pending_sessions + write_event_sessions_threshold) at
    // any time.
    //
    // If total number of tracing records is greater or equal to the limit
    // above, the new trace point is going to be dropped.
    //
    // If current number or records plus the expected number of trace records
    // per session (exp_trace_events_per_session) is greater than the limit
    // above new sessions will be dropped. A new session will also be dropped if
    // there are too many active sessions.
    //
    // When the record or a session is dropped the appropriate statistics
    // counters are updated and there is a rate-limited warning message printed
    // to the log.
    //
    // Every time a number of records pending for write is greater or equal to
    // (write_event_records_threshold) or a number of sessions pending for
    // write is greater or equal to (write_event_sessions_threshold) a write
    // event is issued.
    //
    // Every 2 seconds a timer would write all pending for write records
    // available so far.
    // Total number of records cached in the active sessions that are not going
    // to be written in the next write event
    uint64_t _cached_records = 0;
    // Total number of records that are currently being written to I/O
    uint64_t _flushing_records = 0;
    // Total number of records in the _pending_for_write_records_bulk. All of
    // them are going to be written to the I/O during the next write event.
    uint64_t _pending_for_write_records_count = 0;
    records_bulk _pending_for_write_records_bulk;
    timer<lowres_clock> _write_timer;
    // _down becomes FALSE after the local service is fully initialized and
    // tracing records are allowed to be created and collected. It becomes TRUE
    // after the shutdown() call and prevents further write attempts to I/O
    // backend.
    bool _down = true;
    // If _slow_query_logging_enabled is enabled, a query processor keeps all
    // trace events related to the query until in the end it can decide
    // if the query was slow to be saved.
    bool _slow_query_logging_enabled = false;
    // If _ignore_trace_events is enabled, tracing::trace ignores all tracing
    // events as well as creating trace_state descendants with trace_info to
    // track tracing sessions only. This is used to implement lightweight
    // slow query tracing.
    bool _ignore_trace_events = false;
    std::unique_ptr<i_tracing_backend_helper> _tracing_backend_helper_ptr;
    sstring _thread_name;
    sstring _tracing_backend_helper_class_name;
    seastar::metrics::metric_groups _metrics;
    double _trace_probability = 0.0; // keep this one for querying purposes
    uint64_t _normalized_trace_probability = 0;
    std::ranlux48_base _gen;
    std::chrono::microseconds _slow_query_duration_threshold;
    std::chrono::seconds _slow_query_record_ttl;
public:
    // Initialize a tracing backend (e.g. tracing_keyspace or logstash)
private:
};
}
namespace ser {
template <typename T>
class serializer;
};
namespace query {
struct max_result_size {
    uint64_t soft_limit;
    uint64_t hard_limit;
private:
    uint64_t page_size = 0;
public:
    max_result_size() = delete;
    explicit max_result_size(uint64_t max_size) : soft_limit(max_size), hard_limit(max_size) { }
    explicit max_result_size(uint64_t soft_limit, uint64_t hard_limit) : soft_limit(soft_limit), hard_limit(hard_limit) { }
    max_result_size(uint64_t soft_limit, uint64_t hard_limit, uint64_t page_size)
            : soft_limit(soft_limit)
            , hard_limit(hard_limit)
            , page_size(page_size)
    { }
    uint64_t get_page_size() const ;
    
    friend class ser::serializer<query::max_result_size>;
};
 
}
namespace db {
using allow_per_partition_rate_limit = seastar::bool_class<class allow_per_partition_rate_limit_tag>;
namespace per_partition_rate_limit {
// Tells the replica to account the operation (increase the corresponding counter)
// and accept it regardless from the value of the counter.
//
// Used when the coordinator IS a replica (correct node and shard).
struct account_only {};
// Tells the replica to account the operation and decide whether to reject
// or not, based on the random variable sent by the coordinator.
//
// Used when the coordinator IS NOT a replica (wrong node or shard).
struct account_and_enforce {
    // A random 32-bit number generated by the coordinator.
    // Replicas are supposed to use it in order to decide whether
    // to accept or reject.
    uint32_t random_variable;
     
};
// std::monostate -> do not count to the rate limit and never reject
// account_and_enforce -> account to the rate limit and optionally reject
using info = std::variant<std::monostate, account_only, account_and_enforce>;
} // namespace per_partition_rate_limit
} // namespace db
using query_id = utils::tagged_uuid<struct query_id_tag>;
using cql_protocol_version_type = uint8_t;
// Abstraction of transport protocol-dependent serialization format
// Protocols v1, v2 used 16 bits for collection sizes, while v3 and
// above use 32 bits.  But letting every bit of the code know what
// transport protocol we're using (and in some cases, we aren't using
// any transport -- it's for internal storage) is bad, so abstract it
// away here.
class cql_serialization_format {
    cql_protocol_version_type _version;
public:
    static constexpr cql_protocol_version_type latest_version = 4;
    explicit cql_serialization_format(cql_protocol_version_type version) : _version(version) {}
    static cql_serialization_format latest() { return cql_serialization_format{latest_version}; }
    cql_protocol_version_type protocol_version() const ;
    void ensure_supported() const {
        if (_version < 3) {
            throw std::runtime_error("cql protocol version must be 3 or later");
        }
    }
};
class position_in_partition_view;
class position_in_partition;
class partition_slice_builder;
namespace query {
using column_id_vector = utils::small_vector<column_id, 8>;
template <typename T>
using range = wrapping_range<T>;
using ring_position = dht::ring_position;
// Note: the bounds of a  clustering range don't necessarily satisfy `rb.end()->value() >= lb.end()->value()`,
// where `lb`, `rb` are the left and right bound respectively, if the bounds use non-full clustering
// key prefixes. Inclusiveness of the range's bounds must be taken into account during comparisons.
// For example, consider clustering key type consisting of two ints. Then [0:1, 0:] is a valid non-empty range
// (e.g. it includes the key 0:2) even though 0: < 0:1 w.r.t the clustering prefix order.
using clustering_range = nonwrapping_range<clustering_key_prefix>;
// If `range` was supposed to be used with a comparator `cmp`, then
// `reverse(range)` is supposed to be used with a reversed comparator `c`.
// For instance, if it does make sense to do
//   range.contains(point, cmp);
// then it also makes sense to do
//   reversed(range).contains(point, [](auto x, auto y) { return cmp(y, x); });
// but it doesn't make sense to do
//   reversed(range).contains(point, cmp);

extern const dht::partition_range full_partition_range;
extern const clustering_range full_clustering_range;


typedef std::vector<clustering_range> clustering_row_ranges;
/// Trim the clustering ranges.
///
/// Equivalent of intersecting each clustering range with [pos, +inf) position
/// in partition range, or (-inf, pos] position in partition range if
/// reversed == true. Ranges that do not intersect are dropped. Ranges that
/// partially overlap are trimmed.
/// Result: each range will overlap fully with [pos, +inf), or (-int, pos] if
/// reversed is true.
/// Trim the clustering ranges.
///
/// Equivalent of intersecting each clustering range with (key, +inf) clustering
/// range, or (-inf, key) clustering range if reversed == true. Ranges that do
/// not intersect are dropped. Ranges that partially overlap are trimmed.
/// Result: each range will overlap fully with (key, +inf), or (-int, key) if
/// reversed is true.
class specific_ranges {
public:
    
private:
    friend std::ostream& operator<<(std::ostream& out, const specific_ranges& r);
    partition_key _pk;
    clustering_row_ranges _ranges;
};
constexpr auto max_rows = std::numeric_limits<uint64_t>::max();
constexpr auto partition_max_rows = std::numeric_limits<uint64_t>::max();
constexpr auto max_rows_if_set = std::numeric_limits<uint32_t>::max();
// Specifies subset of rows, columns and cell attributes to be returned in a query.
// Can be accessed across cores.
// Schema-dependent.
//
// COMPATIBILITY NOTE: the partition-slice for reverse queries has two different
// format:
// * legacy format
// * native format
// The wire format uses the legacy format. See docs/dev/reverse-reads.md
// for more details on the formats.
class partition_slice {
    friend class ::partition_slice_builder;
public:
    enum class option {
        send_clustering_key,
        send_partition_key,
        send_timestamp,
        send_expiry,
        reversed,
        distinct,
        collections_as_maps,
        send_ttl,
        allow_short_read,
        with_digest,
        bypass_cache,
        // Normally, we don't return static row if the request has clustering
        // key restrictions and the partition doesn't have any rows matching
        // the restrictions, see #589. This flag overrides this behavior.
        always_return_static_content,
        // Use the new data range scan variant, which builds query::result
        // directly, bypassing the intermediate reconcilable_result format used
        // in pre 4.5 range scans.
        range_scan_data_variant,
    };
    using option_set = enum_set<super_enum<option,
        option::send_clustering_key,
        option::send_partition_key,
        option::send_timestamp,
        option::send_expiry,
        option::reversed,
        option::distinct,
        option::collections_as_maps,
        option::send_ttl,
        option::allow_short_read,
        option::with_digest,
        option::bypass_cache,
        option::always_return_static_content,
        option::range_scan_data_variant>>;
    clustering_row_ranges _row_ranges;
public:
    column_id_vector static_columns; // TODO: consider using bitmap
    column_id_vector regular_columns;  // TODO: consider using bitmap
    option_set options;
private:
    std::unique_ptr<specific_ranges> _specific_ranges;
    uint32_t _partition_row_limit_low_bits;
    uint32_t _partition_row_limit_high_bits;
public:
    partition_slice(clustering_row_ranges row_ranges, column_id_vector static_columns,
        column_id_vector regular_columns, option_set options,
        std::unique_ptr<specific_ranges> specific_ranges,
        cql_serialization_format,
        uint32_t partition_row_limit_low_bits,
        uint32_t partition_row_limit_high_bits);
    partition_slice(clustering_row_ranges row_ranges, column_id_vector static_columns,
        column_id_vector regular_columns, option_set options,
        std::unique_ptr<specific_ranges> specific_ranges = nullptr,
        uint64_t partition_row_limit = partition_max_rows);
    partition_slice(clustering_row_ranges ranges, const schema& schema, const column_set& mask, option_set options);
    partition_slice(const partition_slice&);
    partition_slice(partition_slice&&);
    ;
    
    const clustering_row_ranges& row_ranges(const schema&, const partition_key&) const;
    void set_range(const schema&, const partition_key&, clustering_row_ranges);
    
    
    // FIXME: possibly make this function return a const ref instead.
    const uint32_t partition_row_limit_high_bits() const ;
    const uint64_t partition_row_limit() const ;
    [[nodiscard]]
    bool is_reversed() const {
        return options.contains<query::partition_slice::option::reversed>();
    }
    friend std::ostream& operator<<(std::ostream& out, const partition_slice& ps);
};
// See docs/dev/reverse-reads.md
// In the following functions, `schema` may be reversed or not (both work).
// Fully reverse slice (forward to native reverse or native reverse to forward).
// Also toggles the reversed bit in `partition_slice::options`.
// Half reverse slice (forwad to legacy reverse or legacy reverse to forward).
// Also toggles the reversed bit in `partition_slice::options`.
constexpr auto max_partitions = std::numeric_limits<uint32_t>::max();
constexpr auto max_tombstones = std::numeric_limits<uint64_t>::max();
// Tagged integers to disambiguate constructor arguments.
enum class row_limit : uint64_t { max = max_rows };
enum class partition_limit : uint32_t { max = max_partitions };
enum class tombstone_limit : uint64_t { max = max_tombstones };
using is_first_page = bool_class<class is_first_page_tag>;
// Full specification of a query to the database.
// Intended for passing across replicas.
// Can be accessed across cores.
class read_command {
public:
    table_id cf_id;
    table_schema_version schema_version; // TODO: This should be enough, drop cf_id
    partition_slice slice;
    uint32_t row_limit_low_bits;
    gc_clock::time_point timestamp;
    std::optional<tracing::trace_info> trace_info;
    uint32_t partition_limit; // The maximum number of live partitions to return.
    // The "query_uuid" field is useful in pages queries: It tells the replica
    // that when it finishes the read request prematurely, i.e., reached the
    // desired number of rows per page, it should not destroy the reader object,
    // rather it should keep it alive - at its current position - and save it
    // under the unique key "query_uuid". Later, when we want to resume
    // the read at exactly the same position (i.e., to request the next page)
    // we can pass this same unique id in that query's "query_uuid" field.
    query_id query_uuid;
    // Signal to the replica that this is the first page of a (maybe) paged
    // read request as far the replica is concerned. Can be used by the replica
    // to avoid doing work normally done on paged requests, e.g. attempting to
    // reused suspended readers.
    query::is_first_page is_first_page;
    // The maximum size of the query result, for all queries.
    // We use the entire value range, so we need an optional for the case when
    // the remote doesn't send it.
    std::optional<query::max_result_size> max_result_size;
    uint32_t row_limit_high_bits;
    // Cut the page after processing this many tombstones (even if the page is empty).
    uint64_t tombstone_limit;
    api::timestamp_type read_timestamp; // not serialized
    db::allow_per_partition_rate_limit allow_limit; // not serialized
public:
    // IDL constructor
};
struct forward_request {
    enum class reduction_type {
        count,
        aggregate
    };
    struct aggregation_info {
        db::functions::function_name name;
        std::vector<sstring> column_names;
    };
    struct reductions_info { 
        // Used by selector_factries to prepare reductions information
        std::vector<reduction_type> types;
        std::vector<aggregation_info> infos;
    };
    std::vector<reduction_type> reduction_types;
    query::read_command cmd;
    dht::partition_range_vector pr;
    db::consistency_level cl;
    lowres_system_clock::time_point timeout;
    std::optional<std::vector<aggregation_info>> aggregation_infos;
};
struct forward_result {
    // vector storing query result for each selected column
    std::vector<bytes_opt> query_results;
    struct printer {
        const std::vector<::shared_ptr<db::functions::aggregate_function>> functions;
        const query::forward_result& res;
    };
};
}
namespace query {
class clustering_key_filter_ranges {
    clustering_row_ranges _storage;
    std::reference_wrapper<const clustering_row_ranges> _ref;
public:
    struct reversed { };
    // Returns all clustering ranges determined by `slice` inside partition determined by `key`.
    // If the slice contains the `reversed` option, we assume that it is given in 'half-reversed' format
    // (i.e. the ranges within are given in reverse order, but the ranges themselves are not reversed)
    // with respect to the table order.
    // The ranges will be returned in forward (increasing) order even if the slice is reversed.
    
    // Returns all clustering ranges determined by `slice` inside partition determined by `key`.
    // The ranges will be returned in the same order as stored in the slice.
    
};
}
// combine two sorted uniqued sequences into a single sorted sequence
// unique elements are copied, duplicate elements are merged with a
// binary function.
template <typename InputIterator1,
          typename InputIterator2,
          typename OutputIterator,
          typename Compare,
          typename Merge>
OutputIterator
combine(InputIterator1 begin1, InputIterator1 end1,
        InputIterator2 begin2, InputIterator2 end2,
        OutputIterator out,
        Compare compare,
        Merge merge) {
    while (begin1 != end1 && begin2 != end2) {
        auto& e1 = *begin1;
        auto& e2 = *begin2;
        if (compare(e1, e2)) {
            *out++ = e1;
            ++begin1;
        } else if (compare(e2, e1)) {
            *out++ = e2;
            ++begin2;
        } else {
            *out++ = merge(e1, e2);
            ++begin1;
            ++begin2;
        }
    }
    out = std::copy(begin1, end1, out);
    out = std::copy(begin2, end2, out);
    return out;
}
struct tombstone final {
    api::timestamp_type timestamp;
    gc_clock::time_point deletion_time;
    tombstone(api::timestamp_type timestamp, gc_clock::time_point deletion_time)
        : timestamp(timestamp)
        , deletion_time(deletion_time)
    { }
    tombstone()
        : tombstone(api::missing_timestamp, {})
    { }
    std::strong_ordering operator<=>(const tombstone& t) const = default;
    bool operator==(const tombstone&) const = default;
    explicit operator bool() const {
        return timestamp != api::missing_timestamp;
    }
    void apply(const tombstone& t) noexcept {
        if (*this < t) {
            *this = t;
        }
    }
    // See reversibly_mergeable.hh
    
    // See reversibly_mergeable.hh
    
    tombstone operator+(const tombstone& t) ;
};
template <>
struct fmt::formatter<tombstone> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const tombstone& t, FormatContext& ctx) const {
        if (t) {
            return fmt::format_to(ctx.out(),
                                  "{{tombstone: timestamp={}, deletion_time={}}}",
                                  t.timestamp, t.deletion_time.time_since_epoch().count());
        } else {
            return fmt::format_to(ctx.out(),
                                  "{{tombstone: none}}");
        }
     }
};
static std::ostream& operator<<(std::ostream& out, const tombstone& t) ;
template<>
struct appending_hash<tombstone> {
    template<typename Hasher>
    void operator()(Hasher& h, const tombstone& t) const ;
};
// Determines whether tombstone may be GC-ed.
using can_gc_fn = std::function<bool(tombstone)>;
extern can_gc_fn always_gc;
class abstract_type;
class collection_type_impl;
class atomic_cell_or_collection;
using atomic_cell_value = managed_bytes;
template <mutable_view is_mutable>
using atomic_cell_value_basic_view = managed_bytes_basic_view<is_mutable>;
using atomic_cell_value_view = atomic_cell_value_basic_view<mutable_view::no>;
using atomic_cell_value_mutable_view = atomic_cell_value_basic_view<mutable_view::yes>;
template <typename T>
requires std::is_trivial_v<T>
static void set_field(atomic_cell_value_mutable_view& out, unsigned offset, T val) {
    auto out_view = managed_bytes_mutable_view(out);
    out_view.remove_prefix(offset);
    write<T>(out_view, val);
}
template <typename T>
requires std::is_trivial_v<T>
static void set_field(atomic_cell_value& out, unsigned offset, T val) {
    auto out_view = atomic_cell_value_mutable_view(out);
    set_field(out_view, offset, val);
}
template <FragmentRange Buffer>
static void set_value(managed_bytes& b, unsigned value_offset, const Buffer& value) {
    auto v = managed_bytes_mutable_view(b).substr(value_offset, value.size_bytes());
    for (auto frag : value) {
        write_fragmented(v, single_fragmented_view(frag));
    }
}
template <typename T, FragmentedView Input>
requires std::is_trivial_v<T>
static T get_field(Input in, unsigned offset = 0) {
    in.remove_prefix(offset);
    return read_simple<T>(in);
}
class atomic_cell_type final {
private:
    static constexpr int8_t LIVE_FLAG = 0x01;
    static constexpr int8_t EXPIRY_FLAG = 0x02; // When present, expiry field is present. Set only for live cells
    static constexpr int8_t COUNTER_UPDATE_FLAG = 0x08; // Cell is a counter update.
    static constexpr unsigned flags_size = 1;
    static constexpr unsigned timestamp_offset = flags_size;
    static constexpr unsigned timestamp_size = 8;
    static constexpr unsigned expiry_offset = timestamp_offset + timestamp_size;
    static constexpr unsigned expiry_size = 8;
    static constexpr unsigned deletion_time_offset = timestamp_offset + timestamp_size;
    static constexpr unsigned deletion_time_size = 8;
    static constexpr unsigned ttl_offset = expiry_offset + expiry_size;
    static constexpr unsigned ttl_size = 4;
    friend class counter_cell_builder;
private:
    static bool is_counter_update(atomic_cell_value_view cell) {
        return cell.front() & COUNTER_UPDATE_FLAG;
    }
    static bool is_live(atomic_cell_value_view cell) {
        return cell.front() & LIVE_FLAG;
    }
    static bool is_live_and_has_ttl(atomic_cell_value_view cell) {
        return cell.front() & EXPIRY_FLAG;
    }
    static bool is_dead(atomic_cell_value_view cell) {
        return !is_live(cell);
    }
    // Can be called on live and dead cells
    static api::timestamp_type timestamp(atomic_cell_value_view cell) {
        return get_field<api::timestamp_type>(cell, timestamp_offset);
    }
    static void set_timestamp(atomic_cell_value_mutable_view& cell, api::timestamp_type ts) {
        set_field(cell, timestamp_offset, ts);
    }
    // Can be called on live cells only
private:
    template <mutable_view is_mutable>
    static managed_bytes_basic_view<is_mutable> do_get_value(managed_bytes_basic_view<is_mutable> cell) {
        auto expiry_field_size = bool(cell.front() & EXPIRY_FLAG) * (expiry_size + ttl_size);
        auto value_offset = flags_size + timestamp_size + expiry_field_size;
        cell.remove_prefix(value_offset);
        return cell;
    }
public:
    static atomic_cell_value_view value(managed_bytes_view cell) {
        return do_get_value(cell);
    }
    static atomic_cell_value_mutable_view value(managed_bytes_mutable_view cell) {
        return do_get_value(cell);
    }
    // Can be called on live counter update cells only
    static int64_t counter_update_value(atomic_cell_value_view cell) {
        return get_field<int64_t>(cell, flags_size + timestamp_size);
    }
    // Can be called only when is_dead() is true.
    static gc_clock::time_point deletion_time(atomic_cell_value_view cell) {
        assert(is_dead(cell));
        return gc_clock::time_point(gc_clock::duration(get_field<int64_t>(cell, deletion_time_offset)));
    }
    // Can be called only when is_live_and_has_ttl() is true.
    static gc_clock::time_point expiry(atomic_cell_value_view cell) {
        assert(is_live_and_has_ttl(cell));
        auto expiry = get_field<int64_t>(cell, expiry_offset);
        return gc_clock::time_point(gc_clock::duration(expiry));
    }
    // Can be called only when is_live_and_has_ttl() is true.
    static gc_clock::duration ttl(atomic_cell_value_view cell) {
        assert(is_live_and_has_ttl(cell));
        return gc_clock::duration(get_field<int32_t>(cell, ttl_offset));
    }
    static managed_bytes make_dead(api::timestamp_type timestamp, gc_clock::time_point deletion_time) {
        managed_bytes b(managed_bytes::initialized_later(), flags_size + timestamp_size + deletion_time_size);
        b[0] = 0;
        set_field(b, timestamp_offset, timestamp);
        set_field(b, deletion_time_offset, static_cast<int64_t>(deletion_time.time_since_epoch().count()));
        return b;
    }
    template <FragmentRange Buffer>
    static managed_bytes make_live(api::timestamp_type timestamp, const Buffer& value) {
        auto value_offset = flags_size + timestamp_size;
        managed_bytes b(managed_bytes::initialized_later(), value_offset + value.size_bytes());
        b[0] = LIVE_FLAG;
        set_field(b, timestamp_offset, timestamp);
        set_value(b, value_offset, value);
        return b;
    }
    static managed_bytes make_live_counter_update(api::timestamp_type timestamp, int64_t value) {
        auto value_offset = flags_size + timestamp_size;
        managed_bytes b(managed_bytes::initialized_later(), value_offset + sizeof(value));
        b[0] = LIVE_FLAG | COUNTER_UPDATE_FLAG;
        set_field(b, timestamp_offset, timestamp);
        set_field(b, value_offset, value);
        return b;
    }
    template <FragmentRange Buffer>
    static managed_bytes make_live(api::timestamp_type timestamp, const Buffer& value, gc_clock::time_point expiry, gc_clock::duration ttl) {
        auto value_offset = flags_size + timestamp_size + expiry_size + ttl_size;
        managed_bytes b(managed_bytes::initialized_later(), value_offset + value.size_bytes());
        b[0] = EXPIRY_FLAG | LIVE_FLAG;
        set_field(b, timestamp_offset, timestamp);
        set_field(b, expiry_offset, static_cast<int64_t>(expiry.time_since_epoch().count()));
        set_field(b, ttl_offset, static_cast<int32_t>(ttl.count()));
        set_value(b, value_offset, value);
        return b;
    }
    static managed_bytes make_live_uninitialized(api::timestamp_type timestamp, size_t size) {
        auto value_offset = flags_size + timestamp_size;
        managed_bytes b(managed_bytes::initialized_later(), value_offset + size);
        b[0] = LIVE_FLAG;
        set_field(b, timestamp_offset, timestamp);
        return b;
    }
    template <mutable_view is_mutable>
    friend class basic_atomic_cell_view;
    friend class atomic_cell;
};
/// View of an atomic cell
template<mutable_view is_mutable>
class basic_atomic_cell_view {
protected:
    managed_bytes_basic_view<is_mutable> _view;
	friend class atomic_cell;
protected:
    void set_view(managed_bytes_basic_view<is_mutable> v) {
        _view = v;
    }
    basic_atomic_cell_view() = default;
    explicit basic_atomic_cell_view(managed_bytes_basic_view<is_mutable> v) : _view(std::move(v)) { }
    friend class atomic_cell_or_collection;
public:
    operator basic_atomic_cell_view<mutable_view::no>() const noexcept ;
    bool is_counter_update() const {
        return atomic_cell_type::is_counter_update(_view);
    }
    bool is_live() const {
        return atomic_cell_type::is_live(_view);
    }
    
    
    bool is_live_and_has_ttl() const {
        return atomic_cell_type::is_live_and_has_ttl(_view);
    }
    bool is_dead(gc_clock::time_point now) const ;
    bool is_covered_by(tombstone t, bool is_counter) const ;
    // Can be called on live and dead cells
    api::timestamp_type timestamp() const {
        return atomic_cell_type::timestamp(_view);
    }
    void set_timestamp(api::timestamp_type ts) {
        atomic_cell_type::set_timestamp(_view, ts);
    }
    // Can be called on live cells only
    atomic_cell_value_basic_view<is_mutable> value() const {
        return atomic_cell_type::value(_view);
    }
    // Can be called on live cells only
    size_t value_size() const ;
    // Can be called on live counter update cells only
    int64_t counter_update_value() const {
        return atomic_cell_type::counter_update_value(_view);
    }
    // Can be called only when is_dead(gc_clock::time_point)
    gc_clock::time_point deletion_time() const {
        return !is_live() ? atomic_cell_type::deletion_time(_view) : expiry() - ttl();
    }
    // Can be called only when is_live_and_has_ttl()
    gc_clock::time_point expiry() const {
        return atomic_cell_type::expiry(_view);
    }
    // Can be called only when is_live_and_has_ttl()
    gc_clock::duration ttl() const {
        return atomic_cell_type::ttl(_view);
    }
    // Can be called on live and dead cells
    bool has_expired(gc_clock::time_point now) const ;
    managed_bytes_view serialize() const {
        return _view;
    }
};
class atomic_cell_view final : public basic_atomic_cell_view<mutable_view::no> {
    atomic_cell_view(managed_bytes_view v)
        : basic_atomic_cell_view(v) {}
    template<mutable_view is_mutable>
    atomic_cell_view(basic_atomic_cell_view<is_mutable> view)  ;
    friend class atomic_cell;
public:
    static atomic_cell_view from_bytes(const abstract_type& t, managed_bytes_view v) {
        return atomic_cell_view(v);
    }
    class printer {
        const abstract_type& _type;
        const atomic_cell_view& _cell;
    public:
    };
};
class atomic_cell_mutable_view final : public basic_atomic_cell_view<mutable_view::yes> {
    atomic_cell_mutable_view(managed_bytes_mutable_view data)
        : basic_atomic_cell_view(data) {}
public:
    static atomic_cell_mutable_view from_bytes(const abstract_type& t, managed_bytes_mutable_view v) {
        return atomic_cell_mutable_view(v);
    }
    friend class atomic_cell;
};
using atomic_cell_ref = atomic_cell_mutable_view;
class atomic_cell final : public basic_atomic_cell_view<mutable_view::yes> {
    managed_bytes _data;
    atomic_cell(managed_bytes b) : _data(std::move(b))  {
        set_view(_data);
    }
public:
    class collection_member_tag;
    using collection_member = bool_class<collection_member_tag>;
    atomic_cell(atomic_cell&& o) noexcept : _data(std::move(o._data)) {
        set_view(_data);
    }
    atomic_cell& operator=(const atomic_cell&) = delete;
    
    
    atomic_cell(const abstract_type& t, atomic_cell_view other);
    static atomic_cell make_dead(api::timestamp_type timestamp, gc_clock::time_point deletion_time);
    static atomic_cell make_live(const abstract_type& type, api::timestamp_type timestamp, bytes_view value,
                                 collection_member = collection_member::no);
    static atomic_cell make_live_counter_update(api::timestamp_type timestamp, int64_t value);
    static atomic_cell make_live(const abstract_type&, api::timestamp_type timestamp, bytes_view value,
        gc_clock::time_point expiry, gc_clock::duration ttl, collection_member = collection_member::no);
    static atomic_cell make_live(const abstract_type&, api::timestamp_type timestamp, managed_bytes_view value,
        gc_clock::time_point expiry, gc_clock::duration ttl, collection_member = collection_member::no);
    static atomic_cell make_live(const abstract_type&, api::timestamp_type timestamp, ser::buffer_view<bytes_ostream::fragment_iterator> value,
        gc_clock::time_point expiry, gc_clock::duration ttl, collection_member = collection_member::no);
    static atomic_cell make_live_uninitialized(const abstract_type& type, api::timestamp_type timestamp, size_t size);
    friend class atomic_cell_or_collection;
    
    class printer : atomic_cell_view::printer {
    public:
        
        
    };
};
class column_definition;
std::strong_ordering compare_atomic_cell_for_merge(atomic_cell_view left, atomic_cell_view right);

class abstract_type;
class compaction_garbage_collector;
class row_tombstone;
class collection_mutation;
// An auxiliary struct used to (de)construct collection_mutations.
// Unlike collection_mutation which is a serialized blob, this struct allows to inspect logical units of information
// (tombstone and cells) inside the mutation easily.
struct collection_mutation_description {
    tombstone tomb;
    // FIXME: use iterators?
    // we never iterate over `cells` more than once, so there is no need to store them in memory.
    // In some cases instead of constructing the `cells` vector, it would be more efficient to provide
    // a one-time-use forward iterator which returns the cells.
    utils::chunked_vector<std::pair<bytes, atomic_cell>> cells;
    // Expires cells based on query_time. Expires tombstones based on max_purgeable and gc_before.
    // Removes cells covered by tomb or this->tomb.
    bool compact_and_expire(column_id id, row_tombstone tomb, gc_clock::time_point query_time,
        can_gc_fn&, gc_clock::time_point gc_before, compaction_garbage_collector* collector = nullptr);
    // Packs the data to a serialized blob.
    collection_mutation serialize(const abstract_type&) const;
};
// Similar to collection_mutation_description, except that it doesn't store the cells' data, only observes it.
struct collection_mutation_view_description {
    tombstone tomb;
    // FIXME: use iterators? See the fixme in collection_mutation_description; the same considerations apply here.
    utils::chunked_vector<std::pair<bytes_view, atomic_cell_view>> cells;
    // Copies the observed data, storing it in a collection_mutation_description.
    collection_mutation_description materialize(const abstract_type&) const;
    // Packs the data to a serialized blob.
    collection_mutation serialize(const abstract_type&) const;
};
class collection_mutation_input_stream {
    std::forward_list<bytes> _linearized;
    managed_bytes_view _src;
public:
    collection_mutation_input_stream(const managed_bytes_view& src) : _src(src) {}
    template <Trivial T>
    T read_trivial() {
        return ::read_simple<T>(_src);
    }
    bytes_view read_linearized(size_t n);
    managed_bytes_view read_fragmented(size_t n);
    bool empty() const;
};
// Given a collection_mutation_view, returns an auxiliary struct allowing the inspection of each cell.
// The function needs to be given the type of stored data to reconstruct the structural information.
collection_mutation_view_description deserialize_collection_mutation(const abstract_type&, collection_mutation_input_stream&);
class collection_mutation_view {
public:
    managed_bytes_view data;
    // Is this a noop mutation?
    bool is_empty() const;
    // Is any of the stored cells live (not deleted nor expired) at the time point `tp`,
    // given the later of the tombstones `t` and the one stored in the mutation (if any)?
    // Requires a type to reconstruct the structural information.
    bool is_any_live(const abstract_type&, tombstone t = tombstone(), gc_clock::time_point tp = gc_clock::time_point::min()) const;
    // The maximum of timestamps of the mutation's cells and tombstone.
    api::timestamp_type last_update(const abstract_type&) const;
    // Given a function that operates on a collection_mutation_view_description,
    // calls it on the corresponding description of `this`.
    template <typename F>
    inline decltype(auto) with_deserialized(const abstract_type& type, F f) const {
        collection_mutation_input_stream stream(data);
        return f(deserialize_collection_mutation(type, stream));
    }
    class printer {
        const abstract_type& _type;
        const collection_mutation_view& _cmv;
    public:
        printer(const abstract_type& type, const collection_mutation_view& cmv)  ;
        
    };
};
// A serialized mutation of a collection of cells.
// Used to represent mutations of collections (lists, maps, sets) or non-frozen user defined types.
// It contains a sequence of cells, each representing a mutation of a single entry (element or field) of the collection.
// Each cell has an associated 'key' (or 'path'). The meaning of each (key, cell) pair is:
//  for sets: the key is the serialized set element, the cell contains no data (except liveness information),
//  for maps: the key is the serialized map element's key, the cell contains the serialized map element's value,
//  for lists: the key is a timeuuid identifying the list entry, the cell contains the serialized value,
//  for user types: the key is an index identifying the field, the cell contains the value of the field.
//  The mutation may also contain a collection-wide tombstone.
class collection_mutation {
public:
    managed_bytes _data;
    
    collection_mutation(const abstract_type&, collection_mutation_view);
    collection_mutation(const abstract_type&, managed_bytes);
    operator collection_mutation_view() const;
};
collection_mutation merge(const abstract_type&, collection_mutation_view, collection_mutation_view);
collection_mutation difference(const abstract_type&, collection_mutation_view, collection_mutation_view);
// Serializes the given collection of cells to a sequence of bytes ready to be sent over the CQL protocol.
bytes_ostream serialize_for_cql(const abstract_type&, collection_mutation_view);
namespace cql3 {
class column_specification;
}
class collection_type_impl : public abstract_type {
    static logging::logger _logger;
public:
    static constexpr size_t max_elements = 65535;
protected:
    bool _is_multi_cell;
    explicit collection_type_impl(kind k, sstring name, bool is_multi_cell)  ;
public:
    bool is_multi_cell() const { return _is_multi_cell; }
    virtual data_type name_comparator() const = 0;
    virtual data_type value_comparator() const = 0;
    lw_shared_ptr<cql3::column_specification> make_collection_receiver(const cql3::column_specification& collection, bool is_key) const;
    virtual bool is_compatible_with_frozen(const collection_type_impl& previous) const = 0;
    virtual bool is_value_compatible_with_frozen(const collection_type_impl& previous) const = 0;
    template <typename Iterator>
    requires requires (Iterator it) { {*it} -> std::convertible_to<bytes_view_opt>; }
    static bytes pack(Iterator start, Iterator finish, int elements);
    template <typename Iterator>
    requires requires (Iterator it) { {*it} -> std::convertible_to<managed_bytes_view_opt>; }
    static managed_bytes pack_fragmented(Iterator start, Iterator finish, int elements);
private:
    // Explicitly instantiated in types.cc
    template <FragmentedView View> data_value deserialize_impl(View v) const;
public:
    template <FragmentedView View> data_value deserialize_value(View v) const ;
    data_value deserialize_value(bytes_view v) const ;
};
// a list or a set
class listlike_collection_type_impl : public collection_type_impl {
protected:
    data_type _elements;
    explicit listlike_collection_type_impl(kind k, sstring name, data_type elements,bool is_multi_cell);
public:
    const data_type& get_elements_type() const { return _elements; }
    // A list or set value can be serialized as a vector<pair<timeuuid, data_value>> or
    // vector<pair<data_value, empty>> respectively. Compare this representation with
    // vector<data_value> without transforming either of the arguments. Since Cassandra doesn't
    // allow nested multi-cell collections this representation does not transcend to values, and we
    // don't need to worry about recursing.
    // @param this          type of the listlike value represented as vector<data_value>
    // @param map_type      type of the listlike value represented as vector<pair<data_value, data_value>>
    // @param list          listlike value, represented as vector<data_value>
    // @param map           listlike value represented as vector<pair<data_value, data_value>>
    //
    // This function is used to compare receiver with a literal or parameter marker during condition
    // evaluation.
    
    // A list or set value can be represented as a vector<pair<timeuuid, data_value>> or
    // vector<pair<data_value, empty>> respectively. Serialize this representation
    // as a vector of values, not as a vector of pairs.
    
    // Verify that there are no NULL elements. Throws if there are.
    void validate_for_storage(const FragmentedView auto& value) const;
};
template <typename Iterator>
requires requires (Iterator it) { {*it} -> std::convertible_to<bytes_view_opt>; }
bytes
collection_type_impl::pack(Iterator start, Iterator finish, int elements) {
    size_t len = collection_size_len();
    size_t psz = collection_value_len();
    for (auto j = start; j != finish; j++) {
        auto v = bytes_view_opt(*j);
        len += (v ? v->size() : 0) + psz;
    }
    bytes out(bytes::initialized_later(), len);
    bytes::iterator i = out.begin();
    write_collection_size(i, elements);
    while (start != finish) {
        write_collection_value(i, *start++);
    }
    return out;
}
template <typename Iterator>
requires requires (Iterator it) { {*it} -> std::convertible_to<managed_bytes_view_opt>; }
managed_bytes
collection_type_impl::pack_fragmented(Iterator start, Iterator finish, int elements) {
    size_t len = collection_size_len();
    size_t psz = collection_value_len();
    for (auto j = start; j != finish; j++) {
        auto v = managed_bytes_view_opt(*j);
        len += (v ? v->size() : 0) + psz;
    }
    managed_bytes out(managed_bytes::initialized_later(), len);
    managed_bytes_mutable_view v(out);
    write_collection_size(v, elements);
    while (start != finish) {
        write_collection_value(v, *start++);
    }
    return out;
}
extern
template
void listlike_collection_type_impl::validate_for_storage(const managed_bytes_view& value) const;
extern
template
void listlike_collection_type_impl::validate_for_storage(const fragmented_temporary_buffer::view& value) const;
class user_type_impl;
namespace Json {
class Value;
}
class list_type_impl final : public concrete_type<std::vector<data_value>, listlike_collection_type_impl> {
    using list_type = shared_ptr<const list_type_impl>;
    using intern = type_interning_helper<list_type_impl, data_type, bool>;
public:
    static list_type get_instance(data_type elements, bool is_multi_cell);
    list_type_impl(data_type elements, bool is_multi_cell);
    virtual data_type name_comparator() const override;
    virtual data_type value_comparator() const override;
    virtual data_type freeze() const override;
    virtual bool is_compatible_with_frozen(const collection_type_impl& previous) const override;
    virtual bool is_value_compatible_with_frozen(const collection_type_impl& previous) const override;
    using abstract_type::deserialize;
    using collection_type_impl::deserialize;
    template <FragmentedView View> data_value deserialize(View v) const;
};
data_value make_list_value(data_type type, list_type_impl::native_type value);
class user_type_impl;
namespace Json {
class Value;
}
class map_type_impl final : public concrete_type<std::vector<std::pair<data_value, data_value>>, collection_type_impl> {
    using map_type = shared_ptr<const map_type_impl>;
    using intern = type_interning_helper<map_type_impl, data_type, data_type, bool>;
    data_type _keys;
    data_type _values;
    data_type _key_value_pair_type;
public:
    static shared_ptr<const map_type_impl> get_instance(data_type keys, data_type values, bool is_multi_cell);
    map_type_impl(data_type keys, data_type values, bool is_multi_cell);
    const data_type& get_keys_type() const { return _keys; }
    const data_type& get_values_type() const { return _values; }
    virtual data_type name_comparator() const override { return _keys; }
    virtual data_type value_comparator() const override { return _values; }
    virtual data_type freeze() const override;
    virtual bool is_compatible_with_frozen(const collection_type_impl& previous) const override;
    virtual bool is_value_compatible_with_frozen(const collection_type_impl& previous) const override;
    static std::strong_ordering compare_maps(data_type keys_comparator, data_type values_comparator,
                        managed_bytes_view o1, managed_bytes_view o2);
    using abstract_type::deserialize;
    using collection_type_impl::deserialize;
    template <FragmentedView View> data_value deserialize(View v) const;
    static bytes serialize_partially_deserialized_form(const std::vector<std::pair<bytes_view, bytes_view>>& v);
    static managed_bytes serialize_partially_deserialized_form_fragmented(const std::vector<std::pair<managed_bytes_view, managed_bytes_view>>& v);
    // Serializes a map using the internal cql serialization format
    // Takes a range of pair<const bytes, bytes>
    template <std::ranges::range Range>
    requires std::convertible_to<std::ranges::range_value_t<Range>, std::pair<const bytes, bytes>>
    static bytes serialize_to_bytes(const Range& map_range);
    // Serializes a map using the internal cql serialization format
    // Takes a range of pair<const managed_bytes, managed_bytes>
    template <std::ranges::range Range>
    requires std::convertible_to<std::ranges::range_value_t<Range>, std::pair<const managed_bytes, managed_bytes>>
    static managed_bytes serialize_to_managed_bytes(const Range& map_range);
};

template <std::ranges::range Range>
requires std::convertible_to<std::ranges::range_value_t<Range>, std::pair<const bytes, bytes>>
bytes map_type_impl::serialize_to_bytes(const Range& map_range) {
    size_t serialized_len = 4;
    size_t map_size = 0;
    for (const std::pair<const bytes, bytes>& elem : map_range) {
        serialized_len += 4 + elem.first.size() + 4 + elem.second.size();
        map_size += 1;
    }
    if (map_size > std::numeric_limits<int32_t>::max()) {
        throw exceptions::invalid_request_exception(
            fmt::format("Map size too large: {} > {}", map_size, std::numeric_limits<int32_t>::max()));
    }
    bytes result(bytes::initialized_later(), serialized_len);
    bytes::iterator out = result.begin();
    write_collection_size(out, map_size);
    for (const std::pair<const bytes, bytes>& elem : map_range) {
        if (elem.first.size() > std::numeric_limits<int32_t>::max()) {
            throw exceptions::invalid_request_exception(
                fmt::format("Map key size too large: {} bytes > {}", map_size, std::numeric_limits<int32_t>::max()));
        }
        if (elem.second.size() > std::numeric_limits<int32_t>::max()) {
            throw exceptions::invalid_request_exception(
                fmt::format("Map value size too large: {} bytes > {}", map_size, std::numeric_limits<int32_t>::max()));
        }
        write_collection_value(out, elem.first);
        write_collection_value(out, elem.second);
    }
    return result;
}
template <std::ranges::range Range>
requires std::convertible_to<std::ranges::range_value_t<Range>, std::pair<const managed_bytes, managed_bytes>>
managed_bytes map_type_impl::serialize_to_managed_bytes(const Range& map_range) {
    size_t serialized_len = 4;
    size_t map_size = 0;
    for (const std::pair<const managed_bytes, managed_bytes>& elem : map_range) {
        serialized_len += 4 + elem.first.size() + 4 + elem.second.size();
        map_size += 1;
    }
    if (map_size > std::numeric_limits<int32_t>::max()) {
        throw exceptions::invalid_request_exception(
            fmt::format("Map size too large: {} > {}", map_size, std::numeric_limits<int32_t>::max()));
    }
    managed_bytes result(managed_bytes::initialized_later(), serialized_len);
    managed_bytes_mutable_view out(result);
    write_collection_size(out, map_size);
    for (const std::pair<const managed_bytes, managed_bytes>& elem : map_range) {
        if (elem.first.size() > std::numeric_limits<int32_t>::max()) {
            throw exceptions::invalid_request_exception(
                fmt::format("Map key size too large: {} bytes > {}", map_size, std::numeric_limits<int32_t>::max()));
        }
        if (elem.second.size() > std::numeric_limits<int32_t>::max()) {
            throw exceptions::invalid_request_exception(
                fmt::format("Map value size too large: {} bytes > {}", map_size, std::numeric_limits<int32_t>::max()));
        }
        write_collection_value(out, elem.first);
        write_collection_value(out, elem.second);
    }
    return result;
}
class user_type_impl;
namespace Json {
class Value;
}
class set_type_impl final : public concrete_type<std::vector<data_value>, listlike_collection_type_impl> {
    using set_type = shared_ptr<const set_type_impl>;
    using intern = type_interning_helper<set_type_impl, data_type, bool>;
public:
    static set_type get_instance(data_type elements, bool is_multi_cell);
    set_type_impl(data_type elements, bool is_multi_cell);
    virtual data_type name_comparator() const override { return _elements; }
    virtual data_type value_comparator() const override;
    virtual data_type freeze() const override;
    virtual bool is_compatible_with_frozen(const collection_type_impl& previous) const override;
    virtual bool is_value_compatible_with_frozen(const collection_type_impl& previous) const override;
    using abstract_type::deserialize;
    using collection_type_impl::deserialize;
    template <FragmentedView View> data_value deserialize(View v) const;
    static bytes serialize_partially_deserialized_form(
            const std::vector<bytes_view>& v);
    static managed_bytes serialize_partially_deserialized_form_fragmented(
            const std::vector<managed_bytes_view_opt>& v);
};
struct tuple_deserializing_iterator {
public:
    using iterator_category = std::input_iterator_tag;
    using value_type = const managed_bytes_view_opt;
    using difference_type = std::ptrdiff_t;
    using pointer = const managed_bytes_view_opt*;
    using reference = const managed_bytes_view_opt&;
private:
    managed_bytes_view _v;
    managed_bytes_view_opt _current;
public:
    struct end_tag {};
    tuple_deserializing_iterator(managed_bytes_view v) : _v(v) {
        parse();
    }
    tuple_deserializing_iterator(end_tag, managed_bytes_view v) : _v(v) {
        _v.remove_prefix(_v.size());
    }
    static tuple_deserializing_iterator start(managed_bytes_view v) {
        return tuple_deserializing_iterator(v);
    }
    static tuple_deserializing_iterator finish(managed_bytes_view v) {
        return tuple_deserializing_iterator(end_tag(), v);
    }
    const managed_bytes_view_opt& operator*() const {
        return _current;
    }
    tuple_deserializing_iterator& operator++() {
        skip();
        parse();
        return *this;
    }
    bool operator==(const tuple_deserializing_iterator& x) const {
        return _v == x._v;
    }
private:
    void parse() {
        _current = std::nullopt;
        if (_v.empty()) {
            return;
        }
        // we don't consume _v, otherwise operator==
        // or the copy constructor immediately after
        // parse() yields the wrong results.
        auto tmp = _v;
        auto s = read_simple<int32_t>(tmp);
        if (s < 0) {
            return;
        }
        _current = read_simple_bytes(tmp, s);
    }
    void skip() {
        _v.remove_prefix(4 + (_current ? _current->size() : 0));
    }
};
template <FragmentedView View>
std::optional<View> read_tuple_element(View& v) {
    auto s = read_simple<int32_t>(v);
    if (s < 0) {
        return std::nullopt;
    }
    return read_simple_bytes(v, s);
}
 ;
class tuple_type_impl : public concrete_type<std::vector<data_value>> {
    using intern = type_interning_helper<tuple_type_impl, std::vector<data_type>>;
protected:
    std::vector<data_type> _types;
    
    tuple_type_impl(kind k, sstring name, std::vector<data_type> types, bool freeze_inner);
    tuple_type_impl(std::vector<data_type> types, bool freze_inner);
public:
    tuple_type_impl(std::vector<data_type> types);
    static shared_ptr<const tuple_type_impl> get_instance(std::vector<data_type> types);
    data_type type(size_t i) const {
        return _types[i];
    }
    size_t size() const {
        return _types.size();
    }
    const std::vector<data_type>& all_types() const {
        return _types;
    }
     ;
    template <typename Range> // range of managed_bytes_opt or managed_bytes_view_opt
    requires requires (Range it) { {std::begin(it)->value()} -> std::convertible_to<managed_bytes_view>; }
    static managed_bytes build_value_fragmented(Range&& range) {
        size_t size = 0;
        for (auto&& v : range) {
            size += 4 + (v ? v->size() : 0);
        }
        auto ret = managed_bytes(managed_bytes::initialized_later(), size);
        auto out = managed_bytes_mutable_view(ret);
        for (auto&& v : range) {
            if (v) {
                write<int32_t>(out, v->size());
                write_fragmented(out, managed_bytes_view(*v));
            } else {
                write<int32_t>(out, -1);
            }
        }
        return ret;
    }
private:
    void set_contains_collections();
    friend abstract_type;
};
data_value make_tuple_value(data_type tuple_type, tuple_type_impl::native_type value);
class user_type_impl : public tuple_type_impl, public data_dictionary::keyspace_element {
    using intern = type_interning_helper<user_type_impl, sstring, bytes, std::vector<bytes>, std::vector<data_type>, bool>;
public:
    const sstring _keyspace;
    const bytes _name;
private:
    const std::vector<bytes> _field_names;
    const std::vector<sstring> _string_field_names;
    const bool _is_multi_cell;
public:
    using native_type = std::vector<data_value>;
    user_type_impl(sstring keyspace, bytes name, std::vector<bytes> field_names, std::vector<data_type> field_types, bool is_multi_cell)
            : tuple_type_impl(kind::user, make_name(keyspace, name, field_names, field_types, is_multi_cell), field_types, false )
            , _keyspace(std::move(keyspace))
            , _name(std::move(name))
            , _field_names(std::move(field_names))
            , _string_field_names(boost::copy_range<std::vector<sstring>>(_field_names | boost::adaptors::transformed(
                    [] (const bytes& field_name) { return utf8_type->to_string(field_name); })))
            , _is_multi_cell(is_multi_cell) {
    }
    static shared_ptr<const user_type_impl> get_instance(sstring keyspace, bytes name,
            std::vector<bytes> field_names, std::vector<data_type> field_types, bool multi_cell);
    data_type field_type(size_t i) const ;
    const std::vector<data_type>& field_types() const ;
    bool is_multi_cell() const { return _is_multi_cell; }
    virtual data_type freeze() const override;
    bytes get_name() const ;
    sstring get_name_as_string() const;
    sstring get_name_as_cql_string() const;
    
    
    virtual sstring element_type() const override ;
    virtual std::ostream& describe(std::ostream& os) const override;
private:
    static sstring make_name(sstring keyspace,
                             bytes name,
                             std::vector<bytes> field_names,
                             std::vector<data_type> field_types,
                             bool is_multi_cell);
};
data_value make_user_value(data_type tuple_type, user_type_impl::native_type value);
constexpr size_t max_udt_fields = std::numeric_limits<int16_t>::max();
// The following two functions are used to translate field indices (used to identify fields inside non-frozen UDTs)
// from/to a serialized bytes representation to be stored in mutations and sstables.
// Refer to collection_mutation.hh for a detailed description on how the serialized indices are used inside mutations.
bytes serialize_field_index(size_t);
size_t deserialize_field_index(const bytes_view&);
size_t deserialize_field_index(managed_bytes_view);
namespace utils {
}
struct empty_type_impl final : public abstract_type {
    using native_type = empty_type_representation;
    empty_type_impl();
};
struct counter_type_impl final : public abstract_type {
    counter_type_impl();
};
template <typename T>
struct simple_type_impl : public concrete_type<T> {
    simple_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed);
};
template<typename T>
struct integer_type_impl : public simple_type_impl<T> {
    integer_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed);
};
struct byte_type_impl final : public integer_type_impl<int8_t> {
    byte_type_impl();
};
struct short_type_impl final : public integer_type_impl<int16_t> {
    short_type_impl();
};
struct int32_type_impl final : public integer_type_impl<int32_t> {
    int32_type_impl();
};
struct long_type_impl final : public integer_type_impl<int64_t> {
    long_type_impl();
};
struct boolean_type_impl final : public simple_type_impl<bool> {
    boolean_type_impl();
};
template <typename T>
struct floating_type_impl : public simple_type_impl<T> {
    floating_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed);
};
struct double_type_impl final : public floating_type_impl<double> {
    double_type_impl();
};
struct float_type_impl final : public floating_type_impl<float> {
    float_type_impl();
};
struct duration_type_impl final : public concrete_type<cql_duration> {
    duration_type_impl();
};
struct timestamp_type_impl final : public simple_type_impl<db_clock::time_point> {
    timestamp_type_impl();
    static db_clock::time_point from_sstring(sstring_view s);
};
struct simple_date_type_impl final : public simple_type_impl<uint32_t> {
    simple_date_type_impl();
    static uint32_t from_sstring(sstring_view s);
};
struct time_type_impl final : public simple_type_impl<int64_t> {
    time_type_impl();
    static int64_t from_sstring(sstring_view s);
};
struct string_type_impl : public concrete_type<sstring> {
    string_type_impl(kind k, sstring name);
};
struct ascii_type_impl final : public string_type_impl {
    ascii_type_impl();
};
struct utf8_type_impl final : public string_type_impl {
    utf8_type_impl();
};
struct bytes_type_impl final : public concrete_type<bytes> {
    bytes_type_impl();
};
// This is the old version of timestamp_type_impl, but has been replaced as it
// wasn't comparing pre-epoch timestamps correctly. This is kept for backward
// compatibility but shouldn't be used in new code.
struct date_type_impl final : public concrete_type<db_clock::time_point> {
    date_type_impl();
};
using timestamp_date_base_class = concrete_type<db_clock::time_point>;
struct timeuuid_type_impl final : public concrete_type<utils::UUID> {
    timeuuid_type_impl();
    static utils::UUID from_sstring(sstring_view s);
};
struct inet_addr_type_impl final : public concrete_type<seastar::net::inet_address> {
    inet_addr_type_impl();
    
    
};
struct uuid_type_impl final : public concrete_type<utils::UUID> {
    uuid_type_impl();
    static utils::UUID from_sstring(sstring_view s);
};
template <typename Func> using visit_ret_type = std::invoke_result_t<Func, const ascii_type_impl&>;
template <typename Func> concept CanHandleAllTypes = requires(Func f) {
    { f(*static_cast<const ascii_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const boolean_type_impl*>(nullptr)) }     -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const byte_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const bytes_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const counter_type_impl*>(nullptr)) }     -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const date_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const double_type_impl*>(nullptr)) }      -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const duration_type_impl*>(nullptr)) }    -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const empty_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const float_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const inet_addr_type_impl*>(nullptr)) }   -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const int32_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const list_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const long_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const map_type_impl*>(nullptr)) }         -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const reversed_type_impl*>(nullptr)) }    -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const set_type_impl*>(nullptr)) }         -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const short_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const simple_date_type_impl*>(nullptr)) } -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const time_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const timestamp_type_impl*>(nullptr)) }   -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const timeuuid_type_impl*>(nullptr)) }    -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const tuple_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const user_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const utf8_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const uuid_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
};
template<typename Func>
requires CanHandleAllTypes<Func>
static inline visit_ret_type<Func> visit(const abstract_type& t, Func&& f) {
    switch (t.get_kind()) {
    case abstract_type::kind::ascii:
        return f(*static_cast<const ascii_type_impl*>(&t));
    case abstract_type::kind::boolean:
        return f(*static_cast<const boolean_type_impl*>(&t));
    case abstract_type::kind::byte:
        return f(*static_cast<const byte_type_impl*>(&t));
    case abstract_type::kind::bytes:
        return f(*static_cast<const bytes_type_impl*>(&t));
    case abstract_type::kind::counter:
        return f(*static_cast<const counter_type_impl*>(&t));
    case abstract_type::kind::date:
        return f(*static_cast<const date_type_impl*>(&t));
    case abstract_type::kind::double_kind:
        return f(*static_cast<const double_type_impl*>(&t));
    case abstract_type::kind::duration:
        return f(*static_cast<const duration_type_impl*>(&t));
    case abstract_type::kind::empty:
        return f(*static_cast<const empty_type_impl*>(&t));
    case abstract_type::kind::float_kind:
        return f(*static_cast<const float_type_impl*>(&t));
    case abstract_type::kind::inet:
        return f(*static_cast<const inet_addr_type_impl*>(&t));
    case abstract_type::kind::int32:
        return f(*static_cast<const int32_type_impl*>(&t));
    case abstract_type::kind::list:
        return f(*static_cast<const list_type_impl*>(&t));
    case abstract_type::kind::long_kind:
        return f(*static_cast<const long_type_impl*>(&t));
    case abstract_type::kind::map:
        return f(*static_cast<const map_type_impl*>(&t));
    case abstract_type::kind::reversed:
        return f(*static_cast<const reversed_type_impl*>(&t));
    case abstract_type::kind::set:
        return f(*static_cast<const set_type_impl*>(&t));
    case abstract_type::kind::short_kind:
        return f(*static_cast<const short_type_impl*>(&t));
    case abstract_type::kind::simple_date:
        return f(*static_cast<const simple_date_type_impl*>(&t));
    case abstract_type::kind::time:
        return f(*static_cast<const time_type_impl*>(&t));
    case abstract_type::kind::timestamp:
        return f(*static_cast<const timestamp_type_impl*>(&t));
    case abstract_type::kind::timeuuid:
        return f(*static_cast<const timeuuid_type_impl*>(&t));
    case abstract_type::kind::tuple:
        return f(*static_cast<const tuple_type_impl*>(&t));
    case abstract_type::kind::user:
        return f(*static_cast<const user_type_impl*>(&t));
    case abstract_type::kind::utf8:
        return f(*static_cast<const utf8_type_impl*>(&t));
    case abstract_type::kind::uuid:
        return f(*static_cast<const uuid_type_impl*>(&t));
    }
    __builtin_unreachable();
}
template <typename Func> struct data_value_visitor {
    const void* v;
    Func& f;
    auto operator()(const empty_type_impl& t) { return f(t, v); }
    auto operator()(const counter_type_impl& t) { return f(t, v); }
    auto operator()(const reversed_type_impl& t) { return f(t, v); }
    template <typename T> auto operator()(const T& t) {
        return f(t, reinterpret_cast<const typename T::native_type*>(v));
    }
};
// Given an abstract_type and a void pointer to an object of that
// type, call f with the runtime type of t and v casted to the
// corresponding native type.
// This takes an abstract_type and a void pointer instead of a
// data_value to support reversed_type_impl without requiring that
// each visitor create a new data_value just to recurse.
template <typename Func> inline auto visit(const abstract_type& t, const void* v, Func&& f) {
    return ::visit(t, data_value_visitor<Func>{v, f});
}
template <typename Func>  auto visit(const data_value& v, Func&& f) ;
namespace utils {
// observable/observer - a publish/subscribe utility
//
// An observable is an object that can broadcast notifications
// about changes in some state. An observer listens for such notifications
// in a particular observable it is connected to. Multiple observers can
// observe a single observable.
//
// A connection between an observer and an observable is established when
// the observer is constructed (using observable::observe()); from then
// on their life cycles are separate, either can be moved or destroyed
// without affecting the other.
//
// During construction, the observer specifies how to react to a change
// in the observable's state by specifying a function to be called on
// a state change. An observable causes the function to be executed
// by calling its operator()() method.
//
// All observers are called without preemption, so an observer should have
// a small number of observers.
template <typename... Args>
class observable {
public:
    class observer;
private:
    std::vector<observer*> _observers;
public:
    class observer {
        friend class observable;
        observable* _observable;
        seastar::noncopyable_function<void (Args...)> _callback;
    private:
        
    public:
        
        // Stops observing the observable immediately, instead of
        // during destruction.
    };
    friend class observer;
private:
public:
    // Send args to all connected observers
    // Adds an observer to an observable
};
// An observer<Args...> can receive notifications about changes
// in an observable<Args...>'s state.
template <typename... Args>
using observer = typename observable<Args...>::observer;
 ;
}
// An async action wrapper which ensures that at most one action
// is running at any time.
class serialized_action {
public:
    template <typename... T>
    using future = seastar::future<T...>;
private:
    std::function<future<>()> _func;
    seastar::shared_future<> _pending;
    seastar::semaphore _sem;
private:
public:
    serialized_action(std::function<future<>()> func)
        : _func(std::move(func))
        , _sem(1)
    { }
    // Makes sure that a new action will be started after this call and
    // returns a future which resolves when that action completes.
    // At most one action can run at any given moment.
    // A single action is started on behalf of all earlier triggers.
    //
    // When action is not currently running, it is started immediately if !later or
    // at some point in time soon after current fiber defers when later is true.
    // Like trigger() but can be aborted
    // Like trigger(), but defers invocation of the action to allow for batching
    // more requests.
    future<> trigger_later() ;
    // Waits for all invocations initiated in the past.
    // The adaptor is to be used as an argument to utils::observable.observe()
    // When the notification happens the adaptor just triggers the action
    // Note, that all arguments provided by the notification callback are lost,
    // its up to the action to get the needed values
    // Also, the future<> returned from .trigger() is ignored, the action code
    // runs in the background. The user should .join() the action if it needs
    // to wait for it to finish on stop/drain/shutdown
    class observing_adaptor {
        friend class serialized_action;
        serialized_action& _action;
    public:
         ;;
    };
};
namespace utils {
// This file contains two templates, updateable_value_source<T> and updateable_value<T>.
//
// The two are analogous to T and const T& respectively, with the following additional
// functionality:
//
//  - updateable_value contains a copy of T, so it can be accessed without indirection
//  - updateable_value and updateable_value_source track each other, so if they move,
//    the references are updated
//  - an observe() function is provided (to both) that can be used to attach a callback
//    that is called whenever the value changes
template <typename T>
class updateable_value_source;
class updateable_value_source_base;
// Base class for updateable_value<T>, containing functionality for tracking
// the update source. Used to reduce template bloat and not meant to be used
// directly.
class updateable_value_base {
protected:
    const updateable_value_source_base* _source = nullptr;
public:
    updateable_value_base& operator=(std::nullptr_t);
    friend class updateable_value_source_base;
};
// A T that can be updated at runtime; uses updateable_value_base to track
// the source as the object is moved or copied. Copying across shards is supported
// unless #7316 is still open
template <typename T>
class updateable_value : public updateable_value_base {
    T _value = {};
private:
    const updateable_value_source<T>* source() const;
public:
    updateable_value() = default;
    explicit updateable_value(T value) : _value(std::move(value)) {}
    explicit updateable_value(const updateable_value_source<T>& source);
    updateable_value(const updateable_value& v);
    
    
    
    
    const T& operator()() const ;
    operator const T& () const ;
    const T& get() const ;
    observer<T> observe(std::function<void (const T&)> callback) const;
    friend class updateable_value_source_base;
    template <typename U>
    friend class updateable_value_source;
};
// Contains the mechanisms to track updateable_value_base.  Used to reduce template
// bloat and not meant to be used directly.
class updateable_value_source_base {
protected:
    // This class contains two different types of state: values and
    // references to updateable_value_base. We consider adding and removing
    // such references const operations since they don't change the logical
    // state of the object (they don't allow changing the carried value).
    mutable std::vector<updateable_value_base*> _refs; // all connected updateable_values on this shard
    void for_each_ref(std::function<void (updateable_value_base* ref)> func);
protected:
    ~updateable_value_source_base();
    void add_ref(updateable_value_base* ref) const;
    void del_ref(updateable_value_base* ref) const;
    void update_ref(updateable_value_base* old_ref, updateable_value_base* new_ref) const;
    friend class updateable_value_base;
};
template <typename T>
class updateable_value_source : public updateable_value_source_base {
    T _value;
    mutable observable<T> _updater;
    ;
private:
public:
    friend class updateable_value_base;
};
template <typename T>
observer<T>
updateable_value<T>::observe(std::function<void (const T&)> callback) const {
    auto* src = source();
    return src ? src->observe(std::move(callback)) : dummy_observer<T>();
}
// Automatically updates a value from a utils::updateable_value
// Where they can be of different types.
// An optional transfom function can provide an additional transformation
// when updating the value, like multiplying it by a factor for unit conversion,
// for example.
template <typename ValueType, typename UpdateableValueType>
class transforming_value_updater {
    ValueType& _value;
    utils::updateable_value<UpdateableValueType> _updateable_value;
    serialized_action _updater;
    utils::observer<UpdateableValueType> _observer;
public:
};
}
namespace seastar { class file; }
namespace seastar::json { class json_return_type; }
namespace YAML { class Node; }
namespace utils {
namespace bpo = boost::program_options;
class config_type {
    std::string_view _name;
    std::function<json::json_return_type (const void*)> _to_json;
private:
     ;
public:
    std::string_view name() const ;
    
};
template <typename T>
extern const config_type config_type_for;
class config_file {
    static thread_local unsigned s_shard_id;
    struct any_value {
        
        virtual std::unique_ptr<any_value> clone() const = 0;
        virtual void update_from(const any_value* source) = 0;
    };
    std::vector<std::vector<std::unique_ptr<any_value>>> _per_shard_values { 1 };
public:
    typedef std::unordered_map<sstring, sstring> string_map;
    typedef std::vector<sstring> string_list;
    enum class value_status {
        Used,
        Unused,
        Invalid,
    };
    enum class liveness {
        LiveUpdate,
        MustRestart,
    };
    enum class config_source : uint8_t {
        None,
        SettingsFile,
        CommandLine,
        CQL,
        Internal,
        API,
    };
    struct config_src {
        config_file* _cf;
        std::string_view _name, _alias, _desc;
        const config_type* _type;
        size_t _per_shard_values_offset;
    protected:
        virtual const void* current_value() const = 0;
    public:
        config_src(config_file* cf, std::string_view name, const config_type* type, std::string_view desc) 
        ;
        
        
        config_file * get_config_file() const ;
        bool matches(std::string_view name) const;
        virtual void add_command_line_option(bpo::options_description_easy_init&) = 0;
        virtual void set_value(const YAML::Node&) = 0;
        virtual bool set_value(sstring, config_source = config_source::Internal) = 0;
        virtual future<> set_value_on_all_shards(const YAML::Node&) = 0;
        virtual future<bool> set_value_on_all_shards(sstring, config_source = config_source::Internal) = 0;
        virtual value_status status() const noexcept = 0;
        virtual config_source source() const noexcept = 0;
        sstring source_name() const noexcept;
        
    };
    template<typename T>
    struct named_value : public config_src {
    private:
        friend class config;
        config_source _source = config_source::None;
        value_status _value_status;
        struct the_value_type final : any_value {
            utils::updateable_value_source<T> value;
        };
        liveness _liveness;
        std::vector<T> _allowed_values;
    protected:
        
        virtual const void* current_value() const override ;
    public:
        typedef T type;
        typedef named_value<T> MyType;
        named_value(config_file* file, std::string_view name, std::string_view alias, liveness liveness_, value_status vs, const T& t = T(), std::string_view desc = {},
                std::initializer_list<T> allowed_values = {})
            : config_src(file, name, alias, &config_type_for<T>, desc)
            , _value_status(vs)
            , _liveness(liveness_)
            , _allowed_values(std::move(allowed_values)) {
            file->add(*this, std::make_unique<the_value_type>(std::move(t)));
        }
        
        
        named_value(config_file* file, std::string_view name, value_status vs, const T& t = T(), std::string_view desc = {},
                std::initializer_list<T> allowed_values = {})
                : named_value(file, name, {}, liveness::MustRestart, vs, t, desc, allowed_values) {
        }
        value_status status() const noexcept override ;
        config_source source() const noexcept override ;
        const T& operator()() const ;
        
        
        void add_command_line_option(bpo::options_description_easy_init&) override;
        void set_value(const YAML::Node&) override;
        bool set_value(sstring, config_source = config_source::Internal) override;
        // For setting a single value on all shards,
        // without having to call broadcast_to_all_shards
        // that broadcasts all values to all shards.
        future<> set_value_on_all_shards(const YAML::Node&) override;
        future<bool> set_value_on_all_shards(sstring, config_source = config_source::Internal) override;
    };
    typedef std::reference_wrapper<config_src> cfg_ref;
    config_file(std::initializer_list<cfg_ref> = {});
    
    
    void add(std::initializer_list<cfg_ref>);
    void add(const std::vector<cfg_ref> &);
    boost::program_options::options_description get_options_description();
    boost::program_options::options_description get_options_description(boost::program_options::options_description);
    boost::program_options::options_description_easy_init&
    add_options(boost::program_options::options_description_easy_init&);
    using error_handler = std::function<void(const sstring&, const sstring&, std::optional<value_status>)>;
    void read_from_yaml(const sstring&, error_handler = {});
    void read_from_yaml(const char *, error_handler = {});
    future<> read_from_file(const sstring&, error_handler = {});
    future<> read_from_file(file, error_handler = {});
    using configs = std::vector<cfg_ref>;
    configs set_values() const;
    configs unset_values() const;
    const configs& values() const ;
    future<> broadcast_to_all_shards();
private:
    configs
        _cfgs;
};
template <typename T>
requires requires (const config_file::named_value<T>& nv) {
    { nv().empty() } -> std::same_as<bool>;
}
const config_file::named_value<T>& operator||(const config_file::named_value<T>& a, const config_file::named_value<T>& b) {
    return !a().empty() ? a : b;
}
extern template struct config_file::named_value<seastar::log_level>;
}
namespace YAML {
template<>
struct convert<seastar::sstring> {
    static bool decode(const Node& node, sstring& rhs) ;
};
template<typename K, typename V, typename... Rest>
struct convert<std::unordered_map<K, V, Rest...>> {
    using map_type = std::unordered_map<K, V, Rest...>;
    static bool decode(const Node& node, map_type& rhs) {
        if (!node.IsMap()) {
            return false;
        }
        rhs.clear();
        for (auto& n : node) {
            rhs[n.first.as<K>()] = n.second.as<V>();
        }
        return true;
    }
};
}
namespace std {
;

template<typename V, typename... Args>
std::istream& operator>>(std::istream&, std::vector<V, Args...>&);
template<>
std::istream& operator>>(std::istream&, std::vector<seastar::sstring>&);
extern template
std::istream& operator>>(std::istream&, std::vector<seastar::sstring>&);
 ;
 ;
 ;
}
namespace utils {
namespace {
template<class T, class charT = char>
class typed_value_ex : public bpo::typed_value<T, charT> {
public:
    typedef bpo::typed_value<T, charT> _Super;
};
 ;
template <typename T>
void maybe_multitoken(std::vector<typed_value_ex<T>>* r) {
    r->multitoken();
}
 ;
}

}
class atomic_cell_view;
class collection_mutation_view;
class row_marker;
class row_tombstone;
class range_tombstone;
class tombstone;
class position_in_partition_view;
// When used on an entry, marks the range between this entry and the previous
// one as continuous or discontinuous, excluding the keys of both entries.
// This information doesn't apply to continuity of the entries themselves,
// that is specified by is_dummy flag.
// See class doc of mutation_partition.
using is_continuous = seastar::bool_class<class continuous_tag>;
// Dummy entry is an entry which is incomplete.
// Typically used for marking bounds of continuity range.
// See class doc of mutation_partition.
class dummy_tag {};
using is_dummy = seastar::bool_class<dummy_tag>;
// Guarantees:
//
// - any tombstones which affect cell's liveness are visited before that cell
//
// - rows are visited in ascending order with respect to their keys
//
// - row header (accept_row) is visited before that row's cells
//
// - row tombstones are visited in ascending order with respect to their key prefixes
//
// - cells in given row are visited in ascending order with respect to their column IDs
//
// - static row is visited before any clustered row
//
// - for each column in a row only one variant of accept_(static|row)_cell() is called, appropriate
//   for column's kind (atomic or collection).
//
class mutation_partition_visitor {
public:
    virtual void accept_partition_tombstone(tombstone) = 0;
    virtual void accept_static_cell(column_id, atomic_cell_view) = 0;
    virtual void accept_static_cell(column_id, collection_mutation_view) = 0;
    virtual void accept_row_tombstone(const range_tombstone&) = 0;
    virtual void accept_row(position_in_partition_view key, const row_tombstone& deleted_at, const row_marker& rm,
        is_dummy = is_dummy::no, is_continuous = is_continuous::yes) = 0;
    virtual void accept_row_cell(column_id id, atomic_cell_view) = 0;
    virtual void accept_row_cell(column_id id, collection_mutation_view) = 0;
};
class schema;
class row;
class mutation_partition;
class column_mapping;
class deletable_row;
class column_definition;
class abstract_type;
class atomic_cell_or_collection;
// Mutation partition visitor which applies visited data into
// existing mutation_partition. The visited data may be of a different schema.
// Data which is not representable in the new schema is dropped.
// Weak exception guarantees.
class converting_mutation_partition_applier : public mutation_partition_visitor {
    const schema& _p_schema;
    mutation_partition& _p;
    const column_mapping& _visited_column_mapping;
    deletable_row* _current_row;
private:
    public:
    converting_mutation_partition_applier(
            const column_mapping& visited_column_mapping,
            const schema& target_schema,
            mutation_partition& target);
    virtual void accept_partition_tombstone(tombstone t) override;
    void accept_static_cell(column_id id, atomic_cell cell);
    virtual void accept_static_cell(column_id id, atomic_cell_view cell) override;
    virtual void accept_static_cell(column_id id, collection_mutation_view collection) override;
    virtual void accept_row_tombstone(const range_tombstone& rt) override;
    virtual void accept_row(position_in_partition_view key, const row_tombstone& deleted_at, const row_marker& rm, is_dummy dummy, is_continuous continuous) override;
    void accept_row_cell(column_id id, atomic_cell cell);
    virtual void accept_row_cell(column_id id, atomic_cell_view cell) override;
    virtual void accept_row_cell(column_id id, collection_mutation_view collection) override;
    // Appends the cell to dst upgrading it to the new schema.
    // Cells must have monotonic names.
    static void append_cell(row& dst, column_kind kind, const column_definition& new_def, const column_definition& old_def, const atomic_cell_or_collection& cell);
};
namespace cql3 {
class column_identifier_raw;
class column_identifier final {
public:
    bytes bytes_;
private:
    sstring _text;
public:
    // less comparator sorting by text
    struct text_comparator {
        
    };
    
    column_identifier(bytes bytes_, data_type type);
    column_identifier(bytes bytes_, sstring text);
    bool operator==(const column_identifier& other) const;
    const sstring& text() const { return _text; }
    const bytes& name() const;
    sstring to_string() const;
    sstring to_cql_string() const;
#if 0
    public ColumnIdentifier clone(AbstractAllocator allocator)
    {
        return new ColumnIdentifier(allocator.clone(bytes), text);
    }
#endif
    using raw = column_identifier_raw;
};
class column_identifier_raw final {
private:
    const sstring _raw_text;
    sstring _text;
public:
    // for selectable::with_expression::raw:
    // for selectable::with_expression::raw:
    friend std::hash<column_identifier_raw>;
};
}
namespace std {
template<>
struct hash<cql3::column_identifier> {
    size_t operator()(const cql3::column_identifier& i) const {
        return std::hash<bytes>()(i.bytes_);
    }
};
template<>
struct hash<cql3::column_identifier_raw> {
    size_t operator()(const cql3::column_identifier::raw& r) const {
        return std::hash<sstring>()(r._text);
    }
};
}
namespace cql3 {
class column_specification;
class prepare_context;
}
namespace replica {
class database; // For transition; remove
}
class schema;
using schema_ptr = lw_shared_ptr<const schema>;
class view_ptr;
namespace db {
class config;
class extensions;
}
namespace secondary_index {
class secondary_index_manager;
}
namespace gms {
class feature_service;
}
namespace locator {
class abstract_replication_strategy;
}
// Classes representing the overall schema, but without access to data.
// Useful on the coordinator side (where access to data is via storage_proxy).
//
// Everything here is type-erased to reduce dependencies. No references are
// kept, so lower-level objects like keyspaces and tables must not be held
// across continuations.
namespace data_dictionary {
// Used to forward all operations to the underlying backing store.
class impl;
class user_types_metadata;
class keyspace_metadata;
class no_such_keyspace : public std::runtime_error {
public:
};
class no_such_column_family : public std::runtime_error {
public:
};
class table {
    const impl* _ops;
    const void* _table;
private:
    friend class impl;
public:
};
class keyspace {
    const impl* _ops;
    const void* _keyspace;
private:
    friend class impl;
public:
};
class database {
    const impl* _ops;
    const void* _database;
private:
    friend class impl;
public:
      // throws no_keyspace
      // throws no_such_column_family
      // throws no_such_column_family
      // throws no_such_column_family
      // throws no_such_column_family
     // For transition; remove
};
}
namespace data_dictionary {
class database;
class user_types_metadata;
}
namespace auth {
class resource;
}
namespace cql3 {
class ut_name;
class cql3_type final {
    data_type _type;
public:
    const sstring& to_string() const ;
    // For UserTypes, we need to know the current keyspace to resolve the
    // actual type used, so Raw is a "not yet prepared" CQL3Type.
    class raw {
        
    protected:
        bool _frozen = false;
    public:
        
        
        friend class auth::resource;
    };
private:
    class raw_type;
    class raw_collection;
    class raw_ut;
    class raw_tuple;
public:
    enum class kind : int8_t {
        ASCII, BIGINT, BLOB, BOOLEAN, COUNTER, DOUBLE, EMPTY, FLOAT, INT, SMALLINT, TINYINT, INET, TEXT, TIMESTAMP, UUID, TIMEUUID, DATE, TIME, DURATION
    };
    using kind_enum = super_enum<kind,
        kind::ASCII,
        kind::BIGINT,
        kind::BLOB,
        kind::BOOLEAN,
        kind::COUNTER,
        kind::DOUBLE,
        kind::EMPTY,
        kind::FLOAT,
        kind::INET,
        kind::INT,
        kind::SMALLINT,
        kind::TINYINT,
        kind::TEXT,
        kind::TIMESTAMP,
        kind::UUID,
        kind::TIMEUUID,
        kind::DATE,
        kind::TIME,
        kind::DURATION>;
    using kind_enum_set = enum_set<kind_enum>;
    static thread_local cql3_type ascii;
    static thread_local cql3_type bigint;
    static thread_local cql3_type blob;
    static thread_local cql3_type boolean;
    static thread_local cql3_type double_;
    static thread_local cql3_type empty;
    static thread_local cql3_type float_;
    static thread_local cql3_type int_;
    static thread_local cql3_type smallint;
    static thread_local cql3_type text;
    static thread_local cql3_type timestamp;
    static thread_local cql3_type tinyint;
    static thread_local cql3_type uuid;
    static thread_local cql3_type timeuuid;
    static thread_local cql3_type date;
    static thread_local cql3_type time;
    static thread_local cql3_type inet;
    static thread_local cql3_type counter;
    static thread_local cql3_type duration;
    
public:
};
#if 0
    public static class Custom implements CQL3Type
    {
        private final AbstractType<?> type;
        public Custom(AbstractType<?> type)
        {
            this.type = type;
        }
        public Custom(String className) throws SyntaxException, ConfigurationException
        {
            this(TypeParser.parse(className));
        }
        public boolean isCollection()
        {
            return false;
        }
        public AbstractType<?> getType()
        {
            return type;
        }
        @Override
        public final boolean equals(Object o)
        {
            if(!(o instanceof Custom))
                return false;
            Custom that = (Custom)o;
            return type.equals(that.type);
        }
        @Override
        public final int hashCode()
        {
            return type.hashCode();
        }
        @Override
        public String toString()
        {
            return "'" + type + "'";
        }
    }
    public static class Collection implements CQL3Type
    {
        private final CollectionType type;
        public Collection(CollectionType type)
        {
            this.type = type;
        }
        public AbstractType<?> getType()
        {
            return type;
        }
        public boolean isCollection()
        {
            return true;
        }
        @Override
        public final boolean equals(Object o)
        {
            if(!(o instanceof Collection))
                return false;
            Collection that = (Collection)o;
            return type.equals(that.type);
        }
        @Override
        public final int hashCode()
        {
            return type.hashCode();
        }
        @Override
        public String toString()
        {
            boolean isFrozen = !this.type.isMultiCell();
            StringBuilder sb = new StringBuilder(isFrozen ? "frozen<" : "");
            switch (type.kind)
            {
                case LIST:
                    AbstractType<?> listType = ((ListType)type).getElementsType();
                    sb.append("list<").append(listType.asCQL3Type());
                    break;
                case SET:
                    AbstractType<?> setType = ((SetType)type).getElementsType();
                    sb.append("set<").append(setType.asCQL3Type());
                    break;
                case MAP:
                    AbstractType<?> keysType = ((MapType)type).getKeysType();
                    AbstractType<?> valuesType = ((MapType)type).getValuesType();
                    sb.append("map<").append(keysType.asCQL3Type()).append(", ").append(valuesType.asCQL3Type());
                    break;
                default:
                    throw new AssertionError();
            }
            sb.append(">");
            if (isFrozen)
                sb.append(">");
            return sb.toString();
        }
    }
    public static class UserDefined implements CQL3Type
    {
        // Keeping this separatly from type just to simplify toString()
        private final String name;
        private final UserType type;
        private UserDefined(String name, UserType type)
        {
            this.name = name;
            this.type = type;
        }
        public static UserDefined create(UserType type)
        {
            return new UserDefined(UTF8Type.instance.compose(type.name), type);
        }
        public boolean isCollection()
        {
            return false;
        }
        public AbstractType<?> getType()
        {
            return type;
        }
        @Override
        public final boolean equals(Object o)
        {
            if(!(o instanceof UserDefined))
                return false;
            UserDefined that = (UserDefined)o;
            return type.equals(that.type);
        }
        @Override
        public final int hashCode()
        {
            return type.hashCode();
        }
        @Override
        public String toString()
        {
            return name;
        }
    }
    public static class Tuple implements CQL3Type
    {
        private final TupleType type;
        private Tuple(TupleType type)
        {
            this.type = type;
        }
        public static Tuple create(TupleType type)
        {
            return new Tuple(type);
        }
        public boolean isCollection()
        {
            return false;
        }
        public AbstractType<?> getType()
        {
            return type;
        }
        @Override
        public final boolean equals(Object o)
        {
            if(!(o instanceof Tuple))
                return false;
            Tuple that = (Tuple)o;
            return type.equals(that.type);
        }
        @Override
        public final int hashCode()
        {
            return type.hashCode();
        }
        @Override
        public String toString()
        {
            StringBuilder sb = new StringBuilder();
            sb.append("tuple<");
            for (int i = 0; i < type.size(); i++)
            {
                if (i > 0)
                    sb.append(", ");
                sb.append(type.type(i).asCQL3Type());
            }
            sb.append(">");
            return sb.toString();
        }
    }
#endif
}
namespace auth {
enum class authentication_option {
    password,
    options
};
using authentication_option_set = std::unordered_set<authentication_option>;
using custom_options = std::unordered_map<sstring, sstring>;
struct authentication_options final {
    std::optional<sstring> password;
    std::optional<custom_options> options;
};
class unsupported_authentication_option : public std::invalid_argument {
public:
};
}
template <>
struct fmt::formatter<auth::authentication_option> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const auth::authentication_option a, FormatContext& ctx) const {
        using enum auth::authentication_option;
        switch (a) {
        case password:
            return formatter<std::string_view>::format("PASSWORD", ctx);
        case options:
            return formatter<std::string_view>::format("OPTIONS", ctx);
        }
        std::abort();
    }
};
namespace auth {
enum class permission {
    //Deprecated
    READ,
    //Deprecated
    WRITE,
    // schema management
    CREATE, // required for CREATE KEYSPACE and CREATE TABLE.
    ALTER,  // required for ALTER KEYSPACE, ALTER TABLE, CREATE INDEX, DROP INDEX.
    DROP,   // required for DROP KEYSPACE and DROP TABLE.
    // data access
    SELECT, // required for SELECT.
    MODIFY, // required for INSERT, UPDATE, DELETE, TRUNCATE.
    // permission management
    AUTHORIZE, // required for GRANT and REVOKE.
    DESCRIBE, // required on the root-level role resource to list all roles.
    // function/aggregate/procedure calls
    EXECUTE,
};
typedef enum_set<
        super_enum<
                permission,
                permission::READ,
                permission::WRITE,
                permission::CREATE,
                permission::ALTER,
                permission::DROP,
                permission::SELECT,
                permission::MODIFY,
                permission::AUTHORIZE,
                permission::DESCRIBE,
                permission::EXECUTE>> permission_set;
namespace permissions {
extern const permission_set ALL;
extern const permission_set NONE;
}
}
#ifndef UTILS_HASH_HH_
#define UTILS_HASH_HH_
namespace utils {
// public for unit testing etc
struct tuple_hash {
private:
    // CMH. Add specializations here to handle recursive tuples
     ;
    template<size_t index, typename...Types>
    struct hash_impl {
    };
    template<class...Types>
    struct hash_impl<0, Types...> {
    };
public:
    // All the operator() implementations are templates, so this is transparent.
    using is_transparent = void;
    template<typename T1, typename T2>
    size_t operator()(const std::pair<T1, T2>& p) const ;
    template<typename T1, typename T2>
    size_t operator()(const T1& t1, const T2& t2) const ;
    template<typename... Args>
    size_t operator()(const std::tuple<Args...>& v) const;
};
}
#endif 
namespace auth {
class invalid_resource_name : public std::invalid_argument {
public:
};
enum class resource_kind {
    data, role, service_level, functions
};
///
/// Type tag for constructing data resources.
///
struct data_resource_t final {};
///
/// Type tag for constructing role resources.
///
struct role_resource_t final {};
///
/// Type tag for constructing service_level resources.
///
struct service_level_resource_t final {};
///
/// Type tag for constructing function resources.
///
struct functions_resource_t final {};
///
/// Resources are entities that users can be granted permissions on.
///
/// There are data (keyspaces and tables), role and function resources. There may be other kinds of resources in the future.
///
/// When they are stored as system metadata, resources have the form `root/part_0/part_1/.../part_n`. Each kind of
/// resource has a specific root prefix, followed by a maximum of `n` parts (where `n` is distinct for each kind of
/// resource as well). In this code, this form is called the "name".
///
/// Since all resources have this same structure, all the different kinds are stored in instances of the same class:
/// \ref resource. When we wish to query a resource for kind-specific data (like the table of a "data" resource), we
/// create a kind-specific "view" of the resource.
///
class resource final {
    resource_kind _kind;
    utils::small_vector<sstring, 3> _parts;
public:
    ///
    /// A root resource of a particular kind.
    ///
    resource(functions_resource_t, std::string_view keyspace, std::string_view function_name,
            std::vector<::shared_ptr<cql3::cql3_type::raw>> function_args);
    
    ///
    /// A machine-friendly identifier unique to each resource.
    ///
    
private:
    friend class std::hash<resource>;
    friend class data_resource_view;
    friend class role_resource_view;
    friend class service_level_resource_view;
    friend class functions_resource_view;
    
};

class resource_kind_mismatch : public std::invalid_argument {
public:
    explicit resource_kind_mismatch(resource_kind expected, resource_kind actual)
        : std::invalid_argument(
            format("This resource has kind '{}', but was expected to have kind '{}'.", actual, expected)) {
    }
};
/// A "data" view of \ref resource.
///
/// If neither `keyspace` nor `table` is present, this is the root resource.
class data_resource_view final {
    const resource& _resource;
public:
    ///
    /// \throws `resource_kind_mismatch` if the argument is not a `data` resource.
    ///
    explicit data_resource_view(const resource& r);
    std::optional<std::string_view> keyspace() const;
    std::optional<std::string_view> table() const;
};
std::ostream& operator<<(std::ostream&, const data_resource_view&);
///
/// A "role" view of \ref resource.
///
/// If `role` is not present, this is the root resource.
///
class role_resource_view final {
    const resource& _resource;
public:
    ///
    /// \throws \ref resource_kind_mismatch if the argument is not a "role" resource.
    ///
    explicit role_resource_view(const resource&);
    std::optional<std::string_view> role() const;
};
std::ostream& operator<<(std::ostream&, const role_resource_view&);
///
/// A "service_level" view of \ref resource.
///
class service_level_resource_view final {
public:
    ///
    /// \throws \ref resource_kind_mismatch if the argument is not a "service_level" resource.
    ///
    explicit service_level_resource_view(const resource&);
};
std::ostream& operator<<(std::ostream&, const service_level_resource_view&);
///
/// A "function" view of \ref resource.
///
class functions_resource_view final {
    const resource& _resource;
public:
    ///
    /// \throws \ref resource_kind_mismatch if the argument is not a "function" resource.
    ///
    explicit functions_resource_view(const resource&);
    std::optional<std::string_view> keyspace() const;
    std::optional<std::string_view> function_signature() const;
    std::optional<std::vector<std::string_view>> function_args() const;
};
///
/// Parse a resource from its name.
///
/// \throws \ref invalid_resource_name when the name is malformed.
///
inline resource make_functions_resource(std::string_view keyspace, std::string_view function_name, std::vector<::shared_ptr<cql3::cql3_type::raw>> function_signature) {
    return resource(functions_resource_t{}, keyspace, function_name, function_signature);
}
sstring encode_signature(std::string_view name, std::vector<data_type> args);
std::pair<sstring, std::vector<data_type>> decode_signature(std::string_view encoded_signature);
}
template <>
struct fmt::formatter<auth::resource_kind> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const auth::resource_kind kind, FormatContext& ctx) const {
        using enum auth::resource_kind;
        switch (kind) {
        case data:
            return formatter<std::string_view>::format("data", ctx);
        case role:
            return formatter<std::string_view>::format("role", ctx);
        case service_level:
            return formatter<std::string_view>::format("service_level", ctx);
        case functions:
            return formatter<std::string_view>::format("functions", ctx);
        }
        std::abort();
    }
};
namespace std {
template <>
struct hash<auth::resource> {
    static size_t hash_data(const auth::data_resource_view& dv) {
        return utils::tuple_hash()(std::make_tuple(auth::resource_kind::data, dv.keyspace(), dv.table()));
    }
    static size_t hash_role(const auth::role_resource_view& rv) {
        return utils::tuple_hash()(std::make_tuple(auth::resource_kind::role, rv.role()));
    }
    static size_t hash_service_level(const auth::service_level_resource_view& rv) {
            return utils::tuple_hash()(std::make_tuple(auth::resource_kind::service_level));
    }
    static size_t hash_function(const auth::functions_resource_view& fv) {
        return utils::tuple_hash()(std::make_tuple(auth::resource_kind::functions, fv.keyspace(), fv.function_signature()));
    }
    size_t operator()(const auth::resource& r) const {
        std::size_t value;
        switch (r._kind) {
        case auth::resource_kind::data: value = hash_data(auth::data_resource_view(r)); break;
        case auth::resource_kind::role: value = hash_role(auth::role_resource_view(r)); break;
        case auth::resource_kind::service_level: value = hash_service_level(auth::service_level_resource_view(r)); break;
        case auth::resource_kind::functions: value = hash_function(auth::functions_resource_view(r)); break;
        }
        return value;
    }
};
}
namespace auth {
using resource_set = std::unordered_set<resource>;
//
// A resource and all of its parents.
//
}
namespace auth {
///
/// A type-safe wrapper for the name of a logged-in user, or a nameless (anonymous) user.
///
class authenticated_user final {
public:
    ///
    /// An anonymous user has no name.
    ///
    std::optional<sstring> name{};
    ///
    /// An anonymous user.
    ///
    authenticated_user() = default;
    
    
};

 
}
///
/// The user name, or "anonymous".
///
template <>
struct fmt::formatter<auth::authenticated_user> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const auth::authenticated_user& u, FormatContext& ctx) const {
        if (u.name) {
            return fmt::format_to(ctx.out(), "{}", *u.name);
        } else {
            return fmt::format_to(ctx.out(), "{}", "anonymous");
        }
    }
};
namespace std {
template <>
struct hash<auth::authenticated_user> final {
    size_t operator()(const auth::authenticated_user &u) const {
        return std::hash<std::optional<sstring>>()(u.name);
    }
};
}
namespace auth {
///
/// A stateful SASL challenge which supports many authentication schemes (depending on the implementation).
///
class sasl_challenge {
public:
    virtual ~sasl_challenge() = default;
};
class plain_sasl_challenge : public sasl_challenge {
public:
    using completion_callback = std::function<future<authenticated_user>(std::string_view, std::string_view)>;
private:
    std::optional<sstring> _username, _password;
    completion_callback _when_complete;
};
}
namespace db {
    class config;
}
namespace auth {
class authenticated_user;
///
/// Abstract client for authenticating role identity.
///
/// All state necessary to authorize a role is stored externally to the client instance.
///
class authenticator {
public:
    using ptr_type = std::unique_ptr<authenticator>;
    ///
    /// The name of the key to be used for the user-name part of password authentication with \ref authenticate.
    ///
    static const sstring USERNAME_KEY;
    ///
    /// The name of the key to be used for the password part of password authentication with \ref authenticate.
    ///
    static const sstring PASSWORD_KEY;
    using credentials_map = std::unordered_map<sstring, sstring>;
    ///
    /// A fully-qualified (class with package) Java-like name for this implementation.
    ///
    ///
    /// A subset of `supported_options()` that users are permitted to alter for themselves.
    ///
    ///
    /// Authenticate a user given implementation-specific credentials.
    ///
    /// If this implementation does not require authentication (\ref require_authentication), an anonymous user may
    /// result.
    ///
    /// \returns an exceptional future with \ref exceptions::authentication_exception if given invalid credentials.
    ///
    ///
    /// Create an authentication record for a new user. This is required before the user can log-in.
    ///
    /// The options provided must be a subset of `supported_options()`.
    ///
    ///
    /// Alter the authentication record of an existing user.
    ///
    /// The options provided must be a subset of `supported_options()`.
    ///
    /// Callers must ensure that the specification of `alterable_options()` is adhered to.
    ///
    ///
    /// Delete the authentication record for a user. This will disallow the user from logging in.
    ///
    ///
    /// Query for custom options (those corresponding to \ref authentication_options::options).
    ///
    /// If no options are set the result is an empty container.
    ///
    ///
    /// System resources used internally as part of the implementation. These are made inaccessible to users.
    ///
};
}
namespace auth {
class role_or_anonymous;
struct permission_details {
    sstring role_name;
    ::auth::resource resource;
    permission_set permissions;
};
class unsupported_authorization_operation : public std::invalid_argument {
public:
    using std::invalid_argument::invalid_argument;
};
///
/// Abstract client for authorizing roles to access resources.
///
/// All state necessary to authorize a role is stored externally to the client instance.
///
class authorizer {
public:
    using ptr_type = std::unique_ptr<authorizer>;
    ///
    /// A fully-qualified (class with package) Java-like name for this implementation.
    ///
    ///
    /// Query for the permissions granted directly to a role for a particular \ref resource (and not any of its
    /// parents).
    ///
    /// The optional role name is empty when an anonymous user is authorized. Some implementations may still wish to
    /// grant default permissions in this case.
    ///
    ///
    /// Grant a set of permissions to a role for a particular \ref resource.
    ///
    /// \throws \ref unsupported_authorization_operation if granting permissions is not supported.
    ///
    ///
    /// Revoke a set of permissions from a role for a particular \ref resource.
    ///
    /// \throws \ref unsupported_authorization_operation if revoking permissions is not supported.
    ///
    ///
    /// Query for all directly granted permissions.
    ///
    /// \throws \ref unsupported_authorization_operation if listing permissions is not supported.
    ///
    virtual future<std::vector<permission_details>> list_all() const = 0;
    ///
    /// Revoke all permissions granted directly to a particular role.
    ///
    /// \throws \ref unsupported_authorization_operation if revoking permissions is not supported.
    ///
    ///
    /// Revoke all permissions granted to any role for a particular resource.
    ///
    /// \throws \ref unsupported_authorization_operation if revoking permissions is not supported.
    ///
    ///
    /// System resources used internally as part of the implementation. These are made inaccessible to users.
    ///
};
}
namespace auth {
class role_or_anonymous final {
public:
    std::optional<sstring> name{};
    
};

bool is_anonymous(const role_or_anonymous&) noexcept;
}
namespace std {
template <>
struct hash<auth::role_or_anonymous> {
    size_t operator()(const auth::role_or_anonymous& mr) const {
        return hash<std::optional<sstring>>()(mr.name);
    }
};
}
namespace bi = boost::intrusive;
namespace utils {
struct do_nothing_loading_shared_values_stats {
    static void inc_hits() noexcept ; // Increase the number of times entry was found ready
    static void inc_misses() noexcept ; // Increase the number of times entry was not found
    static void inc_blocks() noexcept ; // Increase the number of times entry was not ready (>= misses)
    static void inc_evictions() noexcept ; // Increase the number of times entry was evicted
};
// Entries stay around as long as there is any live external reference (entry_ptr) to them.
// Supports asynchronous insertion, ensures that only one entry will be loaded.
// InitialBucketsCount is required to be greater than zero. Otherwise a constructor will throw an
// std::invalid_argument exception.
template<typename Key,
         typename Tp,
         typename Hash = std::hash<Key>,
         typename EqualPred = std::equal_to<Key>,
         typename Stats = do_nothing_loading_shared_values_stats,
         size_t InitialBucketsCount = 16>
requires requires () {
    Stats::inc_hits();
    Stats::inc_misses();
    Stats::inc_blocks();
    Stats::inc_evictions();
}
class loading_shared_values {
public:
    using key_type = Key;
    using value_type = Tp;
    static constexpr size_t initial_buckets_count = InitialBucketsCount;
private:
    class entry : public bi::unordered_set_base_hook<bi::store_hash<true>>, public enable_lw_shared_from_this<entry> {
    private:
        loading_shared_values& _parent;
        key_type _key;
        std::optional<value_type> _val;
        shared_promise<> _loaded;
    public:
        const key_type& key() const noexcept {
            return _key;
        }
        const value_type& value() const noexcept {
            return *_val;
        }
        value_type& value() noexcept {
            return *_val;
        }
        /// \brief "Release" the object from the contained value.
        /// After this call the state of the value kept inside this object is undefined and it may no longer be used.
        ///
        /// \return The r-value reference to the value kept inside this object.
        value_type&& release() {
            return *std::move(_val);
        }
        void set_value(value_type new_val) {
            _val.emplace(std::move(new_val));
        }
        bool orphaned() const {
            return !is_linked();
        }
        shared_promise<>& loaded() {
            return _loaded;
        }
        bool ready() const noexcept {
            return bool(_val);
        }
        entry(loading_shared_values& parent, key_type k)
                : _parent(parent), _key(std::move(k)) {}
        ~entry() {
            if (is_linked()) {
                _parent._set.erase(_parent._set.iterator_to(*this));
            }
            Stats::inc_evictions();
        }
        friend bool operator==(const entry& a, const entry& b){
            return EqualPred()(a.key(), b.key());
        }
        friend std::size_t hash_value(const entry& v) {
            return Hash()(v.key());
        }
    };
    template<typename KeyType, typename KeyEqual>
    struct key_eq {
        bool operator()(const KeyType& k, const entry& c) const {
           return KeyEqual()(k, c.key());
        }
        bool operator()(const entry& c, const KeyType& k) const {
           return KeyEqual()(c.key(), k);
        }
    };
    using set_type = bi::unordered_set<entry, bi::power_2_buckets<true>, bi::compare_hash<true>>;
    using bi_set_bucket_traits = typename set_type::bucket_traits;
    using set_iterator = typename set_type::iterator;
    enum class shrinking_is_allowed { no, yes };
public:
    // Pointer to entry value
    class entry_ptr {
        lw_shared_ptr<entry> _e;
    public:
        using element_type = value_type;
        entry_ptr() = default;
        entry_ptr(std::nullptr_t) noexcept : _e() {};
        explicit entry_ptr(lw_shared_ptr<entry> e) : _e(std::move(e)) {}
        entry_ptr& operator=(std::nullptr_t) noexcept {
            _e = nullptr;
            return *this;
        }
        explicit operator bool() const noexcept { return bool(_e); }
        bool operator==(const entry_ptr&) const = default;
        element_type& operator*() const noexcept { return _e->value(); }
        element_type* operator->() const noexcept { return &_e->value(); }
        /// \brief Get the wrapped value. Avoid the copy if this is the last reference to this value.
        /// If this is the last reference then the wrapped value is going to be std::move()ed. Otherwise it's going to
        /// be copied.
        /// \return The wrapped value.
        element_type release() {
            auto res = _e.owned() ? _e->release() : _e->value();
            _e = nullptr;
            return res;
        }
        // Returns the key this entry is associated with.
        // Valid if bool(*this).
        const key_type& key() const {
            return _e->key();
        }
        // Returns true iff the entry is not linked in the set.
        // Call only when bool(*this).
        bool orphaned() const {
            return _e->orphaned();
        }
        friend class loading_shared_values;
        friend std::ostream& operator<<(std::ostream& os, const entry_ptr& ep) {
            return os << ep._e.get();
        }
    };
private:
    std::vector<typename set_type::bucket_type> _buckets;
    set_type _set;
public:
    static const key_type& to_key(const entry_ptr& e_ptr) noexcept {
        return e_ptr._e->key();
    }
    /// \throw std::invalid_argument if InitialBucketsCount is zero
    loading_shared_values()
        : _buckets(InitialBucketsCount)
        , _set(bi_set_bucket_traits(_buckets.data(), _buckets.size()))
    {
        static_assert(noexcept(Stats::inc_evictions()), "Stats::inc_evictions must be non-throwing");
        static_assert(noexcept(Stats::inc_hits()), "Stats::inc_hits must be non-throwing");
        static_assert(noexcept(Stats::inc_misses()), "Stats::inc_misses must be non-throwing");
        static_assert(noexcept(Stats::inc_blocks()), "Stats::inc_blocks must be non-throwing");
        static_assert(InitialBucketsCount && ((InitialBucketsCount & (InitialBucketsCount - 1)) == 0), "Initial buckets count should be a power of two");
    }
    loading_shared_values(loading_shared_values&&) = default;
    loading_shared_values(const loading_shared_values&) = delete;
    ~loading_shared_values() {
         assert(!_set.size());
    }
    /// \brief
    /// Returns a future which resolves with a shared pointer to the entry for the given key.
    /// Always returns a valid pointer if succeeds.
    ///
    /// If entry is missing, the loader is invoked. If entry is already loading, this invocation
    /// will wait for prior loading to complete and use its result when it's done.
    ///
    /// The loader object does not survive deferring, so the caller must deal with its liveness.
    template<typename Loader>
    future<entry_ptr> get_or_load(const key_type& key, Loader&& loader) noexcept {
        static_assert(std::is_same<future<value_type>, typename futurize<std::result_of_t<Loader(const key_type&)>>::type>::value, "Bad Loader signature");
        try {
            auto i = _set.find(key, Hash(), key_eq<key_type, EqualPred>());
            lw_shared_ptr<entry> e;
            future<> f = make_ready_future<>();
            if (i != _set.end()) {
                e = i->shared_from_this();
                // take a short cut if the value is ready
                if (e->ready()) {
                    Stats::inc_hits();
                    return make_ready_future<entry_ptr>(entry_ptr(std::move(e)));
                }
                f = e->loaded().get_shared_future();
            } else {
                Stats::inc_misses();
                e = make_lw_shared<entry>(*this, key);
                rehash_before_insert();
                _set.insert(*e);
                // get_shared_future() may throw, so make sure to call it before invoking the loader(key)
                f = e->loaded().get_shared_future();
                // Future indirectly forwarded to `e`.
                (void)futurize_invoke([&] { return loader(key); }).then_wrapped([e](future<value_type>&& val_fut) mutable {
                    if (val_fut.failed()) {
                        e->loaded().set_exception(val_fut.get_exception());
                    } else {
                        e->set_value(val_fut.get0());
                        e->loaded().set_value();
                    }
                });
            }
            if (!f.available()) {
                Stats::inc_blocks();
                return f.then([e]() mutable {
                    return entry_ptr(std::move(e));
                });
            } else if (f.failed()) {
                return make_exception_future<entry_ptr>(std::move(f).get_exception());
            } else {
                Stats::inc_hits();
                return make_ready_future<entry_ptr>(entry_ptr(std::move(e)));
            }
        } catch (...) {
            return make_exception_future<entry_ptr>(std::current_exception());
        }
    }
    /// \brief Try to rehash the container so that the load factor is between 0.25 and 0.75.
    /// \throw May throw if allocation of a new buckets array throws.
    void rehash() {
        rehash<shrinking_is_allowed::yes>(_set.size());
    }
    size_t buckets_count() const {
        return _buckets.size();
    }
    size_t size() const {
        return _set.size();
    }
    template<typename KeyType, typename KeyHasher, typename KeyEqual>
    entry_ptr find(const KeyType& key, KeyHasher key_hasher_func, KeyEqual key_equal_func) noexcept {
        set_iterator it = _set.find(key, std::move(key_hasher_func), key_eq<KeyType, KeyEqual>());
        if (it == _set.end() || !it->ready()) {
            return entry_ptr();
        }
        return entry_ptr(it->shared_from_this());
    };
    // Removes a given key from this container.
    // If a given key is currently loading, the loading will succeed and will return entry_ptr
    // to the caller, but the value will not be present in the container. It will be removed
    // when the last entry_ptr dies, as usual.
    //
    // Post-condition: !find(key)
    template<typename KeyType, typename KeyHasher, typename KeyEqual>
    void remove(const KeyType& key, KeyHasher key_hasher_func, KeyEqual key_equal_func) {
        set_iterator it = _set.find(key, std::move(key_hasher_func), key_eq<KeyType, KeyEqual>());
        if (it != _set.end()) {
            _set.erase(it);
        }
    }
    // Removes a given key from this container.
    // If a given key is currently loading, the loading will succeed and will return entry_ptr
    // to the caller, but the value will not be present in the container. It will be removed
    // when the last entry_ptr dies, as usual.
    //
    // Post-condition: !find(key)
    template<typename KeyType>
    void remove(const KeyType& key) {
        remove(key, Hash(), EqualPred());
    }
    // Removes all values which match a given predicate or are currently loading.
    // Guarantees that no values which match the predicate and whose loading was initiated
    // before this call will be present after this call (or appear at any time later).
    // Same effects as if remove(e.key()) was called on each matching entry.
    template<typename Pred>
    requires std::is_invocable_r_v<bool, Pred, const Tp&>
    void remove_if(const Pred& pred) {
        auto it = _set.begin();
        while (it != _set.end()) {
            if (!it->ready() || pred(it->value())) {
                auto next = std::next(it);
                _set.erase(it);
                it = next;
            } else {
                ++it;
            }
        }
    }
    // keep the default non-templated overloads to ease on the compiler for specifications
    // that do not require the templated find().
    entry_ptr find(const key_type& key) noexcept {
        return find(key, Hash(), EqualPred());
    }
private:
    void rehash_before_insert() noexcept {
        try {
            rehash<shrinking_is_allowed::no>(_set.size() + 1);
        } catch (...) {
            // if rehashing fails - continue with the current buckets array
        }
    }
    template <shrinking_is_allowed ShrinkingIsAllowed>
    void rehash(size_t new_size) {
        size_t new_buckets_count = 0;
        // Try to keep the load factor between 0.25 (when shrinking is allowed) and 0.75.
        if (ShrinkingIsAllowed == shrinking_is_allowed::yes && new_size < buckets_count() / 4) {
            if (!new_size) {
                new_buckets_count = 1;
            } else {
                new_buckets_count = size_t(1) << log2floor(new_size * 4);
            }
        } else if (new_size > 3 * buckets_count() / 4) {
            new_buckets_count = buckets_count() * 2;
        }
        if (new_buckets_count < InitialBucketsCount) {
            return;
        }
        std::vector<typename set_type::bucket_type> new_buckets(new_buckets_count);
        _set.rehash(bi_set_bucket_traits(new_buckets.data(), new_buckets.size()));
        _buckets = std::move(new_buckets);
    }
};
}
namespace bi = boost::intrusive;
namespace utils {
enum class loading_cache_reload_enabled { no, yes };
struct loading_cache_config final {
    size_t max_size = 0;
    seastar::lowres_clock::duration expiry;
    seastar::lowres_clock::duration refresh;
};
template <typename Tp>
struct simple_entry_size {
    size_t operator()(const Tp& val) ;
};
struct do_nothing_loading_cache_stats {
    // Accounts events when entries are evicted from the unprivileged cache section due to size restriction.
    // These events are interesting because they are an indication of a cache pollution event.
    ;
    // A metric complementary to the above one. Both combined allow to get the total number of cache evictions
    ;
};
/// \brief Loading cache is a cache that loads the value into the cache using the given asynchronous callback.
///
/// Each cached value if reloading is enabled (\tparam ReloadEnabled == loading_cache_reload_enabled::yes) is reloaded after
/// the "refresh" time period since it was loaded for the last time.
///
/// The values are going to be evicted from the cache if they are not accessed during the "expiration" period or haven't
/// been reloaded even once during the same period.
///
/// If "expiration" is set to zero - the caching is going to be disabled and get_XXX(...) is going to call the "loader" callback
/// every time in order to get the requested value.
///
/// \note In order to avoid the eviction of cached entries due to "aging" of the contained value the user has to choose
/// the "expiration" to be at least ("refresh" + "max load latency"). This way the value is going to stay in the cache and is going to be
/// read in a non-blocking way as long as it's frequently accessed. Note however that since reloading is an asynchronous
/// procedure it may get delayed by other running task. Therefore choosing the "expiration" too close to the ("refresh" + "max load latency")
/// value one risks to have his/her cache values evicted when the system is heavily loaded.
///
/// The cache is also limited in size and if adding the next value is going
/// to exceed the cache size limit the least recently used value(s) is(are) going to be evicted until the size of the cache
/// becomes such that adding the new value is not going to break the size limit. If the new entry's size is greater than
/// the cache size then the get_XXX(...) method is going to return a future with the loading_cache::entry_is_too_big exception.
///
/// The cache is comprised of 2 dynamic sections.
/// Total size of both sections should not exceed the maximum cache size.
/// New cache entry is always added to the unprivileged section.
/// After a cache entry is read more than SectionHitThreshold times it moves to the second (privileged) cache section.
/// Both sections' entries obey expiration and reload rules as explained above.
/// When cache entries need to be evicted due to a size restriction unprivileged section least recently used entries are evicted first.
/// If cache size is still too big event after there are no more entries in the unprivileged section the least recently used entries
/// from the privileged section are going to be evicted till the cache size restriction is met.
///
/// The size of the cache is defined as a sum of sizes of all cached entries.
/// The size of each entry is defined by the value returned by the \tparam EntrySize predicate applied on it.
///
/// The get(key) or get_ptr(key) methods ensures that the "loader" callback is called only once for each cached entry regardless of how many
/// callers are calling for the get_XXX(key) for the same "key" at the same time. Only after the value is evicted from the cache
/// it's going to be "loaded" in the context of get_XXX(key). As long as the value is cached get_XXX(key) is going to return the
/// cached value immediately and reload it in the background every "refresh" time period as described above.
///
/// \tparam Key type of the cache key
/// \tparam Tp type of the cached value
/// \tparam SectionHitThreshold number of hits after which a cache item is going to be moved to the privileged cache section.
/// \tparam ReloadEnabled if loading_cache_reload_enabled::yes allow reloading the values otherwise don't reload
/// \tparam EntrySize predicate to calculate the entry size
/// \tparam Hash hash function
/// \tparam EqualPred equality predicate
/// \tparam LoadingSharedValuesStats statistics incrementing class (see utils::loading_shared_values)
/// \tparam Alloc elements allocator
template<typename Key,
         typename Tp,
         int SectionHitThreshold = 0,
         loading_cache_reload_enabled ReloadEnabled = loading_cache_reload_enabled::no,
         typename EntrySize = simple_entry_size<Tp>,
         typename Hash = std::hash<Key>,
         typename EqualPred = std::equal_to<Key>,
         typename LoadingSharedValuesStats = utils::do_nothing_loading_shared_values_stats,
         typename LoadingCacheStats = utils::do_nothing_loading_cache_stats,
         typename Alloc = std::pmr::polymorphic_allocator<>>
class loading_cache {
    using loading_cache_clock_type = seastar::lowres_clock;
    using safe_link_list_hook = bi::list_base_hook<bi::link_mode<bi::safe_link>>;
    class timestamped_val {
    public:
        using value_type = Tp;
        using loading_values_type = typename utils::loading_shared_values<Key, timestamped_val, Hash, EqualPred, LoadingSharedValuesStats, 256>;
        class lru_entry;
        class value_ptr;
    private:
        value_type _value;
        loading_cache_clock_type::time_point _loaded;
        loading_cache_clock_type::time_point _last_read;
        lru_entry* _lru_entry_ptr = nullptr; /// MRU item is at the front, LRU - at the back
        size_t _size = 0;
    public:
        timestamped_val(value_type val)
            : _value(std::move(val))
            , _loaded(loading_cache_clock_type::now())
            , _last_read(_loaded)
            , _size(EntrySize()(_value))
        {}
        timestamped_val(timestamped_val&&) = default;
        timestamped_val& operator=(value_type new_val) {
            assert(_lru_entry_ptr);
            _value = std::move(new_val);
            _loaded = loading_cache_clock_type::now();
            _lru_entry_ptr->owning_section_size() -= _size;
            _size = EntrySize()(_value);
            _lru_entry_ptr->owning_section_size() += _size;
            return *this;
        }
        value_type& value() noexcept { return _value; }
        const value_type& value() const noexcept { return _value; }
        static const timestamped_val& container_of(const value_type& value) {
            return *bi::get_parent_from_member(&value, &timestamped_val::_value);
        }
        loading_cache_clock_type::time_point last_read() const noexcept {
            return _last_read;
        }
        loading_cache_clock_type::time_point loaded() const noexcept {
            return _loaded;
        }
        size_t size() const noexcept {
            return _size;
        }
        bool ready() const noexcept {
            return _lru_entry_ptr;
        }
        lru_entry* lru_entry_ptr() const noexcept {
            return _lru_entry_ptr;
        }
    private:
        void touch() noexcept {
            _last_read = loading_cache_clock_type::now();
            if (_lru_entry_ptr) {
                _lru_entry_ptr->touch();
            }
        }
        void set_anchor_back_reference(lru_entry* lru_entry_ptr) noexcept {
            _lru_entry_ptr = lru_entry_ptr;
        }
    };
private:
    using loading_values_type = typename timestamped_val::loading_values_type;
    using timestamped_val_ptr = typename loading_values_type::entry_ptr;
    using ts_value_lru_entry = typename timestamped_val::lru_entry;
    using lru_list_type = typename ts_value_lru_entry::lru_list_type;
    using list_iterator = typename lru_list_type::iterator;
public:
    using value_type = Tp;
    using key_type = Key;
    using value_ptr = typename timestamped_val::value_ptr;
    class entry_is_too_big : public std::exception {};
private:
    loading_cache(loading_cache_config cfg, logging::logger& logger)
        : _cfg(std::move(cfg))
        , _logger(logger)
        , _timer([this] { on_timer(); })
    {
        static_assert(noexcept(LoadingCacheStats::inc_unprivileged_on_cache_size_eviction()), "LoadingCacheStats::inc_unprivileged_on_cache_size_eviction must be non-throwing");
        static_assert(noexcept(LoadingCacheStats::inc_privileged_on_cache_size_eviction()), "LoadingCacheStats::inc_privileged_on_cache_size_eviction must be non-throwing");
        if (!validate_config(_cfg)) {
            throw exceptions::configuration_exception("loading_cache: caching is enabled but refresh period and/or max_size are zero");
        }
    }
    bool validate_config(const loading_cache_config& cfg) const noexcept {
        // Sanity check: if expiration period is given then non-zero refresh period and maximal size are required
        if (cfg.expiry != loading_cache_clock_type::duration(0) && (cfg.max_size == 0 || cfg.refresh == loading_cache_clock_type::duration(0))) {
            return false;
        }
        return true;
    }
public:
    template<typename Func>
    requires std::is_invocable_r_v<future<value_type>, Func, const key_type&>
    loading_cache(loading_cache_config cfg, logging::logger& logger, Func&& load)
        : loading_cache(std::move(cfg), logger)
    {
        static_assert(ReloadEnabled == loading_cache_reload_enabled::yes, "This constructor should only be invoked when ReloadEnabled == loading_cache_reload_enabled::yes");
        _load = std::forward<Func>(load);
        // If expiration period is zero - caching is disabled
        if (!caching_enabled()) {
            return;
        }
        _timer_period = std::min(_cfg.expiry, _cfg.refresh);
        _timer.arm(_timer_period);
    }
    loading_cache(size_t max_size, lowres_clock::duration expiry, logging::logger& logger)
        : loading_cache({max_size, expiry, loading_cache_clock_type::time_point::max().time_since_epoch()}, logger)
    {
        static_assert(ReloadEnabled == loading_cache_reload_enabled::no, "This constructor should only be invoked when ReloadEnabled == loading_cache_reload_enabled::no");
        // If expiration period is zero - caching is disabled
        if (!caching_enabled()) {
            return;
        }
        _timer_period = _cfg.expiry;
        _timer.arm(_timer_period);
    }
    ~loading_cache() {
        auto value_destroyer = [] (ts_value_lru_entry* ptr) { loading_cache::destroy_ts_value(ptr); };
        _unprivileged_lru_list.erase_and_dispose(_unprivileged_lru_list.begin(), _unprivileged_lru_list.end(), value_destroyer);
        _lru_list.erase_and_dispose(_lru_list.begin(), _lru_list.end(), value_destroyer);
    }
    void reset() noexcept {
        _logger.info("Resetting cache");
        remove_if([](const value_type&){ return true; });
    }
    bool update_config(utils::loading_cache_config cfg) {
        _logger.info("Updating loading cache; max_size: {}, expiry: {}ms, refresh: {}ms", cfg.max_size,
                     std::chrono::duration_cast<std::chrono::milliseconds>(cfg.expiry).count(),
                     std::chrono::duration_cast<std::chrono::milliseconds>(cfg.refresh).count());
        if (!validate_config(cfg)) {
            _logger.debug("loading_cache: caching is enabled but refresh period and/or max_size are zero");
            return false;
        }
        _updated_cfg.emplace(std::move(cfg));
        // * If the timer is already armed we need to rearm it so that the changes on config can take place.
        // * If timer is not armed and caching is enabled, it means that on_timer was executed but its continuation hasn't finished yet,
        //   so we don't need to rearm it here, since on_timer's continuation will take care of that
        // * If caching is disabled and it's being enabled here on update_config, we also need to arm the timer, so that the changes on config
        //   can take place
        if (_timer.armed() ||
            (!caching_enabled() && _updated_cfg->expiry != loading_cache_clock_type::duration(0))) {
            _timer.rearm(loading_cache_clock_type::now() + loading_cache_clock_type::duration(std::chrono::milliseconds(1)));
        }
        return true;
    }
    template <typename LoadFunc>
    requires std::is_invocable_r_v<future<value_type>, LoadFunc, const key_type&>
    future<value_ptr> get_ptr(const Key& k, LoadFunc&& load) {
        // We shouldn't be here if caching is disabled
        assert(caching_enabled());
        return _loading_values.get_or_load(k, [load = std::forward<LoadFunc>(load)] (const Key& k) mutable {
            return load(k).then([] (value_type val) {
                return timestamped_val(std::move(val));
            });
        }).then([this, k] (timestamped_val_ptr ts_val_ptr) {
            // check again since it could have already been inserted and initialized
            if (!ts_val_ptr->ready() && !ts_val_ptr.orphaned()) {
                _logger.trace("{}: storing the value for the first time", k);
                if (ts_val_ptr->size() > _cfg.max_size) {
                    return make_exception_future<value_ptr>(entry_is_too_big());
                }
                ts_value_lru_entry* new_lru_entry = Alloc().template allocate_object<ts_value_lru_entry>();
                // Remove the least recently used items if map is too big.
                shrink();
                new(new_lru_entry) ts_value_lru_entry(std::move(ts_val_ptr), *this);
                // This will "touch" the entry and add it to the LRU list - we must do this before the shrink() call.
                value_ptr vp(new_lru_entry->timestamped_value_ptr());
                return make_ready_future<value_ptr>(std::move(vp));
            }
            return make_ready_future<value_ptr>(std::move(ts_val_ptr));
        });
    }
    future<value_ptr> get_ptr(const Key& k) {
        static_assert(ReloadEnabled == loading_cache_reload_enabled::yes, "");
        return get_ptr(k, _load);
    }
    future<Tp> get(const Key& k) {
        static_assert(ReloadEnabled == loading_cache_reload_enabled::yes, "");
        // If caching is disabled - always load in the foreground
        if (!caching_enabled()) {
            return _load(k);
        }
        return get_ptr(k).then([] (value_ptr v_ptr) {
            return make_ready_future<Tp>(*v_ptr);
        });
    }
    future<> stop() {
        return _timer_reads_gate.close().finally([this] { _timer.cancel(); });
    }
    /// Find a value for a specific Key value and touch() it.
    /// \tparam KeyType Key type
    /// \tparam KeyHasher Hash functor type
    /// \tparam KeyEqual Equality functor type
    ///
    /// \param key Key value to look for
    /// \param key_hasher_func Hash functor
    /// \param key_equal_func Equality functor
    /// \return cache_value_ptr object pointing to the found value or nullptr otherwise.
    template<typename KeyType, typename KeyHasher, typename KeyEqual>
    value_ptr find(const KeyType& key, KeyHasher key_hasher_func, KeyEqual key_equal_func) noexcept {
        // cache_value_ptr constructor is going to update a "last read" timestamp of the corresponding object and move
        // the object to the front of the LRU
        return set_find(key, std::move(key_hasher_func), std::move(key_equal_func));
    };
    value_ptr find(const Key& k) noexcept {
        return set_find(k);
    }
    // Removes all values matching a given predicate and values which are currently loading.
    // Guarantees that no values which match the predicate and whose loading was initiated
    // before this call will be present after this call (or appear at any time later).
    // The predicate may be invoked multiple times on the same value.
    // It must return the same result for a given value (it must be a pure function).
    template <typename Pred>
    requires std::is_invocable_r_v<bool, Pred, const value_type&>
    void remove_if(Pred&& pred) {
        auto cond_pred = [&pred] (const ts_value_lru_entry& v) {
            return pred(v.timestamped_value().value());
        };
        auto value_destroyer = [] (ts_value_lru_entry* p) {
            loading_cache::destroy_ts_value(p);
        };
        _unprivileged_lru_list.remove_and_dispose_if(cond_pred, value_destroyer);
        _lru_list.remove_and_dispose_if(cond_pred, value_destroyer);
        _loading_values.remove_if([&pred] (const timestamped_val& v) {
            return pred(v.value());
        });
    }
    // Removes a given key from the cache.
    // The key is removed immediately.
    // After this, get_ptr() is guaranteed to reload the value before returning it.
    // As a consequence of the above, if there is a concurrent get_ptr() in progress with this,
    // its value will not populate the cache. It will still succeed.
    void remove(const Key& k) {
        remove_ts_value(set_find(k));
        // set_find() returns nullptr for a key which is currently loading, which we want to remove too.
        _loading_values.remove(k);
    }
    // Removes a given key from the cache.
    // Same guarantees as with remove(key).
    template<typename KeyType, typename KeyHasher, typename KeyEqual>
    void remove(const KeyType& key, KeyHasher key_hasher_func, KeyEqual key_equal_func) noexcept {
        remove_ts_value(set_find(key, key_hasher_func, key_equal_func));
        // set_find() returns nullptr for a key which is currently loading, which we want to remove too.
        _loading_values.remove(key, key_hasher_func, key_equal_func);
    }
    size_t size() const {
        return _lru_list.size() + _unprivileged_lru_list.size();
    }
    /// \brief returns the memory size the currently cached entries occupy according to the EntrySize predicate.
    size_t memory_footprint() const noexcept {
        return _unprivileged_section_size + _privileged_section_size;
    }
    /// \brief returns the memory size the currently cached entries occupy in the privileged section according to the EntrySize predicate.
    size_t privileged_section_memory_footprint() const noexcept {
        return _privileged_section_size;
    }
    /// \brief returns the memory size the currently cached entries occupy in the unprivileged section according to the EntrySize predicate.
    size_t unprivileged_section_memory_footprint() const noexcept {
        return _unprivileged_section_size;
    }
private:
    void remove_ts_value(timestamped_val_ptr ts_ptr) {
        if (!ts_ptr) {
            return;
        }
        ts_value_lru_entry* lru_entry_ptr = ts_ptr->lru_entry_ptr();
        lru_list_type& entry_list = container_list(*lru_entry_ptr);
        entry_list.erase_and_dispose(entry_list.iterator_to(*lru_entry_ptr), [] (ts_value_lru_entry* p) { loading_cache::destroy_ts_value(p); });
    }
    timestamped_val_ptr ready_entry_ptr(timestamped_val_ptr tv_ptr) {
        if (!tv_ptr || !tv_ptr->ready()) {
            return nullptr;
        }
        return std::move(tv_ptr);
    }
    lru_list_type& container_list(const ts_value_lru_entry& lru_entry_ptr) noexcept {
        return (lru_entry_ptr.touch_count() > SectionHitThreshold) ? _lru_list : _unprivileged_lru_list;
    }
    template<typename KeyType, typename KeyHasher, typename KeyEqual>
    timestamped_val_ptr set_find(const KeyType& key, KeyHasher key_hasher_func, KeyEqual key_equal_func) noexcept {
        return ready_entry_ptr(_loading_values.find(key, std::move(key_hasher_func), std::move(key_equal_func)));
    }
    // keep the default non-templated overloads to ease on the compiler for specifications
    // that do not require the templated find().
    timestamped_val_ptr set_find(const Key& key) noexcept {
        return ready_entry_ptr(_loading_values.find(key));
    }
    bool caching_enabled() const {
        return _cfg.expiry != lowres_clock::duration(0);
    }
    static void destroy_ts_value(ts_value_lru_entry* val) noexcept {
        Alloc().delete_object(val);
    }
    /// This is the core method in the 2 sections LRU implementation.
    /// Set the given item as the most recently used item at the corresponding cache section.
    /// The MRU item is going to be at the front of the list, the LRU item - at the back.
    /// The entry is initially entering the "unprivileged" section (represented by a _unprivileged_lru_list).
    /// After an entry is touched more than SectionHitThreshold times it moves to a "privileged" section
    /// (represented by an _lru_list).
    ///
    /// \param lru_entry Cache item that has been "touched"
    void touch_lru_entry_2_sections(ts_value_lru_entry& lru_entry) {
        if (lru_entry.is_linked()) {
            lru_list_type& lru_list = container_list(lru_entry);
            lru_list.erase(lru_list.iterator_to(lru_entry));
        }
        if (lru_entry.touch_count() < SectionHitThreshold) {
            _logger.trace("Putting key {} into the unprivileged section", lru_entry.key());
            _unprivileged_lru_list.push_front(lru_entry);
            lru_entry.inc_touch_count();
        } else {
            _logger.trace("Putting key {} into the privileged section", lru_entry.key());
            _lru_list.push_front(lru_entry);
            // Bump it up only once to avoid a wrap around
            if (lru_entry.touch_count() == SectionHitThreshold) {
                // This code will run only once, when a promotion
                // from unprivileged to privileged section happens.
                // Update section size bookkeeping.
                lru_entry.owning_section_size() -= lru_entry.timestamped_value().size();
                lru_entry.inc_touch_count();
                lru_entry.owning_section_size() += lru_entry.timestamped_value().size();
            }
        }
    }
    future<> reload(timestamped_val_ptr ts_value_ptr) {
        const Key& key = loading_values_type::to_key(ts_value_ptr);
        // Do nothing if the entry has been dropped before we got here (e.g. by the _load() call on another key that is
        // also being reloaded).
        if (!ts_value_ptr->lru_entry_ptr()) {
            _logger.trace("{}: entry was dropped before the reload", key);
            return make_ready_future<>();
        }
        return _load(key).then_wrapped([this, ts_value_ptr = std::move(ts_value_ptr), &key] (auto&& f) mutable {
            // if the entry has been evicted by now - simply end here
            if (!ts_value_ptr->lru_entry_ptr()) {
                _logger.trace("{}: entry was dropped during the reload", key);
                return make_ready_future<>();
            }
            // The exceptions are related to the load operation itself.
            // We should ignore them for the background reads - if
            // they persist the value will age and will be reloaded in
            // the forground. If the foreground READ fails the error
            // will be propagated up to the user and will fail the
            // corresponding query.
            try {
                *ts_value_ptr = f.get0();
            } catch (std::exception& e) {
                _logger.debug("{}: reload failed: {}", key, e.what());
            } catch (...) {
                _logger.debug("{}: reload failed: unknown error", key);
            }
            return make_ready_future<>();
        });
    }
    void drop_expired() {
        auto now = loading_cache_clock_type::now();
        auto expiration_cond = [now, this] (const ts_value_lru_entry& lru_entry) {
            using namespace std::chrono;
            // An entry should be discarded if it hasn't been reloaded for too long or nobody cares about it anymore
            const timestamped_val& v = lru_entry.timestamped_value();
            auto since_last_read = now - v.last_read();
            auto since_loaded = now - v.loaded();
            if (_cfg.expiry < since_last_read || (ReloadEnabled == loading_cache_reload_enabled::yes && _cfg.expiry < since_loaded)) {
                _logger.trace("drop_expired(): {}: dropping the entry: expiry {},  ms passed since: loaded {} last_read {}", lru_entry.key(), _cfg.expiry.count(), duration_cast<milliseconds>(since_loaded).count(), duration_cast<milliseconds>(since_last_read).count());
                return true;
            }
            return false;
        };
        auto value_destroyer = [] (ts_value_lru_entry* p) {
            loading_cache::destroy_ts_value(p);
        };
        _unprivileged_lru_list.remove_and_dispose_if(expiration_cond, value_destroyer);
        _lru_list.remove_and_dispose_if(expiration_cond, value_destroyer);
    }
    // Shrink the cache to the max_size discarding the least recently used items.
    // Get rid from the entries that were used exactly once first.
    void shrink() noexcept {
        using namespace std::chrono;
        auto drop_privileged_entry = [&] {
            ts_value_lru_entry& lru_entry = *_lru_list.rbegin();
            _logger.trace("shrink(): {}: dropping the entry: ms since last_read {}", lru_entry.key(), duration_cast<milliseconds>(loading_cache_clock_type::now() - lru_entry.timestamped_value().last_read()).count());
            loading_cache::destroy_ts_value(&lru_entry);
            LoadingCacheStats::inc_privileged_on_cache_size_eviction();
        };
        auto drop_unprivileged_entry = [&] {
            ts_value_lru_entry& lru_entry = *_unprivileged_lru_list.rbegin();
            _logger.trace("shrink(): {}: dropping the unprivileged entry: ms since last_read {}", lru_entry.key(), duration_cast<milliseconds>(loading_cache_clock_type::now() - lru_entry.timestamped_value().last_read()).count());
            loading_cache::destroy_ts_value(&lru_entry);
            LoadingCacheStats::inc_unprivileged_on_cache_size_eviction();
        };
        // When cache entries need to be evicted due to a size restriction,
        // unprivileged section entries are evicted first.
        //
        // However, we make sure that the unprivileged section does not get
        // too small, because this could lead to starving the unprivileged section.
        // For example if the cache could store at most 50 entries and there are 49 entries in
        // privileged section, after adding 5 entries (that would go to unprivileged
        // section) 4 of them would get evicted and only the 5th one would stay.
        // This caused problems with BATCH statements where all prepared statements
        // in the batch have to stay in cache at the same time for the batch to correctly
        // execute.
        auto minimum_unprivileged_section_size = _cfg.max_size / 2;
        while (memory_footprint() >= _cfg.max_size && _unprivileged_section_size > minimum_unprivileged_section_size) {
            drop_unprivileged_entry();
        }
        while (memory_footprint() >= _cfg.max_size && !_lru_list.empty()) {
            drop_privileged_entry();
        }
        // If dropping entries from privileged section did not help,
        // we have to drop entries from unprivileged section,
        // going below minimum_unprivileged_section_size.
        while (memory_footprint() >= _cfg.max_size) {
            drop_unprivileged_entry();
        }
    }
    // Try to bring the load factors of the _loading_values into a known range.
    void periodic_rehash() noexcept {
        try {
            _loading_values.rehash();
        } catch (...) {
            // if rehashing fails - continue with the current buckets array
        }
    }
    void on_timer() {
        _logger.trace("on_timer(): start");
        if (_updated_cfg) {
            _cfg = *_updated_cfg;
            _updated_cfg.reset();
            _timer_period = std::min(_cfg.expiry, _cfg.refresh);
        }
        // Caching might have been disabled during a config update
        if (!caching_enabled()) {
            reset();
            return;
        }
        // Clean up items that were not touched for the whole expiry period.
        drop_expired();
        // check if rehashing is needed and do it if it is.
        periodic_rehash();
        if constexpr (ReloadEnabled == loading_cache_reload_enabled::no) {
            _logger.trace("on_timer(): rearming");
            _timer.arm(loading_cache_clock_type::now() + _timer_period);
            return;
        }
        // Reload all those which value needs to be reloaded.
        // Future is waited on indirectly in `stop()` (via `_timer_reads_gate`).
        // FIXME: error handling
        (void)with_gate(_timer_reads_gate, [this] {
            auto to_reload = boost::copy_range<utils::chunked_vector<timestamped_val_ptr>>(boost::range::join(_unprivileged_lru_list, _lru_list)
                    | boost::adaptors::filtered([this] (ts_value_lru_entry& lru_entry) {
                        return lru_entry.timestamped_value().loaded() + _cfg.refresh < loading_cache_clock_type::now();
                    })
                    | boost::adaptors::transformed([] (ts_value_lru_entry& lru_entry) {
                        return lru_entry.timestamped_value_ptr();
                    }));
            return parallel_for_each(std::move(to_reload), [this] (timestamped_val_ptr ts_value_ptr) {
                _logger.trace("on_timer(): {}: reloading the value", loading_values_type::to_key(ts_value_ptr));
                return this->reload(std::move(ts_value_ptr));
            }).finally([this] {
                _logger.trace("on_timer(): rearming");
                // If the config was updated after on_timer and before this continuation finished
                // it's necessary to run on_timer again to make sure that everything will be reloaded correctly
                if (_updated_cfg) {
                    _timer.arm(loading_cache_clock_type::now() + loading_cache_clock_type::duration(std::chrono::milliseconds(1)));
                } else {
                    _timer.arm(loading_cache_clock_type::now() + _timer_period);
                }
            });
        });
    }
    loading_values_type _loading_values;
    lru_list_type _lru_list;              // list containing "privileged" section entries
    lru_list_type _unprivileged_lru_list; // list containing "unprivileged" section entries
    size_t _privileged_section_size = 0;
    size_t _unprivileged_section_size = 0;
    loading_cache_clock_type::duration _timer_period;
    loading_cache_config _cfg;
    std::optional<loading_cache_config> _updated_cfg;
    logging::logger& _logger;
    std::function<future<Tp>(const Key&)> _load;
    timer<loading_cache_clock_type> _timer;
    seastar::gate _timer_reads_gate;
};
template<typename Key, typename Tp, int SectionHitThreshold, loading_cache_reload_enabled ReloadEnabled, typename EntrySize, typename Hash, typename EqualPred, typename LoadingSharedValuesStats, typename LoadingCacheStats, typename Alloc>
class loading_cache<Key, Tp, SectionHitThreshold, ReloadEnabled, EntrySize, Hash, EqualPred, LoadingSharedValuesStats, LoadingCacheStats, Alloc>::timestamped_val::value_ptr {
private:
    using loading_values_type = typename timestamped_val::loading_values_type;
public:
    using timestamped_val_ptr = typename loading_values_type::entry_ptr;
    using value_type = Tp;
private:
    timestamped_val_ptr _ts_val_ptr;
public:
    value_ptr(std::nullptr_t) noexcept : _ts_val_ptr() {}
    bool operator==(const value_ptr&) const = default;
};
/// \brief This is and LRU list entry which is also an anchor for a loading_cache value.
template<typename Key, typename Tp, int SectionHitThreshold, loading_cache_reload_enabled ReloadEnabled, typename EntrySize, typename Hash, typename EqualPred, typename LoadingSharedValuesStats, typename  LoadingCacheStats, typename Alloc>
class loading_cache<Key, Tp, SectionHitThreshold, ReloadEnabled, EntrySize, Hash, EqualPred, LoadingSharedValuesStats, LoadingCacheStats, Alloc>::timestamped_val::lru_entry : public safe_link_list_hook {
private:
    using loading_values_type = typename timestamped_val::loading_values_type;
public:
    using lru_list_type = bi::list<lru_entry>;
    using timestamped_val_ptr = typename loading_values_type::entry_ptr;
private:
    timestamped_val_ptr _ts_val_ptr;
    loading_cache& _parent;
    int _touch_count;
public:
    
    
    
    
    
    
    
    timestamped_val_ptr timestamped_value_ptr() noexcept ;
};
}
namespace std {
}
namespace db {
class config;
}
namespace auth {
class service;
class permissions_cache final {
    using cache_type = utils::loading_cache<
            std::pair<role_or_anonymous, resource>,
            permission_set,
            1,
            utils::loading_cache_reload_enabled::yes,
            utils::simple_entry_size<permission_set>,
            utils::tuple_hash>;
    using key_type = typename cache_type::key_type;
    cache_type _cache;
public:
};
}
namespace auth {
struct role_config final {
    bool is_superuser{false};
    bool can_login{false};
};
///
/// Differential update for altering existing roles.
///
struct role_config_update final {
    std::optional<bool> is_superuser{};
    std::optional<bool> can_login{};
};
///
/// A logical argument error for a role-management operation.
///
class roles_argument_exception : public exceptions::invalid_request_exception {
public:
    using exceptions::invalid_request_exception::invalid_request_exception;
};
class role_already_exists : public roles_argument_exception {
public:
};
class nonexistant_role : public roles_argument_exception {
public:
};
class role_already_included : public roles_argument_exception {
public:
};
class revoke_ungranted_role : public roles_argument_exception {
public:
};
using role_set = std::unordered_set<sstring>;
enum class recursive_role_query { yes, no };
///
/// Abstract client for managing roles.
///
/// All state necessary for managing roles is stored externally to the client instance.
///
/// All implementations should throw role-related exceptions as documented. Authorization is not addressed here, and
/// access-control should never be enforced in implementations.
///
class role_manager {
public:
    // this type represents a mapping between a role and some attribute value.
    // i.e: given attribute name  'a' this map holds role name and it's assigned
    // value of 'a'.
    using attribute_vals = std::unordered_map<sstring, sstring>;
    using ptr_type = std::unique_ptr<role_manager>;
public:
    ///
    /// \returns an exceptional future with \ref role_already_exists for a role that has previously been created.
    ///
    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    ///
    /// Grant `role_name` to `grantee_name`.
    ///
    /// \returns an exceptional future with \ref nonexistant_role if either the role or the grantee do not exist.
    ///
    /// \returns an exceptional future with \ref role_already_included if granting the role would be redundant, or
    /// create a cycle.
    ///
    ///
    /// Revoke `role_name` from `revokee_name`.
    ///
    /// \returns an exceptional future with \ref nonexistant_role if either the role or the revokee do not exist.
    ///
    /// \returns an exceptional future with \ref revoke_ungranted_role if the role was not granted.
    ///
    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    ///
    /// \returns the value of the named attribute, if one is set.
    ///
    virtual future<std::optional<sstring>> get_attribute(std::string_view role_name, std::string_view attribute_name) = 0;
    ///
    /// \returns a mapping of each role's value for the named attribute, if one is set for the role.
    ///
    /// Sets `attribute_name` with `attribute_value` for `role_name`.
    /// \returns an exceptional future with nonexistant_role if the role does not exist.
    ///
    /// Removes `attribute_name` for `role_name`.
    /// \returns an exceptional future with nonexistant_role if the role does not exist.
    /// \note: This is a no-op if the role does not have the named attribute set.
    ///
};
}
namespace cql3 {
class query_processor;
}
namespace service {
class migration_manager;
class migration_notifier;
class migration_listener;
}
namespace auth {
class role_or_anonymous;
struct service_config final {
    sstring authorizer_java_name;
    sstring authenticator_java_name;
    sstring role_manager_java_name;
};
///
/// Due to poor (in this author's opinion) decisions of Apache Cassandra, certain choices of one role-manager,
/// authenticator, or authorizer imply restrictions on the rest.
///
/// This exception is thrown when an invalid combination of modules is selected, with a message explaining the
/// incompatibility.
///
class incompatible_module_combination : public std::invalid_argument {
public:
    using std::invalid_argument::invalid_argument;
};
///
/// Client for access-control in the system.
///
/// Access control encompasses user/role management, authentication, and authorization. This client provides access to
/// the dynamically-loaded implementations of these modules (through the `underlying_*` member functions), but also
/// builds on their functionality with caching and abstractions for common operations.
///
/// All state associated with access-control is stored externally to any particular instance of this class.
///
/// peering_sharded_service inheritance is needed to be able to access shard local authentication service
/// given an object from another shard. Used for bouncing lwt requests to correct shard.
class service final : public seastar::peering_sharded_service<service> {
    utils::loading_cache_config _loading_cache_config;
    std::unique_ptr<permissions_cache> _permissions_cache;
    cql3::query_processor& _qp;
    ::service::migration_notifier& _mnotifier;
    authorizer::ptr_type _authorizer;
    authenticator::ptr_type _authenticator;
    role_manager::ptr_type _role_manager;
    // Only one of these should be registered, so we end up with some unused instances. Not the end of the world.
    std::unique_ptr<::service::migration_listener> _migration_listener;
    std::function<void(uint32_t)> _permissions_cache_cfg_cb;
    serialized_action _permissions_cache_config_action;
    utils::observer<uint32_t> _permissions_cache_max_entries_observer;
    utils::observer<uint32_t> _permissions_cache_update_interval_in_ms_observer;
    utils::observer<uint32_t> _permissions_cache_validity_in_ms_observer;
public:
    ///
    /// This constructor is intended to be used when the class is sharded via \ref seastar::sharded. In that case, the
    /// arguments must be copyable, which is why we delay construction with instance-construction instructions instead
    /// of the instances themselves.
    ///
    ///
    /// \returns an exceptional future with \ref nonexistant_role if the named role does not exist.
    ///
    ///
    /// Like \ref get_permissions, but never returns cached permissions.
    ///
    ///
    /// Query whether the named role has been granted a role that is a superuser.
    ///
    /// A role is always granted to itself. Therefore, a role that "is" a superuser also "has" superuser.
    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    ///
    /// Return the set of all roles granted to the given role, including itself and roles granted through other roles.
    ///
    /// \returns an exceptional future with \ref nonexistent_role if the role does not exist.
private:
};
///
/// Access-control is "enforcing" when either the authenticator or the authorizer are not their "allow-all" variants.
///
/// Put differently, when access control is not enforcing, all operations on resources will be allowed and users do not
/// need to authenticate themselves.
///
/// A description of a CQL command from which auth::service can tell whether or not this command could endanger
/// internal data on which auth::service depends.
struct command_desc {
    auth::permission permission; ///< Nature of the command's alteration.
    const ::auth::resource& resource; ///< Resource impacted by this command.
    enum class type {
        ALTER_WITH_OPTS, ///< Command is ALTER ... WITH ...
        OTHER
    } type_ = type::OTHER;
};
///
/// Protected resources cannot be modified even if the performer has permissions to do so.
///
///
/// Create a role with optional authentication information.
///
/// \returns an exceptional future with \ref role_already_exists if the user or role exists.
///
/// \returns an exceptional future with \ref unsupported_authentication_option if an unsupported option is included.
///
///
/// Alter an existing role and its authentication information.
///
/// \returns an exceptional future with \ref nonexistant_role if the named role does not exist.
///
/// \returns an exceptional future with \ref unsupported_authentication_option if an unsupported option is included.
///
///
/// Drop a role from the system, including all permissions and authentication information.
///
/// \returns an exceptional future with \ref nonexistant_role if the named role does not exist.
///
///
/// Check if `grantee` has been granted the named role.
///
/// \returns an exceptional future with \ref nonexistent_role if `grantee` or `name` do not exist.
///
///
/// Check if the authenticated user has been granted the named role.
///
/// \returns an exceptional future with \ref nonexistent_role if the user or `name` do not exist.
///
///
/// \returns an exceptional future with \ref nonexistent_role if the named role does not exist.
///
/// \returns an exceptional future with \ref unsupported_authorization_operation if granting permissions is not
/// supported.
///
///
/// Like \ref grant_permissions, but grants all applicable permissions on the resource.
///
/// \returns an exceptional future with \ref nonexistent_role if the named role does not exist.
///
/// \returns an exceptional future with \ref unsupported_authorization_operation if granting permissions is not
/// supported.
///
///
/// \returns an exceptional future with \ref nonexistent_role if the named role does not exist.
///
/// \returns an exceptional future with \ref unsupported_authorization_operation if revoking permissions is not
/// supported.
///
using recursive_permissions = bool_class<struct recursive_permissions_tag>;
///
/// Query for all granted permissions according to filtering criteria.
///
/// Only permissions included in the provided set are included.
///
/// If a role name is provided, only permissions granted (directly or recursively) to the role are included.
///
/// If a resource filter is provided, only permissions granted on the resource are included. When \ref
/// recursive_permissions is `true`, permissions on a parent resource are included.
///
/// \returns an exceptional future with \ref nonexistent_role if a role name is included which refers to a role that
/// does not exist.
///
/// \returns an exceptional future with \ref unsupported_authorization_operation if listing permissions is not
/// supported.
///
future<std::vector<permission_details>> list_filtered_permissions(
        const service&,
        permission_set,
        std::optional<std::string_view> role_name,
        const std::optional<std::pair<resource, recursive_permissions>>& resource_filter);
}
namespace unimplemented {
enum class cause {
    API,
    INDEXES,
    LWT,
    PAGING,
    AUTH,
    PERMISSIONS,
    TRIGGERS,
    COUNTERS,
    METRICS,
    MIGRATIONS,
    GOSSIP,
    TOKEN_RESTRICTION,
    LEGACY_COMPOSITE_KEYS,
    COLLECTION_RANGE_TOMBSTONES,
    RANGE_DELETES,
    THRIFT,
    VALIDATION,
    REVERSED,
    COMPRESSION,
    NONATOMIC,
    CONSISTENCY,
    HINT,
    SUPER,
    WRAP_AROUND, // Support for handling wrap around ranges in queries on database level and below
    STORAGE_SERVICE,
    SCHEMA_CHANGE,
    MIXED_CF,
    SSTABLE_FORMAT_M,
};
[[noreturn]] void fail(cause what);
}
namespace std {
template <>
struct hash<unimplemented::cause> : seastar::enum_hash<unimplemented::cause> {};
}
namespace db {
using timeout_clock = seastar::lowres_clock;
using timeout_semaphore = seastar::basic_semaphore<seastar::default_timeout_exception_factory, timeout_clock>;
using timeout_semaphore_units = seastar::semaphore_units<seastar::default_timeout_exception_factory, timeout_clock>;
static constexpr timeout_clock::time_point no_timeout = timeout_clock::time_point::max();
}
namespace db { class config; }
class updateable_timeout_config;
/// timeout_config represents a snapshot of the options stored in it when
/// an instance of this class is created. so far this class is only used by
/// client_state and thrift_handler. so either these classes are obliged to
/// update it by themselves, or they are fine with using the maybe-updated
/// options in the lifecycle of a client / connection even if some of these
/// options are changed whtn the client / connection is still alive.
struct timeout_config {
    using duration_t = db::timeout_clock::duration;
    duration_t read_timeout;
    duration_t write_timeout;
    duration_t range_read_timeout;
    duration_t counter_write_timeout;
    duration_t truncate_timeout;
    duration_t cas_timeout;
    duration_t other_timeout;
};
struct updateable_timeout_config {
    using timeout_option_t = utils::updateable_value<uint32_t>;
    timeout_option_t read_timeout_in_ms;
    timeout_option_t write_timeout_in_ms;
    timeout_option_t range_read_timeout_in_ms;
    timeout_option_t counter_write_timeout_in_ms;
    timeout_option_t truncate_timeout_in_ms;
    timeout_option_t cas_timeout_in_ms;
    timeout_option_t other_timeout_in_ms;
    timeout_config current_values() const;
};
using timeout_config_selector = db::timeout_clock::duration (timeout_config::*);
extern const timeout_config infinite_timeout_config;
using inet_address_vector_replica_set = utils::small_vector<gms::inet_address, 3>;
using inet_address_vector_topology_change = utils::small_vector<gms::inet_address, 1>;
namespace cql3{
class query_options;
struct raw_value_view;
struct raw_value_view_vector_with_unset;
namespace statements {
class prepared_statement;
}
}
namespace tracing {
using prepared_checked_weak_ptr = seastar::checked_ptr<seastar::weak_ptr<cql3::statements::prepared_statement>>;
class trace_state final {
public:
    // A primary session may be in 3 states:
    //   - "inactive": between the creation and a begin() call.
    //   - "foreground": after a begin() call and before a
    //     stop_foreground_and_write() call.
    //   - "background": after a stop_foreground_and_write() call and till the
    //     state object is destroyed.
    //
    // - Traces are not allowed while state is in an "inactive" state.
    // - The time the primary session was in a "foreground" state is the time
    //   reported as a session's "duration".
    // - Traces that have arrived during the "background" state will be recorded
    //   as usual but their "elapsed" time will be greater or equal to the
    //   session's "duration".
    //
    // Secondary sessions may only be in an "inactive" or in a "foreground"
    // states.
    enum class state {
        inactive,
        foreground,
        background
    };
private:
    shared_ptr<tracing> _local_tracing_ptr;
    trace_state_props_set _state_props;
    lw_shared_ptr<one_session_records> _records;
    // Used for calculation of time passed since the beginning of a tracing
    // session till each tracing event. Secondary slow-query-logging sessions inherit `_start` from parents.
    elapsed_clock::time_point _start;
    std::optional<uint64_t> _supplied_start_ts_us; // Parent's `_start`, as microseconds from POSIX epoch.
    std::chrono::microseconds _slow_query_threshold;
    state _state = state::inactive;
    struct params_values;
    struct params_values_deleter {
        void operator()(params_values* pv) ;
    };
    class params_ptr {
    private:
        std::unique_ptr<params_values, params_values_deleter> _vals;
    public:
    } _params_ptr;
public:
private:
    template <typename Func>
    requires std::is_invocable_r_v<sstring, Func>
    void begin(const seastar::lazy_eval<Func>& lf, gms::inet_address client) {
        begin(lf(), client);
    }
    void set_batchlog_endpoints(const inet_address_vector_replica_set& val);
    ;
    ;
    ;
};
class trace_state_ptr final {
private:
    lw_shared_ptr<trace_state> _state_ptr;
public:
    trace_state_ptr() = default;
    trace_state_ptr(lw_shared_ptr<trace_state> state_ptr)
        : _state_ptr(std::move(state_ptr))
    {}
    trace_state_ptr(std::nullptr_t)
        : _state_ptr(nullptr)
    {}
    explicit operator bool() const noexcept {
        return __builtin_expect(bool(_state_ptr), false);
    }
    trace_state* operator->() const noexcept {
        return _state_ptr.get();
    }
    trace_state& operator*() const noexcept {
        return *_state_ptr;
    }
};
 void set_user_timestamp(const trace_state_ptr& p, api::timestamp_type val) ;
 void add_prepared_statement(const trace_state_ptr& p, prepared_checked_weak_ptr& prepared) ;
 void set_username(const trace_state_ptr& p, const std::optional<auth::authenticated_user>& user) ;
 ;
 ;
// global_trace_state_ptr is a helper class that may be used for creating spans
// of an existing tracing session on other shards. When a tracing span on a
// different shard is needed global_trace_state_ptr would create a secondary
// tracing session on that shard similarly to what we do when we create tracing
// spans on remote Nodes.
//
// The usage is straight forward:
// 1. Create a global_trace_state_ptr from the existing trace_state_ptr object.
// 2. Pass it to the execution unit that (possibly) runs on a different shard
//    and pass the global_trace_state_ptr object instead of a trace_state_ptr
//    object.
class global_trace_state_ptr {
    unsigned _cpu_of_origin;
    trace_state_ptr _ptr;
public:
    // Note: the trace_state_ptr must come from the current shard
    // May be invoked across shards.
    // May be invoked across shards.
    // May be invoked across shards.
    // May be invoked across shards.
};
}
namespace cql_transport {
enum class cql_protocol_extension {
    LWT_ADD_METADATA_MARK,
    RATE_LIMIT_ERROR
};
using cql_protocol_extension_enum = super_enum<cql_protocol_extension,
    cql_protocol_extension::LWT_ADD_METADATA_MARK,
    cql_protocol_extension::RATE_LIMIT_ERROR>;
using cql_protocol_extension_enum_set = enum_set<cql_protocol_extension_enum>;
} // namespace cql_transport
namespace qos {
struct service_level_options {
    struct unset_marker {
        ;
    };
    struct delete_marker {
        ;
    };
    enum class workload_type {
        unspecified, batch, interactive, delete_marker
    };
    using timeout_type = std::variant<unset_marker, delete_marker, lowres_clock::duration>;
    timeout_type timeout = unset_marker{};
    workload_type workload = workload_type::unspecified;
    // Merges the values of two service level options. The semantics depends
    // on the type of the parameter - e.g. for timeouts, a min value is preferred.
};
using service_levels_info = std::map<sstring, service_level_options>;
///
/// A logical argument error for a service_level statement operation.
///
class service_level_argument_exception : public std::invalid_argument {
public:
    using std::invalid_argument::invalid_argument;
};
///
/// An exception to indicate that the service level given as parameter doesn't exist.
///
class nonexistant_service_level_exception : public service_level_argument_exception {
public:
};
}
// This class supports atomic removes (by using a lock and returning a
// future) and non atomic insert and iteration (by using indexes).
template <typename T>
class atomic_vector {
    std::vector<T> _vec;
    seastar::rwlock _vec_lock;
public:
    // This must be called on a thread. The callback function must not
    // call remove.
    //
    // We would take callbacks that take a T&, but we had bugs in the
    // past with some of those callbacks holding that reference past a
    // preemption.
    // The callback function must not call remove.
    //
    // We would take callbacks that take a T&, but we had bugs in the
    // past with some of those callbacks holding that reference past a
    // preemption.
};
namespace service {
class endpoint_lifecycle_subscriber {
public:
};
class endpoint_lifecycle_notifier {
    atomic_vector<endpoint_lifecycle_subscriber*> _subscribers;
public:
};
}
namespace qos {
    struct service_level_info {
        sstring name;
    };
    class qos_configuration_change_subscriber {
    public:
        ;
    };
}
namespace db {
    class system_distributed_keyspace;
}
namespace qos {
struct service_level {
     service_level_options slo;
     bool marked_for_deletion;
     bool is_static;
};
class service_level_controller : public peering_sharded_service<service_level_controller>, public service::endpoint_lifecycle_subscriber {
public:
    class service_level_distributed_data_accessor {
    public:
    };
    using service_level_distributed_data_accessor_ptr = ::shared_ptr<service_level_distributed_data_accessor>;
private:
    struct global_controller_data {
        service_levels_info  static_configurations{};
        int schedg_group_cnt = 0;
        int io_priority_cnt = 0;
        service_level_options default_service_level_config;
        // The below future is used to serialize work so no reordering can occur.
        // This is needed so for example: delete(x), add(x) will not reverse yielding
        // a completely different result than the one intended.
        future<> work_future = make_ready_future();
        semaphore notifications_serializer = semaphore(1);
        future<> distributed_data_update = make_ready_future();
        abort_source dist_data_update_aborter;
    };
    std::unique_ptr<global_controller_data> _global_controller_db;
    static constexpr shard_id global_controller = 0;
    std::map<sstring, service_level> _service_levels_db;
    std::unordered_map<sstring, sstring> _role_to_service_level;
    service_level _default_service_level;
    service_level_distributed_data_accessor_ptr _sl_data_accessor;
    sharded<auth::service>& _auth_service;
    std::chrono::time_point<seastar::lowres_clock> _last_successful_config_update;
    unsigned _logged_intervals;
    atomic_vector<qos_configuration_change_subscriber*> _subscribers;
public:
    service_level_controller(sharded<auth::service>& auth_service, service_level_options default_service_level_config);
    future<std::optional<service_level_options>> find_service_level(auth::role_set roles);
private:
    enum class  set_service_level_op_type {
        add_if_not_exists,
        add,
        alter
    };
public:
    static sstring default_service_level_name;
};
}
namespace auth {
class resource;
}
namespace data_dictionary {
class database;
}
namespace service {
class client_state {
public:
    enum class auth_state : uint8_t {
        UNINITIALIZED, AUTHENTICATION, READY
    };
    using workload_type = qos::service_level_options::workload_type;
    // This class is used to move client_state between shards
    // It is created on a shard that owns client_state than passed
    // to a target shard where client_state_for_another_shard::get()
    // can be called to obtain a shard local copy.
    class client_state_for_another_shard {
    private:
        const client_state* _cs;
        seastar::sharded<auth::service>* _auth_service;
        friend client_state;
    public:
    };
private:
    friend client_state_for_another_shard;
private:
    sstring _keyspace;
#if 0
    private static final Logger logger = LoggerFactory.getLogger(ClientState.class);
    public static final SemanticVersion DEFAULT_CQL_VERSION = org.apache.cassandra.cql3.QueryProcessor.CQL_VERSION;
    private static final Set<IResource> READABLE_SYSTEM_RESOURCES = new HashSet<>();
    private static final Set<IResource> PROTECTED_AUTH_RESOURCES = new HashSet<>();
    static
    {
        // We want these system cfs to be always readable to authenticated users since many tools rely on them
        // (nodetool, cqlsh, bulkloader, etc.)
        for (String cf : Iterables.concat(Arrays.asList(SystemKeyspace.LOCAL, SystemKeyspace.PEERS), LegacySchemaTables.ALL))
            READABLE_SYSTEM_RESOURCES.add(DataResource.columnFamily(SystemKeyspace.NAME, cf));
        PROTECTED_AUTH_RESOURCES.addAll(DatabaseDescriptor.getAuthenticator().protectedResources());
        PROTECTED_AUTH_RESOURCES.addAll(DatabaseDescriptor.getAuthorizer().protectedResources());
    }
    // Current user for the session
    private volatile AuthenticatedUser user;
    private volatile String keyspace;
#endif
    std::optional<auth::authenticated_user> _user;
    std::optional<sstring> _driver_name, _driver_version;
    auth_state _auth_state = auth_state::UNINITIALIZED;
    // isInternal is used to mark ClientState as used by some internal component
    // that should have an ability to modify system keyspace.
    bool _is_internal;
    bool _is_thrift;
    // The biggest timestamp that was returned by getTimestamp/assigned to a query
    static thread_local api::timestamp_type _last_timestamp_micros;
    // Address of a client
    socket_address _remote_address;
    // Only populated for external client state.
    auth::service* _auth_service{nullptr};
    qos::service_level_controller* _sl_controller{nullptr};
    // For restoring default values in the timeout config
    timeout_config _default_timeout_config;
    timeout_config _timeout_config;
    workload_type _workload_type = workload_type::unspecified;
public:
    struct internal_tag {};
    struct external_tag {};
    const timeout_config& get_timeout_config() const ;
    qos::service_level_controller& get_service_level_controller() const ;
    ///
    /// `nullptr` for internal instances.
    ///
    api::timestamp_type get_timestamp() ;
    
#if 0
    public SocketAddress getRemoteAddress()
    {
        return remoteAddress;
    }
#endif
    
    sstring& get_raw_keyspace() noexcept ;
public:
    /// \brief A user can login if it's anonymous, or if it exists and the `LOGIN` option for the user is `true`.
private:
public:
     // unauthorized_exception on error
#if 0
    public void ensureIsSuper(String message) throws UnauthorizedException
    {
        if (DatabaseDescriptor.getAuthenticator().requireAuthentication() && (user == null || !user.isSuper()))
            throw new UnauthorizedException(message);
    }
    private static void validateKeyspace(String keyspace) throws InvalidRequestException
    {
        if (keyspace == null)
            throw new InvalidRequestException("You have not set a keyspace for this session");
    }
#endif
#if 0
    public static SemanticVersion[] getCQLSupportedVersion()
    {
        return new SemanticVersion[]{ QueryProcessor.CQL_VERSION };
    }
    private Set<Permission> authorize(IResource resource)
    {
        // AllowAllAuthorizer or manually disabled caching.
        if (Auth.permissionsCache == null)
            return DatabaseDescriptor.getAuthorizer().authorize(user, resource);
        try
        {
            return Auth.permissionsCache.get(Pair.create(user, resource));
        }
        catch (ExecutionException e)
        {
            throw new RuntimeException(e);
        }
    }
#endif
private:
    cql_transport::cql_protocol_extension_enum_set _enabled_protocol_extensions;
public:
};
}
class service_permit {
    seastar::lw_shared_ptr<seastar::semaphore_units<>> _permit;
    service_permit(seastar::semaphore_units<>&& u) : _permit(seastar::make_lw_shared<seastar::semaphore_units<>>(std::move(u))) {}
    friend service_permit make_service_permit(seastar::semaphore_units<>&& permit);
    friend service_permit empty_service_permit();
public:
    size_t count() const ;;
};
inline service_permit make_service_permit(seastar::semaphore_units<>&& permit) {
    return service_permit(std::move(permit));
}
inline service_permit empty_service_permit() {
    return make_service_permit(seastar::semaphore_units<>());
}
#ifndef SERVICE_QUERY_STATE_HH
#define SERVICE_QUERY_STATE_HH
namespace qos {
class service_level_controller;
}
namespace service {
class query_state final {
private:
    client_state& _client_state;
    tracing::trace_state_ptr _trace_state_ptr;
    service_permit _permit;
public:
    query_state(client_state& client_state, service_permit permit)
            : _client_state(client_state)
            , _trace_state_ptr(tracing::trace_state_ptr())
            , _permit(std::move(permit))
    {}
    query_state(client_state& client_state, tracing::trace_state_ptr trace_state_ptr, service_permit permit)
        : _client_state(client_state)
        , _trace_state_ptr(std::move(trace_state_ptr))
        , _permit(std::move(permit))
    { }
    const tracing::trace_state_ptr& get_trace_state() const {
        return _trace_state_ptr;
    }
    tracing::trace_state_ptr& get_trace_state() {
        return _trace_state_ptr;
    }
    client_state& get_client_state() {
        return _client_state;
    }
    const client_state& get_client_state() const {
        return _client_state;
    }
    api::timestamp_type get_timestamp() {
        return _client_state.get_timestamp();
    }
    service_permit get_permit() const& {
        return _permit;
    }
    service_permit&& get_permit() && {
        return std::move(_permit);
    }
    qos::service_level_controller& get_service_level_controller() const {
        return _client_state.get_service_level_controller();
    }
};
}
#endif
namespace db {
enum class read_repair_decision {
  NONE,
  GLOBAL,
  DC_LOCAL
};
inline std::ostream&  operator<<(std::ostream& out, db::read_repair_decision d) {
    switch (d) {
    case db::read_repair_decision::NONE: out << "NONE"; break;
    case db::read_repair_decision::GLOBAL: out << "GLOBAL"; break;
    case db::read_repair_decision::DC_LOCAL: out << "DC_LOCAL"; break;
    default: out << "ERR"; break;
    }
    return out;
}
}
enum class bound_kind : uint8_t {
    excl_end = 0,
    incl_start = 1,
    // values 2 to 5 are reserved for forward Origin compatibility
    incl_end = 6,
    excl_start = 7,
};
std::ostream& operator<<(std::ostream& out, const bound_kind k);
// Swaps start <-> end && incl <-> excl

// Swaps start <-> end

int32_t weight(bound_kind k);
class bound_view {
    const static thread_local clustering_key _empty_prefix;
    std::reference_wrapper<const clustering_key_prefix> _prefix;
    bound_kind _kind;
public:
    bound_view(const clustering_key_prefix& prefix, bound_kind kind)
        : _prefix(prefix)
        , _kind(kind)
    { }
    
    
    bound_kind kind() const { return _kind; }
    const clustering_key_prefix& prefix() const { return _prefix; }
    struct tri_compare {
        // To make it assignable and to avoid taking a schema_ptr, we
        // wrap the schema reference.
        std::reference_wrapper<const schema> _s;
        tri_compare(const schema& s) : _s(s)
        { }
        std::strong_ordering operator()(const clustering_key_prefix& p1, int32_t w1, const clustering_key_prefix& p2, int32_t w2) const {
            auto type = _s.get().clustering_key_prefix_type();
            auto res = prefix_equality_tri_compare(type->types().begin(),
                type->begin(p1.representation()), type->end(p1.representation()),
                type->begin(p2.representation()), type->end(p2.representation()),
                ::tri_compare);
            if (res != 0) {
                return res;
            }
            auto d1 = p1.size(_s);
            auto d2 = p2.size(_s);
            return ((d1 <= d2) ? w1 << 1 : 1) <=> ((d2 <= d1) ? w2 << 1 : 1);
        }
        std::strong_ordering operator()(const bound_view b, const clustering_key_prefix& p) const ;
        
        
    };
    struct compare {
        // To make it assignable and to avoid taking a schema_ptr, we
        // wrap the schema reference.
        tri_compare _cmp;
        compare(const schema& s) : _cmp(s)
        { }
        bool operator()(const clustering_key_prefix& p1, int32_t w1, const clustering_key_prefix& p2, int32_t w2) const {
            return _cmp(p1, w1, p2, w2) < 0;
        }
        bool operator()(const bound_view b, const clustering_key_prefix& p) const ;
        bool operator()(const clustering_key_prefix& p, const bound_view b) const ;
        bool operator()(const bound_view b1, const bound_view b2) const {
            return operator()(b1._prefix, weight(b1._kind), b2._prefix, weight(b2._kind));
        }
    };
    bool equal(const schema& s, const bound_view other) const {
        return _kind == other._kind && _prefix.get().equal(s, other._prefix.get());
    }
    bool adjacent(const schema& s, const bound_view other) const ;
    static bound_view bottom() {
        return {_empty_prefix, bound_kind::incl_start};
    }
    static bound_view top() {
        return {_empty_prefix, bound_kind::incl_end};
    }
    template<template<typename> typename R>
    requires Range<R, clustering_key_prefix_view>
    static bound_view from_range_start(const R<clustering_key_prefix>& range) {
        return range.start()
               ? bound_view(range.start()->value(), range.start()->is_inclusive() ? bound_kind::incl_start : bound_kind::excl_start)
               : bottom();
    }
    template<template<typename> typename R>
    requires Range<R, clustering_key_prefix>
    static bound_view from_range_end(const R<clustering_key_prefix>& range) {
        return range.end()
               ? bound_view(range.end()->value(), range.end()->is_inclusive() ? bound_kind::incl_end : bound_kind::excl_end)
               : top();
    }
    template<template<typename> typename R>
    requires Range<R, clustering_key_prefix>
    static std::pair<bound_view, bound_view> from_range(const R<clustering_key_prefix>& range) {
        return {from_range_start(range), from_range_end(range)};
    }
    template<template<typename> typename R>
    requires Range<R, clustering_key_prefix_view>
    static std::optional<typename R<clustering_key_prefix_view>::bound> to_range_bound(const bound_view& bv) {
        if (&bv._prefix.get() == &_empty_prefix) {
            return {};
        }
        bool inclusive = bv._kind != bound_kind::excl_end && bv._kind != bound_kind::excl_start;
        return {typename R<clustering_key_prefix_view>::bound(bv._prefix.get().view(), inclusive)};
    }
    friend std::ostream& operator<<(std::ostream& out, const bound_view& b) {
        return out << "{bound: prefix=" << b._prefix.get() << ", kind=" << b._kind << "}";
    }
};
lexicographical_relation relation_for_lower_bound(composite_view v) ;

enum class bound_weight : int8_t {
    before_all_prefixed = -1,
    equal = 0,
    after_all_prefixed = 1,
};

inline
bound_weight position_weight(bound_kind k) {
    switch (k) {
    case bound_kind::excl_end:
    case bound_kind::incl_start:
        return bound_weight::before_all_prefixed;
    case bound_kind::incl_end:
    case bound_kind::excl_start:
        return bound_weight::after_all_prefixed;
    }
    abort();
}
enum class partition_region : uint8_t {
    partition_start,
    static_row,
    clustered,
    partition_end,
};
struct view_and_holder;
template <>
struct fmt::formatter<partition_region> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const ::partition_region& r, FormatContext& ctx) const {
        switch (r) {
            case partition_region::partition_start:
                return formatter<std::string_view>::format("partition_start", ctx);
            case partition_region::static_row:
                return formatter<std::string_view>::format("static_row", ctx);
            case partition_region::clustered:
                return formatter<std::string_view>::format("clustered", ctx);
            case partition_region::partition_end:
                return formatter<std::string_view>::format("partition_end", ctx);
        }
        std::abort(); // compiler will error before we reach here
    }
};
partition_region parse_partition_region(std::string_view);
class position_in_partition_view {
    friend class position_in_partition;
    partition_region _type;
    bound_weight _bound_weight = bound_weight::equal;
    const clustering_key_prefix* _ck; // nullptr when _type != clustered
public:
    position_in_partition_view(partition_region type, bound_weight weight, const clustering_key_prefix* ck)
        : _type(type)
        , _bound_weight(weight)
        , _ck(ck)
    { }
    bool is_before_key() const ;
    
private:
    // Returns placement of this position_in_partition relative to *_ck,
    // or lexicographical_relation::at_prefix if !_ck.
    
public:
    struct partition_start_tag_t { };
    struct end_of_partition_tag_t { };
    struct static_row_tag_t { };
    struct clustering_row_tag_t { };
    struct range_tag_t { };
    using range_tombstone_tag_t = range_tag_t;
    position_in_partition_view(clustering_row_tag_t, const clustering_key_prefix& ck)
        : _type(partition_region::clustered), _ck(&ck) { }
    position_in_partition_view(range_tag_t, bound_view bv)
        : _type(partition_region::clustered), _bound_weight(position_weight(bv.kind())), _ck(&bv.prefix()) { }
    
    static position_in_partition_view for_key(const clustering_key& ck) {
        return {clustering_row_tag_t(), ck};
    }
    // Returns a view, as the first element of the returned pair, to before_key(pos._ck)
    // if pos.is_clustering_row() else returns pos as-is.
    // The second element of the pair needs to be kept alive as long as the first element is used.
    // The returned view is valid as long as the view passed to this method is valid.
    // Returns a view to after_all_prefixed(pos._ck) if pos.is_clustering_row() else returns pos as-is.
    // Returns a view to before_key(pos._ck) if pos.is_clustering_row() else returns pos as-is.
    
    // Returns true if all fragments that can be seen for given schema have
    // positions >= than this. partition_start is ignored.
    
    // Valid when has_key() == true
    
    // Can be called only when !is_static_row && !is_clustering_row().
    bound_view as_start_bound_view() const {
        assert(_bound_weight != bound_weight::equal);
        return bound_view(*_ck, _bound_weight == bound_weight::before_all_prefixed ? bound_kind::incl_start : bound_kind::excl_start);
    }
    bound_view as_end_bound_view() const {
        assert(_bound_weight != bound_weight::equal);
        return bound_view(*_ck, _bound_weight == bound_weight::before_all_prefixed ? bound_kind::excl_end : bound_kind::incl_end);
    }
    class printer {
        const schema& _schema;
        const position_in_partition_view& _pipv;
    public:
        friend fmt::formatter<printer>;
    };
    // Create a position which is the same as this one but governed by a schema with reversed clustering key order.
    position_in_partition_view reversed() const ;
    friend fmt::formatter<printer>;
    friend fmt::formatter<position_in_partition_view>;
    friend bool no_clustering_row_between(const schema&, position_in_partition_view, position_in_partition_view);
};
template <>
struct fmt::formatter<position_in_partition_view> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const ::position_in_partition_view& pos, FormatContext& ctx) const {
        fmt::format_to(ctx.out(), "{{position: {}, ", pos._type);
        if (pos._ck) {
            fmt::format_to(ctx.out(), "{}, ", *pos._ck);
        } else {
            fmt::format_to(ctx.out(), "null, ");
        }
        return fmt::format_to(ctx.out(), "{}}}", int32_t(pos._bound_weight));
    }
};
template <>
struct fmt::formatter<position_in_partition_view::printer> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const ::position_in_partition_view::printer& p, FormatContext& ctx) const {
        auto& pos = p._pipv;
        fmt::format_to(ctx.out(), "{{position: {},", pos._type);
        if (pos._ck) {
            fmt::format_to(ctx.out(), "{}", clustering_key_prefix::with_schema_wrapper(p._schema, *pos._ck));
        } else {
            fmt::format_to(ctx.out(), "null");
        }
        return fmt::format_to(ctx.out(), ", {}}}", int32_t(pos._bound_weight));
    }
};
class position_in_partition {
    partition_region _type;
    bound_weight _bound_weight = bound_weight::equal;
    std::optional<clustering_key_prefix> _ck;
public:
    friend class clustering_interval_set;
    struct partition_start_tag_t { };
    struct end_of_partition_tag_t { };
    struct static_row_tag_t { };
    struct after_static_row_tag_t { };
    struct clustering_row_tag_t { };
    struct after_clustering_row_tag_t { };
    struct before_clustering_row_tag_t { };
    struct range_tag_t { };
    using range_tombstone_tag_t = range_tag_t;
    position_in_partition(before_clustering_row_tag_t, clustering_key_prefix ck)
        : _type(partition_region::clustered), _bound_weight(bound_weight::before_all_prefixed), _ck(std::move(ck)) { }
    position_in_partition(before_clustering_row_tag_t, position_in_partition_view pos)
        : _type(partition_region::clustered)
        , _bound_weight(pos._bound_weight != bound_weight::equal ? pos._bound_weight : bound_weight::before_all_prefixed)
        , _ck(*pos._ck) { }
    position_in_partition(range_tag_t, bound_view bv)
        : _type(partition_region::clustered), _bound_weight(position_weight(bv.kind())), _ck(bv.prefix()) { }
    position_in_partition(range_tag_t, bound_kind kind, clustering_key_prefix&& prefix)
        : _type(partition_region::clustered), _bound_weight(position_weight(kind)), _ck(std::move(prefix)) { }
    position_in_partition(after_static_row_tag_t)  ;
    explicit position_in_partition(position_in_partition_view view) 
        ;
    position_in_partition& operator=(position_in_partition_view view) {
        _type = view._type;
        _bound_weight = view._bound_weight;
        if (view._ck) {
            _ck = *view._ck;
        } else {
            _ck.reset();
        }
        return *this;
    }
    static position_in_partition before_all_clustered_rows() ;
    static position_in_partition after_all_clustered_rows() ;
    // If given position is a clustering row position, returns a position
    // right before it. Otherwise, returns it unchanged.
    // The position "pos" must be a clustering position.
    static position_in_partition before_key(position_in_partition_view pos) ;
    static position_in_partition before_key(clustering_key ck) ;
    static position_in_partition after_key(const schema& s, clustering_key ck) ;
    // If given position is a clustering row position, returns a position
    // right after it. Otherwise returns it unchanged.
    // The position "pos" must be a clustering position.
    
    
    
    
    static position_in_partition for_partition_end() ;
    static position_in_partition for_static_row() ;
    static position_in_partition min() ;
    static position_in_partition for_range_start(const query::clustering_range&);
    static position_in_partition for_range_end(const query::clustering_range&);
     ;
    const clustering_key_prefix& key() const ;
    operator position_in_partition_view() const {
        return { _type, _bound_weight, _ck ? &*_ck : nullptr };
    }
    size_t external_memory_usage() const ;
    // Defines total order on the union of position_and_partition and composite objects.
    //
    // The ordering is compatible with position_range (r). The following is satisfied for
    // all cells with name c included by the range:
    //
    //   r.start() <= c < r.end()
    //
    // The ordering on composites given by this is compatible with but weaker than the cell name order.
    //
    // The ordering on position_in_partition given by this is compatible but weaker than the ordering
    // given by position_in_partition::tri_compare.
    //
    class composite_tri_compare {
        const schema& _s;
    public:
        static int rank(partition_region t) {
            return static_cast<int>(t);
        }
        
        
    };
    // Less comparator giving the same order as composite_tri_compare.
    class composite_less_compare {
        composite_tri_compare _cmp;
    public:
         ;
    };
    class tri_compare {
        bound_view::tri_compare _cmp;
    private:
        template<typename T, typename U>
        std::strong_ordering compare(const T& a, const U& b) const {
            if (a._type != b._type) {
                return composite_tri_compare::rank(a._type) <=> composite_tri_compare::rank(b._type);
            }
            if (!a._ck) {
                return std::strong_ordering::equal;
            }
            return _cmp(*a._ck, int8_t(a._bound_weight), *b._ck, int8_t(b._bound_weight));
        }
    public:
        tri_compare(const schema& s) : _cmp(s) { }
        std::strong_ordering operator()(const position_in_partition& a, const position_in_partition& b) const {
            return compare(a, b);
        }
        std::strong_ordering operator()(const position_in_partition_view& a, const position_in_partition_view& b) const {
            return compare(a, b);
        }
        std::strong_ordering operator()(const position_in_partition& a, const position_in_partition_view& b) const {
            return compare(a, b);
        }
        std::strong_ordering operator()(const position_in_partition_view& a, const position_in_partition& b) const {
            return compare(a, b);
        }
    };
    class less_compare {
        tri_compare _cmp;
    public:
        less_compare(const schema& s) : _cmp(s) { }
        bool operator()(const position_in_partition& a, const position_in_partition& b) const {
            return _cmp(a, b) < 0;
        }
        bool operator()(const position_in_partition_view& a, const position_in_partition_view& b) const ;
        bool operator()(const position_in_partition& a, const position_in_partition_view& b) const {
            return _cmp(a, b) < 0;
        }
        bool operator()(const position_in_partition_view& a, const position_in_partition& b) const ;
    };
    class equal_compare {
        clustering_key_prefix::equality _equal;
        template<typename T, typename U>
        bool compare(const T& a, const U& b) const ;
    public:
        
    };
    
    // Create a position which is the same as this one but governed by a schema with reversed clustering key order.
    // Create a position which is the same as this one but governed by a schema with reversed clustering key order.
};
template <>
struct fmt::formatter<position_in_partition> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const ::position_in_partition& pos, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", position_in_partition_view(pos));
    }
};
struct view_and_holder {
    std::optional<position_in_partition> holder;
    position_in_partition_view view;
    
};
// Returns true if and only if there can't be any clustering_row with position > a and < b.
// It is assumed that a <= b.
bool no_clustering_row_between(const schema& s, position_in_partition_view a, position_in_partition_view b) ;
// Returns true if and only if there can't be any clustering_row with position >= a and < b.
// It is assumed that a <= b.

// Includes all position_in_partition objects "p" for which: start <= p < end
// And only those.
class position_range {
private:
    position_in_partition _start;
    position_in_partition _end;
public:
    static position_range from_range(const query::clustering_range&);
    static position_range for_static_row() ;
    static position_range full() ;
    // Constructs position_range which covers the same rows as given clustering_range.
    // position_range includes a fragment if it includes position of that fragment.
    position_range(const query::clustering_range&);
    position_range(query::clustering_range&&);
    position_range(position_in_partition start, position_in_partition end)
        : _start(std::move(start))
        , _end(std::move(end))
    { }
    void set_start(position_in_partition pos) ;
    void set_end(position_in_partition pos) ;
    const position_in_partition& start() const& ;
    position_in_partition&& start() && ;
    const position_in_partition& end() const& ;
    
    
    // Returns true iff this range contains all keys contained by position_range(start, end).
};
class clustering_interval_set;
// Assumes that the bounds of `r` are of 'clustered' type
// and that `r` is non-empty (the left bound is smaller than the right bound).
//
// If `r` does not contain any keys, returns nullopt.
namespace locator {
using host_id = utils::tagged_uuid<struct host_id_tag>;
}
namespace service {
namespace pager {
class paging_state final {
public:
    using replicas_per_token_range = std::unordered_map<dht::token_range, std::vector<locator::host_id>>;
private:
    partition_key _partition_key;
    std::optional<clustering_key> _clustering_key;
    uint32_t _remaining_low_bits;
    query_id _query_uuid;
    replicas_per_token_range _last_replicas;
    std::optional<db::read_repair_decision> _query_read_repair_decision;
    uint32_t _rows_fetched_for_last_partition_low_bits;
    uint32_t _remaining_high_bits;
    uint32_t _rows_fetched_for_last_partition_high_bits;
    bound_weight _ck_weight = bound_weight::equal;
    partition_region _region = partition_region::partition_start;
public:
    // IDL ctor
    // sets position to at the given clustering key
};
}
}
template<typename... Ts> struct overloaded_functor : Ts... { using Ts::operator()...; };
;
namespace cql3 {
struct null_value {
};
class raw_value;
/// \brief View to a raw CQL protocol value.
///
/// \see raw_value
class raw_value_view {
    std::variant<fragmented_temporary_buffer::view, managed_bytes_view, null_value> _data;
    // Temporary storage is only useful if a raw_value_view needs to be instantiated
    // with a value which lifetime is bounded only to the view itself.
    // This hack is introduced in order to avoid storing temporary storage
    // in an external container, which may cause memory leaking problems.
    // This pointer is disengaged for regular raw_value_view instances.
    // Data is stored in a shared pointer for two reasons:
    // - pointers are cheap to copy
    // - it makes the view keep its semantics - it's safe to copy a view multiple times
    //   and all copies still refer to the same underlying data.
    lw_shared_ptr<managed_bytes> _temporary_storage = nullptr;
    // This constructor is only used by make_temporary() and it acquires ownership
    // of the given buffer. The view created that way refers to its own temporary storage.
public:
    
    
    // An empty value is not null, but it has 0 bytes of data.
    // An empty int value can be created in CQL using blobasint(0x).
    bool is_empty_value() const ;
    bool is_value() const ;
    
    template <typename Func>
    requires std::invocable<Func, const managed_bytes_view&> && std::invocable<Func, const fragmented_temporary_buffer::view&>
    decltype(auto) with_value(Func f) const {
        switch (_data.index()) {
        case 0: return f(std::get<fragmented_temporary_buffer::view>(_data));
        default: return f(std::get<managed_bytes_view>(_data));
        }
    }
    template <typename Func>
    requires std::invocable<Func, bytes_view>
    decltype(auto) with_linearized(Func f) const {
        return with_value([&] (const FragmentedView auto& v) {
            return ::with_linearized(v, std::forward<Func>(f));
        });
    }
    size_t size_bytes() const {
        return with_value([&] (const FragmentedView auto& v) {
            return v.size_bytes();
        });
    }
    template <typename ValueType>
    ValueType deserialize(const abstract_type& t) const {
        return value_cast<ValueType>(with_value([&] (const FragmentedView auto& v) { return t.deserialize(v); }));
    }
    template <typename ValueType>
    ValueType deserialize(const collection_type_impl& t) const {
        return value_cast<ValueType>(with_value([&] (const FragmentedView auto& v) { return t.deserialize(v); }));
    }
    void validate(const abstract_type& t) const {
        return with_value([&] (const FragmentedView auto& v) { return t.validate(v); });
    }
    template <typename ValueType>
    ValueType validate_and_deserialize(const collection_type_impl& t) const {
        return with_value([&] (const FragmentedView auto& v) {
            t.validate(v);
            return value_cast<ValueType>(t.deserialize(v));
        });
    }
    template <typename ValueType>
    ValueType validate_and_deserialize(const abstract_type& t) const {
        return with_value([&] (const FragmentedView auto& v) {
            t.validate(v);
            return value_cast<ValueType>(t.deserialize(v));
        });
    }
    friend managed_bytes_opt to_managed_bytes_opt(const cql3::raw_value_view& view) {
        if (view.is_value()) {
            return view.with_value([] (const FragmentedView auto& v) { return managed_bytes(v); });
        }
        return managed_bytes_opt();
    }
    friend managed_bytes_opt to_managed_bytes_opt(cql3::raw_value_view&& view) {
        if (view._temporary_storage) {
            return std::move(*view._temporary_storage);
        }
        return to_managed_bytes_opt(view);
    }
    friend std::ostream& operator<<(std::ostream& os, const raw_value_view& value);
    friend class raw_value;
};
/// \brief Raw CQL protocol value.
///
/// The `raw_value` type represents an uninterpreted value from the CQL wire
/// protocol. A raw value can hold either a null value, or a byte
/// blob that represents the value.
class raw_value {
    std::variant<bytes, managed_bytes, null_value> _data;
public:
    // An empty value is not null, but it has 0 bytes of data.
    // An empty int value can be created in CQL using blobasint(0x).
    friend class raw_value_view;
};
}
namespace cql3 {
class cql_config;
extern const cql_config default_cql_config;
class column_specification;
using computed_function_values = std::unordered_map<uint8_t, bytes_opt>;
using unset_bind_variable_vector = utils::small_vector<bool, 16>;
// Matches a raw_value_view with an unset vector to support CQL binary protocol
// "unset" values.
struct raw_value_view_vector_with_unset {
    std::vector<raw_value_view> values;
    unset_bind_variable_vector unset;
    // Constructor with no unset support, for tests and internal queries
};
// Matches a raw_value with an unset vector to support CQL binary protocol
// "unset" values.
struct raw_value_vector_with_unset {
    std::vector<raw_value> values;
    unset_bind_variable_vector unset;
    // Constructor with no unset support, for tests and internal queries
    // Mostly for testing.
};
class query_options {
public:
    // Options that are likely to not be present in most queries
    struct specific_options final {
        static thread_local const specific_options DEFAULT;
        const int32_t page_size;
        const lw_shared_ptr<service::pager::paging_state> state;
        const std::optional<db::consistency_level> serial_consistency;
        const api::timestamp_type timestamp;
    };
private:
    const cql_config& _cql_config;
    const db::consistency_level _consistency;
    const std::optional<std::vector<sstring_view>> _names;
    std::vector<cql3::raw_value> _values;
    std::vector<cql3::raw_value_view> _value_views;
    unset_bind_variable_vector _unset;
    const bool _skip_metadata;
    const specific_options _options;
    std::optional<std::vector<query_options>> _batch_options;
    // We must use the same microsecond-precision timestamp for
    // all cells created by an LWT statement or when a statement
    // has a user-provided timestamp. In case the statement or
    // a BATCH appends many values to a list, each value should
    // get a unique and monotonic timeuuid. This sequence is
    // used to make all time-based UUIDs:
    // 1) share the same microsecond,
    // 2) monotonic
    // 3) unique.
    mutable int _list_append_seq = 0;
    // Cached `function_call` evaluation results. `function_call` AST nodes
    // are created for each function with side effects in a CQL query, i.e.
    // non-deterministic functions (`uuid()`, `now()` and some others
    // timeuuid-related).
    //
    // These nodes are evaluated either when a query itself is executed
    // or query restrictions are computed (e.g. partition/clustering
    // key ranges for LWT requests).
    //
    // We need to cache the calls since otherwise when handling a
    // `bounce_to_shard` request for an LWT query, we can possibly enter an
    // infinite bouncing loop (in case a function is used to calculate
    // partition key ranges for a query), since the results can be different
    // each time. Furthermore, we don't support bouncing more than one time.
    // Refs: #8604 (https://github.com/scylladb/scylla/issues/8604)
    //
    // Using mutable because `query_state` is not available at
    // evaluation sites and we only have a const reference to `query_options`.
    mutable computed_function_values _cached_pk_fn_calls;
private:
    // Batch constructor.
    template <typename Values>
    requires std::same_as<Values, raw_value_vector_with_unset> || std::same_as<Values, raw_value_view_vector_with_unset>
    explicit query_options(query_options&& o, std::vector<Values> values_ranges);
public:
    template <typename Values>
    requires std::same_as<Values, raw_value_vector_with_unset> || std::same_as<Values, raw_value_view_vector_with_unset>
    static query_options make_batch_options(query_options&& o, std::vector<Values> values_ranges) {
        return query_options(std::move(o), std::move(values_ranges));
    }
    // It can't be const because of prepare()
    static thread_local query_options DEFAULT;
    // forInternalUse
    // Mainly for the sake of BatchQueryOptions
    const std::optional<std::vector<sstring_view>>& get_names() const noexcept {
        return _names;
    }
    const std::vector<cql3::raw_value_view>& get_values() const noexcept {
        return _value_views;
    }
    const cql_config& get_cql_config() const {
        return _cql_config;
    }
    // Generate a next unique list sequence for list append, e.g.
    // a = a + [val1, val2, ...]
    int next_list_append_seq() const {
        return _list_append_seq++;
    }
    // To preserve prepend monotonicity within a batch, each next
    // value must get a timestamp that's smaller than the previous one:
    // BEGIN BATCH
    //      UPDATE t SET l = [1, 2] + l WHERE pk = 0;
    //      UPDATE t SET l = [3] + l WHERE pk = 0;
    //      UPDATE t SET l = [4] + l WHERE pk = 0;
    // APPLY BATCH
    // SELECT l FROM t WHERE pk = 0;
    //  l
    // ------------
    // [4, 3, 1, 2]
    //
    // This function reserves the given number of prepend entries
    // and returns an id for the first prepended entry (it
    // got to be the smallest one, to preserve the order of
    // a multi-value append).
    //
    // @retval sequence number of the first entry of a multi-value
    // append. To get the next value, add 1.
    int next_list_prepend_seq(int num_entries, int max_entries) const {
        if (_list_append_seq + num_entries < max_entries) {
            _list_append_seq += num_entries;
            return max_entries - _list_append_seq;
        }
        return max_entries;
    }
    void prepare(const std::vector<lw_shared_ptr<column_specification>>& specs);
private:
};
template <typename Values>
requires std::same_as<Values, raw_value_vector_with_unset> || std::same_as<Values, raw_value_view_vector_with_unset>
query_options::query_options(query_options&& o, std::vector<Values> values_ranges)
    : query_options(std::move(o))
{
    std::vector<query_options> tmp;
    tmp.reserve(values_ranges.size());
    std::transform(values_ranges.begin(), values_ranges.end(), std::back_inserter(tmp), [this](auto& values_range) {
        return query_options(_cql_config, _consistency, {}, std::move(values_range), _skip_metadata, _options);
    });
    _batch_options = std::move(tmp);
}
}
namespace query {
enum class digest_algorithm : uint8_t {
    none = 0,  // digest not required
    MD5 = 1,
    legacy_xxHash_without_null_digest = 2,
    xxHash = 3, // default algorithm
};
}
struct full_position;
struct full_position_view {
    const partition_key_view partition;
    const position_in_partition_view position;
};
struct full_position {
    partition_key partition;
    position_in_partition position;
};
namespace query {
struct short_read_tag { };
using short_read = bool_class<short_read_tag>;
// result_memory_limiter, result_memory_accounter and result_memory_tracker
// form an infrastructure for limiting size of query results.
//
// result_memory_limiter is a shard-local object which ensures that all results
// combined do not use more than 10% of the shard memory.
//
// result_memory_accounter is used by result producers, updates the shard-local
// limits as well as keeps track of the individual maximum result size limit
// which is 1 MB.
//
// result_memory_tracker is just an object that makes sure the
// result_memory_limiter is notified when memory is released (but not sooner).
class result_memory_accounter;
class result_memory_limiter {
    const size_t _maximum_total_result_memory;
    semaphore _memory_limiter;
public:
    static constexpr size_t minimum_result_size = 4 * 1024;
    static constexpr size_t maximum_result_size = 1 * 1024 * 1024;
    static constexpr size_t unlimited_result_size = std::numeric_limits<size_t>::max();
public:
    // Reserves minimum_result_size and creates new memory accounter for
    // mutation query. Uses the specified maximum result size and may be
    // stopped before reaching it due to memory pressure on shard.
    // Reserves minimum_result_size and creates new memory accounter for
    // data query. Uses the specified maximum result size, result will *not*
    // be stopped due to on shard memory pressure in order to avoid digest
    // mismatches.
    // Creates a memory accounter for digest reads. Such accounter doesn't
    // contribute to the shard memory usage, but still stops producing the
    // result after individual limit has been reached.
    // Checks whether the result can grow any more, takes into account only
    // the per shard limit.
    // Consumes n bytes from memory limiter and checks whether the result
    // can grow any more (considering just the per-shard limit).
};
class result_memory_tracker {
    semaphore_units<> _units;
    size_t _used_memory;
private:
    static thread_local semaphore _dummy;
public:
    result_memory_tracker() noexcept : _units(_dummy, 0), _used_memory(0) { }
};
class result_memory_accounter {
    result_memory_limiter* _limiter = nullptr;
    size_t _blocked_bytes = 0;
    size_t _used_memory = 0;
    size_t _total_used_memory = 0;
    query::max_result_size _maximum_result_size;
    stop_iteration _stop_on_global_limit;
    short_read _short_read_allowed;
    mutable bool _below_soft_limit = true;
private:
    // Mutation query accounter. Uses provided individual result size limit and
    // will stop when shard memory pressure grows too high.
    struct mutation_query_tag { };
    // Data query accounter. Uses provided individual result size limit and
    // will *not* stop even though shard memory pressure grows too high.
    struct data_query_tag { };
    // Digest query accounter. Uses provided individual result size limit and
    // will *not* stop even though shard memory pressure grows too high. This
    // accounter does not contribute to the shard memory limits.
    struct digest_query_tag { };
    
    stop_iteration check_local_limit() const;
    friend class result_memory_limiter;
public:
    explicit result_memory_accounter(size_t max_size) noexcept
        : _blocked_bytes(0)
        , _maximum_result_size(max_size) {
    }
    // Consume n more bytes for the result. Returns stop_iteration::yes if
    // the result cannot grow any more (taking into account both individual
    // and per-shard limits).
    stop_iteration update_and_check(size_t n) ;
    // Checks whether the result can grow any more.
    stop_iteration check() const ;
    // Consume n more bytes for the result.
    void update(size_t n) ;
    result_memory_tracker done() && ;
};
enum class result_request {
    only_result,
    only_digest,
    result_and_digest,
};
struct result_options {
    result_request request = result_request::only_result;
    digest_algorithm digest_algo = query::digest_algorithm::none;
    static result_options only_result() ;
};
class result_digest {
public:
    using type = std::array<uint8_t, 16>;
private:
    type _digest;
public:
};
//
// The query results are stored in a serialized form. This is in order to
// address the following problems, which a structured format has:
//
//   - high level of indirection (vector of vectors of vectors of blobs), which
//     is not CPU cache friendly
//
//   - high allocation rate due to fine-grained object structure
//
// On replica side, the query results are probably going to be serialized in
// the transport layer anyway, so serializing the results up-front doesn't add
// net work. There is no processing of the query results on replica other than
// concatenation in case of range queries and checksum calculation. If query
// results are collected in serialized form from different cores, we can
// concatenate them without copying by simply appending the fragments into the
// packet.
//
// On coordinator side, the query results would have to be parsed from the
// transport layer buffers anyway, so the fact that iterators parse it also
// doesn't add net work, but again saves allocations and copying. The CQL
// server doesn't need complex data structures to process the results, it just
// goes over it linearly consuming it.
//
// The coordinator side could be optimized even further for CQL queries which
// do not need processing (eg. select * from cf where ...). We could make the
// replica send the query results in the format which is expected by the CQL
// binary protocol client. So in the typical case the coordinator would just
// pass the data using zero-copy to the client, prepending a header.
//
// Users which need more complex structure of query results can convert this
// to query::result_set.
//
// Related headers:
//  - query-result-reader.hh
//  - query-result-writer.hh
class result {
    bytes_ostream _w;
    std::optional<result_digest> _digest;
    std::optional<uint32_t> _row_count_low_bits;
    api::timestamp_type _last_modified = api::missing_timestamp;
    short_read _short_read;
    query::result_memory_tracker _memory_tracker;
    std::optional<uint32_t> _partition_count;
    std::optional<uint32_t> _row_count_high_bits;
    std::optional<full_position> _last_position;
public:
    class builder;
    class partition_writer;
    friend class result_merger;
    // Return _last_position if replica filled it, otherwise calculate it based
    // on the content (by looking up the last row in the last partition).
    struct printer {
        schema_ptr s;
        const query::partition_slice& slice;
        const query::result& res;
    };
};
}
namespace ser {
template <>
struct serializer<clustering_key_prefix> {
  ;
  ;
  ;
};
template <>
struct serializer<const clustering_key_prefix> : public serializer<clustering_key_prefix>
{};
template <>
struct serializer<partition_key> {
  ;
  ;
  ;
};
template <>
struct serializer<const partition_key> : public serializer<partition_key>
{};
} // ser
namespace ser {
template <>
struct serializer<query::digest_algorithm> {
  ;
  ;
  ;
};
template <>
struct serializer<const query::digest_algorithm> : public serializer<query::digest_algorithm>
{};
} // ser
namespace ser {
// frame represents a place holder for object size which will be known later
template<typename Output>
struct place_holder { };
template<typename Output>
struct frame { };
template<>
struct place_holder<bytes_ostream> {
    bytes_ostream::place_holder<size_type> ph;
};
template<>
struct frame<bytes_ostream> : public place_holder<bytes_ostream> {
    bytes_ostream::size_type offset;
};
struct vector_position {
    bytes_ostream::position pos;
    size_type count;
};
//empty frame, behave like a place holder, but is used when no place holder is needed
template<typename Output>
struct empty_frame {
};
 ;
template<>
struct place_holder<seastar::measuring_output_stream> {
    void set(seastar::measuring_output_stream&, size_type) { }
};
template<>
struct frame<seastar::measuring_output_stream> : public place_holder<seastar::measuring_output_stream> {
    void end(seastar::measuring_output_stream& out) { }
};
template<>
class place_holder<seastar::simple_output_stream> {
    seastar::simple_output_stream _substream;
public:
    place_holder(seastar::simple_output_stream substream)
        : _substream(substream) { }
    void set(seastar::simple_output_stream& out, size_type v) {
        serialize(_substream, v);
    }
};
template<>
class frame<seastar::simple_output_stream> : public place_holder<seastar::simple_output_stream> {
    char* _start;
public:
    frame(seastar::simple_output_stream ph, char* start)
        : place_holder(ph), _start(start) { }
    void end(seastar::simple_output_stream& out) {
        set(out, out.begin() - _start);
    }
};
template<typename Iterator>
class place_holder<seastar::memory_output_stream<Iterator>> {
    seastar::memory_output_stream<Iterator> _substream;
public:
    place_holder(seastar::memory_output_stream<Iterator> substream)
        : _substream(substream) { }
    void set(seastar::memory_output_stream<Iterator>& out, size_type v) {
        serialize(_substream, v);
    }
};
template<typename Iterator>
class frame<seastar::memory_output_stream<Iterator>> : public place_holder<seastar::memory_output_stream<Iterator>> {
    size_t _start_left;
public:
    frame(seastar::memory_output_stream<Iterator> ph, size_t start_left)
        : place_holder<seastar::memory_output_stream<Iterator>>(ph), _start_left(start_left) { }
    void end(seastar::memory_output_stream<Iterator>& out) {
        this->set(out, _start_left - out.size());
    }
};
template<typename Iterator>
inline place_holder<seastar::memory_output_stream<Iterator>> start_place_holder(seastar::memory_output_stream<Iterator>& out) {
    return { out.write_substream(sizeof(size_type)) };
}
template<typename Iterator>
inline frame<seastar::memory_output_stream<Iterator>> start_frame(seastar::memory_output_stream<Iterator>& out) {
    auto start_left = out.size();
    auto substream = out.write_substream(sizeof(size_type));
    {
        auto sstr = substream;
        serialize(sstr, size_type(0));
    }
    return frame<seastar::memory_output_stream<Iterator>>(substream, start_left);
}
}
namespace ser {
} // ser
namespace ser {
struct qr_cell_view {
    utils::input_stream v;
};
template<>
struct serializer<qr_cell_view> {
     ;
     ;
     ;
};
struct qr_row_view {
    utils::input_stream v;
};
template<>
struct serializer<qr_row_view> {
     ;
     ;
     ;
};
struct qr_clustered_row_view {
    utils::input_stream v;
};
template<>
struct serializer<qr_clustered_row_view> {
     ;
     ;
     ;
};
struct qr_partition_view {
    utils::input_stream v;
};
template<>
struct serializer<qr_partition_view> {
     ;
     ;
     ;
};
struct query_result_view {
    utils::input_stream v;
};
template<>
struct serializer<query_result_view> {
     ;
     ;
     ;
};
////// State holders
template<typename Output>
struct state_of_qr_cell {
    frame<Output> f;
};
template<typename Output>
struct state_of_qr_row {
    frame<Output> f;
};
template<typename Output>
struct state_of_qr_clustered_row {
    frame<Output> f;
};
template<typename Output>
struct state_of_qr_clustered_row__cells {
    frame<Output> f;
    state_of_qr_clustered_row<Output> _parent;
};
template<typename Output>
struct state_of_qr_partition {
    frame<Output> f;
};
template<typename Output>
struct state_of_qr_partition__static_row {
    frame<Output> f;
    state_of_qr_partition<Output> _parent;
};
template<typename Output>
struct state_of_query_result {
    frame<Output> f;
};
////// Nodes
template<typename Output>
struct after_qr_cell__ttl {
    Output& _out;
    state_of_qr_cell<Output> _state;
};
template<typename Output>
struct after_qr_cell__value {
    Output& _out;
    state_of_qr_cell<Output> _state;
};
template<typename Output>
struct after_qr_cell__expiry {
    Output& _out;
    state_of_qr_cell<Output> _state;
     ;
};
template<typename Output>
struct after_qr_cell__timestamp {
    Output& _out;
    state_of_qr_cell<Output> _state;
};
template<typename Output>
struct writer_of_qr_cell {
    Output& _out;
    state_of_qr_cell<Output> _state;
};
template<typename Output>
struct after_qr_row__cells {
    Output& _out;
    state_of_qr_row<Output> _state;
};
template<typename Output>
struct writer_of_std__optional__qr_cell {
    Output& _out;
};
template<typename Output>
struct qr_row__cells {
    Output& _out;
    state_of_qr_row<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct writer_of_qr_row {
    Output& _out;
    state_of_qr_row<Output> _state;
    qr_row__cells<Output> start_cells() && ;
    after_qr_row__cells<Output> skip_cells() && ;
};
template<typename Output>
struct after_qr_clustered_row__cells {
    Output& _out;
    state_of_qr_clustered_row<Output> _state;
    void  end_qr_clustered_row() ;
};
template<typename Output>
struct after_qr_clustered_row__cells__cells {
    Output& _out;
    state_of_qr_clustered_row__cells<Output> _state;
    after_qr_clustered_row__cells<Output>  end_cells() && ;
};
template<typename Output>
struct qr_clustered_row__cells__cells {
    Output& _out;
    state_of_qr_clustered_row__cells<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
    
  
  void add(std::optional<qr_cell_view> v) ;
  after_qr_clustered_row__cells__cells<Output> end_cells() && ;
  
  
};
template<typename Output>
struct qr_clustered_row__cells {
    Output& _out;
    state_of_qr_clustered_row__cells<Output> _state;
    qr_clustered_row__cells(Output& out, state_of_qr_clustered_row<Output> state)
            : _out(out)
            , _state{start_frame(out), std::move(state)}
            {}
    qr_clustered_row__cells__cells<Output> start_cells() && ;
    after_qr_clustered_row__cells__cells<Output> skip_cells() && ;
};
template<typename Output>
struct after_qr_clustered_row__key {
    Output& _out;
    state_of_qr_clustered_row<Output> _state;
    qr_clustered_row__cells<Output> start_cells() && ;
     ;
};
template<typename Output>
struct writer_of_qr_clustered_row {
    Output& _out;
    state_of_qr_clustered_row<Output> _state;
    
    after_qr_clustered_row__key<Output> skip_key() && ;
    after_qr_clustered_row__key<Output> write_key(const clustering_key& t) && ;
};
template<typename Output>
struct after_qr_partition__rows {
    Output& _out;
    state_of_qr_partition<Output> _state;
    void  end_qr_partition() ;
};
template<typename Output>
struct qr_partition__rows {
    Output& _out;
    state_of_qr_partition<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
    qr_partition__rows(Output& out, state_of_qr_partition<Output> state) 
            ;
  writer_of_qr_clustered_row<Output> add() ;
  void add(qr_clustered_row_view v) ;
  after_qr_partition__rows<Output> end_rows() && ;
  vector_position pos() const ;
  void rollback(const vector_position& vp) ;
};
template<typename Output>
struct after_qr_partition__static_row {
    Output& _out;
    state_of_qr_partition<Output> _state;
    qr_partition__rows<Output> start_rows() && ;
    after_qr_partition__rows<Output> skip_rows() && ;
};
template<typename Output>
struct after_qr_partition__static_row__cells {
    Output& _out;
    state_of_qr_partition__static_row<Output> _state;
    after_qr_partition__static_row<Output>  end_static_row() && ;
};
template<typename Output>
struct qr_partition__static_row__cells {
    Output& _out;
    state_of_qr_partition__static_row<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
    
  
  void add(std::optional<qr_cell_view> v) ;
  after_qr_partition__static_row__cells<Output> end_cells() && ;
  
  
};
template<typename Output>
struct qr_partition__static_row {
    Output& _out;
    state_of_qr_partition__static_row<Output> _state;
    qr_partition__static_row(Output& out, state_of_qr_partition<Output> state)
            : _out(out)
            , _state{start_frame(out), std::move(state)}
            {}
    qr_partition__static_row__cells<Output> start_cells() && ;
    after_qr_partition__static_row__cells<Output> skip_cells() && ;
};
template<typename Output>
struct after_qr_partition__key {
    Output& _out;
    state_of_qr_partition<Output> _state;
    qr_partition__static_row<Output> start_static_row() && ;
     ;
};
template<typename Output>
struct writer_of_qr_partition {
    Output& _out;
    state_of_qr_partition<Output> _state;
};
template<typename Output>
struct after_query_result__partitions {
    Output& _out;
    state_of_query_result<Output> _state;
};
template<typename Output>
struct query_result__partitions {
    Output& _out;
    state_of_query_result<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct writer_of_query_result {
    Output& _out;
    state_of_query_result<Output> _state;
    
    query_result__partitions<Output> start_partitions() && ;
};
} // ser
namespace query {
using result_bytes_view = ser::buffer_view<bytes_ostream::fragment_iterator>;
class result_atomic_cell_view {
    ser::qr_cell_view _view;
public:
};
// Contains cells in the same order as requested by partition_slice.
// Contains only live cells.
class result_row_view {
    ser::qr_row_view _v;
public:
    class iterator_type {
        using cells_deserializer = ser::vector_deserializer<std::optional<ser::qr_cell_view>>;
        cells_deserializer _cells;
        cells_deserializer::iterator _i;
    public:
        ;
    };
};
// Describes expectations about the ResultVisitor concept.
//
// Interaction flow:
//   -> accept_new_partition()
//   -> accept_new_row()
//   -> accept_new_row()
//   -> accept_partition_end()
//   -> accept_new_partition()
//   -> accept_new_row()
//   -> accept_new_row()
//   -> accept_new_row()
//   -> accept_partition_end()
//   ...
//
struct result_visitor {
};
template<typename Visitor>
concept ResultVisitor = requires(Visitor visitor, const partition_key& pkey,
                                      uint64_t row_count, const clustering_key& ckey,
                                      const result_row_view& static_row, const result_row_view& row)
{
    visitor.accept_new_partition(pkey, row_count);
    visitor.accept_new_partition(row_count);
    visitor.accept_new_row(ckey, static_row, row);
    visitor.accept_new_row(static_row, row);
    visitor.accept_partition_end(static_row);
};
class result_view {
    ser::query_result_view _v;
    friend class result_merger;
public:
     ;
     ;
     ;
};
}
namespace cql3 {
class assignment_testable {
public:
    enum class test_result {
        EXACT_MATCH,
        WEAKLY_ASSIGNABLE,
        NOT_ASSIGNABLE,
    };
    // for error reporting
};
std::ostream&
operator<<(std::ostream& os, const assignment_testable& at) ;
}
class mutation;
class atomic_cell_or_collection;
using counter_id = utils::tagged_uuid<struct counter_id_tag>;
template<mutable_view is_mutable>
class basic_counter_shard_view {
    enum class offset : unsigned {
        id = 0u,
        value = unsigned(id) + sizeof(counter_id),
        logical_clock = unsigned(value) + sizeof(int64_t),
        total_size = unsigned(logical_clock) + sizeof(int64_t),
    };
private:
    managed_bytes_basic_view<is_mutable> _base;
private:
    template<typename T>
    T read(offset off) const {
        auto v = _base;
        v.remove_prefix(size_t(off));
        return read_simple_native<T>(v);
    }
public:
    static constexpr auto size = size_t(offset::total_size);
public:
    basic_counter_shard_view() = default;
    explicit basic_counter_shard_view(managed_bytes_basic_view<is_mutable> v) noexcept
        : _base(v) { }
    counter_id id() const { return read<counter_id>(offset::id); }
    int64_t value() const { return read<int64_t>(offset::value); }
    int64_t logical_clock() const { return read<int64_t>(offset::logical_clock); }
    void swap_value_and_clock(basic_counter_shard_view& other) noexcept {
        static constexpr size_t off = size_t(offset::value);
        static constexpr size_t size = size_t(offset::total_size) - off;
        signed char tmp[size];
        auto tmp_view = single_fragmented_mutable_view(bytes_mutable_view(std::data(tmp), std::size(tmp)));
        managed_bytes_mutable_view this_view = _base.substr(off, size);
        managed_bytes_mutable_view other_view = other._base.substr(off, size);
        copy_fragmented_view(tmp_view, this_view);
        copy_fragmented_view(this_view, other_view);
        copy_fragmented_view(other_view, tmp_view);
    }
    void set_value_and_clock(const basic_counter_shard_view& other) noexcept {
        static constexpr size_t off = size_t(offset::value);
        static constexpr size_t size = size_t(offset::total_size) - off;
        managed_bytes_mutable_view this_view = _base.substr(off, size);
        managed_bytes_mutable_view other_view = other._base.substr(off, size);
        copy_fragmented_view(this_view, other_view);
    }
    bool operator==(const basic_counter_shard_view& other) const ;
    struct less_compare_by_id {
        bool operator()(const basic_counter_shard_view& x, const basic_counter_shard_view& y) const {
            return x.id() < y.id();
        }
    };
};
using counter_shard_view = basic_counter_shard_view<mutable_view::no>;
class counter_shard {
    counter_id _id;
    int64_t _value;
    int64_t _logical_clock;
private:
    // Shared logic for applying counter_shards and counter_shard_views.
    // T is either counter_shard or basic_counter_shard_view<U>.
    template<typename T>
    requires requires(T shard) {
        { shard.value() } -> std::same_as<int64_t>;
        { shard.logical_clock() } -> std::same_as<int64_t>;
    }
    counter_shard& do_apply(T&& other) noexcept {
        auto other_clock = other.logical_clock();
        if (_logical_clock < other_clock) {
            _logical_clock = other_clock;
            _value = other.value();
        }
        return *this;
    }
public:
    counter_shard(counter_id id, int64_t value, int64_t logical_clock) noexcept
        : _id(id)
        , _value(value)
        , _logical_clock(logical_clock)
    { }
    explicit counter_shard(counter_shard_view csv) noexcept
        : _id(csv.id())
        , _value(csv.value())
        , _logical_clock(csv.logical_clock())
    { }
    counter_id id() const { return _id; }
    int64_t value() const { return _value; }
    int64_t logical_clock() const { return _logical_clock; }
    counter_shard& update(int64_t value_delta, int64_t clock_increment) noexcept {
        _value = uint64_t(_value) + uint64_t(value_delta); // signed int overflow is undefined hence the cast
        _logical_clock += clock_increment;
        return *this;
    }
    counter_shard& apply(counter_shard_view other) noexcept {
        return do_apply(other);
    }
    counter_shard& apply(const counter_shard& other) noexcept {
        return do_apply(other);
    }
    static constexpr size_t serialized_size() {
        return counter_shard_view::size;
    }
    void serialize(atomic_cell_value_mutable_view& out) const {
        write_native<counter_id>(out, _id);
        write_native<int64_t>(out, _value);
        write_native<int64_t>(out, _logical_clock);
    }
};
class counter_cell_builder {
    std::vector<counter_shard> _shards;
    bool _sorted = true;
private:
public:
    counter_cell_builder() = default;
    void add_shard(const counter_shard& cs) {
        _shards.emplace_back(cs);
    }
    void sort_and_remove_duplicates() ;
    size_t serialized_size() const {
        return _shards.size() * counter_shard::serialized_size();
    }
    
    
    atomic_cell build(api::timestamp_type timestamp) const {
        auto ac = atomic_cell::make_live_uninitialized(*counter_type, timestamp, serialized_size());
        auto dst = ac.value();
        for (auto&& cs : _shards) {
            cs.serialize(dst);
        }
        return ac;
    }
    static atomic_cell from_single_shard(api::timestamp_type timestamp, const counter_shard& cs) ;
    class inserter_iterator {
    public:
        using iterator_category = std::output_iterator_tag;
        using value_type = counter_shard;
        using difference_type = std::ptrdiff_t;
        using pointer = counter_shard*;
        using reference = counter_shard&;
    private:
        counter_cell_builder* _builder;
    public:
        explicit inserter_iterator(counter_cell_builder& b) : _builder(&b) { }
        inserter_iterator& operator=(const counter_shard& cs) {
            _builder->add_shard(cs);
            return *this;
        }
        inserter_iterator& operator=(const counter_shard_view& csv) {
            return this->operator=(counter_shard(csv));
        }
        inserter_iterator& operator++() { return *this; }
        inserter_iterator& operator++(int) { return *this; }
        inserter_iterator& operator*() { return *this; };
    };
    inserter_iterator inserter() {
        return inserter_iterator(*this);
    }
};
// <counter_id>   := <int64_t><int64_t>
// <shard>        := <counter_id><int64_t:value><int64_t:logical_clock>
// <counter_cell> := <shard>*
template<mutable_view is_mutable>
class basic_counter_cell_view {
protected:
    basic_atomic_cell_view<is_mutable> _cell;
private:
    class shard_iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = basic_counter_shard_view<is_mutable>;
        using difference_type = std::ptrdiff_t;
        using pointer = basic_counter_shard_view<is_mutable>*;
        using reference = basic_counter_shard_view<is_mutable>&;
    private:
        managed_bytes_basic_view<is_mutable> _current;
        basic_counter_shard_view<is_mutable> _current_view;
        size_t _pos = 0;
    public:
        shard_iterator(managed_bytes_basic_view<is_mutable> v, size_t offset) noexcept
            : _current(v), _current_view(_current), _pos(offset) { }
        basic_counter_shard_view<is_mutable>& operator*() noexcept {
            return _current_view;
        }
        basic_counter_shard_view<is_mutable>* operator->() noexcept {
            return &_current_view;
        }
        shard_iterator& operator++() noexcept {
            _pos += counter_shard_view::size;
            _current_view = basic_counter_shard_view<is_mutable>(_current.substr(_pos, counter_shard_view::size));
            return *this;
        }
        shard_iterator operator++(int) noexcept ;
        shard_iterator& operator--() noexcept {
            _pos -= counter_shard_view::size;
            _current_view = basic_counter_shard_view<is_mutable>(_current.substr(_pos, counter_shard_view::size));
            return *this;
        }
        shard_iterator operator--(int) noexcept ;
        bool operator==(const shard_iterator& other) const noexcept {
            return _pos == other._pos;
        }
    };
public:
    boost::iterator_range<shard_iterator> shards() const {
        auto value = _cell.value();
        auto begin = shard_iterator(value, 0);
        auto end = shard_iterator(value, value.size());
        return boost::make_iterator_range(begin, end);
    }
    size_t shard_count() const {
        return _cell.value().size() / counter_shard_view::size;
    }
public:
    // ac must be a live counter cell
    explicit basic_counter_cell_view(basic_atomic_cell_view<is_mutable> ac) noexcept
        : _cell(ac)
    {
        assert(_cell.is_live());
        assert(!_cell.is_counter_update());
    }
    api::timestamp_type timestamp() const { return _cell.timestamp(); }
    static data_type total_value_type() ;
    int64_t total_value() const ;
    
    
};
struct counter_cell_view : basic_counter_cell_view<mutable_view::no> {
    using basic_counter_cell_view::basic_counter_cell_view;
    // Reversibly applies two counter cells, at least one of them must be live.
    static void apply(const column_definition& cdef, atomic_cell_or_collection& dst, atomic_cell_or_collection& src);
    // Computes a counter cell containing minimal amount of data which, when
    // applied to 'b' returns the same cell as 'a' and 'b' applied together.
    static std::optional<atomic_cell> difference(atomic_cell_view a, atomic_cell_view b);
    friend std::ostream& operator<<(std::ostream& os, counter_cell_view ccv);
};
struct counter_cell_mutable_view : basic_counter_cell_view<mutable_view::yes> {
    using basic_counter_cell_view::basic_counter_cell_view;
    explicit counter_cell_mutable_view(atomic_cell_mutable_view ac) noexcept
        : basic_counter_cell_view<mutable_view::yes>(ac)
    {
    }
    void set_timestamp(api::timestamp_type ts) { _cell.set_timestamp(ts); }
};
// Transforms mutation dst from counter updates to counter shards using state
// stored in current_state.
// If current_state is present it has to be in the same schema as dst.
template<>
struct appending_hash<counter_shard_view> {
     ;
};
template<>
struct appending_hash<counter_cell_view> {
     ;
};
namespace cql3 {
namespace selection {
class result_set_builder;
class selector : public assignment_testable {
public:
    class factory;
};
class selector::factory {
public:
    virtual std::optional<std::pair<query::forward_request::reduction_type, query::forward_request::aggregation_info>> 
    get_reduction() const {return std::nullopt;}
    virtual bool is_write_time_selector_factory() const {
        return false;
    }
    virtual bool is_ttl_selector_factory() const {
        return false;
    }
    virtual sstring column_name() const = 0;
};
}
}
namespace cql3 {
namespace functions {
using function = db::functions::function;
}
}
namespace cql3 {
namespace functions {
using function_name = db::functions::function_name;
}
}
namespace cql3 {
namespace selection {
class selectable;
class selectable {
public:
    virtual ~selectable() ;
    
protected:
public:
    class writetime_or_ttl;
    class with_function;
    class with_anonymous_function;
    class with_field_selection;
    class with_cast;
};
class selectable::with_function : public selectable {
    functions::function_name _function_name;
    std::vector<shared_ptr<selectable>> _args;
public:
};
class selectable::with_anonymous_function : public selectable {
    shared_ptr<functions::function> _function;
    std::vector<shared_ptr<selectable>> _args;
public:
};
class selectable::with_cast : public selectable {
    ::shared_ptr<selectable> _arg;
    data_type _type;
public:
};
}
}
namespace cql3 {
class result_set;
class result_set_builder;
class metadata;
class query_options;
namespace restrictions {
class statement_restrictions;
}
namespace selection {
class raw_selector;
class selector_factories;
class selectors {
public:
};
class selection {
private:
    schema_ptr _schema;
    std::vector<const column_definition*> _columns;
    ::shared_ptr<metadata> _metadata;
    const bool _collect_timestamps;
    const bool _collect_TTLs;
    const bool _contains_static_columns;
    bool _is_trivial;
protected:
    using trivial = bool_class<class trivial_tag>;
public:
    // Overriden by SimpleSelection when appropriate.
private:
public:
     ;
    friend class result_set_builder;
};
class result_set_builder {
private:
    std::unique_ptr<result_set> _result_set;
    std::unique_ptr<selectors> _selectors;
    const std::vector<size_t> _group_by_cell_indices; ///< Indices in \c current of cells holding GROUP BY values.
    std::vector<managed_bytes_opt> _last_group; ///< Previous row's group: all of GROUP BY column values.
    bool _group_began; ///< Whether a group began being formed.
public:
    std::optional<std::vector<managed_bytes_opt>> current;
private:
    std::vector<api::timestamp_type> _timestamps;
    std::vector<int32_t> _ttls;
    const gc_clock::time_point _now;
public:
     ;
    class nop_filter {
    public:
    };
    class restrictions_filter {
        const ::shared_ptr<const restrictions::statement_restrictions> _restrictions;
        const query_options& _options;
        const bool _skip_pk_restrictions;
        const bool _skip_ck_restrictions;
        mutable bool _current_partition_key_does_not_match = false;
        mutable bool _current_static_row_does_not_match = false;
        mutable uint64_t _rows_dropped = 0;
        mutable uint64_t _remaining;
        schema_ptr _schema;
        mutable uint64_t _per_partition_limit;
        mutable uint64_t _per_partition_remaining;
        mutable uint64_t _rows_fetched_for_last_partition;
        mutable std::optional<partition_key> _last_pkey;
        mutable bool _is_first_partition_on_page = true;
    public:
    private:
    };
    // Implements ResultVisitor concept from query.hh
    template<typename Filter = nop_filter>
    class visitor {
    protected:
        result_set_builder& _builder;
        const schema& _schema;
        const selection& _selection;
        uint64_t _row_count;
        std::vector<bytes> _partition_key;
        std::vector<bytes> _clustering_key;
        Filter _filter;
    public:
    };
private:
    /// True iff the \c current row ends a previously started group, either according to
    /// _group_by_cell_indices or aggregation.
    /// If there is a valid row in this->current, process it; if \p more_rows_coming, get ready to
    /// receive another.
    /// Gets output row from _selectors and resets them.
    /// Updates _last_group from the \c current row.
};
}
}
namespace cql3 {
class update_parameters final {
public:
    // Option set for partition_slice to be used when fetching prefetch_data
    static constexpr query::partition_slice::option_set options = query::partition_slice::option_set::of<
        query::partition_slice::option::send_partition_key,
        query::partition_slice::option::send_clustering_key,
        query::partition_slice::option::collections_as_maps>();
    // Holder for data for
    // 1) CQL list updates which depend on current state of the list
    // 2) cells needed to check conditions of a CAS statement,
    // 3) rows of CAS result set.
    struct prefetch_data {
        using key = std::pair<partition_key, clustering_key>;
        using key_view = std::pair<const partition_key&, const clustering_key&>;
        struct key_less {
            partition_key::tri_compare pk_cmp;
            clustering_key::tri_compare ck_cmp;
            // Allow mixing std::pair<partition_key, clustering_key> and
            // std::pair<const partition_key&, const clustering_key&> during lookup
             ;
        };
    public:
        struct row {
            bool has_static;
            // indexes are determined by prefetch_data::selection
            std::vector<managed_bytes_opt> cells;
            // Return true if this row has at least one static column set.
        };
        // Use an ordered map since CAS result set must be naturally ordered
        // when returned to the client.
        std::map<key, row, key_less> rows;
        schema_ptr schema;
        shared_ptr<cql3::selection::selection> selection;
    public:
        // Find a row object for either static or regular subset of cells, depending
        // on whether clustering key is empty or not.
        // A particular cell within the row can then be found using a column id.
    };
    // Note: value (mutation) only required to contain the rows we are interested in
private:
    const std::optional<gc_clock::duration> _ttl;
    // For operations that require a read-before-write, stores prefetched cell values.
    // For CAS statements, stores values of conditioned columns.
    // Is a reference to an outside prefetch_data container since a CAS BATCH statement
    // prefetches all rows at once, for all its nested modification statements.
    const prefetch_data& _prefetched;
public:
    const api::timestamp_type _timestamp;
    const gc_clock::time_point _local_deletion_time;
    const schema_ptr _schema;
    const query_options& _options;
    ;
    ;
    std::optional<std::vector<std::pair<data_value, data_value>>>
    get_prefetched_list(const partition_key& pkey, const clustering_key& ckey, const column_definition& column) const;
};
}
namespace cql3 {
namespace statements {
enum class bound : int32_t { START = 0, END };

}
}
namespace utils {
// Given type T and an std::variant Variant, return std::true_type if T is a variant element
template <class T, class Variant>
struct is_variant_element;
template <class T, class... Elements>
struct is_variant_element<T, std::variant<Elements...>> : std::bool_constant<(std::is_same_v<T, Elements> || ...)> {
};
// Givent type T and std::variant, true if T is one of the variant elements.
template <typename T, typename Variant>
concept VariantElement = is_variant_element<T, Variant>::value;
}
class row;
namespace db {
namespace functions {
    class function;
}
}
namespace secondary_index {
class index;
class secondary_index_manager;
} // namespace secondary_index
namespace query {
    class result_row_view;
} // namespace query
namespace cql3 {
struct prepare_context;
class column_identifier_raw;
class query_options;
namespace selection {
    class selection;
} // namespace selection
namespace restrictions {
    class restriction;
}
namespace expr {
struct allow_local_index_tag {};
using allow_local_index = bool_class<allow_local_index_tag>;
struct binary_operator;
struct conjunction;
struct column_value;
struct subscript;
struct unresolved_identifier;
struct column_mutation_attribute;
struct function_call;
struct cast;
struct field_selection;
struct bind_variable;
struct untyped_constant;
struct constant;
struct tuple_constructor;
struct collection_constructor;
struct usertype_constructor;
template <typename T>
concept ExpressionElement
        = std::same_as<T, conjunction>
        || std::same_as<T, binary_operator>
        || std::same_as<T, column_value>
        || std::same_as<T, subscript>
        || std::same_as<T, unresolved_identifier>
        || std::same_as<T, column_mutation_attribute>
        || std::same_as<T, function_call>
        || std::same_as<T, cast>
        || std::same_as<T, field_selection>
        || std::same_as<T, bind_variable>
        || std::same_as<T, untyped_constant>
        || std::same_as<T, constant>
        || std::same_as<T, tuple_constructor>
        || std::same_as<T, collection_constructor>
        || std::same_as<T, usertype_constructor>
        ;
template <typename Func>
concept invocable_on_expression
        = std::invocable<Func, conjunction>
        && std::invocable<Func, binary_operator>
        && std::invocable<Func, column_value>
        && std::invocable<Func, subscript>
        && std::invocable<Func, unresolved_identifier>
        && std::invocable<Func, column_mutation_attribute>
        && std::invocable<Func, function_call>
        && std::invocable<Func, cast>
        && std::invocable<Func, field_selection>
        && std::invocable<Func, bind_variable>
        && std::invocable<Func, untyped_constant>
        && std::invocable<Func, constant>
        && std::invocable<Func, tuple_constructor>
        && std::invocable<Func, collection_constructor>
        && std::invocable<Func, usertype_constructor>
        ;
template <typename Func>
concept invocable_on_expression_ref
        = std::invocable<Func, conjunction&>
        && std::invocable<Func, binary_operator&>
        && std::invocable<Func, column_value&>
        && std::invocable<Func, subscript&>
        && std::invocable<Func, unresolved_identifier&>
        && std::invocable<Func, column_mutation_attribute&>
        && std::invocable<Func, function_call&>
        && std::invocable<Func, cast&>
        && std::invocable<Func, field_selection&>
        && std::invocable<Func, bind_variable&>
        && std::invocable<Func, untyped_constant&>
        && std::invocable<Func, constant&>
        && std::invocable<Func, tuple_constructor&>
        && std::invocable<Func, collection_constructor&>
        && std::invocable<Func, usertype_constructor&>
        ;
/// A CQL expression -- union of all possible expression types.
class expression final {
    // 'impl' holds a variant of all expression types, but since 
    // variants of incomplete types are not allowed, we forward declare it
    // here and fully define it later.
    struct impl;                 
    std::unique_ptr<impl> _v;
public:
     // FIXME: remove
    template <invocable_on_expression Visitor>
    friend decltype(auto) visit(Visitor&& visitor, const expression& e);
    template <invocable_on_expression_ref Visitor>
    friend decltype(auto) visit(Visitor&& visitor, expression& e);
    ;
    ;
    ;
    ;
    // Prints given expression using additional options
    struct printer {
        const expression& expr_to_print;
        bool debug_mode = true;
    };
};
/// Checks if two expressions are equal. If they are, they definitely
/// perform the same computation. If they are unequal, they may perform
/// the same computation or different computations.
// An expression that doesn't contain subexpressions
template <typename E>
concept LeafExpression
        = std::same_as<unresolved_identifier, E>
        || std::same_as<bind_variable, E> 
        || std::same_as<untyped_constant, E> 
        || std::same_as<constant, E>
        || std::same_as<column_value, E>
        ;
/// A column, usually encountered on the left side of a restriction.
/// An expression like `mycol < 5` would be expressed as a binary_operator
/// with column_value on the left hand side.
struct column_value {
    const column_definition* col;
};
/// A subscripted value, eg list_colum[2], val[sub]
struct subscript {
    expression val;
    expression sub;
    data_type type; // may be null before prepare
};
/// Gets the subscripted column_value out of the subscript.
/// Only columns can be subscripted in CQL, so we can expect that the subscripted expression is a column_value.
/// Gets the column_definition* out of expression that can be a column_value or subscript
/// Only columns can be subscripted in CQL, so we can expect that the subscripted expression is a column_value.
enum class oper_t { EQ, NEQ, LT, LTE, GTE, GT, IN, CONTAINS, CONTAINS_KEY, IS_NOT, LIKE };
/// Describes the nature of clustering-key comparisons.  Useful for implementing SCYLLA_CLUSTERING_BOUND.
enum class comparison_order : char {
    cql, ///< CQL order. (a,b)>(1,1) is equivalent to a>1 OR (a=1 AND b>1).
    clustering, ///< Table's clustering order. (a,b)>(1,1) means any row past (1,1) in storage.
};
enum class null_handling_style {
    sql,           // evaluate(NULL = NULL) -> NULL, evaluate(NULL < x) -> NULL
    lwt_nulls,     // evaluate(NULL = NULL) -> TRUE, evaluate(NULL < x) -> exception
};
/// Operator restriction: LHS op RHS.
struct binary_operator {
    expression lhs;
    oper_t op;
    expression rhs;
    comparison_order order;
    null_handling_style null_handling = null_handling_style::sql;
};
/// A conjunction of restrictions.
struct conjunction {
    std::vector<expression> children;
};
// Gets resolved eventually into a column_value.
struct unresolved_identifier {
    ::shared_ptr<column_identifier_raw> ident;
};
// An attribute attached to a column mutation: writetime or ttl
struct column_mutation_attribute {
    enum class attribute_kind { writetime, ttl };
    attribute_kind kind;
    // note: only unresolved_identifier is legal here now. One day, when prepare()
    // on expressions yields expressions, column_value will also be legal here.
    expression column;
};
struct function_call {
    std::variant<functions::function_name, shared_ptr<db::functions::function>> func;
    std::vector<expression> args;
    // 0-based index of the function call within a CQL statement.
    // Used to populate the cache of execution results while passing to
    // another shard (handling `bounce_to_shard` messages) in LWT statements.
    //
    // The id is set only for the function calls that are a part of LWT
    // statement restrictions for the partition key. Otherwise, the id is not
    // set and the call is not considered when using or populating the cache.
    //
    // For example in a query like:
    // INSERT INTO t (pk) VALUES (uuid()) IF NOT EXISTS
    // The query should be executed on a shard that has the pk partition,
    // but it changes with each uuid() call.
    // uuid() call result is cached and sent to the proper shard.
    //
    // Cache id is kept in shared_ptr because of how prepare_context works.
    // During fill_prepare_context all function cache ids are collected
    // inside prepare_context.
    // Later when some condition occurs we might decide to clear
    // cache ids of all function calls found in prepare_context.
    // However by this time these function calls could have been
    // copied multiple times. Prepare_context keeps a shared_ptr
    // to function_call ids, and then clearing the shared id
    // clears it in all possible copies.
    // This logic was introduced back when everything was shared_ptr<term>,
    // now a better solution might exist.
    //
    // This field can be nullptr, it means that there is no cache id set.
    ::shared_ptr<std::optional<uint8_t>> lwt_cache_id;
};
struct cast {
    expression arg;
    std::variant<data_type, shared_ptr<cql3_type::raw>> type;
};
struct field_selection {
    expression structure;
    shared_ptr<column_identifier_raw> field;
    data_type type; // may be null before prepare
};
struct bind_variable {
    int32_t bind_index;
    // Describes where this bound value will be assigned.
    // Contains value type and other useful information.
    ::lw_shared_ptr<column_specification> receiver;
};
// A constant which does not yet have a date type. It is partially typed
// (we know if it's floating or int) but not sized.
struct untyped_constant {
    enum type_class { integer, floating_point, string, boolean, duration, uuid, hex, null };
    type_class partial_type;
    sstring raw_text;
};
// Represents a constant value with known value and type
// For null and unset the type can sometimes be set to empty_type
struct constant {
    cql3::raw_value value;
    // Never nullptr, for NULL and UNSET might be empty_type
    data_type type;
};
// Denotes construction of a tuple from its elements, e.g.  ('a', ?, some_column) in CQL.
struct tuple_constructor {
    std::vector<expression> elements;
    // Might be nullptr before prepare.
    // After prepare always holds a valid type, although it might be reversed_type(tuple_type).
    data_type type;
};
// Constructs a collection of same-typed elements
struct collection_constructor {
    enum class style_type { list, set, map };
    style_type style;
    std::vector<expression> elements;
    // Might be nullptr before prepare.
    // After prepare always holds a valid type, although it might be reversed_type(collection_type).
    data_type type;
};
// Constructs an object of a user-defined type
struct usertype_constructor {
    using elements_map_type = std::unordered_map<column_identifier, expression>;
    elements_map_type elements;
    // Might be nullptr before prepare.
    // After prepare always holds a valid type, although it might be reversed_type(user_type).
    data_type type;
};
// now that all expression types are fully defined, we can define expression::impl
struct expression::impl final {
    using variant_type = std::variant<
            conjunction, binary_operator, column_value, unresolved_identifier,
            column_mutation_attribute, function_call, cast, field_selection,
            bind_variable, untyped_constant, constant, tuple_constructor, collection_constructor,
            usertype_constructor, subscript>;
    variant_type v;
};
template <invocable_on_expression Visitor>
decltype(auto) visit(Visitor&& visitor, const expression& e) {
    return std::visit(std::forward<Visitor>(visitor), e._v->v);
}
template <invocable_on_expression_ref Visitor>
decltype(auto) visit(Visitor&& visitor, expression& e) {
    return std::visit(std::forward<Visitor>(visitor), e._v->v);
}
 ;
 ;
 ;
 ;
/// Creates a conjunction of a and b.  If either a or b is itself a conjunction, its children are inserted
/// directly into the resulting conjunction's children, flattening the expression tree.
// Input data needed to evaluate an expression. Individual members can be
// null if not applicable (e.g. evaluating outside a row context)
struct evaluation_inputs {
    const std::vector<bytes>* partition_key = nullptr;
    const std::vector<bytes>* clustering_key = nullptr;
    const std::vector<managed_bytes_opt>* static_and_regular_columns = nullptr; // indexes match `selection` member
    const cql3::selection::selection* selection = nullptr;
    const query_options* options = nullptr;
};
/// Helper for generating evaluation_inputs::static_and_regular_columns
/// Helper for accessing a column value from evaluation_inputs
/// True iff restr evaluates to true, given these inputs
/// A set of discrete values.
using value_list = std::vector<managed_bytes>; // Sorted and deduped using value comparator.
/// General set of values.  Empty set and single-element sets are always value_list.  nonwrapping_range is
/// never singular and never has start > end.  Universal set is a nonwrapping_range with both bounds null.
using value_set = std::variant<value_list, nonwrapping_range<managed_bytes>>;
/// A set of all column values that would satisfy an expression. The _token_values variant finds
/// matching values for the partition token function call instead of the column.
///
/// An expression restricts possible values of a column or token:
/// - `A>5` restricts A from below
/// - `A>5 AND A>6 AND B<10 AND A=12 AND B>0` restricts A to 12 and B to between 0 and 10
/// - `A IN (1, 3, 5)` restricts A to 1, 3, or 5
/// - `A IN (1, 3, 5) AND A>3` restricts A to just 5
/// - `A=1 AND A<=0` restricts A to an empty list; no value is able to satisfy the expression
/// - `A>=NULL` also restricts A to an empty list; all comparisons to NULL are false
/// - an expression without A "restricts" A to unbounded range
/// Turns value_set into a range, unless it's a multi-valued list (in which case this throws).
/// A range of all X such that X op val.
/// True iff the index can support the entire expression.
/// True iff any of the indices from the manager can support the entire expression.  If allow_local, use all
/// indices; otherwise, use only global indices.
// Looks at each column indivudually and checks whether some index can support restrictions on this single column.
// Expression has to consist only of single column restrictions.
extern std::ostream& operator<<(std::ostream&, const expression::printer&);
// Looks into the expression and finds the given expression variant
// for which the predicate function returns true.
// If nothing is found returns nullptr.
// For example:
// find_in_expression<binary_operator>(e, [](const binary_operator&) {return true;})
// Will return the first binary operator found in the expression
template<ExpressionElement ExprElem, class Fn>
requires std::invocable<Fn, const ExprElem&>
      && std::same_as<std::invoke_result_t<Fn, const ExprElem&>, bool>
const ExprElem* find_in_expression(const expression& e, Fn predicate_fun) {
    const ExprElem* ret = nullptr;
    recurse_until(e, [&] (const expression& e) {
        if (auto expr_elem = as_if<ExprElem>(&e)) {
            if (predicate_fun(*expr_elem)) {
                ret = expr_elem;
                return true;
            }
        }
        return false;
    });
    return ret;
}
/// If there is a binary_operator atom b for which f(b) is true, returns it.  Otherwise returns null.
template<class Fn>
requires std::invocable<Fn, const binary_operator&>
      && std::same_as<std::invoke_result_t<Fn, const binary_operator&>, bool>
const binary_operator* find_binop(const expression& e, Fn predicate_fun) {
    return find_in_expression<binary_operator>(e, predicate_fun);
}
// Goes over each expression of the specified type and calls for_each_func for each of them.
// For example:
// for_each_expression<column_vaue>(e, [](const column_value& cval) {std::cout << cval << '\n';});
// Will print all column values in an expression
template<ExpressionElement ExprElem, class Fn>
requires std::invocable<Fn, const ExprElem&>
void for_each_expression(const expression& e, Fn for_each_func) {
    recurse_until(e, [&] (const expression& cur_expr) -> bool {
        if (auto expr_elem = as_if<ExprElem>(&cur_expr)) {
            for_each_func(*expr_elem);
        }
        return false;
    });
}
/// Counts binary_operator atoms b for which f(b) is true.
size_t count_if(const expression& e, const noncopyable_function<bool (const binary_operator&)>& f);
 bool has_slice(const expression& e) ;
 bool is_compare(oper_t op) ;
// Check whether the given expression represents
// a call to the token() function.
/// Check whether the expression contains a binary_operator whose LHS is a call to the token
/// function representing a partition key token.
/// Examples:
/// For expression: "token(p1, p2, p3) < 123 AND c = 2" returns true
/// For expression: "p1 = token(1, 2, 3) AND c = 2" return false
/// Given a Boolean expression, compute its factors such as e=f1 AND f2 AND f3 ...
/// If the expression is TRUE, may return no factors (happens today for an
/// empty conjunction).
/// Run the given function for each element in the top level conjunction.
/// True iff binary_operator involves a collection.
// Checks whether the given column occurs in the expression.
// Uses column_defintion::operator== for comparison, columns with the same name but different schema will not be equal.
// Checks whether this expression contains a nonpure function.
// The expression must be prepared, so that function names are converted to function pointers.
// Checks whether the given column has an EQ restriction in the expression.
// EQ restriction is `col = ...` or `(col, col2) = ...`
// IN restriction is NOT an EQ restriction, this function will not look for IN restrictions.
// Uses column_defintion::operator== for comparison, columns with the same name but different schema will not be equal.
/// Replaces every column_definition in an expression with this one.  Throws if any LHS is not a single
/// column_value.
// Replaces all occurences of token(p1, p2) on the left hand side with the given colum.
// For example this changes token(p1, p2) < token(1, 2) to my_column_name < token(1, 2).
// Schema is needed to find out which calls to token() describe the partition token.
// Recursively copies e and returns it. Calls replace_candidate() on all nodes. If it returns nullopt,
// continue with the copying. If it returns an expression, that expression replaces the current node.
// Adjust an expression for rows that were fetched using query::partition_slice::options::collections_as_maps
// Prepares a binary operator received from the parser.
// Does some basic type checks but no advanced validation.
// Pre-compile any constant LIKE patterns and return equivalent expression
// Test all elements of exprs for assignment. If all are exact match, return exact match. If any is not assignable,
// return not assignable. Otherwise, return weakly assignable.
// Extracts all binary operators which have the given column on their left hand side.
// Extracts only single-column restrictions.
// Does not include multi-column restrictions.
// Does not include token() restrictions.
// Does not include boolean constant restrictions.
// For example "WHERE c = 1 AND (a, c) = (2, 1) AND token(p) < 2 AND FALSE" will return {"c = 1"}.
// Takes a prepared expression and calculates its value.
// Evaluates bound values, calls functions and returns just the bytes and type.
std::vector<std::pair<managed_bytes, managed_bytes>> get_map_elements(const cql3::raw_value&);
// Gets the elements of a constant which can be a list, set, tuple or user type
// Get elements of list<tuple<>> as vector<vector<managed_bytes_opt>
// It is useful with IN restrictions like (a, b) IN [(1, 2), (3, 4)].
// `type` parameter refers to the list<tuple<>> type.
utils::chunked_vector<std::vector<managed_bytes_opt>> get_list_of_tuples_elements(const cql3::raw_value&, const abstract_type& type);
// Retrieves information needed in prepare_context.
// Collects the column specification for the bind variables in this expression.
// Sets lwt_cache_id field in function_calls.
// Checks whether there is a bind_variable inside this expression
// It's important to note, that even when there are no bind markers,
// there can be other things that prevent immediate evaluation of an expression.
// For example an expression can contain calls to nonpure functions.
// Checks whether this expression contains restrictions on one single column.
// There might be more than one restriction, but exactly one column.
// The expression must be prepared.
// Gets the only column from a single_column_restriction expression.
// A comparator that orders columns by their position in the schema
// For primary key columns the `id` field is used to determine their position.
// Other columns are assumed to have position std::numeric_limits<uint32_t>::max().
// In case the position is the same they are compared by their name.
// This comparator has been used in the original restricitons code to keep
// restrictions for each column sorted by their place in the schema.
// It's not recommended to use this comparator with columns of different kind
// (partition/clustering/nonprimary) because the id field is unique
// for (kind, schema). So a partition and clustering column might
// have the same id within one schema.
struct schema_pos_column_definition_comparator {
};
// Extracts column_defs from the expression and sorts them using schema_pos_column_definition_comparator.
// Extracts column_defs and returns the last one according to schema_pos_column_definition_comparator.
// A map of single column restrictions for each column
using single_column_restrictions_map = std::map<const column_definition*, expression, schema_pos_column_definition_comparator>;
// Extracts map of single column restrictions for each column from expression
// Checks whether this expression is empty - doesn't restrict anything
// Finds common columns between both expressions and prints them to a string.
// Uses schema_pos_column_definition_comparator for comparison.
// Finds the value of the given column in the expression
// In case of multpiple possible values calls on_internal_error
} // namespace expr
} // namespace cql3
/// Custom formatter for an expression. Use {:user} for user-oriented
/// output, {:debug} for debug-oriented output. Debug is the default.
///
/// Required for fmt::join() to work on expression.
template <>
class fmt::formatter<cql3::expr::expression> {
    bool _debug = true;
private:
    constexpr static bool try_match_and_advance(format_parse_context& ctx, std::string_view s) {
        auto [ctx_end, s_end] = std::ranges::mismatch(ctx, s);
        if (s_end == s.end()) {
            ctx.advance_to(ctx_end);
            return true;
        }
        return false;
    }
public:
    constexpr auto parse(format_parse_context& ctx) {
        using namespace std::string_view_literals;
        if (try_match_and_advance(ctx, "debug"sv)) {
            _debug = true;
        } else if (try_match_and_advance(ctx, "user"sv)) {
            _debug = false;
        }
        return ctx.begin();
    }
    template <typename FormatContext>
    auto format(const cql3::expr::expression& expr, FormatContext& ctx) const {
        std::ostringstream os;
        os << cql3::expr::expression::printer{.expr_to_print = expr, .debug_mode = _debug};
        return fmt::format_to(ctx.out(), "{}", os.str());
    }
};
/// Required for fmt::join() to work on expression::printer.
template <>
struct fmt::formatter<cql3::expr::expression::printer> {
    constexpr auto parse(format_parse_context& ctx) {
        return ctx.end();
    }
    template <typename FormatContext>
    auto format(const cql3::expr::expression::printer& pr, FormatContext& ctx) const {
        std::ostringstream os;
        os << pr;
        return fmt::format_to(ctx.out(), "{}", os.str());
    }
};
/// Required for fmt::join() to work on ExpressionElement, and for {:user}/{:debug} to work on ExpressionElement.
template <cql3::expr::ExpressionElement E>
struct fmt::formatter<E> : public fmt::formatter<cql3::expr::expression> {
};
namespace cql3 {
class query_options;
}
namespace cql3::expr {
// Some expression users can behave differently if the expression is a bind variable
// and if that bind variable is unset. unset_bind_variable_guard encapsulates the two
// conditions.
class unset_bind_variable_guard {
    // Disengaged if the operand is not exactly a single bind variable.
    std::optional<bind_variable> _var;
public:
    
};
}
namespace cql3 {
namespace statements::broadcast_tables {
    struct prepared_update;
}
class update_parameters;
class operation {
public:
    // the column the operation applies to
    // We can hold a reference because all operations have life bound to their statements and
    // statements pin the schema.
    const column_definition& column;
protected:
    // Value involved in the operation. In theory this should not be here since some operation
    // may require none of more than one expression, but most need 1 so it simplify things a bit.
    std::optional<expr::expression> _e;
    // A guard to check if the operation should be skipped due to unset operand.
    expr::unset_bind_variable_guard _unset_guard;
public:
    
    
    
    
    
    class raw_update {
    public:
        
    };
    class raw_deletion {
    public:
        virtual ~raw_deletion() = default;
    };
    class set_value;
    class set_counter_value_from_tuple_list;
    class set_element : public raw_update {
        const expr::expression _selector;
        const expr::expression _value;
        const bool _by_uuid;
    private:
    public:
    };
    // Set a single field inside a user-defined type.
    class set_field : public raw_update {
        const shared_ptr<column_identifier> _field;
        const expr::expression _value;
    private:
    public:
    };
    // Delete a single field inside a user-defined type.
    // Equivalent to setting the field to null.
    class field_deletion : public raw_deletion {
        const shared_ptr<column_identifier::raw> _id;
        const shared_ptr<column_identifier> _field;
    public:
    };
    class addition : public raw_update {
        const expr::expression _value;
    private:
    public:
    };
    class subtraction : public raw_update {
        const expr::expression _value;
    private:
    public:
    };
    class prepend : public raw_update {
        expr::expression _value;
    private:
    public:
    };
    class column_deletion;
    class element_deletion : public raw_deletion {
        shared_ptr<column_identifier::raw> _id;
        expr::expression _element;
    public:
    };
};
class operation_skip_if_unset : public operation {
public:
};
class operation_no_unset_support : public operation {
public:
};
}
namespace cql3 {
class lists {
public:
public:
    class setter : public operation_skip_if_unset {
    public:
    };
    class setter_by_index : public operation_skip_if_unset {
    protected:
        expr::expression _idx;
    public:
        
    };
    class setter_by_uuid : public setter_by_index {
    public:
    };
    class appender : public operation_skip_if_unset {
    public:
        using operation_skip_if_unset::operation_skip_if_unset;
    };
    class prepender : public operation_skip_if_unset {
    public:
        using operation_skip_if_unset::operation_skip_if_unset;
    };
    class discarder : public operation_skip_if_unset {
    public:
    };
    class discarder_by_index : public operation_skip_if_unset {
    public:
    };
};
}
// A variant type that can hold either an atomic_cell, or a serialized collection.
// Which type is stored is determined by the schema.
// Has an "empty" state.
// Objects moved-from are left in an empty state.
class atomic_cell_or_collection final {
    managed_bytes _data;
private:
public:
    atomic_cell_or_collection(atomic_cell ac) : _data(std::move(ac._data)) {}
    
    
    atomic_cell_view as_atomic_cell(const column_definition& cdef) const { return atomic_cell_view::from_bytes(*cdef.type, _data); }
    atomic_cell_mutable_view as_mutable_atomic_cell(const column_definition& cdef) { return atomic_cell_mutable_view::from_bytes(*cdef.type, _data); }
    atomic_cell_or_collection(collection_mutation cm) : _data(std::move(cm._data)) { }
    atomic_cell_or_collection copy(const abstract_type&) const;
    collection_mutation_view as_collection_mutation() const;
    bool equals(const abstract_type& type, const atomic_cell_or_collection& other) const;
    size_t external_memory_usage(const abstract_type&) const;
    class printer {
        const column_definition& _cdef;
        const atomic_cell_or_collection& _cell;
    public:
        printer(const column_definition& cdef, const atomic_cell_or_collection& cell)  ;
        
        
        
    };
    
};
// Not part of atomic_cell.hh to avoid cyclic dependency between types.hh and atomic_cell.hh
template<>
struct appending_hash<collection_mutation_view> {
    template<typename Hasher>
    void operator()(Hasher& h, collection_mutation_view cell, const column_definition& cdef) const ;
};
template<>
struct appending_hash<atomic_cell_view> {
    template<typename Hasher>
    void operator()(Hasher& h, atomic_cell_view cell, const column_definition& cdef) const ;
};
template<>
struct appending_hash<atomic_cell> {
    template<typename Hasher>
    void operator()(Hasher& h, const atomic_cell& cell, const column_definition& cdef) const ;
};
template<>
struct appending_hash<collection_mutation> {
    template<typename Hasher>
    void operator()(Hasher& h, const collection_mutation& cm, const column_definition& cdef) const ;
};
template<>
struct appending_hash<atomic_cell_or_collection> {
    template<typename Hasher>
    void operator()(Hasher& h, const atomic_cell_or_collection& c, const column_definition& cdef) const ;
};
// Calculates a hash of a mutation_partition which is consistent with
// mutation equality. For any equal mutations, no matter which schema
// version they were generated under, the hash fed will be the same for both of them.
template<typename Hasher>
class hashing_partition_visitor : public mutation_partition_visitor {
    Hasher& _h;
    const schema& _s;
public:
    
    
    virtual void accept_row_cell(column_id id, collection_mutation_view cell) override ;
};
namespace bi = boost::intrusive;
class range_tombstone final {
public:
    clustering_key_prefix start;
    bound_kind start_kind;
    clustering_key_prefix end;
    bound_kind end_kind;
    tombstone tomb;
    range_tombstone(clustering_key_prefix start, bound_kind start_kind, clustering_key_prefix end, bound_kind end_kind, tombstone tomb)
            : start(std::move(start))
            , start_kind(start_kind)
            , end(std::move(end))
            , end_kind(end_kind)
            , tomb(std::move(tomb))
    { }
    range_tombstone(bound_view start, bound_view end, tombstone tomb)
            : range_tombstone(start.prefix(), start.kind(), end.prefix(), end.kind(), std::move(tomb))
    { }
    // Can be called only when both start and end are !is_static_row && !is_clustering_row().
    range_tombstone(position_in_partition_view start, position_in_partition_view end, tombstone tomb)
            : range_tombstone(start.as_start_bound_view(), end.as_end_bound_view(), tomb)
    {}
    range_tombstone(clustering_key_prefix&& start, clustering_key_prefix&& end, tombstone tomb)
            : range_tombstone(std::move(start), bound_kind::incl_start, std::move(end), bound_kind::incl_end, std::move(tomb))
    { }
    // IDL constructor
    const bound_view start_bound() const {
        return bound_view(start, start_kind);
    }
    const bound_view end_bound() const {
        return bound_view(end, end_kind);
    }
    // Range tombstone covers all rows with positions p such that: position() <= p < end_position()
    position_in_partition_view position() const;
    position_in_partition_view end_position() const;
    bool empty() const noexcept ;
    explicit operator bool() const noexcept ;
    bool equal(const schema& s, const range_tombstone& other) const {
        return tomb == other.tomb && start_bound().equal(s, other.start_bound()) && end_bound().equal(s, other.end_bound());
    }
    struct compare {
        bound_view::compare _c;
        compare(const schema& s) : _c(s) {}
        bool operator()(const range_tombstone& rt1, const range_tombstone& rt2) const {
            return _c(rt1.start_bound(), rt2.start_bound());
        }
    };
    friend void swap(range_tombstone& rt1, range_tombstone& rt2) noexcept ;
    static bool is_single_clustering_row_tombstone(const schema& s, const clustering_key_prefix& start,
        bound_kind start_kind, const clustering_key_prefix& end, bound_kind end_kind)
    {
        return start.is_full(s) && start_kind == bound_kind::incl_start
            && end_kind == bound_kind::incl_end && start.equal(s, end);
    }
    // Applies src to this. The tombstones may be overlapping.
    // If the tombstone with larger timestamp has the smaller range the remainder
    // is returned, it guaranteed not to overlap with this.
    // The start bounds of this and src are required to be equal. The start bound
    // of this is not changed. The start bound of the remainder (if there is any)
    // is larger than the end bound of this.
    std::optional<range_tombstone> apply(const schema& s, range_tombstone&& src);
    // Intersects the range of this tombstone with [pos, +inf) and replaces
    // the range of the tombstone if there is an overlap.
    // Returns true if there is an overlap. When returns false, the tombstone
    // is not modified.
    //
    // pos must satisfy:
    //   1) before_all_clustered_rows() <= pos
    //   2) !pos.is_clustering_row() - because range_tombstone bounds can't represent such positions
    
    // Intersects the range of this tombstone with [start, end) and replaces
    // the range of the tombstone if there is an overlap.
    // Returns true if there is an overlap and false otherwise. When returns false, the tombstone
    // is not modified.
    //
    // start and end must satisfy:
    //   1) has_clustering_key() == true
    //   2) is_clustering_row() == false
    //
    // Also: start <= end
    
    // Assumes !pos.is_clustering_row(), because range_tombstone bounds can't represent such positions
    
    // Assumes !pos.is_clustering_row(), because range_tombstone bounds can't represent such positions
    
    // Swap bounds to reverse range-tombstone -- as if it came from a table with
    // reverse native order. See docs/dev/reverse-reads.md.
    void reverse() ;
    
    
    size_t memory_usage(const schema& s) const noexcept ;
    size_t minimal_memory_usage(const schema& s) const noexcept ;
};
template<>
struct appending_hash<range_tombstone>  {
     ;
};
// The accumulator expects the incoming range tombstones and clustered rows to
// follow the ordering used by the mutation readers.
//
// Unless the accumulator is in the reverse mode, after apply(rt) or
// tombstone_for_row(ck) are called there are followng restrictions for
// subsequent calls:
//  - apply(rt1) can be invoked only if rt.start_bound() < rt1.start_bound()
//    and ck < rt1.start_bound()
//  - tombstone_for_row(ck1) can be invoked only if rt.start_bound() < ck1
//    and ck < ck1
//
// In other words position in partition of the mutation fragments passed to the
// accumulator must be increasing.
//
// If the accumulator was created with the reversed flag set it expects the
// stream of the range tombstone to come from a reverse partitions and follow
// the ordering that they use. In particular, the restrictions from non-reversed
// mode change to:
//  - apply(rt1) can be invoked only if rt.end_bound() > rt1.end_bound() and
//    ck > rt1.end_bound()
//  - tombstone_for_row(ck1) can be invoked only if rt.end_bound() > ck1 and
//    ck > ck1.
class range_tombstone_accumulator {
    bound_view::compare _cmp;
    tombstone _partition_tombstone;
    std::deque<range_tombstone> _range_tombstones;
    tombstone _current_tombstone;
private:
public:
    void set_partition_tombstone(tombstone t) ;
};
template<>
struct fmt::formatter<range_tombstone> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const range_tombstone& rt, FormatContext& ctx) const {
        if (rt) {
            return fmt::format_to(ctx.out(), "{{range_tombstone: start={}, end={}, {}}}",
                                  rt.position(), rt.end_position(), rt.tomb);
        } else {
            return fmt::format_to(ctx.out(), "{{range_tombstone: none}}");
        }
    }
};
class is_preemptible_tag;
using is_preemptible = bool_class<is_preemptible_tag>;
/// A function which decides when to preempt.
/// If it returns true then the algorithm will be interrupted.
using preemption_check = noncopyable_function<bool() noexcept>;
class position_in_partition_view;
class range_tombstone_entry {
    range_tombstone _tombstone;
    bi::set_member_hook<bi::link_mode<bi::auto_unlink>> _link;
public:
    struct compare {
        range_tombstone::compare _c;
        compare(const schema& s) : _c(s) {}
        bool operator()(const range_tombstone_entry& rt1, const range_tombstone_entry& rt2) const {
            return _c(rt1._tombstone, rt2._tombstone);
        }
    };
    using container_type = bi::set<range_tombstone_entry,
            bi::member_hook<range_tombstone_entry, bi::set_member_hook<bi::link_mode<bi::auto_unlink>>, &range_tombstone_entry::_link>,
            bi::compare<range_tombstone_entry::compare>,
            bi::constant_time_size<false>>;
    range_tombstone_entry(const range_tombstone_entry& rt)
        : _tombstone(rt._tombstone)
    {
    }
    range_tombstone_entry(range_tombstone_entry&& rt) noexcept
            : _tombstone(std::move(rt._tombstone))
    {
        update_node(rt._link);
    }
    range_tombstone_entry(range_tombstone&& rt) noexcept
        : _tombstone(std::move(rt))
    { }
    range_tombstone_entry& operator=(range_tombstone_entry&& rt) noexcept {
        update_node(rt._link);
        _tombstone = std::move(rt._tombstone);
        return *this;
    }
    range_tombstone& tombstone() noexcept { return _tombstone; }
    const range_tombstone& tombstone() const noexcept { return _tombstone; }
    const bound_view start_bound() const ;
    const bound_view end_bound() const { return _tombstone.end_bound(); }
    position_in_partition_view position() const { return _tombstone.position(); }
    position_in_partition_view end_position() const { return _tombstone.end_position(); }
    size_t memory_usage(const schema& s) const noexcept ;
private:
    void update_node(bi::set_member_hook<bi::link_mode<bi::auto_unlink>>& other_link) noexcept {
        if (other_link.is_linked()) {
            // Move the link in case we're being relocated by LSA.
            container_type::node_algorithms::replace_node(other_link.this_ptr(), _link.this_ptr());
            container_type::node_algorithms::init(other_link.this_ptr());
        }
    }
};
template <>
struct fmt::formatter<range_tombstone_entry> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const range_tombstone_entry& rt, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", rt.tombstone());
    }
};
class range_tombstone_list final {
    using range_tombstones_type = range_tombstone_entry::container_type;
    class insert_undo_op {
        const range_tombstone_entry& _new_rt;
    public:
        insert_undo_op(const range_tombstone_entry& new_rt)
                : _new_rt(new_rt) { }
        void undo(const schema& s, range_tombstone_list& rt_list) noexcept;
    };
    class erase_undo_op {
        alloc_strategy_unique_ptr<range_tombstone_entry> _rt;
    public:
        erase_undo_op(range_tombstone_entry& rt)
                : _rt(&rt) { }
        void undo(const schema& s, range_tombstone_list& rt_list) noexcept;
    };
    class update_undo_op {
        range_tombstone _old_rt;
        const range_tombstone_entry& _new_rt;
    public:
        update_undo_op(range_tombstone&& old_rt, const range_tombstone_entry& new_rt)
                : _old_rt(std::move(old_rt)), _new_rt(new_rt) { }
        void undo(const schema& s, range_tombstone_list& rt_list) noexcept;
    };
    class reverter {
    private:
        using op = std::variant<erase_undo_op, insert_undo_op, update_undo_op>;
        utils::chunked_vector<op> _ops;
        const schema& _s;
    protected:
        range_tombstone_list& _dst;
    public:
        reverter(const schema& s, range_tombstone_list& dst)
                : _s(s)
                , _dst(dst) { }
        
        
        
        
        virtual range_tombstones_type::iterator insert(range_tombstones_type::iterator it, range_tombstone_entry& new_rt);
        virtual range_tombstones_type::iterator erase(range_tombstones_type::iterator it);
        virtual void update(range_tombstones_type::iterator it, range_tombstone&& new_rt);
        void revert() noexcept;
        void cancel() noexcept {
            _ops.clear();
        }
    };
    class nop_reverter : public reverter {
    public:
        nop_reverter(const schema& s, range_tombstone_list& rt_list)
                : reverter(s, rt_list) { }
        virtual range_tombstones_type::iterator insert(range_tombstones_type::iterator it, range_tombstone_entry& new_rt) override;
        virtual range_tombstones_type::iterator erase(range_tombstones_type::iterator it) override;
        virtual void update(range_tombstones_type::iterator it, range_tombstone&& new_rt) override;
    };
private:
    range_tombstones_type _tombstones;
public:
    // ForwardIterator<range_tombstone>
    using iterator = range_tombstones_type::iterator;
    using reverse_iterator = range_tombstones_type::reverse_iterator;
    using const_iterator = range_tombstones_type::const_iterator;
    struct copy_comparator_only { };
    range_tombstone_list(const schema& s)
        : _tombstones(range_tombstone_entry::compare(s))
    { }
    range_tombstone_list(const range_tombstone_list& x, copy_comparator_only)
        : _tombstones(x._tombstones.key_comp())
    { }
    range_tombstone_list(const range_tombstone_list&);
    
    
    range_tombstone_list& operator=(range_tombstone_list&&) = default;
    ~range_tombstone_list();
    size_t size() const noexcept ;
    bool empty() const noexcept ;
    auto begin() noexcept {
        return _tombstones.begin();
    }
    auto begin() const noexcept {
        return _tombstones.begin();
    }
    auto rbegin() noexcept ;
    auto rbegin() const noexcept ;
    auto end() noexcept {
        return _tombstones.end();
    }
    auto end() const noexcept {
        return _tombstones.end();
    }
    auto rend() noexcept ;
    
    
    void apply(const schema& s, const range_tombstone& rt) ;
    void apply(const schema& s, range_tombstone&& rt) {
        apply(s, std::move(rt.start), rt.start_kind, std::move(rt.end), rt.end_kind, std::move(rt.tomb));
    }
    void apply(const schema& s, clustering_key_prefix start, bound_kind start_kind,
               clustering_key_prefix end, bound_kind end_kind, tombstone tomb) {
        nop_reverter rev(s, *this);
        apply_reversibly(s, std::move(start), start_kind, std::move(end), end_kind, std::move(tomb), rev);
    }
    // Monotonic exception guarantees. In case of failure the object will contain at least as much information as before the call.
    void apply_monotonically(const schema& s, const range_tombstone& rt);
    // Merges another list with this object.
    // Monotonic exception guarantees. In case of failure the object will contain at least as much information as before the call.
    /// Merges another list with this object.
    /// The other list must be governed by the same allocator as this object.
    ///
    /// Monotonic exception guarantees. In case of failure the object will contain at least as much information as before the call.
    /// The other list will be left in a state such that it would still commute with this object to the same state as it
    /// would if the call didn't fail.
public:
    using iterator_range = boost::iterator_range<const_iterator>;
    // Returns range tombstones which overlap with given range
    // Returns range tombstones which overlap with [start, end)
    // Returns range tombstones with ends inside [start, before).
    // Returns range tombstones with starts inside (after, end].
    // Pops the first element and bans (in theory) further additions
    // The list is assumed not to be empty
    // Ensures that every range tombstone is strictly contained within given clustering ranges.
    // Preserves all information which may be relevant for rows from that ranges.
    void trim(const schema& s, const query::clustering_row_ranges&);
    range_tombstone_list difference(const schema& s, const range_tombstone_list& rt_list) const;
    // Erases the range tombstones for which filter returns true.
    template <typename Pred>
    requires std::is_invocable_r_v<bool, Pred, const range_tombstone&>
    void erase_where(Pred filter) {
        auto it = begin();
        while (it != end()) {
            if (filter(it->tombstone())) {
                it = _tombstones.erase_and_dispose(it, current_deleter<range_tombstone_entry>());
            } else {
                ++it;
            }
        }
    }
    void clear() noexcept {
        _tombstones.clear_and_dispose(current_deleter<range_tombstone_entry>());
    }
    range_tombstone pop(iterator it) {
        range_tombstone rt(std::move(it->tombstone()));
        _tombstones.erase_and_dispose(it, current_deleter<range_tombstone_entry>());
        return rt;
    }
    // Removes elements of this list in batches.
    // Returns stop_iteration::yes iff there is no more elements to remove.
    stop_iteration clear_gently() noexcept;
    void apply(const schema& s, const range_tombstone_list& rt_list);
    // See reversibly_mergeable.hh
    reverter apply_reversibly(const schema& s, range_tombstone_list& rt_list);
    bool equal(const schema&, const range_tombstone_list&) const;
    size_t external_memory_usage(const schema& s) const noexcept ;
private:
    void apply_reversibly(const schema& s, clustering_key_prefix start, bound_kind start_kind,
                          clustering_key_prefix end, bound_kind end_kind, tombstone tomb, reverter& rev);
    void insert_from(const schema& s,
                     range_tombstones_type::iterator it,
                     position_in_partition start,
                     position_in_partition end,
                     tombstone tomb,
                     reverter& rev);
    range_tombstones_type::iterator find(const schema& s, const range_tombstone_entry& rt);
};
template <>
struct fmt::formatter<range_tombstone_list> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const range_tombstone_list& list, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{{{}}}", fmt::join(list, ", "));
    }
};
template <typename Func, typename T>
concept Disposer = requires (Func f, T* val) { 
    { f(val) } noexcept -> std::same_as<void>;
};
template <typename Key1, typename Key2, typename Less>
concept LessComparable = requires (const Key1& a, const Key2& b, Less less) {
    { less(a, b) } -> std::same_as<bool>;
    { less(b, a) } -> std::same_as<bool>;
};
template <typename Key1, typename Key2, typename Less>
concept LessNothrowComparable = LessComparable<Key1, Key2, Less> && std::is_nothrow_invocable_v<Less, Key1, Key2>;
template <typename T1, typename T2, typename Compare>
concept Comparable = requires (const T1& a, const T2& b, Compare cmp) {
    // The Comparable is trichotomic comparator that should return 
    //   negative value when a < b
    //   zero when a == b
    //   positive value when a > b
    { cmp(a, b) } -> std::same_as<std::strong_ordering>;
};
namespace utils {
template <bool Debug>
struct neat_id {
    unsigned int operator()() const noexcept ;
};
template <>
struct neat_id<true> {
    unsigned int _id;
    static unsigned int _next() noexcept ;
    
    
};
} // namespace
namespace intrusive_b {
template <typename Func, typename T>
concept KeyCloner = requires (Func f, T* val) {
    { f(val) } -> std::same_as<T*>;
};
template <typename Pointer, typename T>
concept KeyPointer = std::is_nothrow_move_constructible_v<Pointer> &&
    requires (Pointer p) { { *p } -> std::same_as<T&>; } &&
    requires (Pointer p) { { p.release() } noexcept -> std::same_as<T*>; };
enum class with_debug { no, yes };
enum class key_search { linear, binary, both };
class member_hook;
// The LinearThreshold is explained below, see NODE_LINEAR flag
template <typename Key, member_hook Key::* Hook, typename Compare, size_t NodeSize, size_t LinearThreshold, key_search Search, with_debug Debug> class node;
template <typename Key, member_hook Key::*, typename Compare, size_t NodeSize, size_t LinearThreshold> class validator;
// For .{do_something_with_data}_and_dispose methods below
template <typename T>
void default_dispose(T* value) noexcept ;
using key_index = size_t;
using kid_index = size_t;
class node_base {
    template <typename K, member_hook K::* H, typename C, size_t NS, size_t LT, key_search KS, with_debug D> friend class node;
    node_base(unsigned short n, unsigned short cap, unsigned short f) noexcept : num_keys(n), flags(f), capacity(cap) {}
public:
    unsigned short num_keys;
    unsigned short flags;
    unsigned short capacity; // used by linear node only
    member_hook* keys[0];
    static constexpr unsigned short NODE_ROOT = 0x1;
    static constexpr unsigned short NODE_LEAF = 0x2;
    static constexpr unsigned short NODE_LEFTMOST = 0x4; // leaf with smallest keys in the tree
    static constexpr unsigned short NODE_RIGHTMOST = 0x8; // leaf with greatest keys in the tree
    static constexpr unsigned short NODE_LINEAR = 0x10;
    static constexpr unsigned short NODE_INLINE = 0x20;
    struct inline_tag{};
    node_base(inline_tag) noexcept : num_keys(0), flags(NODE_ROOT | NODE_LEAF | NODE_INLINE), capacity(1) {}
    bool is_root() const noexcept { return flags & NODE_ROOT; }
    bool is_leaf() const noexcept { return flags & NODE_LEAF; }
    bool is_leftmost() const noexcept { return flags & NODE_LEFTMOST; }
    bool is_rightmost() const noexcept { return flags & NODE_RIGHTMOST; }
    bool is_linear() const noexcept { return flags & NODE_LINEAR; }
    bool is_inline() const noexcept { return flags & NODE_INLINE; }
    node_base(const node_base&) = delete;
    node_base(node_base&&) = delete;
    key_index index_for(const member_hook* hook) const noexcept {
        for (key_index i = 0; i < num_keys; i++) {
            if (keys[i] == hook) {
                return i;
            }
        }
        std::abort();
    }
    bool empty() const noexcept { return num_keys == 0; }
private:
    friend class member_hook;
    void reattach(member_hook* to, member_hook* from) noexcept {
        key_index idx = index_for(from);
        keys[idx] = to;
    }
};
class member_hook {
    template <typename K, member_hook K::* H, typename C, size_t NS, size_t LT> friend class validator;
    template <typename K, member_hook K::* H, typename C, size_t NS, size_t LT, key_search KS, with_debug D> friend class node;
private:
    node_base* _node = nullptr;
public:
    bool attached() const noexcept { return _node != nullptr; }
    node_base* node() const noexcept { return _node; }
    void attach_first(node_base& to) noexcept {
        assert(to.num_keys == 0);
        to.num_keys = 1;
        to.keys[0] = this;
        _node = &to;
    }
    member_hook() noexcept = default;
    member_hook(const member_hook&) = delete;
    ~member_hook() {
        assert(!attached());
    }
    member_hook(member_hook&& other) noexcept : _node(other._node) {
        if (attached()) {
            _node->reattach(this, &other);
            other._node = nullptr;
        }
    }
    template <typename K, member_hook K::* Hook>
    const K* to_key() const noexcept ;
    template <typename K, member_hook K::* Hook>
    K* to_key() noexcept {
        return boost::intrusive::get_parent_from_member(this, Hook);
    }
};
struct stats {
    unsigned long nodes;
    std::vector<unsigned long> nodes_filled;
    unsigned long leaves;
    std::vector<unsigned long> leaves_filled;
    unsigned long linear_keys;
};
template <typename Key, member_hook Key::* Hook, typename Compare, size_t NodeSize, size_t LinearThreshold, key_search Search, with_debug Debug = with_debug::no>
requires Comparable<Key, Key, Compare>
class tree {
    // Sanity not to allow slow key-search in non-debug mode
    static_assert(Debug == with_debug::yes || Search != key_search::both);
public:
    friend class node<Key, Hook, Compare, NodeSize, LinearThreshold, Search, Debug>;
    friend class validator<Key, Hook, Compare, NodeSize, LinearThreshold>;
    using node = class node<Key, Hook, Compare, NodeSize, LinearThreshold, Search, Debug>;
    class iterator;
    class const_iterator;
private:
    node* _root = nullptr;
    struct corners {
        node* left;
        node* right;
        corners() noexcept : left(nullptr), right(nullptr) {}
    };
    union {
        corners _corners;
        node_base _inline;
        static_assert(sizeof(corners) >= sizeof(node_base) + sizeof(member_hook*));
    };
    static const tree* from_inline(const node_base* n) noexcept {
        assert(n->is_inline());
        return boost::intrusive::get_parent_from_member(n, &tree::_inline);
    }
    static tree* from_inline(node_base* n) noexcept {
        assert(n->is_inline());
        return boost::intrusive::get_parent_from_member(n, &tree::_inline);
    }
    struct cursor {
        node* n;
        kid_index idx;
        void descend() noexcept {
            n = n->_kids[idx];
            __builtin_prefetch(n);
        }
        template <typename Pointer>
        iterator insert(Pointer kptr) {
            if (n->is_linear()) {
                n = n->check_linear_capacity(idx);
            }
            Key& k = *kptr;
            n->insert(idx, std::move(kptr));
            return iterator(k.*Hook, 0);
        }
    };
    template <typename K>
    bool key_lower_bound(const K& key, const Compare& cmp, cursor& cur) const {
        cur.n = _root;
        while (true) {
            bool match;
            cur.idx = cur.n->index_for(key, cmp, match);
            assert(cur.idx <= cur.n->_base.num_keys);
            if (match || cur.n->is_leaf()) {
                return match;
            }
            cur.descend();
        }
    }
    void do_set_root(node& n) noexcept {
        assert(n.is_root());
        n._parent.t = this;
        _root = &n;
    }
    void do_set_left(node& n) noexcept {
        assert(n.is_leftmost());
        if (!n.is_linear()) {
            n._leaf_tree = this;
        }
        _corners.left = &n;
    }
    void do_set_right(node& n) noexcept {
        assert(n.is_rightmost());
        if (!n.is_linear()) {
            n._leaf_tree = this;
        }
        _corners.right = &n;
    }
    template <typename Pointer>
    iterator insert_into_inline(Pointer kptr) noexcept {
        member_hook* hook = &(kptr.release()->*Hook);
        hook->attach_first(_inline);
        return iterator(*hook, 0);
    }
    template <typename K>
    std::strong_ordering find_in_inline(const K& k, const Compare& cmp) const {
        return _inline.empty() ? std::strong_ordering::greater : cmp(k, *(_inline.keys[0]->to_key<Key, Hook>()));
    }
    void break_inline() {
        node* n = node::create_empty_root();
        _inline.keys[0]->attach_first(n->_base);
        do_set_root(*n);
        do_set_left(*n);
        do_set_right(*n);
    }
    const node_base* rightmost_node() const noexcept {
        return _root == nullptr ? &_inline : &_corners.right->_base;
    }
    node_base* rightmost_node() noexcept {
        return _root == nullptr ? &_inline : &_corners.right->_base;
    }
    const node_base* leftmost_node() const noexcept {
        return _root == nullptr ? &_inline : &_corners.left->_base;
    }
    node_base* leftmost_node() noexcept {
        return _root == nullptr ? &_inline : &_corners.left->_base;
    }
    bool inline_root() const noexcept { return _root == nullptr; }
public:
    tree() noexcept : _root(nullptr), _inline(node_base::inline_tag{}) {}
    tree(tree&& other) noexcept : tree() {
        if (!other.inline_root()) {
            do_set_root(*other._root);
            do_set_left(*other._corners.left);
            do_set_right(*other._corners.right);
            other._root = nullptr;
            other._corners.left = nullptr;
            other._corners.right = nullptr;
        } else if (!other._inline.empty()) {
            other._inline.keys[0]->attach_first(_inline);
            other._inline.num_keys = 0;
        }
    }
    tree(const tree& other) = delete;
    ~tree() noexcept {
        if (!inline_root()) {
            assert(_root->is_leaf());
            node::destroy(*_root);
        } else {
            assert(_inline.empty());
        }
    }
    template <typename Pointer>
    requires KeyPointer<Pointer, Key>
    std::pair<iterator, bool> insert(Pointer kptr, Compare cmp) {
        seastar::memory::on_alloc_point();
        cursor cur;
        if (inline_root()) {
            if (_inline.empty()) {
                return std::pair(insert_into_inline(std::move(kptr)), true);
            }
            break_inline();
        }
        if (key_lower_bound(*kptr, cmp, cur)) {
            return std::pair(iterator(cur), false);
        }
        return std::pair(cur.insert(std::move(kptr)), true);
    }
    template <typename Pointer>
    requires KeyPointer<Pointer, Key>
    std::pair<iterator, bool> insert_before_hint(iterator hint, Pointer kptr, Compare cmp) {
        seastar::memory::on_alloc_point();
        auto x = std::strong_ordering::less;
        if (hint != end()) {
            x = cmp(*kptr, *hint);
            if (x == 0) {
                return std::pair(iterator(hint), false);
            }
        }
        if (x < 0) {
            x = std::strong_ordering::greater;
            if (hint != begin()) {
                auto prev = std::prev(hint);
                x = cmp(*kptr, *prev);
                if (x == 0) {
                    return std::pair(iterator(prev), false);
                }
            }
            if (x > 0) {
                return std::pair(hint.insert_before(std::move(kptr)), true);
            }
        }
        return insert(std::move(kptr), std::move(cmp));
    }
    template <typename Pointer>
    requires KeyPointer<Pointer, Key>
    iterator insert_before(iterator it, Pointer kptr) {
        seastar::memory::on_alloc_point();
        return it.insert_before(std::move(kptr));
    }
    template <typename K>
    requires Comparable<K, Key, Compare>
    const_iterator find(const K& k, Compare cmp) const {
        cursor cur;
        if (inline_root()) {
            if (find_in_inline(k, cmp) == 0) {
                return const_iterator(*_inline.keys[0], 0);
            }
            return cend();
        }
        if (!key_lower_bound(k, cmp, cur)) {
            return cend();
        }
        return const_iterator(cur);
    }
    template <typename K>
    requires Comparable<K, Key, Compare>
    iterator find(const K& k, Compare cmp) {
        return iterator(const_cast<const tree*>(this)->find(k, cmp));
    }
    template <typename K>
    requires Comparable<K, Key, Compare>
    const_iterator lower_bound(const K& k, bool& match, Compare cmp) const {
        if (inline_root()) {
            auto x = find_in_inline(k, cmp);
            if (x <= 0) {
                match = x == 0;
                return const_iterator(*_inline.keys[0], 0);
            }
            match = false;
            return cend();
        }
        if (_root->_base.num_keys == 0) {
            match = false;
            return cend();
        }
        cursor cur;
        match = key_lower_bound(k, cmp, cur);
        if (!match && cur.idx == cur.n->_base.num_keys) {
            assert(cur.idx > 0);
            cur.idx--;
            return ++const_iterator(cur);
        }
        return const_iterator(cur);
    }
    template <typename K>
    requires Comparable<K, Key, Compare>
    iterator lower_bound(const K& k, bool& match, Compare cmp) {
        return iterator(const_cast<const tree*>(this)->lower_bound(k, match, cmp));
    }
    template <typename K>
    requires Comparable<K, Key, Compare>
    const_iterator lower_bound(const K& k, Compare cmp) const {
        bool match;
        return lower_bound(k, match, cmp);
    }
    template <typename K>
    requires Comparable<K, Key, Compare>
    iterator lower_bound(const K& k, Compare cmp) {
        bool match;
        return lower_bound(k, match, cmp);
    }
    template <typename K>
    requires Comparable<K, Key, Compare>
    const_iterator upper_bound(const K& k, Compare cmp) const {
        bool match;
        const_iterator ret = lower_bound(k, match, cmp);
        if (match) {
            ret++;
        }
        return ret;
    }
    template <typename K>
    requires Comparable<K, Key, Compare>
    iterator upper_bound(const K& k, Compare cmp) {
        return iterator(const_cast<const tree*>(this)->upper_bound(k, cmp));
    }
    template <typename K, typename Disp>
    requires Comparable<K, Key, Compare> && Disposer<Disp, Key>
    iterator erase_and_dispose(const K& k, Compare cmp, Disp&& disp) {
        cursor cur;
        if (inline_root()) {
            if (find_in_inline(k, cmp) == 0) {
                node::dispose_key(_inline.keys[0], disp);
                _inline.num_keys = 0;
            }
            return cend();
        }
        if (!key_lower_bound(k, cmp, cur)) {
            return end();
        }
        iterator it(cur);
        member_hook* hook = it._hook;
        it++;
        cur.n->remove(cur.idx);
        node::dispose_key(hook, disp);
        return it;
    }
    template <typename Disp>
    requires Disposer<Disp, Key>
    iterator erase_and_dispose(iterator from, iterator to, Disp&& disp) noexcept {
        while (from != to) {
            from = from.erase_and_dispose(disp);
        }
        return to;
    }
    template <typename Disp>
    requires Disposer<Disp, Key>
    iterator erase_and_dispose(const_iterator from, const_iterator to, Disp&& disp) noexcept {
        return erase_and_dispose(iterator(from), iterator(to), std::forward<Disp>(disp));
    }
    template <typename Disp>
    requires Disposer<Disp, Key>
    iterator erase_and_dispose(iterator it, Disp&& disp) noexcept {
        return it.erase_and_dispose(disp);
    }
    Key* unlink_leftmost_without_rebalance() noexcept {
        node_base* nb = leftmost_node();
        if (nb->num_keys == 0) {
            return nullptr;
        }
        member_hook* hook = nb->keys[0];
        node::dispose_key(hook, default_dispose<Key>);
        if (nb->is_inline()) {
            nb->num_keys = 0;
        } else {
            node* n = node::from_base(nb);
            assert(n->is_leaf());
            n->remove_leftmost_light_rebalance();
        }
        return hook->to_key<Key, Hook>();
    }
    template <typename... Args>
    iterator erase(Args&&... args) { return erase_and_dispose(std::forward<Args>(args)..., default_dispose<Key>); }
    template <typename Func>
    requires Disposer<Func, Key>
    void clear_and_dispose(Func&& disp) noexcept {
        if (!inline_root()) {
            _root->clear([&disp] (member_hook* h) { node::dispose_key(h, disp); });
            node::destroy(*_root);
            _root = nullptr;
            // Both left and right leaves pointers are not touched as this
            // initialization of inline node overwrites them anyway
            new (&_inline) node_base(node_base::inline_tag{});
        } else if (!_inline.empty()) {
            node::dispose_key(_inline.keys[0], disp);
            _inline.num_keys = 0;
        }
    }
    void clear() noexcept { clear_and_dispose(default_dispose<Key>); }
    template <typename Cloner, typename Deleter>
    requires KeyCloner<Cloner, Key> && Disposer<Deleter, Key>
    void clone_from(const tree& t, Cloner&& cloner, Deleter&& deleter) {
        clear_and_dispose(deleter);
        if (!t.inline_root()) {
            node* left = nullptr;
            node* right = nullptr;
            _root = t._root->clone(left, right, cloner, deleter);
            left->_base.flags |= node_base::NODE_LEFTMOST;
            do_set_left(*left);
            right->_base.flags |= node_base::NODE_RIGHTMOST;
            do_set_right(*right);
            _root->_base.flags |= node_base::NODE_ROOT;
            do_set_root(*_root);
        } else if (!t._inline.empty()) {
            Key* key = cloner(t._inline.keys[0]->template to_key<Key, Hook>());
            (key->*Hook).attach_first(_inline);
        }
    }
    template <bool Const>
    class iterator_base {
    protected:
        using tree_ptr = std::conditional_t<Const, const tree*, tree*>;
        using key_hook_ptr = std::conditional_t<Const, const member_hook*, member_hook*>;
        using node_base_ptr = std::conditional_t<Const, const node_base*, node_base*>;
        using node_ptr = std::conditional_t<Const, const node*, node*>;
        // The end() iterator uses _tree pointer, all the others use _hook.
        union {
            tree_ptr _tree;
            key_hook_ptr _hook;
        };
        key_index _idx;
        // No keys can be at this index, so it's used as the "end" mark.
        static constexpr key_index npos = LinearThreshold;
        explicit iterator_base(tree_ptr t) noexcept : _tree(t), _idx(npos) {}
        iterator_base(key_hook_ptr h, key_index idx) noexcept : _hook(h), _idx(idx) {
            assert(!is_end());
            assert(h->attached());
        }
        explicit iterator_base(const cursor& cur) noexcept : _idx(cur.idx) {
            assert(_idx < cur.n->_base.num_keys);
            _hook = cur.n->_base.keys[_idx];
            assert(_hook->attached());
        }
        iterator_base() noexcept : _tree(static_cast<tree_ptr>(nullptr)), _idx(npos) {}
        bool is_end() const noexcept { return _idx == npos; }
        node_base_ptr revalidate() noexcept {
            assert(!is_end());
            node_base_ptr n = _hook->node();
            if (_idx >= n->num_keys || n->keys[_idx] != _hook) {
                _idx = n->index_for(_hook);
            }
            return n;
        }
    public:
        using iterator_category = std::bidirectional_iterator_tag;
        using value_type = std::conditional_t<Const, const Key, Key>;
        using difference_type = ssize_t;
        using pointer = value_type*;
        using reference = value_type&;
        iterator_base(const iterator_base& other) noexcept {
            if (other.is_end()) {
                _idx = npos;
                _tree = other._tree;
            } else {
                _idx = other._idx;
                _hook = other._hook;
            }
        }
        reference operator*() const noexcept { return *_hook->template to_key<Key, Hook>(); }
        pointer operator->() const noexcept { return _hook->template to_key<Key, Hook>(); }
        iterator_base& operator++() noexcept {
            node_base_ptr n = revalidate();
            if (n->is_leaf()) [[likely]] {
                if (_idx < n->num_keys - 1u) [[likely]] {
                    _idx++;
                    _hook = n->keys[_idx];
                } else if (n->is_inline()) {
                    _idx = npos;
                    _tree = tree::from_inline(n);
                } else if (n->is_rightmost()) {
                    _idx = npos;
                    _tree = node::from_base(n)->corner_tree();
                } else {
                    node_ptr nd = node::from_base(n);
                    do {
                        node_ptr p = nd->_parent.n;
                        _idx = p->index_for(nd);
                        nd = p;
                    } while (_idx == nd->_base.num_keys);
                    _hook = nd->_base.keys[_idx];
                }
            } else {
                node_ptr nd = node::from_base(n);
                nd = nd->_kids[_idx + 1];
                while (!nd->is_leaf()) {
                    nd = nd->_kids[0];
                }
                _idx = 0;
                _hook = nd->_base.keys[_idx];
            }
            return *this;
        }
        iterator_base& operator--() noexcept {
            if (is_end()) {
                node_base_ptr n = _tree->rightmost_node();
                assert(n->num_keys > 0);
                _idx = n->num_keys - 1u;
                _hook = n->keys[_idx];
                return *this;
            }
            node_ptr n = node::from_base(revalidate());
            if (n->is_leaf()) {
                while (_idx == 0) {
                    node_ptr p = n->_parent.n;
                    _idx = p->index_for(n);
                    n = p;
                }
                _idx--;
            } else {
                n = n->_kids[_idx];
                while (!n->is_leaf()) {
                    n = n->_kids[n->_base.num_keys];
                }
                _idx = n->_base.num_keys - 1;
            }
            _hook = n->_base.keys[_idx];
            return *this;
        }
        iterator_base operator++(int) noexcept {
            iterator_base cur = *this;
            operator++();
            return cur;
        }
        iterator_base operator--(int) noexcept {
            iterator_base cur = *this;
            operator--();
            return cur;
        }
        bool operator==(const iterator_base& o) const noexcept { return is_end() ? o.is_end() : _hook == o._hook; }
        operator bool() const noexcept { return !is_end(); }
        iterator_base(pointer key) noexcept : iterator_base(&(key->*Hook), 0) {
            revalidate();
        }
        tree_ptr tree_if_singular() noexcept {
            node_base* n = revalidate();
            if (n->is_root() && n->is_leaf() && n->num_keys == 1) {
                return n->is_inline() ? tree::from_inline(n) : node::from_base(n)->_parent.t;
            } else {
                return nullptr;
            }
        }
    };
    using iterator_base_const = iterator_base<true>;
    using iterator_base_nonconst = iterator_base<false>;
    class const_iterator final : public iterator_base_const {
        friend class tree;
        using super = iterator_base_const;
        explicit const_iterator(const tree* t) noexcept : super(t) {}
        explicit const_iterator(const cursor& cur) noexcept : super(cur) {}
        const_iterator(const member_hook& h, key_index idx) noexcept : super(&h, idx) {}
    public:
        const_iterator() noexcept : super() {}
        const_iterator(const iterator_base_const& other) noexcept : super(other) {}
        const_iterator(const iterator& other) noexcept {
            if (other.is_end()) {
                super::_idx = super::npos;
                super::_tree = const_cast<const tree*>(other._tree);
            } else {
                super::_idx = other._idx;
                super::_hook = const_cast<const member_hook*>(other._hook);
            }
        }
    };
    class iterator final : public iterator_base_nonconst {
        friend class tree;
        friend class key_grabber;
        using super = iterator_base_nonconst;
        explicit iterator(const tree* t) noexcept : super(t) {}
        explicit iterator(const cursor& cur) noexcept : super(cur) {}
        iterator(member_hook& h, key_index idx) noexcept : super(&h, idx) {}
    public:
        iterator() noexcept : super() {}
        iterator(const iterator_base_nonconst& other) noexcept : super(other) {}
        iterator(const const_iterator& other) noexcept {
            if (other.is_end()) {
                super::_idx = super::npos;
                super::_tree = const_cast<tree*>(other._tree);
            } else {
                super::_idx = other._idx;
                super::_hook = const_cast<member_hook*>(other._hook);
            }
        }
    private:
        template <typename Disp>
        requires Disposer<Disp, Key>
        iterator erase_and_dispose(Disp&& disp) noexcept {
            node_base* nb = super::revalidate();
            iterator cur;
            if (nb->is_inline()) {
                cur._idx = super::npos;
                cur._tree = tree::from_inline(nb);
                nb->num_keys = 0;
            } else {
                cur = *this;
                cur++;
                node::from_base(nb)->remove(super::_idx);
                if (cur._hook->node() == nb && cur._idx > 0) {
                    cur._idx--;
                }
            }
            node::dispose_key(super::_hook, disp);
            return cur;
        }
        template <typename Pointer>
        iterator insert_before(Pointer kptr) {
            cursor cur;
            if (super::is_end()) {
                tree* t = super::_tree;
                if (t->inline_root()) {
                    if (t->_inline.empty()) {
                        return t->insert_into_inline(std::move(kptr));
                    }
                    t->break_inline();
                }
                cur.n = t->_corners.right;
                cur.idx = cur.n->_base.num_keys;
            } else {
                node_base* n = super::revalidate();
                if (n->is_inline()) {
                    tree* t = tree::from_inline(n);
                    t->break_inline();
                    cur.n = t->_root;
                    cur.idx = 0;
                } else {
                    cur.n = node::from_base(n);
                    cur.idx = super::_idx;
                    while (!cur.n->is_leaf()) {
                        cur.descend();
                        cur.idx = cur.n->_base.num_keys;
                    }
                }
            }
            return cur.insert(std::move(kptr));
        }
    };
    bool empty() const noexcept { return inline_root() ? _inline.empty() : _root->_base.empty(); }
    const_iterator cbegin() const noexcept {
        const node_base* n = leftmost_node();
        return n->num_keys == 0 ? cend() : const_iterator(*n->keys[0], 0);
    }
    const_iterator cend() const noexcept {
        return const_iterator(this);
    }
    const_iterator begin() const noexcept { return cbegin(); }
    const_iterator end() const noexcept { return cend(); }
    iterator begin() noexcept {
        return iterator(const_cast<const tree*>(this)->cbegin());
    }
    iterator end() noexcept {
        return iterator(const_cast<const tree*>(this)->cend());
    }
    using reverse_iterator = std::reverse_iterator<iterator>;
    reverse_iterator rbegin() noexcept { return std::make_reverse_iterator(end()); }
    reverse_iterator rend() noexcept { return std::make_reverse_iterator(begin()); }
    using const_reverse_iterator = std::reverse_iterator<const_iterator>;
    const_reverse_iterator crbegin() const noexcept { return std::make_reverse_iterator(cend()); }
    const_reverse_iterator crend() const noexcept { return std::make_reverse_iterator(cbegin()); }
    const_reverse_iterator rbegin() const noexcept { return crbegin(); }
    const_reverse_iterator rend() const noexcept { return crend(); }
    size_t calculate_size() const noexcept {
        return inline_root() ? _inline.num_keys : _root->size_slow();
    }
    size_t external_memory_usage() const noexcept {
        return inline_root() ? 0 : _root->external_memory_usage();
    }
    class key_grabber {
        iterator& _it;
    public:
        explicit key_grabber(iterator& it) : _it(it) {
            assert(!_it.is_end());
        }
        key_grabber(const key_grabber&) = delete;
        key_grabber(key_grabber&&) noexcept = default;
        Key& operator*() const noexcept { return *_it; }
        template <typename Disp>
        requires Disposer<Disp, Key>
        void release(Disp&& disp) {
            _it = _it.erase_and_dispose(std::move(disp));
        }
        Key* release() noexcept {
            Key& key = *_it;
            release(default_dispose<Key>);
            return &key;
        }
    };
    struct stats get_stats() const noexcept {
        struct stats st;
        st.nodes = 0;
        st.leaves = 0;
        st.linear_keys = 0;
        if (!inline_root()) {
            st.nodes_filled.resize(NodeSize + 1);
            st.leaves_filled.resize(NodeSize + 1);
            _root->fill_stats(st);
        }
        return st;
    }
};
template <typename K, typename Key, member_hook Key::* Hook, typename Compare, key_search Search>
struct searcher { };
template <typename K, typename Key, member_hook Key::* Hook, typename Compare>
struct searcher<K, Key, Hook, Compare, key_search::linear> {
    static key_index ge(const K& k, const node_base& node, const Compare& cmp, bool& match) {
        key_index i;
        match = false;
        for (i = 0; i < node.num_keys; i++) {
            if (i + 1 < node.num_keys) {
                __builtin_prefetch(node.keys[i + 1]->to_key<Key, Hook>());
            }
            auto x = cmp(k, *node.keys[i]->to_key<Key, Hook>());
            if (x <= 0) {
                match = x == 0;
                break;
            }
        }
        return i;
    };
};
template <typename K, typename Key, member_hook Key::* Hook, typename Compare>
struct searcher<K, Key, Hook, Compare, key_search::binary> {
    static key_index ge(const K& k, const node_base& node, const Compare& cmp, bool& match) ;
};
template <typename K, typename Key, member_hook Key::* Hook, typename Compare>
struct searcher<K, Key, Hook, Compare, key_search::both> {
    static key_index ge(const K& k, const node_base& node, const Compare& cmp, bool& match) ;
};
template <typename Key, member_hook Key::* Hook, typename Compare, size_t NodeSize, size_t LinearThreshold, key_search Search, with_debug Debug>
class node {
    friend class tree<Key, Hook, Compare, NodeSize, LinearThreshold, Search, Debug>;
    friend class validator<Key, Hook, Compare, NodeSize, LinearThreshold>;
    using tree = class tree<Key, Hook, Compare, NodeSize, LinearThreshold, Search, Debug>;
    class prealloc;
    [[no_unique_address]] utils::neat_id<Debug == with_debug::yes> id;
    static constexpr size_t NodeHalf = ((NodeSize - 1) / 2);
    static_assert(NodeHalf >= 1);
    static_assert(LinearThreshold >= NodeSize);
    static_assert(LinearThreshold <= NodeSize * (NodeSize + 1) + NodeSize);
    // Hint for the compiler when not to mess with linear stuff at all
    static constexpr bool make_linear_root = (LinearThreshold > NodeSize);
    union node_or_tree {
        node* n;
        tree* t;
    };
    // root node keeps .t pointer on tree, all others -- .n on parents
    node_or_tree _parent;
    node_base _base;
    char __room_for_keys[NodeSize * sizeof(member_hook*)];
    static_assert(offsetof(node_base, keys[NodeSize]) == sizeof(node_base) + NodeSize * sizeof(member_hook*));
    union {
        node* _kids[0];
        tree* _leaf_tree;
    };
    tree* corner_tree() const noexcept {
        assert(is_leaf());
        if (!is_linear()) {
            return _leaf_tree;
        }
        assert(is_root());
        return _parent.t;
    }
public:
    static constexpr size_t leaf_node_size = sizeof(node);
    static constexpr size_t inner_node_size = sizeof(node) - sizeof(tree*) + (NodeSize + 1) * sizeof(node*);
    static size_t linear_node_size(size_t cap) {
        return sizeof(node) - sizeof(tree*) - NodeSize * sizeof(member_hook*) + cap * sizeof(member_hook*);
    }
private:
    bool is_root() const noexcept { return _base.is_root(); }
    bool is_leaf() const noexcept { return _base.is_leaf(); }
    bool is_leftmost() const noexcept { return _base.is_leftmost(); }
    bool is_rightmost() const noexcept { return _base.is_rightmost(); }
    bool is_linear() const noexcept { return make_linear_root && _base.is_linear(); }
    // Helpers to move keys/kids around
    // ... locally
    void move_key(key_index f, key_index t) noexcept {
        _base.keys[t] = _base.keys[f];
    }
    void move_kid(kid_index f, kid_index t) noexcept {
        _kids[t] = _kids[f];
    }
    void set_key(key_index idx, member_hook* hook) noexcept {
        _base.keys[idx] = hook;
        hook->_node = &_base;
    }
    void set_kid(kid_index idx, node* n) noexcept {
        _kids[idx] = n;
        n->_parent.n = this;
    }
    // ... to other nodes
    void move_key(key_index f, node& n, key_index t) noexcept {
        n.set_key(t, _base.keys[f]);
    }
    void move_kid(kid_index f, node& n, kid_index t) noexcept {
        n.set_kid(t, _kids[f]);
    }
    void unlink_corner_leaf() noexcept ;
    static const node* from_base(const node_base* nb) noexcept ;
    static node* from_base(node_base* nb) noexcept {
        assert(!nb->is_inline());
        return boost::intrusive::get_parent_from_member(nb, &node::_base);
    }
    template <typename Disp>
    static void dispose_key(member_hook* hook, Disp&& disp) noexcept {
        hook->_node = nullptr;
        disp(hook->to_key<Key, Hook>());
    }
public:
    node(size_t cap, unsigned short flags) noexcept : _base(0, cap, flags) { }
    node(node&& other) noexcept : node(other._base.capacity, std::move(other)) {}
    node(size_t cap, node&& other) noexcept : _base(0, cap, other._base.flags) {
        if (is_leaf()) {
            if (is_leftmost()) {
                other.corner_tree()->do_set_left(*this);
            }
            if (is_rightmost()) {
                other.corner_tree()->do_set_right(*this);
            }
            other._base.flags &= ~(node_base::NODE_LEFTMOST | node_base::NODE_RIGHTMOST);
        } else {
            other.move_kid(0, *this, 0);
        }
        other.move_to(*this, 0, other._base.num_keys);
        if (!is_root()) {
            _parent.n = other._parent.n;
            kid_index i = _parent.n->index_for(&other);
            _parent.n->_kids[i] = this;
        } else {
            other._parent.t->do_set_root(*this);
        }
    }
    node(const node& other) = delete;
    ~node() {
        assert(_base.num_keys == 0);
    }
    size_t storage_size() const noexcept {
        return is_linear() ? linear_node_size(_base.capacity) :
            is_leaf() ? leaf_node_size : inner_node_size;
    }
private:
    template <typename... Args>
    static node* construct(size_t size, Args&&... args) {
        void* mem = current_allocator().alloc<node>(size);
        return new (mem) node(std::forward<Args>(args)...);
    }
    static node* create_leaf() { return construct(leaf_node_size, NodeSize, node_base::NODE_LEAF); }
    static node* create_inner() { return construct(inner_node_size, NodeSize, 0); }
    static node* create_empty_root() {
        if (make_linear_root) {
            return construct(node::linear_node_size(1), 1,
                    node_base::NODE_LINEAR | node_base::NODE_ROOT | node_base::NODE_LEAF |
                    node_base::NODE_LEFTMOST | node_base::NODE_RIGHTMOST);
        } else {
            node* n = node::create_leaf();
            n->_base.flags |= node_base::NODE_ROOT | node_base::NODE_LEFTMOST | node_base::NODE_RIGHTMOST;
            return n;
        }
    }
    static void destroy(node& n) noexcept {
        current_allocator().destroy(&n);
    }
    void drop() noexcept {
        assert(!(is_leftmost() || is_rightmost()));
        if (Debug == with_debug::yes && !is_root()) {
            node* p = _parent.n;
            if (p->_base.num_keys != 0) {
                for (kid_index i = 0; i <= p->_base.num_keys; i++) {
                    assert(p->_kids[i] != this);
                }
            }
        }
        destroy(*this);
    }
    template <typename K>
    key_index index_for(const K& k, const Compare& cmp, bool& match) const {
        return searcher<K, Key, Hook, Compare, Search>::ge(k, _base, cmp, match);
    }
    // Two helpers for raw pointers lookup.
    kid_index index_for(const node* kid) const noexcept {
        assert(!is_leaf());
        for (kid_index i = 0; i <= _base.num_keys; i++) {
            if (_kids[i] == kid) {
                return i;
            }
        }
        std::abort();
    }
    
    
    bool can_grab_from() const noexcept ;
    bool can_push_to() const noexcept {
        return _base.num_keys < NodeSize;
    }
    bool can_merge_with(const node& n) const noexcept ;
    // Make a room for a new key (and kid) at \at position
    void shift_right(size_t at) noexcept {
        for (size_t i = _base.num_keys; i > at; i--) {
            move_key(i - 1, i);
            if (!is_leaf()) {
                move_kid(i, i + 1);
            }
        }
        _base.num_keys++;
    }
    // Occupy the hole at \at after key (and kid) removal
    void shift_left(size_t at) noexcept {
        _base.num_keys--;
        for (size_t i = at; i < _base.num_keys; i++) {
            move_key(i + 1, i);
            if (!is_leaf()) {
                move_kid(i + 2, i + 1);
            }
        }
    }
    // Move keys (and kids) to other node
    void move_to(node& to, size_t off, size_t nr) noexcept {
        for (size_t i = 0; i < nr; i++) {
            move_key(i + off, to, to._base.num_keys + i);
            if (!is_leaf()) {
                move_kid(i + off + 1, to, to._base.num_keys + i + 1);
            }
        }
        _base.num_keys -= nr;
        to._base.num_keys += nr;
    }
    void maybe_allocate_nodes(prealloc& nodes) const {
        // this is full leaf
        nodes.push(node::create_leaf());
        if (is_root()) {
            nodes.push(node::create_inner());
            return;
        }
        const node* cur = _parent.n;
        while (cur->_base.num_keys == NodeSize) {
            nodes.push(node::create_inner());
            if (cur->is_root()) {
                nodes.push(node::create_inner());
                break;
            }
            cur = cur->_parent.n;
        }
    }
    // Constants for linear node shattering into a tree
    // Nr of leaves to keep LinearThreshold keys (inc. keys in the root)
    static constexpr size_t ShatterLeaves = (LinearThreshold + NodeSize + 1) / (NodeSize + 1);
    // This many keys will be put into leaves themselves
    static constexpr size_t ShatterKeysInLeaves = LinearThreshold - (ShatterLeaves - 1);
    // Each leaf gets this amount of keys ...
    static constexpr size_t ShatterKeysPerLeaf = ShatterKeysInLeaves / ShatterLeaves;
    // ... plus 0 or 1 from the remainder
    static constexpr size_t ShatterKeysRemain = ShatterKeysInLeaves % ShatterLeaves;
    node* shatter(prealloc& nodes, kid_index& idx) noexcept {
        node* new_insertion = nullptr;
        node* root = nodes.pop(false);
        root->_base.flags |= node_base::NODE_ROOT;
        _parent.t->do_set_root(*root);
        node* leaf = nodes.pop(true);
        root->set_kid(root->_base.num_keys, leaf);
        leaf->_base.flags |= node_base::NODE_LEFTMOST;
        _parent.t->do_set_left(*leaf);
        key_index src = 0;
        ssize_t rem = ShatterKeysRemain;
        auto adjust_idx = [&] () noexcept {
            if (new_insertion == nullptr && src == idx) {
                new_insertion = leaf;
                idx = leaf->_base.num_keys;
            }
        };
        while (true) {
            adjust_idx();
            move_key(src++, *leaf, leaf->_base.num_keys++);
            if (src == _base.num_keys) {
                leaf->_base.flags |= node_base::NODE_RIGHTMOST;
                _parent.t->do_set_right(*leaf);
                break;
            }
            if (leaf->_base.num_keys == ShatterKeysPerLeaf + (rem > 0 ? 1 : 0)) {
                rem--;
                adjust_idx();
                move_key(src++, *root, root->_base.num_keys++);
                leaf = nodes.pop(true);
                root->set_kid(root->_base.num_keys, leaf);
                assert(src != _base.num_keys); // need more keys for the next leaf
            }
        }
        adjust_idx();
        _base.num_keys = 0;
        _base.flags &= ~(node_base::NODE_LEFTMOST | node_base::NODE_RIGHTMOST);
        drop();
        assert(new_insertion != nullptr);
        return new_insertion;
    }
    node* check_linear_capacity(kid_index& idx) {
        assert(make_linear_root && is_root() && is_leaf());
        if (_base.num_keys < _base.capacity) {
            return this;
        }
        if (_base.capacity < LinearThreshold) {
            size_t ncap = std::min<size_t>(LinearThreshold, _base.capacity * 2);
            node* n = node::construct(linear_node_size(ncap), ncap, std::move(*this));
            drop();
            return n;
        }
        prealloc nodes;
        nodes.push(node::create_inner());
        for (size_t i = 0; i < ShatterLeaves; i++) {
            nodes.push(node::create_leaf());
        }
        return shatter(nodes, idx);
    }
    template <typename KeyPointer>
    void insert(kid_index idx, KeyPointer kptr) {
        assert(is_leaf());
        if (_base.num_keys < _base.capacity) {
            do_insert(idx, kptr.release()->*Hook, nullptr);
            return;
        }
        prealloc nodes;
        maybe_allocate_nodes(nodes);
        insert_into_full(idx, kptr.release()->*Hook, nullptr, nodes);
    }
    void insert(kid_index idx, member_hook& key, node* kid, prealloc& nodes) noexcept {
        if (_base.num_keys < NodeSize) {
            do_insert(idx, key, kid);
        } else {
            insert_into_full(idx, key, kid, nodes);
        }
    }
    void insert_into_full(kid_index idx, member_hook& key, node* kid, prealloc& nodes) noexcept {
        if (!is_root()) {
            node* p = _parent.n;
            kid_index i = p->index_for(this);
            if (i > 0) {
                node* left = p->_kids[i - 1];
                if (left->can_push_to()) {
                    if (idx > 0) {
                        left->grab_from_right(this, i - 1);
                        idx--;
                    } else if (is_leaf()) {
                        assert(kid == nullptr);
                        p->move_key(i - 1, *left, left->_base.num_keys);
                        left->_base.num_keys++;
                        p->set_key(i - 1, &key);
                        return;
                    }
                }
            }
            if (i < p->_base.num_keys) {
                node* right = p->_kids[i + 1];
                if (right->can_push_to()) {
                    if (idx < _base.num_keys) {
                        right->grab_from_left(this, i + 1);
                    } else if (is_leaf()) {
                        assert(kid == nullptr);
                        right->shift_right(0);
                        p->move_key(i, *right, 0);
                        p->set_key(i, &key);
                        return;
                    }
                }
            }
            if (_base.num_keys < NodeSize) {
                do_insert(idx, key, kid);
                return;
            }
        }
        split_and_insert(idx, key, kid, nodes);
    }
    void split_and_insert(kid_index idx, member_hook& key, node* kid, prealloc& nodes) noexcept {
        node* n = nodes.pop(is_leaf());
        size_t off = NodeHalf + 1;
        if (is_leaf() && is_rightmost()) {
            _base.flags &= ~node_base::NODE_RIGHTMOST;
            n->_base.flags |= node_base::NODE_RIGHTMOST;
            corner_tree()->do_set_right(*n);
        }
        if (idx == off) {
            move_to(*n, off, NodeSize - off);
            if (!is_leaf()) {
                n->_kids[0] = kid;
                kid->_parent.n = n;
            }
            insert_into_parent(key, n, nodes);
        } else {
            if (idx < off) {
                move_to(*n, off, NodeSize - off);
                do_insert(idx, key, kid);
            } else {
                off++;
                move_to(*n, off, NodeSize - off);
                n->do_insert(idx - off, key, kid);
            }
            if (!is_leaf()) {
                move_kid(_base.num_keys, *n, 0);
            }
            _base.num_keys--;
            insert_into_parent(*_base.keys[_base.num_keys], n, nodes);
        }
    }
    void do_insert(kid_index idx, member_hook& key, node* kid) noexcept {
        shift_right(idx);
        set_key(idx, &key);
        if (kid != nullptr) {
            _kids[idx + 1] = kid;
            kid->_parent.n = this;
        }
    }
    void insert_into_parent(member_hook& key, node* kid, prealloc& nodes) noexcept {
        if (is_root()) {
            insert_into_root(key, kid, nodes);
        } else {
            kid_index idx = _parent.n->index_for(this);
            _parent.n->insert(idx, key, kid, nodes);
        }
    }
    void insert_into_root(member_hook& key, node* kid, prealloc& nodes) noexcept {
        tree* t = _parent.t;
        node* nr = nodes.pop(false);
        nr->_base.num_keys = 1;
        nr->set_key(0, &key);
        nr->_kids[0] = this;
        this->_parent.n = nr;
        nr->_kids[1] = kid;
        kid->_parent.n = nr;
        _base.flags &= ~node_base::NODE_ROOT;
        nr->_base.flags |= node_base::NODE_ROOT;
        t->do_set_root(*nr);
    }
    void collapse_root() noexcept ;
    void grab_from_left(node* left, key_index idx) noexcept {
        shift_right(0);
        _parent.n->move_key(idx - 1, *this, 0);
        left->move_key(left->_base.num_keys - 1, *_parent.n, idx - 1);
        if (!is_leaf()) {
            move_kid(0, 1);
            left->move_kid(left->_base.num_keys, *this, 0);
        }
        left->_base.num_keys--;
    }
    void grab_from_right(node* right, key_index idx) noexcept {
        _parent.n->move_key(idx, *this, _base.num_keys);
        right->move_key(0, *_parent.n, idx);
        if (!is_leaf()) {
            right->move_kid(0, *this, _base.num_keys + 1);
            right->move_kid(1, 0);
        }
        right->shift_left(0);
        _base.num_keys++;
    }
    
    
    void refill() noexcept ;
    void light_refill() noexcept ;
    void remove_from_inner(kid_index idx) noexcept ;
    template <typename KFunc>
    void clear(KFunc&& k_clear) noexcept {
        size_t nk = _base.num_keys;
        _base.num_keys = 0;
        if (!is_leaf()) {
            for (kid_index i = 0; i <= nk; i++) {
                _kids[i]->clear(k_clear);
                destroy(*_kids[i]);
            }
        }
        for (key_index i = 0; i < nk; i++) {
            k_clear(_base.keys[i]);
        }
    }
    template <typename Cloner, typename Deleter>
    node* clone(node*& left_leaf, node*& right_leaf, Cloner&& cloner, Deleter&& deleter) const ;
    size_t size_slow() const noexcept ;
    
    
    class prealloc {
        node* _nodes;
        node** _tail = &_nodes;
        node* pop() noexcept {
            assert(!empty());
            node* ret = _nodes;
            _nodes = ret->_parent.n;
            if (_tail == &ret->_parent.n) {
                _tail = &_nodes;
            }
            return ret;
        }
        bool empty() const noexcept { return _tail == &_nodes; }
        void drain() noexcept {
            while (!empty()) {
                node* n = pop();
                node::destroy(*n);
            }
        }
    public:
        void push(node* n) noexcept {
            *_tail = n;
            _tail = &n->_parent.n;
        }
        node* pop(bool leaf) noexcept {
            node* ret = pop();
            assert(leaf == ret->is_leaf());
            return ret;
        }
        ~prealloc() {
            drain();
        }
    };
};
} // namespace
class evictable {
    friend class lru;
    // For bookkeeping, we want the unlinking of evictables to be explicit.
    // E.g. if the cache's internal data structure consists of multiple lists, we would
    // like to know which list is an element being removed from.
    // Therefore, we are using auto_unlink only to be able to call unlink() in the move constructor
    // and we do NOT rely on automatic unlinking in _lru_link's destructor.
    // It's the programmer's responsibility. to call lru::remove on the evictable before its destruction.
    // Failure to do so is a bug, and it will trigger an assertion in the destructor.
    using lru_link_type = boost::intrusive::list_member_hook<
        boost::intrusive::link_mode<boost::intrusive::auto_unlink>>;
    lru_link_type _lru_link;
protected:
    // Prevent destruction via evictable pointer. LRU is not aware of allocation strategy.
    // Prevent destruction of a linked evictable. While we could unlink the evictable here
    // in the destructor, we can't perform proper accounting for that without access to the
    // head of the containing list.
    ~evictable() {
        assert(!_lru_link.is_linked());
    }
public:
    ;
    virtual void on_evicted() noexcept = 0;
    // Used for testing to avoid cascading eviction of the containing object.
    virtual void on_evicted_shallow() noexcept { on_evicted(); }
    void swap(evictable& o) noexcept ;
};
class lru {
private:
    friend class evictable;
    using lru_type = boost::intrusive::list<evictable,
        boost::intrusive::member_hook<evictable, evictable::lru_link_type, &evictable::_lru_link>,
        boost::intrusive::constant_time_size<false>>; // we need this to have bi::auto_unlink on hooks.
    lru_type _list;
public:
    using reclaiming_result = seastar::memory::reclaiming_result;
    
    
    // Like add(e) but makes sure that e is evicted right before "more_recent" in the absence of later touches.
    // Evicts a single element from the LRU
     ;
    // Evicts a single element from the LRU.
    // Evicts a single element from the LRU.
    // Will call on_evicted_shallow() instead of on_evicted().
    // Evicts all elements.
    // May stall the reactor, use only in tests.
};
template<typename T>
requires std::is_nothrow_move_constructible_v<T>
class managed;
//
// Similar to std::unique_ptr<>, but for LSA-allocated objects. Remains
// valid across deferring points. See make_managed().
//
// std::unique_ptr<> can't be used with LSA-allocated objects because
// it assumes that the object doesn't move after being allocated. This
// is not true for LSA, which moves objects during compaction.
//
// Also works for objects allocated using standard allocators, though
// there the extra space overhead of a pointer is not justified.
// It still make sense to use it in places which are meant to work
// with either kind of allocator.
//
template<typename T>
struct managed_ref {
    managed<T>* _ptr;
    T& operator*() {
        return _ptr->_value;
    }
    const T& operator*() const ;
    T* operator->() ;
    const T* operator->() const ;
    explicit operator bool() const {
        return _ptr != nullptr;
    }
    size_t external_memory_usage() const ;
};
template<typename T>
requires std::is_nothrow_move_constructible_v<T>
class managed {
    managed<T>** _backref;
    T _value;
    template<typename T_>
    friend struct managed_ref;
public:
    managed(managed<T>** backref, T&& v) noexcept
        : _backref(backref)
        , _value(std::move(v))
    {
        *_backref = this;
    }
    managed(managed&& other) noexcept
        : _backref(other._backref)
        , _value(std::move(other._value))
    {
        *_backref = this;
    }
};
//
// Allocates T using given AllocationStrategy and returns a managed_ref owning the
// allocated object.
//
template<typename T, typename... Args>
managed_ref<T>
make_managed(Args&&... args) {
    managed_ref<T> ref;
    current_allocator().construct<managed<T>>(&ref._ptr, T(std::forward<Args>(args)...));
    return ref;
}
namespace utils {
static constexpr int64_t simple_key_unused_value = std::numeric_limits<int64_t>::min();
int array_search_gt(int64_t val, const int64_t* array, const int capacity, const int size);
static inline unsigned array_search_4_eq(uint8_t val, const uint8_t* array) {
    // Unrolled loop is few %s faster
    if (array[0] == val) {
        return 0;
    } else if (array[1] == val) {
        return 1;
    } else if (array[2] == val) {
        return 2;
    } else if (array[3] == val) {
        return 3;
    } else {
        return 4;
    }
}
static inline unsigned array_search_8_eq(uint8_t val, const uint8_t* array) {
    for (unsigned i = 0; i < 8; i++) {
        if (array[i] == val) {
            return i;
        }
    }
    return 8;
}
unsigned array_search_16_eq(uint8_t val, const uint8_t* array);
unsigned array_search_32_eq(uint8_t val, const uint8_t* array);
unsigned array_search_x32_eq(uint8_t val, const uint8_t* array, int nr);
}
class size_calculator;
namespace compact_radix_tree {
template <typename T, typename Idx> class printer;
template <unsigned Size>
inline unsigned find_in_array(uint8_t val, const uint8_t* arr);
template <>
inline unsigned find_in_array<4>(uint8_t val, const uint8_t* arr) {
    return utils::array_search_4_eq(val, arr);
}
template <>
inline unsigned find_in_array<8>(uint8_t val, const uint8_t* arr) {
    return utils::array_search_8_eq(val, arr);
}
template <>
inline unsigned find_in_array<16>(uint8_t val, const uint8_t* arr) {
    return utils::array_search_16_eq(val, arr);
}
template <>
inline unsigned find_in_array<32>(uint8_t val, const uint8_t* arr) {
    return utils::array_search_32_eq(val, arr);
}
template <> unsigned find_in_array<64>(uint8_t val, const uint8_t* arr) ;
// A union of any number of types.
template <typename... Ts>
struct variadic_union;
template <typename Tx>
struct variadic_union<Tx> {
    union {
        Tx _this;
    };
    variadic_union() noexcept {}
    ~variadic_union() {}
};
template <typename Tx, typename Ty, typename... Ts>
struct variadic_union<Tx, Ty, Ts...> {
    union {
        Tx _this;
        variadic_union<Ty, Ts...> _other;
    };
    variadic_union() noexcept {}
    ~variadic_union() {}
};
template <typename T, typename Index = unsigned int>
requires std::is_nothrow_move_constructible_v<T> && std::is_integral_v<Index>
class tree {
    friend class ::size_calculator;
    template <typename A, typename I> friend class printer;
    class leaf_node;
    class inner_node;
    struct node_head;
    class node_head_ptr;
public:
    using key_t = std::make_unsigned_t<Index>;
    using node_index_t = uint8_t;
    static constexpr unsigned radix_bits = 7;
    static constexpr key_t  radix_mask = (1 << radix_bits) - 1;
    static constexpr unsigned leaf_depth = (8 * sizeof(key_t) + radix_bits - 1) / radix_bits - 1;
    static constexpr unsigned node_index_limit = 1 << radix_bits;
    static_assert(node_index_limit != 0);
    static constexpr node_index_t unused_node_index = node_index_limit;
private:
    enum class layout : uint8_t { nil,
        indirect_tiny, indirect_small, indirect_medium, indirect_large,
        direct_dynamic, direct_static, };
    static constexpr key_t prefix_len_mask = radix_mask;
    static constexpr key_t prefix_mask = ~prefix_len_mask;
    static key_t make_prefix(key_t key, unsigned len) noexcept {
        return (key & prefix_mask) + len;
    }
    static key_t prefix_mask_at(unsigned depth) noexcept {
        return prefix_mask << (radix_bits * (leaf_depth - depth));
    }
    static unsigned common_prefix_len(key_t k1, key_t k2) noexcept {
        static constexpr unsigned trailing_bits = (8 * sizeof(key_t)) % radix_bits;
        static constexpr unsigned round_up_delta = trailing_bits == 0 ? 0 : radix_bits - trailing_bits;
        return (__builtin_clz(k1 ^ k2) + round_up_delta) / radix_bits;
    }
    static node_index_t node_index(key_t key, unsigned depth) noexcept {
        return (key >> (radix_bits * (leaf_depth - depth))) & radix_mask;
    }
    enum class erase_mode { real, cleanup, };
    enum class erase_result { nothing, empty, shrink, squash, };
    template <unsigned Threshold>
    static erase_result after_drop(unsigned count) noexcept {
        if (count == 0) {
            return erase_result::empty;
        }
        if (count == 1) {
            return erase_result::squash;
        }
        if constexpr (Threshold != 0) {
            if (count <= Threshold) {
                return erase_result::shrink;
            }
        }
        return erase_result::nothing;
    }
    struct lower_bound_res {
        const T* elem;
        const node_head* leaf;
        key_t key;
        lower_bound_res(const T* e, const node_head& l, key_t k) noexcept : elem(e), leaf(&l), key(k) {}
        lower_bound_res() noexcept : elem(nullptr), leaf(nullptr), key(0) {}
    };
    using allocate_res = std::pair<T*, bool>;
    using clone_res = std::pair<node_head*, std::exception_ptr>;
    struct node_head {
        node_head_ptr* _backref;
        // Prefix for squashed nodes
        key_t _prefix;
        const layout _base_layout;
        // Number of keys on the node
        uint8_t _size;
        // How many slots are there. Used only by direct dynamic nodes
        const uint8_t _capacity;
        node_head() noexcept : _backref(nullptr), _prefix(0), _base_layout(layout::nil), _size(0), _capacity(0) {}
        node_head(key_t prefix, layout lt, uint8_t capacity) noexcept
                : _backref(nullptr)
                , _prefix(prefix)
                , _base_layout(lt)
                , _size(0)
                , _capacity(capacity) {}
        node_head(node_head&& o) noexcept
                : _backref(std::exchange(o._backref, nullptr))
                , _prefix(o._prefix)
                , _base_layout(o._base_layout)
                , _size(std::exchange(o._size, 0))
                , _capacity(o._capacity) {
            if (_backref != nullptr) {
                *_backref = this;
            }
        }
        node_head(const node_head&) = delete;
        ~node_head() { assert(_size == 0); }
        template <typename NBT>
        NBT& as_base() noexcept {
            return *boost::intrusive::get_parent_from_member(this, &NBT::_head);
        }
        template <typename NBT>
        const NBT& as_base() const noexcept {
            return *boost::intrusive::get_parent_from_member(this, &NBT::_head);
        }
        template <typename NT>
        typename NT::node_type& as_base_of() noexcept {
            return as_base<typename NT::node_type>();
        }
        template <typename NT>
        const typename NT::node_type& as_base_of() const noexcept {
            return as_base<typename NT::node_type>();
        }
        template <typename NT>
        NT& as_node() noexcept {
            return *boost::intrusive::get_parent_from_member(&as_base_of<NT>(), &NT::_base);
        }
        template <typename NT>
        const NT& as_node() const noexcept {
            return *boost::intrusive::get_parent_from_member(&as_base_of<NT>(), &NT::_base);
        }
        // Construct a key from leaf node prefix and index
        key_t key_of(node_index_t ni) const noexcept {
            return (_prefix & prefix_mask) + ni;
        }
        // Prefix manipulations
        unsigned prefix_len() const noexcept { return _prefix & prefix_len_mask; }
        void trim_prefix(unsigned v) noexcept { _prefix -= v; }
        void bump_prefix(unsigned v) noexcept { _prefix += v; }
        bool check_prefix(key_t key, unsigned& depth) const noexcept {
            unsigned real_depth = depth + prefix_len();
            key_t mask = prefix_mask_at(real_depth);
            if ((key & mask) != (_prefix & mask)) {
                return false;
            }
            depth = real_depth;
            return true;
        }
        const T* get(key_t key, unsigned depth) const noexcept {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().get(key, depth);
            } else {
                return as_base_of<inner_node>().get(key, depth);
            }
        }
        lower_bound_res lower_bound(key_t key, unsigned depth) const noexcept {
            unsigned real_depth = depth + prefix_len();
            key_t mask = prefix_mask_at(real_depth);
            if ((key & mask) > (_prefix & mask)) {
                return lower_bound_res();
            }
            depth = real_depth;
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().lower_bound(key, depth);
            } else {
                return as_base_of<inner_node>().lower_bound(key, depth);
            }
        }
        allocate_res alloc(key_t key, unsigned depth) {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().alloc(key, depth);
            } else {
                return as_base_of<inner_node>().alloc(key, depth);
            }
        }
        erase_result erase(key_t key, unsigned depth, erase_mode erm) noexcept {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().erase(key, depth, erm);
            } else {
                return as_base_of<inner_node>().erase(key, depth, erm);
            }
        }
        template <typename Fn>
        erase_result weed(Fn&& filter, unsigned depth) {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().weed(filter, depth);
            } else {
                return as_base_of<inner_node>().weed(filter, depth);
            }
        }
        node_head* grow(key_t key, unsigned depth) {
            node_index_t ni = node_index(key, depth);
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().template grow<leaf_node>(ni);
            } else {
                return as_base_of<inner_node>().template grow<inner_node>(ni);
            }
        }
        node_head* shrink(unsigned depth) {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().template shrink<leaf_node>();
            } else {
                return as_base_of<inner_node>().template shrink<inner_node>();
            }
        }
        template <typename Visitor>
        bool visit(Visitor&& v, unsigned depth) const {
            bool ret = true;
            depth += prefix_len();
            if (v(*this, depth, true)) {
                if (depth == leaf_depth) {
                    ret = as_base_of<leaf_node>().visit(v, depth);
                } else {
                    ret = as_base_of<inner_node>().visit(v, depth);
                }
                v(*this, depth, false);
            }
            return ret;
        }
        template <typename Fn>
        clone_res clone(Fn&& cloner, unsigned depth) const noexcept {
            depth += prefix_len();
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().template clone<leaf_node, Fn>(cloner, depth);
            } else {
                return as_base_of<inner_node>().template clone<inner_node, Fn>(cloner, depth);
            }
        }
        void free(unsigned depth) noexcept {
            if (depth == leaf_depth) {
                leaf_node::free(as_node<leaf_node>());
            } else {
                inner_node::free(as_node<inner_node>());
            }
        }
        size_t node_size(unsigned depth) const noexcept {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().node_size();
            } else {
                return as_base_of<inner_node>().node_size();
            }
        }
        lower_bound_res lower_bound(key_t key) const noexcept {
            return as_base_of<leaf_node>().lower_bound(key, leaf_depth);
        }
        void set_lower(node_index_t ni, node_head* n) noexcept {
            as_node<inner_node>().set_lower(ni, n);
        }
        node_head_ptr pop_lower() noexcept {
            return as_node<inner_node>().pop_lower();
        }
    };
    class node_head_ptr {
        node_head* _v;
    public:
        node_head_ptr(node_head* v) noexcept : _v(v) {}
        node_head_ptr(const node_head_ptr&) = delete;
        node_head_ptr(node_head_ptr&& o) noexcept : _v(std::exchange(o._v, nullptr)) {
            if (_v != nullptr) {
                _v->_backref = this;
            }
        }
        node_head& operator*() const noexcept { return *_v; }
        node_head* operator->() const noexcept { return _v; }
        node_head* raw() const noexcept { return _v; }
        operator bool() const noexcept { return _v != nullptr; }
        bool is(const node_head& n) const noexcept { return _v == &n; }
        node_head_ptr& operator=(node_head* v) noexcept {
            _v = v;
            // Checking (_v != &nil_root) is not needed for correctness, since
            // nil_root's _backref is never read anyway. But we do this check for
            // performance reasons: since nil_root is shared between shards,
            // writing to it would cause serious cache contention.
            if (_v != nullptr && _v != &nil_root) {
                _v->_backref = this;
            }
            return *this;
        }
    };
    template <typename Slot, typename... Layouts>
    struct node_base {
        node_head _head;
        variadic_union<Layouts...> _layouts;
        template <typename Tx>
        static size_t node_size(layout lt, uint8_t capacity) noexcept {
            return sizeof(node_head) + Tx::layout_size(capacity);
        }
        template <typename Tx, typename Ty, typename... Ts>
        static size_t node_size(layout lt, uint8_t capacity) noexcept {
            return lt == Tx::this_layout ? sizeof(node_head) + Tx::layout_size(capacity) : node_size<Ty, Ts...>(lt, capacity);
        }
        static size_t node_size(layout lt, uint8_t capacity) noexcept {
            return node_size<Layouts...>(lt, capacity);
        }
        size_t node_size() const noexcept {
            return node_size(_head._base_layout, _head._capacity);
        }
        // construct
        template <typename Tx>
        void construct(variadic_union<Tx>& cur) noexcept {
            new (&cur._this) Tx(_head);
        }
        template <typename Tx, typename Ty, typename... Ts>
        void construct(variadic_union<Tx, Ty, Ts...>& cur) noexcept {
            if (_head._base_layout == Tx::this_layout) {
                new (&cur._this) Tx(_head);
                return;
            }
            construct<Ty, Ts...>(cur._other);
        }
        node_base(key_t prefix, layout lt, uint8_t capacity) noexcept
                : _head(prefix, lt, capacity) {
            construct<Layouts...>(_layouts);
        }
        node_base(const node_base&) = delete;
        template <typename Tx>
        void move_construct(variadic_union<Tx>& cur, variadic_union<Tx>&& o) noexcept {
            new (&cur._this) Tx(std::move(o._this), _head);
        }
        template <typename Tx, typename Ty, typename... Ts>
        void move_construct(variadic_union<Tx, Ty, Ts...>& cur, variadic_union<Tx, Ty, Ts...>&& o) noexcept {
            if (_head._base_layout == Tx::this_layout) {
                new (&cur._this) Tx(std::move(o._this), _head);
                return;
            }
            move_construct<Ty, Ts...>(cur._other, std::move(o._other));
        }
        node_base(node_base&& o) noexcept
                : _head(std::move(o._head)) {
            move_construct<Layouts...>(_layouts, std::move(o._layouts));
        }
        ~node_base() { }
        // get value by key
        template <typename Tx>
        const T* get(const variadic_union<Tx>& cur, key_t key, unsigned depth) const noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.get(_head, key, depth);
            }
            return (const T*)nullptr;
        }
        template <typename Tx, typename Ty, typename... Ts>
        const T* get(const variadic_union<Tx, Ty, Ts...>& cur, key_t key, unsigned depth) const noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.get(_head, key, depth);
            }
            return get<Ty, Ts...>(cur._other, key, depth);
        }
        const T* get(key_t key, unsigned depth) const noexcept {
            return get<Layouts...>(_layouts, key, depth);
        }
        // finds a lowed-bound element
        template <typename Tx>
        lower_bound_res lower_bound(const variadic_union<Tx>& cur, key_t key, unsigned depth) const noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.lower_bound(_head, key, depth);
            }
            return lower_bound_res();
        }
        template <typename Tx, typename Ty, typename... Ts>
        lower_bound_res lower_bound(const variadic_union<Tx, Ty, Ts...>& cur, key_t key, unsigned depth) const noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.lower_bound(_head, key, depth);
            }
            return lower_bound<Ty, Ts...>(cur._other, key, depth);
        }
        lower_bound_res lower_bound(key_t key, unsigned depth) const noexcept {
            return lower_bound<Layouts...>(_layouts, key, depth);
        }
        // erase by key
        template <typename Tx>
        erase_result erase(variadic_union<Tx>& cur, key_t key, unsigned depth, erase_mode erm) noexcept {
            return cur._this.erase(_head, key, depth, erm);
        }
        template <typename Tx, typename Ty, typename... Ts>
        erase_result erase(variadic_union<Tx, Ty, Ts...>& cur, key_t key, unsigned depth, erase_mode erm) noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.erase(_head, key, depth, erm);
            }
            return erase<Ty, Ts...>(cur._other, key, depth, erm);
        }
        erase_result erase(key_t key, unsigned depth, erase_mode erm) noexcept {
            return erase<Layouts...>(_layouts, key, depth, erm);
        }
        // weed values with filter
        template <typename Fn, typename Tx>
        erase_result weed(variadic_union<Tx>& cur, Fn&& filter, unsigned depth) {
            return cur._this.weed(_head, filter, _head._prefix, depth);
        }
        template <typename Fn, typename Tx, typename Ty, typename... Ts>
        erase_result weed(variadic_union<Tx, Ty, Ts...>& cur, Fn&& filter, unsigned depth) {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.weed(_head, filter, _head._prefix, depth);
            }
            return weed<Fn, Ty, Ts...>(cur._other, filter, depth);
        }
        template <typename Fn>
        erase_result weed(Fn&& filter, unsigned depth) {
            return weed<Fn, Layouts...>(_layouts, filter, depth);
        }
        // allocate new slot
        template <typename Tx>
        allocate_res alloc(variadic_union<Tx>& cur, key_t key, unsigned depth) {
            return cur._this.alloc(_head, key, depth);
        }
        template <typename Tx, typename Ty, typename... Ts>
        allocate_res alloc(variadic_union<Tx, Ty, Ts...>& cur, key_t key, unsigned depth) {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.alloc(_head, key, depth);
            }
            return alloc<Ty, Ts...>(cur._other, key, depth);
        }
        allocate_res alloc(key_t key, unsigned depth) {
            return alloc<Layouts...>(_layouts, key, depth);
        }
        // append slot to node
        template <typename Tx>
        void append(variadic_union<Tx>& cur, node_index_t ni, Slot&& val) noexcept {
            cur._this.append(_head, ni, std::move(val));
        }
        template <typename Tx, typename Ty, typename... Ts>
        void append(variadic_union<Tx, Ty, Ts...>& cur, node_index_t ni, Slot&& val) noexcept {
            if (_head._base_layout == Tx::this_layout) {
                cur._this.append(_head, ni, std::move(val));
                return;
            }
            append<Ty, Ts...>(cur._other, ni, std::move(val));
        }
        void append(node_index_t ni, Slot&& val) noexcept {
            return append<Layouts...>(_layouts, ni, std::move(val));
        }
        // find and remove some element (usually the last one)
        template <typename Tx>
        Slot pop(variadic_union<Tx>& cur) noexcept {
            return cur._this.pop(_head);
        }
        template <typename Tx, typename Ty, typename... Ts>
        Slot pop(variadic_union<Tx, Ty, Ts...>& cur) noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.pop(_head);
            }
            return pop<Ty, Ts...>(cur._other);
        }
        Slot pop() noexcept {
            return pop<Layouts...>(_layouts);
        }
        // visiting
        template <typename Visitor, typename Tx>
        bool visit(const variadic_union<Tx>& cur, Visitor&& v, unsigned depth) const {
            return cur._this.visit(_head, v, depth);
        }
        template <typename Visitor, typename Tx, typename Ty, typename... Ts>
        bool visit(const variadic_union<Tx, Ty, Ts...>& cur, Visitor&& v, unsigned depth) const {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.visit(_head, v, depth);
            }
            return visit<Visitor, Ty, Ts...>(cur._other, v, depth);
        }
        template <typename Visitor>
        bool visit(Visitor&& v, unsigned depth) const {
            return visit<Visitor, Layouts...>(_layouts, v, depth);
        }
        // cloning
        template <typename NT, typename Fn, typename Tx>
        clone_res clone(const variadic_union<Tx>& cur, Fn&& cloner, unsigned depth) const noexcept {
            return cur._this.template clone<NT, Fn>(_head, cloner, depth);
        }
        template <typename NT, typename Fn, typename Tx, typename Ty, typename... Ts>
        clone_res clone(const variadic_union<Tx, Ty, Ts...>& cur, Fn&& cloner, unsigned depth) const noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.template clone<NT, Fn>(_head, cloner, depth);
            }
            return clone<NT, Fn, Ty, Ts...>(cur._other, cloner, depth);
        }
        template <typename NT, typename Fn>
        clone_res clone(Fn&& cloner, unsigned depth) const noexcept {
            return clone<NT, Fn, Layouts...>(_layouts, cloner, depth);
        }
        // growing into larger layout
        template <typename NT, typename Tx>
        node_head* grow(variadic_union<Tx>& cur, node_index_t want_ni) {
            if constexpr (Tx::growable) {
                return cur._this.template grow<NT>(_head, want_ni);
            }
            std::abort();
        }
        template <typename NT, typename Tx, typename Ty, typename... Ts>
        node_head* grow(variadic_union<Tx, Ty, Ts...>& cur, node_index_t want_ni) {
            if constexpr (Tx::growable) {
                if (_head._base_layout == Tx::this_layout) {
                    return cur._this.template grow<NT>(_head, want_ni);
                }
            }
            return grow<NT, Ty, Ts...>(cur._other, want_ni);
        }
        template <typename NT>
        node_head* grow(node_index_t want_ni) {
            return grow<NT, Layouts...>(_layouts, want_ni);
        }
        // shrinking into smaller layout
        template <typename NT, typename Tx>
        node_head* shrink(variadic_union<Tx>& cur) {
            if constexpr (Tx::shrinkable) {
                return cur._this.template shrink<NT>(_head);
            }
            std::abort();
        }
        template <typename NT, typename Tx, typename Ty, typename... Ts>
        node_head* shrink(variadic_union<Tx, Ty, Ts...>& cur) {
            if constexpr (Tx::shrinkable) {
                if (_head._base_layout == Tx::this_layout) {
                    return cur._this.template shrink<NT>(_head);
                }
            }
            return shrink<NT, Ty, Ts...>(cur._other);
        }
        template <typename NT>
        node_head* shrink() {
            return shrink<NT, Layouts...>(_layouts);
        }
    };
    template <typename Slot, layout Layout, layout GrowInto, unsigned GrowThreshold, layout ShrinkInto, unsigned ShrinkThreshold>
    struct direct_layout {
        static constexpr bool shrinkable = ShrinkInto != layout::nil;
        static constexpr bool growable = GrowInto != layout::nil;
        static constexpr layout this_layout = Layout;
        static bool check_capacity(const node_head& head, node_index_t ni) noexcept {
            if constexpr (this_layout == layout::direct_static) {
                return true;
            } else {
                return ni < head._capacity;
            }
        }
        static unsigned capacity(const node_head& head) noexcept {
            if constexpr (this_layout == layout::direct_static) {
                return node_index_limit;
            } else {
                return head._capacity;
            }
        }
        struct array_of_non_node_head_ptr {
            std::bitset<node_index_limit> _present;
            Slot _slots[0];
            array_of_non_node_head_ptr(const node_head& head) noexcept {
                _present.reset();
            }
            array_of_non_node_head_ptr(array_of_non_node_head_ptr&& o, const node_head& head) noexcept
                    : _present(std::move(o._present)) {
                for (unsigned i = 0; i < capacity(head); i++) {
                    if (o.has(i)) {
                        new (&_slots[i]) Slot(std::move(o._slots[i]));
                        o._slots[i].~Slot();
                    }
                }
            }
            array_of_non_node_head_ptr(const array_of_non_node_head_ptr&) = delete;
            bool has(unsigned i) const noexcept { return _present.test(i); }
            bool has(const node_head& h, unsigned i) const noexcept { return has(i); }
            void add(node_head& head, unsigned i) noexcept { _present.set(i); }
            void del(node_head& head, unsigned i) noexcept { _present.set(i, false); }
            unsigned count(const node_head& head) const noexcept { return _present.count(); }
        };
        struct array_of_node_head_ptr {
            Slot _slots[0];
            array_of_node_head_ptr(const node_head& head) noexcept {
                for (unsigned i = 0; i < capacity(head); i++) {
                    new (&_slots[i]) node_head_ptr(nullptr);
                }
            }
            array_of_node_head_ptr(array_of_node_head_ptr&& o, const node_head& head) noexcept {
                for (unsigned i = 0; i < capacity(head); i++) {
                    new (&_slots[i]) Slot(std::move(o._slots[i]));
                    o._slots[i].~Slot();
                }
            }
            array_of_node_head_ptr(const array_of_node_head_ptr&) = delete;
            bool has(unsigned i) const noexcept { return _slots[i]; }
            bool has(const node_head& h, unsigned i) const noexcept { return check_capacity(h, i) && _slots[i]; }
            void add(node_head& head, unsigned i) noexcept { head._size++; }
            void del(node_head& head, unsigned i) noexcept { head._size--; }
            unsigned count(const node_head& head) const noexcept { return head._size; }
        };
        using array_of_slot = std::conditional_t<std::is_same_v<Slot, node_head_ptr>, array_of_node_head_ptr, array_of_non_node_head_ptr>;
        array_of_slot _data;
        direct_layout(const node_head& head) noexcept : _data(head) {}
        direct_layout(direct_layout&& o, const node_head& head) noexcept : _data(std::move(o._data), head) {}
        direct_layout(const direct_layout&) = delete;
        const T* get(const node_head& head, key_t key, unsigned depth) const noexcept {
            node_index_t ni = node_index(key, depth);
            if (!_data.has(head, ni)) {
                return nullptr;
            }
            return get_at(_data._slots[ni], key, depth + 1);
        }
        Slot pop(node_head& head) noexcept {
            for (unsigned i = 0; i < capacity(head); i++) {
                if (_data.has(i)) {
                    Slot ret = std::move(_data._slots[i]);
                    _data.del(head, i);
                    _data._slots[i].~Slot();
                    return ret;
                }
            }
            return nullptr;
        }
        allocate_res alloc(node_head& head, key_t key, unsigned depth) {
            node_index_t ni = node_index(key, depth);
            if (!check_capacity(head, ni)) {
                return allocate_res(nullptr, false);
            }
            bool exists = _data.has(ni);
            if (!exists) {
                populate_slot(_data._slots[ni], key, depth + 1);
                _data.add(head, ni);
            }
            return allocate_on(_data._slots[ni], key, depth + 1, !exists);
        }
        void append(node_head& head, node_index_t ni, Slot&& val) noexcept {
            assert(check_capacity(head, ni));
            assert(!_data.has(ni));
            _data.add(head, ni);
            new (&_data._slots[ni]) Slot(std::move(val));
        }
        erase_result erase(node_head& head, key_t key, unsigned depth, erase_mode erm) noexcept {
            node_index_t ni = node_index(key, depth);
            if (_data.has(head, ni)) {
                if (erase_from_slot(&_data._slots[ni], key, depth + 1, erm)) {
                    _data.del(head, ni);
                    return after_drop<ShrinkThreshold>(_data.count(head));
                }
            }
            return erase_result::nothing;
        }
        template <typename Fn>
        erase_result weed(node_head& head, Fn&& filter, key_t pfx, unsigned depth) {
            bool removed_something = false;
            for (unsigned i = 0; i < capacity(head); i++) {
                if (_data.has(i)) {
                    if (weed_from_slot(head, i, &_data._slots[i], filter, depth + 1)) {
                        _data.del(head, i);
                        removed_something = true;
                    }
                }
            }
            return removed_something ? after_drop<ShrinkThreshold>(_data.count(head)) : erase_result::nothing;
        }
        template <typename NT, typename Cloner>
        clone_res clone(const node_head& head, Cloner&& clone, unsigned depth) const noexcept {
            NT* nn;
            try {
                nn = NT::allocate(head._prefix, head._base_layout, head._capacity);
            } catch (...) {
                return clone_res(nullptr, std::current_exception());
            }
            auto ex = copy_slots(head, _data._slots, capacity(head), depth, nn->_base,
                        [this] (unsigned i) noexcept { return _data.has(i) ? i : unused_node_index; }, clone);
            return std::make_pair(&nn->_base._head, std::move(ex));
        }
        template <typename NT>
        node_head* grow(node_head& head, node_index_t want_ni) {
            static_assert(GrowInto == layout::direct_dynamic && GrowThreshold == 0);
            uint8_t next_cap = head._capacity << 1;
            while (want_ni >= next_cap) {
                next_cap <<= 1;
            }
            assert(next_cap > head._capacity);
            NT* nn = NT::allocate(head._prefix, layout::direct_dynamic, next_cap);
            move_slots(_data._slots, head._capacity, head._capacity + 1, nn->_base,
                    [this] (unsigned i) noexcept { return _data.has(i) ? i : unused_node_index; });
            head._size = 0;
            return &nn->_base._head;
        }
        template <typename NT>
        node_head* shrink(node_head& head) {
            static_assert(shrinkable && ShrinkThreshold != 0);
            NT* nn = NT::allocate(head._prefix, ShrinkInto);
            move_slots(_data._slots, node_index_limit, ShrinkThreshold, nn->_base,
                    [this] (unsigned i) noexcept { return _data.has(i) ? i : unused_node_index; });
            head._size = 0;
            return &nn->_base._head;
        }
        lower_bound_res lower_bound(const node_head& head, key_t key, unsigned depth) const noexcept {
            node_index_t ni = node_index(key, depth);
            if (_data.has(head, ni)) {
                lower_bound_res ret = lower_bound_at(&_data._slots[ni], head, ni, key, depth);
                if (ret.elem != nullptr) {
                    return ret;
                }
            }
            for (unsigned i = ni + 1; i < capacity(head); i++) {
                if (_data.has(i)) {
                    return lower_bound_at(&_data._slots[i], head, i, 0, depth);
                }
            }
            return lower_bound_res();
        }
        template <typename Visitor>
        bool visit(const node_head& head, Visitor&& v, unsigned depth) const {
            for (unsigned i = 0; i < capacity(head); i++) {
                if (_data.has(i)) {
                    if (!visit_slot(v, head, i, &_data._slots[i], depth)) {
                        return false;
                    }
                }
            }
            return true;
        }
        static size_t layout_size(uint8_t capacity) noexcept {
            if constexpr (this_layout == layout::direct_static) {
                return sizeof(direct_layout) + node_index_limit * sizeof(Slot);
            } else {
                assert(capacity != 0);
                return sizeof(direct_layout) + capacity * sizeof(Slot);
            }
        }
    };
    template <typename Slot, layout Layout, unsigned Size, layout GrowInto, unsigned GrowThreshold, layout ShrinkInto, unsigned ShrinkThreshold>
    struct indirect_layout {
        static constexpr bool shrinkable = ShrinkInto != layout::nil;
        static constexpr bool growable = GrowInto != layout::nil;
        static constexpr unsigned size = Size;
        static constexpr layout this_layout = Layout;
        node_index_t _idx[Size];
        Slot _slots[0];
        bool has(unsigned i) const noexcept { return _idx[i] != unused_node_index; }
        void unset(unsigned i) noexcept { _idx[i] = unused_node_index; }
        indirect_layout(const node_head& head) noexcept {
            for (unsigned i = 0; i < Size; i++) {
                _idx[i] = unused_node_index;
            }
        }
        indirect_layout(indirect_layout&& o, const node_head& head) noexcept {
            for (unsigned i = 0; i < Size; i++) {
                _idx[i] = o._idx[i];
                if (o.has(i)) {
                    new (&_slots[i]) Slot(std::move(o._slots[i]));
                    o._slots[i].~Slot();
                }
            }
        }
        indirect_layout(const indirect_layout&) = delete;
        const T* get(const node_head& head, key_t key, unsigned depth) const noexcept {
            node_index_t ni = node_index(key, depth);
            unsigned i = find_in_array<Size>(ni, _idx);
            if (i >= Size) {
                return nullptr;
            }
            return get_at(_slots[i], key, depth + 1);
        }
        Slot pop(node_head& head) noexcept {
            for (unsigned i = 0; i < Size; i++) {
                if (has(i)) {
                    Slot ret = std::move(_slots[i]);
                    head._size--;
                    _slots[i].~Slot();
                    return ret;
                }
            }
            return nullptr;
        }
        allocate_res alloc(node_head& head, key_t key, unsigned depth) {
            node_index_t ni = node_index(key, depth);
            bool new_slot = false;
            unsigned i = find_in_array<Size>(ni, _idx);
            if (i >= Size) {
                i = find_in_array<Size>(unused_node_index, _idx);
                if (i >= Size) {
                    return allocate_res(nullptr, false);
                }
                populate_slot(_slots[i], key, depth + 1);
                _idx[i] = ni;
                head._size++;
                new_slot = true;
            }
            return allocate_on(_slots[i], key, depth + 1, new_slot);
        }
        void append(node_head& head, node_index_t ni, Slot&& val) noexcept {
            unsigned i = head._size++;
            assert(i < Size);
            assert(_idx[i] == unused_node_index);
            _idx[i] = ni;
            new (&_slots[i]) Slot(std::move(val));
        }
        erase_result erase(node_head& head, key_t key, unsigned depth, erase_mode erm) noexcept {
            node_index_t ni = node_index(key, depth);
            unsigned i = find_in_array<Size>(ni, _idx);
            if (i < Size) {
                if (erase_from_slot(&_slots[i], key, depth + 1, erm)) {
                    unset(i);
                    head._size--;
                    return after_drop<ShrinkThreshold>(head._size);
                }
            }
            return erase_result::nothing;
        }
        template <typename Fn>
        erase_result weed(node_head& head, Fn&& filter, key_t pfx, unsigned depth) {
            bool removed_something = false;
            for (unsigned i = 0; i < Size; i++) {
                if (has(i)) {
                    if (weed_from_slot(head, _idx[i], &_slots[i], filter, depth + 1)) {
                        unset(i);
                        head._size--;
                        removed_something = true;
                    }
                }
            }
            return removed_something ? after_drop<ShrinkThreshold>(head._size) : erase_result::nothing;
        }
        template <typename NT, typename Cloner>
        clone_res clone(const node_head& head, Cloner&& clone, unsigned depth) const noexcept {
            NT* nn;
            try {
                nn = NT::allocate(head._prefix, head._base_layout, head._capacity);
            } catch (...) {
                return clone_res(nullptr, std::current_exception());
            }
            auto ex = copy_slots(head, _slots, Size, depth, nn->_base, [this] (unsigned i) noexcept { return _idx[i]; }, clone);
            return std::make_pair(&nn->_base._head, std::move(ex));
        }
        template <typename NT>
        node_head* grow(node_head& head, node_index_t want_ni) {
            static_assert(growable && GrowThreshold == 0);
            NT* nn = NT::allocate(head._prefix, GrowInto);
            move_slots(_slots, Size, Size + 1, nn->_base, [this] (unsigned i) noexcept { return _idx[i]; });
            head._size = 0;
            return &nn->_base._head;
        }
        template <typename NT>
        node_head* shrink(node_head& head) {
            static_assert(shrinkable && ShrinkThreshold != 0);
            NT* nn = NT::allocate(head._prefix, ShrinkInto);
            move_slots(_slots, Size, ShrinkThreshold, nn->_base, [this] (unsigned i) noexcept { return _idx[i]; });
            head._size = 0;
            return &nn->_base._head;
        }
        lower_bound_res lower_bound(const node_head& head, key_t key, unsigned depth) const noexcept {
            node_index_t ni = node_index(key, depth);
            unsigned i = find_in_array<Size>(ni, _idx);
            if (i < Size) {
                lower_bound_res ret = lower_bound_at(&_slots[i], head, _idx[i], key, depth);
                if (ret.elem != nullptr) {
                    return ret;
                }
            }
            unsigned ui = Size;
            for (unsigned i = 0; i < Size; i++) {
                if (has(i) && _idx[i] > ni && (ui == Size || _idx[i] < _idx[ui])) {
                    ui = i;
                }
            }
            if (ui == Size) {
                return lower_bound_res();
            }
            // See comment in direct_layout about the zero key argument
            return lower_bound_at(&_slots[ui], head, _idx[ui], 0, depth);
        }
        template <typename Visitor>
        bool visit(const node_head& head, Visitor&& v, unsigned depth) const {
            if (head._size == 0) {
                return true;
            }
            if (head._size == 1 && has(0)) {
                return visit_slot(v, head, _idx[0], &_slots[0], depth);
            }
            unsigned indices[Size];
            unsigned sz = 0;
            for (unsigned i = 0; i < Size; i++) {
                if (has(i)) {
                    indices[sz++] = i;
                }
            }
            if (v.sorted) {
                std::sort(indices, indices + sz, [this] (int a, int b) {
                    return _idx[a] < _idx[b];
                });
            }
            for (unsigned i = 0; i < sz; i++) {
                unsigned pos = indices[i];
                if (!visit_slot(v, head, _idx[pos], &_slots[pos], depth)) {
                    return false;
                }
            }
            return true;
        }
        static size_t layout_size(uint8_t capacity) noexcept { return sizeof(indirect_layout) + size * sizeof(Slot); }
    };
    template <typename SlotType, typename FN>
    static void move_slots(SlotType* slots, unsigned nr, unsigned thresh, auto& into, FN&& node_index_of) noexcept {
        unsigned count = 0;
        for (unsigned i = 0; i < nr; i++) {
            node_index_t ni = node_index_of(i);
            if (ni != unused_node_index) {
                into.append(ni, std::move(slots[i]));
                slots[i].~SlotType();
                if (++count >= thresh) {
                    break;
                }
            }
        }
    }
    template <typename FN, typename Cloner>
    static std::exception_ptr copy_slots(const node_head& h, const T* slots, unsigned nr, unsigned depth, auto& into, FN&& node_index_of, Cloner&& cloner) noexcept {
        for (unsigned i = 0; i < nr; i++) {
            node_index_t ni = node_index_of(i);
            if (ni != unused_node_index) {
                try {
                    into.append(ni, cloner(h.key_of(ni), slots[i]));
                } catch (...) {
                    return std::current_exception();
                }
            }
        }
        return nullptr;
    }
    template <typename FN, typename Cloner>
    static std::exception_ptr copy_slots(const node_head& h, const node_head_ptr* slots, unsigned nr, unsigned depth, auto& into, FN&& node_index_of, Cloner&& cloner) noexcept {
        for (unsigned i = 0; i < nr; i++) {
            node_index_t ni = node_index_of(i);
            if (ni != unused_node_index) {
                clone_res res = slots[i]->clone(cloner, depth + 1);
                if (res.first != nullptr) {
                    into.append(ni, std::move(res.first));
                }
                if (res.second) {
                    return res.second;
                }
            }
        }
        return nullptr;
    }
    static node_head* expand(node_head& n, key_t key, unsigned& depth) {
        key_t n_prefix = n._prefix;
        unsigned plen = common_prefix_len(key, n_prefix);
        assert(plen >= depth);
        plen -= depth;
        depth += plen;
        assert(n.prefix_len() > plen);
        node_index_t ni = node_index(n_prefix, depth);
        node_head* nn = inner_node::allocate_initial(make_prefix(key, plen), ni);
        // Trim all common nodes + nn one from n
        n.trim_prefix(plen + 1);
        nn->set_lower(ni, &n);
        return nn;
    }
    static node_head* squash(node_head* n, unsigned depth) noexcept {
        const node_head_ptr np = n->pop_lower();
        node_head* kid = np.raw();
        assert(kid != nullptr);
        // Kid has n and it's prefix squashed
        kid->bump_prefix(n->prefix_len() + 1);
        return kid;
    }
    static bool maybe_drop_from(node_head_ptr* np, erase_result res, unsigned depth) noexcept {
        node_head* n = np->raw();
        switch (res) {
        case erase_result::empty:
            n->free(depth);
            *np = nullptr;
            return true;
        case erase_result::squash:
            if (depth != leaf_depth) {
                *np = squash(n, depth);
                n->free(depth);
            }
            break;
        case erase_result::shrink:
            try {
                *np = n->shrink(depth);
                n->free(depth);
            } catch(...) {
            }
            break;
        case erase_result::nothing: ; // make compiler happy
        }
        return false;
    }
    static const T* get_at(const T& val, key_t key, unsigned depth) noexcept { return &val; }
    static const T* get_at(const node_head_ptr& np, key_t key, unsigned depth = 0) noexcept {
        if (!np->check_prefix(key, depth)) {
            return nullptr;
        }
        return np->get(key, depth);
    }
    static allocate_res allocate_on(T& val, key_t key, unsigned depth, bool allocated) noexcept {
        return allocate_res(&val, allocated);
    }
    static allocate_res allocate_on(node_head_ptr& n, key_t key, unsigned depth = 0, bool _ = false) {
        if (!n->check_prefix(key, depth)) {
            n = expand(*n, key, depth);
        }
        allocate_res ret = n->alloc(key, depth);
        if (ret.first == nullptr) {
            node_head* nn = n->grow(key, depth);
            n->free(depth);
            n = nn;
            ret = nn->alloc(key, depth);
            assert(ret.first != nullptr);
        }
        return ret;
    }
    // Populating value slot happens in tree::emplace
    static void populate_slot(T& val, key_t key, unsigned depth) noexcept { }
    static void populate_slot(node_head_ptr& np, key_t key, unsigned depth) {
        assert(leaf_depth >= depth);
        np = leaf_node::allocate_initial(make_prefix(key, leaf_depth - depth));
    }
    template <typename Visitor>
    static bool visit_slot(Visitor&& v, const node_head& n, node_index_t ni, const T* val, unsigned depth) {
        return v(n.key_of(ni), *val);
    }
    template <typename Visitor>
    static bool visit_slot(Visitor&& v, const node_head& n, node_index_t, const node_head_ptr* ptr, unsigned depth) {
        return (*ptr)->visit(v, depth + 1);
    }
    static lower_bound_res lower_bound_at(const T* val, const node_head& n, node_index_t ni, key_t, unsigned) noexcept {
        return lower_bound_res(val, n, n.key_of(ni));
    }
    static lower_bound_res lower_bound_at(const node_head_ptr* ptr, const node_head&, node_index_t, key_t key, unsigned depth) noexcept {
        return (*ptr)->lower_bound(key, depth + 1);
    }
    template <typename Fn>
    static bool weed_from_slot(node_head& n, node_index_t ni, T* val, Fn&& filter, unsigned depth) {
        if (!filter(n.key_of(ni), *val)) {
            return false;
        }
        val->~T();
        return true;
    }
    template <typename Fn>
    static bool weed_from_slot(node_head&, node_index_t, node_head_ptr* np, Fn&& filter, unsigned depth) {
        return weed_from_slot(np, filter, depth);
    }
    template <typename Fn>
    static bool weed_from_slot(node_head_ptr* np, Fn&& filter, unsigned depth) {
        node_head* n = np->raw();
        depth += n->prefix_len();
        erase_result er = n->weed(filter, depth);
        // FIXME -- after weed the node might want to shrink into
        // even smaller, than just previous, layout
        return maybe_drop_from(np, er, depth);
    }
    static bool erase_from_slot(T* val, key_t key, unsigned depth, erase_mode erm) noexcept {
        if (erm == erase_mode::real) {
            val->~T();
        }
        return true;
    }
    static bool erase_from_slot(node_head_ptr* np, key_t key, unsigned depth, erase_mode erm) noexcept {
        node_head* n = np->raw();
        assert(n->check_prefix(key, depth));
        erase_result er = n->erase(key, depth, erm);
        if (erm == erase_mode::cleanup) {
            return false;
        }
        return maybe_drop_from(np, er, depth);
    }
    template <typename Visitor>
    void visit(Visitor&& v) const {
        if (!_root.is(nil_root)) {
            _root->visit(std::move(v), 0);
        }
    }
    template <typename Visitor>
    void visit(Visitor&& v) {
        struct adaptor {
            Visitor&& v;
            bool sorted;
            bool operator()(key_t key, const T& val) {
                return v(key, const_cast<T&>(val));
            }
            bool operator()(const node_head& n, unsigned depth, bool enter) {
                return v(const_cast<node_head&>(n), depth, enter);
            }
        };
        const_cast<const tree*>(this)->visit(adaptor{std::move(v), v.sorted});
    }
    class leaf_node {
        template <typename A, typename B> friend class printer;
        friend class tree;
        friend class node_head;
        template <typename A, layout L, unsigned S, layout GL, unsigned GT, layout SL, unsigned ST> friend class indirect_layout;
        template <typename A, layout L, layout GL, unsigned GT, layout SL, unsigned ST> friend class direct_layout;
        using tiny_node = indirect_layout<T, layout::indirect_tiny, 4, layout::indirect_small, 0, layout::nil, 0>;
        using small_node = indirect_layout<T, layout::indirect_small, 8, layout::indirect_medium, 0, layout::indirect_tiny, 4>;
        using medium_node = indirect_layout<T, layout::indirect_medium, 16, layout::indirect_large, 0, layout::indirect_small, 8>;
        using large_node = indirect_layout<T, layout::indirect_large, 32, layout::direct_static, 0, layout::indirect_medium, 16>;
        using direct_node = direct_layout<T, layout::direct_static, layout::nil, 0, layout::indirect_large, 32>;
    public:
        using node_type = node_base<T, tiny_node, small_node, medium_node, large_node, direct_node>;
        leaf_node(leaf_node&& other) noexcept : _base(std::move(other._base)) {}
        ~leaf_node() { }
        size_t storage_size() const noexcept {
            return _base.node_size();
        }
    private:
        node_type _base;
        leaf_node(key_t prefix, layout lt, uint8_t capacity) noexcept : _base(prefix, lt, capacity) { }
        leaf_node(const leaf_node&) = delete;
        static node_head* allocate_initial(key_t prefix) {
            return &allocate(prefix, layout::indirect_tiny)->_base._head;
        }
        static leaf_node* allocate(key_t prefix, layout lt, uint8_t capacity = 0) {
            void* mem = current_allocator().alloc<leaf_node>(node_type::node_size(lt, capacity));
            return new (mem) leaf_node(prefix, lt, capacity);
        }
        static void free(leaf_node& node) noexcept {
            node.~leaf_node();
            current_allocator().free(&node, node._base.node_size());
        }
    };
    class inner_node {
        template <typename A, typename B> friend class printer;
        friend class tree;
        friend class node_head;
        template <typename A, layout L, unsigned S, layout GL, unsigned GT, layout SL, unsigned ST> friend class indirect_layout;
        template <typename A, layout L, layout GL, unsigned GT, layout SL, unsigned ST> friend class direct_layout;
        static constexpr uint8_t initial_capacity = 4;
        using dynamic_node = direct_layout<node_head_ptr, layout::direct_dynamic, layout::direct_dynamic, 0, layout::nil, 0>;
    public:
        using node_type = node_base<node_head_ptr, dynamic_node>;
        inner_node(inner_node&& other) noexcept : _base(std::move(other._base)) {}
        ~inner_node() {}
        size_t storage_size() const noexcept {
            return _base.node_size();
        }
    private:
        node_type _base;
        inner_node(key_t prefix, layout lt, uint8_t capacity) noexcept : _base(prefix, lt, capacity) {}
        inner_node(const inner_node&) = delete;
        static node_head* allocate_initial(key_t prefix, node_index_t want_ni) {
            uint8_t capacity = initial_capacity;
            while (want_ni >= capacity) {
                capacity <<= 1;
            }
            return &allocate(prefix, layout::direct_dynamic, capacity)->_base._head;
        }
        static inner_node* allocate(key_t prefix, layout lt, uint8_t capacity = 0) {
            void* mem = current_allocator().alloc<inner_node>(node_type::node_size(lt, capacity));
            return new (mem) inner_node(prefix, lt, capacity);
        }
        static void free(inner_node& node) noexcept {
            node.~inner_node();
            current_allocator().free(&node, node._base.node_size());
        }
        node_head_ptr pop_lower() noexcept {
            return _base.pop();
        }
        void set_lower(node_index_t ni, node_head* n) noexcept {
            _base.append(ni, node_head_ptr(n));
        }
    };
    node_head_ptr _root;
    static inline node_head nil_root;
public:
    tree() noexcept : _root(&nil_root) {}
    ~tree() {
        clear();
    }
    tree(const tree&) = delete;
    tree(tree&& o) noexcept : _root(std::exchange(o._root, &nil_root)) {}
    const T* get(key_t key) const noexcept {
        return get_at(_root, key);
    }
    T* get(key_t key) noexcept {
        return const_cast<T*>(get_at(_root, key));
    }
    const T* lower_bound(key_t key) const noexcept {
        return _root->lower_bound(key, 0).elem;
    }
    T* lower_bound(key_t key) noexcept {
        return const_cast<T*>(const_cast<const tree*>(this)->lower_bound(key));
    }
    template <typename... Args>
    void emplace(key_t key, Args&&... args) {
        if (_root.is(nil_root)) {
            populate_slot(_root, key, 0);
        }
        allocate_res v = allocate_on(_root, key);
        if (!v.second) {
            v.first->~T();
        }
        try {
            new (v.first) T(std::forward<Args>(args)...);
        } catch (...) {
            erase_from_slot(&_root, key, 0, erase_mode::cleanup);
            throw;
        }
    }
    void erase(key_t key) noexcept {
        if (!_root.is(nil_root)) {
            erase_from_slot(&_root, key, 0, erase_mode::real);
            if (!_root) {
                _root = &nil_root;
            }
        }
    }
    void clear() noexcept {
        struct clearing_visitor {
            bool sorted = false;
            bool operator()(key_t key, T& val) noexcept {
                val.~T();
                return true;
            }
            bool operator()(node_head& n, unsigned depth, bool enter) noexcept {
                if (!enter) {
                    n._size = 0;
                    n.free(depth);
                }
                return true;
            }
        };
        visit(clearing_visitor{});
        _root = &nil_root;
    }
    template <typename Cloner>
    requires std::is_invocable_r<T, Cloner, key_t, const T&>::value
    void clone_from(const tree& tree, Cloner&& cloner) {
        assert(_root.is(nil_root));
        if (!tree._root.is(nil_root)) {
            clone_res cres = tree._root->clone(cloner, 0);
            if (cres.first != nullptr) {
                _root = cres.first;
            }
            if (cres.second) {
                clear();
                std::rethrow_exception(cres.second);
            }
        }
    }
    template <typename Fn>
    requires std::is_invocable_r<bool, Fn, key_t, T&>::value
    void weed(Fn&& filter) {
        if (!_root.is(nil_root)) {
            weed_from_slot(&_root, filter, 0);
            if (!_root) {
                _root = &nil_root;
            }
        }
    }
private:
    template <typename Fn, bool Const>
    struct walking_visitor {
            Fn&& fn;
            bool sorted;
            using value_t = std::conditional_t<Const, const T, T>;
            using node_t = std::conditional_t<Const, const node_head, node_head>;
            bool operator()(key_t key, value_t& val) {
                return fn(key, val);
            }
            bool operator()(node_t& n, unsigned depth, bool enter) noexcept {
                return true;
            }
    };
public:
    template <typename Fn>
    requires std::is_invocable_r<bool, Fn, key_t, const T&>::value
    void walk(Fn&& fn, bool sorted = true) const {
        visit(walking_visitor<Fn, true>{std::move(fn), sorted});
    }
    template <typename Fn>
    requires std::is_invocable_r<bool, Fn, key_t, T&>::value
    void walk(Fn&& fn, bool sorted = true) {
        visit(walking_visitor<Fn, false>{std::move(fn), sorted});
    }
    template <bool Const>
    class iterator_base {
    public:
        using iterator_category = std::forward_iterator_tag;
        using value_type = std::conditional_t<Const, const T, T>;
        using difference_type = ssize_t;
        using pointer = value_type*;
        using reference = value_type&;
    private:
        key_t _key = 0;
        pointer _value = nullptr;
        const tree* _tree = nullptr;
        const node_head* _leaf = nullptr;
    public:
        key_t key() const noexcept { return _key; }
        iterator_base() noexcept = default;
        iterator_base(const tree* t) noexcept : _tree(t) {
            lower_bound_res res = _tree->_root->lower_bound(_key, 0);
            _leaf = res.leaf;
            _value = const_cast<pointer>(res.elem);
            _key = res.key;
        }
        iterator_base& operator++() noexcept {
            if (_value == nullptr) {
                _value = nullptr;
                return *this;
            }
            _key++;
            if (node_index(_key, leaf_depth) != 0) {
                lower_bound_res res = _leaf->lower_bound(_key);
                _value = const_cast<pointer>(res.elem);
                if (_value != nullptr) {
                    _key = res.key;
                    return *this;
                }
                 _key += node_index_limit;
                 _key &= ~radix_mask;
            }
            lower_bound_res res = _tree->_root->lower_bound(_key, 0);
            _leaf = res.leaf;
            _value = const_cast<pointer>(res.elem);
            _key = res.key;
            return *this;
        }
        iterator_base operator++(int) noexcept {
            iterator_base cur = *this;
            operator++();
            return cur;
        }
        pointer operator->() const noexcept { return _value; }
        reference operator*() const noexcept { return *_value; }
        bool operator==(const iterator_base& o) const noexcept { return _value == o._value; }
    };
    using iterator = iterator_base<false>;
    using const_iterator = iterator_base<true>;
    iterator begin() noexcept { return iterator(this); }
    iterator end() noexcept { return iterator(); }
    const_iterator cbegin() const noexcept { return const_iterator(this); }
    const_iterator cend() const noexcept { return const_iterator(); }
    const_iterator begin() const noexcept { return cbegin(); }
    const_iterator end() const noexcept { return cend(); }
    bool empty() const noexcept { return _root.is(nil_root); }
    template <typename Fn>
    requires std::is_nothrow_invocable_r<size_t, Fn, key_t, const T&>::value
    size_t memory_usage(Fn&& entry_mem_usage) const noexcept {
        struct counting_visitor {
                Fn&& entry_mem_usage;
                bool sorted = false;
                size_t mem = 0;
                bool operator()(key_t key, const T& val) {
                    mem += entry_mem_usage(key, val);
                    return true;
                }
                bool operator()(const node_head& n, unsigned depth, bool enter) noexcept {
                    if (enter) {
                        mem += n.node_size(depth);
                    }
                    return true;
                }
        };
        counting_visitor v{std::move(entry_mem_usage)};
        visit(v);
        return v.mem;
    }
    struct stats {
        struct node_stats {
            unsigned long indirect_tiny = 0;
            unsigned long indirect_small = 0;
            unsigned long indirect_medium = 0;
            unsigned long indirect_large = 0;
            unsigned long direct_static = 0;
            unsigned long direct_dynamic = 0;
        };
        node_stats inners;
        node_stats leaves;
    };
    stats get_stats() const noexcept {
        struct counting_visitor {
            bool sorted = false;
            stats st;
            bool operator()(key_t key, const T& val) noexcept { std::abort(); }
            void update(typename stats::node_stats& ns, const node_head& n) const noexcept {
                switch (n._base_layout) {
                case layout::indirect_tiny: ns.indirect_tiny++; break;
                case layout::indirect_small: ns.indirect_small++; break;
                case layout::indirect_medium: ns.indirect_medium++; break;
                case layout::indirect_large: ns.indirect_large++; break;
                case layout::direct_static: ns.direct_static++; break;
                case layout::direct_dynamic: ns.direct_dynamic++; break;
                default: break;
                }
            }
            bool operator()(const node_head& n, unsigned depth, bool enter) noexcept {
                if (!enter) {
                    return true;
                }
                if (depth == leaf_depth) {
                    update(st.leaves, n);
                    return false; // don't visit elements
                } else {
                    update(st.inners, n);
                    return true;
                }
            }
        };
        counting_visitor v;
        visit(v);
        return v.st;
    }
};
} // namespace
namespace utils {
template <typename Collection>
class immutable_collection {
    Collection* _col;
public:
    immutable_collection(Collection& col)  ;
#define DO_WRAP_METHOD(method, is_const)                                                                           \
    template <typename... Args>                                                                                    \
    auto method(Args&&... args) is_const noexcept(noexcept(std::declval<is_const Collection>().method(args...))) { \
        return _col->method(std::forward<Args>(args)...);                                                           \
    }
#define WRAP_CONST_METHOD(method)    \
    DO_WRAP_METHOD(method, const)
#define WRAP_METHOD(method)          \
    WRAP_CONST_METHOD(method)        \
    DO_WRAP_METHOD(method, )
    ;  ;   ;  ;;   ; WRAP_CONST_METHOD(cbegin)
    WRAP_CONST_METHOD(cend)
    WRAP_CONST_METHOD(crbegin)
    WRAP_CONST_METHOD(crend)
#undef WRAP_METHOD
#undef WRAP_CONST_METHOD
#undef DO_WRAP_METHOD
};
} // namespace utils
namespace dht {
class decorated_key;
using token_range = nonwrapping_interval<token>;
}
namespace data_dictionary {
class database;
}
class repair_history_map;
using per_table_history_maps = std::unordered_map<table_id, seastar::lw_shared_ptr<repair_history_map>>;
class tombstone_gc_options;
class tombstone_gc_state {
    per_table_history_maps* _repair_history_maps;
public:
    tombstone_gc_state() = delete;
    tombstone_gc_state(per_table_history_maps* maps) noexcept : _repair_history_maps(maps) {}
    struct get_gc_before_for_range_result {
        gc_clock::time_point min_gc_before;
        gc_clock::time_point max_gc_before;
        bool knows_entire_range;
    };
    get_gc_before_for_range_result get_gc_before_for_range(schema_ptr s, const dht::token_range& range, const gc_clock::time_point& query_time) const;
    gc_clock::time_point get_gc_before_for_key(schema_ptr s, const dht::decorated_key& dk, const gc_clock::time_point& query_time) const;
    void update_repair_time(table_id id, const dht::token_range& range, gc_clock::time_point repair_time);
};
void validate_tombstone_gc_options(const tombstone_gc_options* options, data_dictionary::database db, sstring ks_name);
class mutation_fragment;
class mutation_partition_view;
class mutation_partition_visitor;
namespace query {
    class clustering_key_filter_ranges;
} // namespace query
struct cell_hash {
    using size_type = uint64_t;
    static constexpr size_type no_hash = 0;
    size_type hash = no_hash;
    explicit operator bool() const noexcept ;
};
template<>
struct appending_hash<cell_hash> {
    template<typename Hasher>
    void operator()(Hasher& h, const cell_hash& ch) const ;
};
using cell_hash_opt = seastar::optimized_optional<cell_hash>;
struct cell_and_hash {
    atomic_cell_or_collection cell;
    mutable cell_hash_opt hash;
    cell_and_hash() = default;
    
    
    cell_and_hash(atomic_cell_or_collection&& cell, cell_hash_opt hash)
        : cell(std::move(cell))
        , hash(hash)
    { }
};
class compaction_garbage_collector;
//
// Container for cells of a row. Cells are identified by column_id.
//
// All cells must belong to a single column_kind. The kind is not stored
// for space-efficiency reasons. Whenever a method accepts a column_kind,
// the caller must always supply the same column_kind.
//
//
class row {
    friend class size_calculator;
    using size_type = std::make_unsigned_t<column_id>;
    size_type _size = 0;
    using sparse_array_type = compact_radix_tree::tree<cell_and_hash, column_id>;
    sparse_array_type _cells;
public:
    row();
    ~row();
    row(const schema&, column_kind, const row&);
    row(row&& other) noexcept;
    row& operator=(row&& other) noexcept;
    size_t size() const ;
    bool empty() const ;
    const atomic_cell_or_collection& cell_at(column_id id) const;
    // Returns a pointer to cell's value or nullptr if column is not set.
    const atomic_cell_or_collection* find_cell(column_id id) const;
    // Returns a pointer to cell's value and hash or nullptr if column is not set.
    const cell_and_hash* find_cell_and_hash(column_id id) const;
    template<typename Func>
    void remove_if(Func&& func) {
        _cells.weed([func, this] (column_id id, cell_and_hash& cah) {
            if (!func(id, cah.cell)) {
                return false;
            }
            _size--;
            return true;
        });
    }
private:
    template<typename Func>
    void consume_with(Func&&);
    // Func obeys the same requirements as for for_each_cell below.
    template<typename Func, typename MaybeConstCellAndHash>
    static constexpr auto maybe_invoke_with_hash(Func& func, column_id id, MaybeConstCellAndHash& c_a_h) {
        if constexpr (std::is_invocable_v<Func, column_id, const cell_and_hash&>) {
            return func(id, c_a_h);
        } else {
            return func(id, c_a_h.cell);
        }
    }
public:
    // Calls Func(column_id, cell_and_hash&) or Func(column_id, atomic_cell_and_collection&)
    // for each cell in this row, depending on the concrete Func type.
    // noexcept if Func doesn't throw.
    template<typename Func>
    void for_each_cell(Func&& func) ;
    template<typename Func>
    void for_each_cell(Func&& func) const {
        _cells.walk([func] (column_id id, const cell_and_hash& cah) {
            maybe_invoke_with_hash(func, id, cah);
            return true;
        });
    }
    template<typename Func>
    void for_each_cell_until(Func&& func) const ;
    // Merges cell's value into the row.
    // Weak exception guarantees.
    void apply(const column_definition& column, const atomic_cell_or_collection& cell, cell_hash_opt hash = cell_hash_opt());
    // Merges cell's value into the row.
    // Weak exception guarantees.
    void apply(const column_definition& column, atomic_cell_or_collection&& cell, cell_hash_opt hash = cell_hash_opt());
    // Monotonic exception guarantees. In case of exception the sum of cell and this remains the same as before the exception.
    void apply_monotonically(const column_definition& column, atomic_cell_or_collection&& cell, cell_hash_opt hash = cell_hash_opt());
    // Adds cell to the row. The column must not be already set.
    void append_cell(column_id id, atomic_cell_or_collection cell);
    // Weak exception guarantees
    void apply(const schema&, column_kind, const row& src);
    // Weak exception guarantees
    void apply(const schema&, column_kind, row&& src);
    // Monotonic exception guarantees
    void apply_monotonically(const schema&, column_kind, row&& src);
    // Expires cells based on query_time. Expires tombstones based on gc_before
    // and max_purgeable. Removes cells covered by tomb.
    // Returns true iff there are any live cells left.
    bool compact_and_expire(
            const schema& s,
            column_kind kind,
            row_tombstone tomb,
            gc_clock::time_point query_time,
            can_gc_fn&,
            gc_clock::time_point gc_before,
            const row_marker& marker,
            compaction_garbage_collector* collector = nullptr);
    bool compact_and_expire(
            const schema& s,
            column_kind kind,
            row_tombstone tomb,
            gc_clock::time_point query_time,
            can_gc_fn&,
            gc_clock::time_point gc_before,
            compaction_garbage_collector* collector = nullptr);
    row difference(const schema&, column_kind, const row& other) const;
    bool equal(column_kind kind, const schema& this_schema, const row& other, const schema& other_schema) const;
    
    
    
    
    bool is_live(const schema&, column_kind kind, tombstone tomb = tombstone(), gc_clock::time_point now = gc_clock::time_point::min()) const;
    class printer {
        const schema& _schema;
        column_kind _kind;
        const row& _row;
    public:
        printer(const schema& s, column_kind k, const row& r)  ;
        
        
        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);
};
// Like row, but optimized for the case where the row doesn't exist (e.g. static rows)
class lazy_row {
    managed_ref<row> _row;
    static inline const row _empty_row;
public:
    lazy_row() = default;
    explicit lazy_row(row&& r) ;
    lazy_row(const schema& s, column_kind kind, const lazy_row& r) ;
    lazy_row(const schema& s, column_kind kind, const row& r) ;
    row& maybe_create() {
        if (!_row) {
            _row = make_managed<row>();
        }
        return *_row;
    }
    const row& get_existing() const & ;
    row& get_existing() & ;
    row&& get_existing() && ;
    const row& get() const ;
    size_t size() const ;
    bool empty() const ;
    void reserve(column_id nr) ;
    // Returns a pointer to cell's value or nullptr if column is not set.
    // Returns a pointer to cell's value and hash or nullptr if column is not set.
    // Calls Func(column_id, cell_and_hash&) or Func(column_id, atomic_cell_and_collection&)
    // for each cell in this row, depending on the concrete Func type.
    // noexcept if Func doesn't throw.
     ;
    template<typename Func>
    void for_each_cell(Func&& func) const {
        if (!_row) {
            return;
        }
        _row->for_each_cell(std::forward<Func>(func));
    }
     ;
    // Merges cell's value into the row.
    // Weak exception guarantees.
    
    // Merges cell's value into the row.
    // Weak exception guarantees.
    void apply(const column_definition& column, atomic_cell_or_collection&& cell, cell_hash_opt hash = cell_hash_opt()) {
        maybe_create().apply(column, std::move(cell), std::move(hash));
    }
    // Monotonic exception guarantees. In case of exception the sum of cell and this remains the same as before the exception.
    // Adds cell to the row. The column must not be already set.
    // Weak exception guarantees
    // Weak exception guarantees
    // Weak exception guarantees
    void apply(const schema& s, column_kind kind, row&& src) ;
    // Monotonic exception guarantees
    void apply_monotonically(const schema& s, column_kind kind, row&& src) ;
    // Monotonic exception guarantees
    void apply_monotonically(const schema& s, column_kind kind, lazy_row&& src) ;
    // Expires cells based on query_time. Expires tombstones based on gc_before
    // and max_purgeable. Removes cells covered by tomb.
    // Returns true iff there are any live cells left.
    bool compact_and_expire(
            const schema& s,
            column_kind kind,
            row_tombstone tomb,
            gc_clock::time_point query_time,
            can_gc_fn& can_gc,
            gc_clock::time_point gc_before,
            const row_marker& marker,
            compaction_garbage_collector* collector = nullptr);
    bool compact_and_expire(
            const schema& s,
            column_kind kind,
            row_tombstone tomb,
            gc_clock::time_point query_time,
            can_gc_fn& can_gc,
            gc_clock::time_point gc_before,
            compaction_garbage_collector* collector = nullptr);
    lazy_row difference(const schema& s, column_kind kind, const lazy_row& other) const ;
    
    
    
    class printer {
        const schema& _schema;
        column_kind _kind;
        const lazy_row& _row;
    public:
        
        printer(printer&&) = delete;
        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
};
// Used to return the timestamp of the latest update to the row
struct max_timestamp {
    api::timestamp_type max = api::missing_timestamp;
    void update(api::timestamp_type ts) ;
};
template<>
struct appending_hash<row> {
    static constexpr int null_hash_value = 0xbeefcafe;
    template<typename Hasher>
    void operator()(Hasher& h, const row& cells, const schema& s, column_kind kind, const query::column_id_vector& columns, max_timestamp& max_ts) const;
};
class row_marker;
int compare_row_marker_for_merge(const row_marker& left, const row_marker& right) noexcept;
class row_marker {
    static constexpr gc_clock::duration no_ttl { 0 };
    static constexpr gc_clock::duration dead { -1 };
    static constexpr gc_clock::time_point no_expiry { gc_clock::duration(0) };
    api::timestamp_type _timestamp = api::missing_timestamp;
    gc_clock::duration _ttl = no_ttl;
    gc_clock::time_point _expiry = no_expiry;
public:
    row_marker() = default;
    explicit row_marker(api::timestamp_type created_at) : _timestamp(created_at) { }
    row_marker(api::timestamp_type created_at, gc_clock::duration ttl, gc_clock::time_point expiry)
        : _timestamp(created_at), _ttl(ttl), _expiry(expiry)
    { }
    explicit row_marker(tombstone deleted_at)
        : _timestamp(deleted_at.timestamp), _ttl(dead), _expiry(deleted_at.deletion_time)
    { }
    bool is_missing() const {
        return _timestamp == api::missing_timestamp;
    }
    bool is_live() const {
        return !is_missing() && _ttl != dead;
    }
    bool is_live(tombstone t, gc_clock::time_point now) const ;
    // Can be called only when !is_missing().
    bool is_dead(gc_clock::time_point now) const ;
    // Can be called only when is_live().
    bool is_expiring() const {
        return _ttl != no_ttl;
    }
    // Can be called only when is_expiring().
    gc_clock::duration ttl() const ;
    // Can be called only when is_expiring().
    gc_clock::time_point expiry() const {
        return _expiry;
    }
    // Should be called when is_dead() or is_expiring().
    // Safe to be called when is_missing().
    // When is_expiring(), returns the the deletion time of the marker when it finally expires.
    gc_clock::time_point deletion_time() const {
        return _ttl == dead ? _expiry : _expiry - _ttl;
    }
    api::timestamp_type timestamp() const {
        return _timestamp;
    }
    void apply(const row_marker& rm) {
        if (compare_row_marker_for_merge(*this, rm) < 0) {
            *this = rm;
        }
    }
    // Expires cells and tombstones. Removes items covered by higher level
    // tombstones.
    // Returns true if row marker is live.
    bool compact_and_expire(tombstone tomb, gc_clock::time_point now,
            can_gc_fn& can_gc, gc_clock::time_point gc_before, compaction_garbage_collector* collector = nullptr);
    // Consistent with feed_hash()
    
    // Consistent with operator==()
     ;
    friend std::ostream& operator<<(std::ostream& os, const row_marker& rm);
};
template<>
struct appending_hash<row_marker> {
    template<typename Hasher>
    void operator()(Hasher& h, const row_marker& m) const ;
};
class shadowable_tombstone {
    tombstone _tomb;
public:
    explicit shadowable_tombstone(api::timestamp_type timestamp, gc_clock::time_point deletion_time)
            : _tomb(timestamp, deletion_time) {
    }
    explicit shadowable_tombstone(tombstone tomb = tombstone())
            : _tomb(std::move(tomb)) {
    }
    
    
    explicit operator bool() const {
        return bool(_tomb);
    }
    const tombstone& tomb() const ;
    // A shadowable row tombstone is valid only if the row has no live marker. In other words,
    // the row tombstone is only valid as long as no newer insert is done (thus setting a
    // live row marker; note that if the row timestamp set is lower than the tombstone's,
    // then the tombstone remains in effect as usual). If a row has a shadowable tombstone
    // with timestamp Ti and that row is updated with a timestamp Tj, such that Tj > Ti
    // (and that update sets the row marker), then the shadowable tombstone is shadowed by
    // that update. A concrete consequence is that if the update has cells with timestamp
    // lower than Ti, then those cells are preserved (since the deletion is removed), and
    // this is contrary to a regular, non-shadowable row tombstone where the tombstone is
    // preserved and such cells are removed.
    bool is_shadowed_by(const row_marker& marker) const {
        return marker.is_live() && marker.timestamp() > _tomb.timestamp;
    }
    void maybe_shadow(tombstone t, row_marker marker) noexcept {
        if (is_shadowed_by(marker)) {
            _tomb = std::move(t);
        }
    }
    void apply(tombstone t) noexcept {
        _tomb.apply(t);
    }
    void apply(shadowable_tombstone t) noexcept {
        _tomb.apply(t._tomb);
    }
};
template <>
struct fmt::formatter<shadowable_tombstone> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const shadowable_tombstone& t, FormatContext& ctx) const {
        if (t) {
            return fmt::format_to(ctx.out(),
                                  "{{shadowable tombstone: timestamp={}, deletion_time={}}}",
                                  t.tomb().timestamp, t.tomb(), t.tomb().deletion_time.time_since_epoch().count());
        } else {
            return fmt::format_to(ctx.out(),
                                  "{{shadowable tombstone: none}}");
        }
     }
};
template<>
struct appending_hash<shadowable_tombstone> {
    template<typename Hasher>
    void operator()(Hasher& h, const shadowable_tombstone& t) const ;
};
class row_tombstone {
    tombstone _regular;
    shadowable_tombstone _shadowable; // _shadowable is always >= _regular
public:
    explicit row_tombstone(tombstone regular, shadowable_tombstone shadowable)
            : _regular(std::move(regular))
            , _shadowable(std::move(shadowable)) {
    }
    explicit row_tombstone(tombstone regular)  ;
    row_tombstone() = default;
    std::strong_ordering operator<=>(const row_tombstone& t) const ;
    bool operator==(const row_tombstone& t) const ;
    explicit operator bool() const {
        return bool(_shadowable);
    }
    const tombstone& tomb() const ;
    const gc_clock::time_point max_deletion_time() const ;
    const tombstone& regular() const ;
    const shadowable_tombstone& shadowable() const ;
    bool is_shadowable() const ;
    void maybe_shadow(const row_marker& marker) noexcept {
        _shadowable.maybe_shadow(_regular, marker);
    }
    void apply(tombstone regular) noexcept {
        _shadowable.apply(regular);
        _regular.apply(regular);
    }
    void apply(shadowable_tombstone shadowable, row_marker marker) noexcept ;
    void apply(row_tombstone t, row_marker marker) noexcept {
        _regular.apply(t._regular);
        _shadowable.apply(t._shadowable);
        _shadowable.maybe_shadow(_regular, marker);
    }
    friend std::ostream& operator<<(std::ostream& out, const row_tombstone& t) ;
};
template<>
struct appending_hash<row_tombstone> {
    template<typename Hasher>
    void operator()(Hasher& h, const row_tombstone& t) const ;
};
class deletable_row final {
    row_tombstone _deleted_at;
    row_marker _marker;
    row _cells;
public:
    deletable_row() {}
    deletable_row(const schema& s, const deletable_row& other)
        : _deleted_at(other._deleted_at)
        , _marker(other._marker)
        , _cells(s, column_kind::regular_column, other._cells)
    { }
    deletable_row(row_tombstone&& tomb, row_marker&& marker, row&& cells)
        : _deleted_at(std::move(tomb)), _marker(std::move(marker)), _cells(std::move(cells))
    {}
    void apply(tombstone deleted_at) {
        _deleted_at.apply(deleted_at);
        maybe_shadow();
    }
    void apply(shadowable_tombstone deleted_at) ;
    void apply(row_tombstone deleted_at) {
        _deleted_at.apply(deleted_at, _marker);
    }
    void apply(const row_marker& rm) {
        _marker.apply(rm);
        maybe_shadow();
    }
    void remove_tombstone() ;
    void maybe_shadow() {
        _deleted_at.maybe_shadow(_marker);
    }
    // Weak exception guarantees. After exception, both src and this will commute to the same value as
    // they would should the exception not happen.
    void apply(const schema& s, const deletable_row& src);
    
    
    void apply_monotonically(const schema& s, deletable_row&& src);
public:
    row_tombstone deleted_at() const ;
    api::timestamp_type created_at() const ;
    // Call `maybe_shadow()` if the marker's timestamp is mutated.
    row_marker& marker() ;
    const row_marker& marker() const ;
    const row& cells() const ;
    row& cells() { return _cells; }
    bool equal(column_kind, const schema& s, const deletable_row& other, const schema& other_schema) const;
    bool is_live(const schema& s, column_kind kind, tombstone base_tombstone = tombstone(), gc_clock::time_point query_time = gc_clock::time_point::min()) const;
    bool empty() const ;
    deletable_row difference(const schema&, column_kind, const deletable_row& other) const;
    // Expires cells and tombstones. Removes items covered by higher level
    // tombstones.
    // Returns true iff the row is still alive.
    // When empty() after the call, the row can be removed without losing writes
    // given that tomb will be still in effect for the row after it is removed,
    // as a range tombstone, partition tombstone, etc.
    bool compact_and_expire(const schema&,
                            tombstone tomb,
                            gc_clock::time_point query_time,
                            can_gc_fn& can_gc,
                            gc_clock::time_point gc_before,
                            compaction_garbage_collector* collector = nullptr);
    class printer {
        const schema& _schema;
        const deletable_row& _deletable_row;
    public:
        printer(const schema& s, const deletable_row& r)  ;
        printer(const printer&) = delete;
        
        
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);
};
class cache_tracker;
class rows_entry final : public evictable {
    friend class size_calculator;
    intrusive_b::member_hook _link;
    clustering_key _key;
    deletable_row _row;
    // Given p is the preceding rows_entry&,
    // this tombstone applies to the range (p.position(), position()] if continuous()
    // and to [position(), position()] if !continuous().
    // So the tombstone applies only to the continuous interval, to the left.
    // On top of that, _row.deleted_at() may still apply new information.
    // So it's not deoverlapped with the row tombstone.
    // Set only when in mutation_partition_v2.
    tombstone _range_tombstone;
    struct flags {
        // _before_ck and _after_ck encode position_in_partition::weight
        bool _before_ck : 1;
        bool _after_ck : 1;
        bool _continuous : 1; // See doc of is_continuous.
        bool _dummy : 1;
        // Marks a dummy entry which is after_all_clustered_rows() position.
        // Needed so that eviction, which can't use comparators, can check if it's dealing with it.
        bool _last_dummy : 1;
        flags() : _before_ck(0), _after_ck(0), _continuous(true), _dummy(false), _last_dummy(false) { }
    } _flags{};
public:
    struct last_dummy_tag {};
    explicit rows_entry(clustering_key&& key)
        : _key(std::move(key))
    { }
    explicit rows_entry(const clustering_key& key)
        : _key(key)
    { }
    rows_entry(rows_entry&& o) noexcept;
    rows_entry(const schema& s, const rows_entry& e) 
    ;
    // Valid only if !dummy()
    clustering_key& key() ;
    // Valid only if !dummy()
    const clustering_key& key() const ;
    deletable_row& row() {
        return _row;
    }
    const deletable_row& row() const ;
    position_in_partition_view position() const {
        return position_in_partition_view(partition_region::clustered, bound_weight(_flags._after_ck - _flags._before_ck), &_key);
    }
    is_continuous continuous() const ;
    is_dummy dummy() const ;
    bool is_last_dummy() const ;
    
    
    void replace_with(rows_entry&& other) noexcept;
    void apply(row_tombstone t) ;
    void apply_monotonically(const schema& s, rows_entry&& e) ;
    bool empty() const ;
    struct tri_compare {
        position_in_partition::tri_compare _c;
        explicit tri_compare(const schema& s) : _c(s) {}
        std::strong_ordering operator()(const rows_entry& e1, const rows_entry& e2) const {
            return _c(e1.position(), e2.position());
        }
        std::strong_ordering operator()(const clustering_key& key, const rows_entry& e) const {
            return _c(position_in_partition_view::for_key(key), e.position());
        }
        
        
        
        
    };
    struct compare {
        tri_compare _c;
        explicit compare(const schema& s) : _c(s) {}
        template <typename K1, typename K2>
        bool operator()(const K1& k1, const K2& k2) const ;
    };
    
    
    size_t memory_usage(const schema&) const;
    // Handles eviction of the row, but doesn't attempt to handle eviction
    // of the containing partition_entry in case this is the last row.
    // Used by tests which don't keep the partition_entry inside a row_cache instance.
    void on_evicted_shallow() noexcept override {}
    void on_evicted(cache_tracker&) noexcept ;
    void on_evicted() noexcept override {}
    void compact(const schema&, tombstone);
    class printer {
        const schema& _schema;
        const rows_entry& _rows_entry;
    public:
        printer(const schema& s, const rows_entry& r)  ;
        
        
    };
    using container_type = intrusive_b::tree<rows_entry, &rows_entry::_link, rows_entry::tri_compare, 12, 20, intrusive_b::key_search::linear>;
};
struct mutation_application_stats {
    uint64_t row_hits = 0;
    uint64_t row_writes = 0;
    uint64_t rows_compacted_with_tombstones = 0;
    uint64_t rows_dropped_by_tombstones = 0;
};
struct apply_resume {
    enum class stage {
        start,
        range_tombstone_compaction,
        merging_range_tombstones,
        partition_tombstone_compaction,
        merging_rows,
        done
    };
    position_in_partition _pos;
    stage _stage;
};
// Represents a set of writes made to a single partition.
//
// The object is schema-dependent. Each instance is governed by some
// specific schema version. Accessors require a reference to the schema object
// of that version.
//
// There is an operation of addition defined on mutation_partition objects
// (also called "apply"), which gives as a result an object representing the
// sum of writes contained in the addends. For instances governed by the same
// schema, addition is commutative and associative.
//
// In addition to representing writes, the object supports specifying a set of
// partition elements called "continuity". This set can be used to represent
// lack of information about certain parts of the partition. It can be
// specified which ranges of clustering keys belong to that set. We say that a
// key range is continuous if all keys in that range belong to the continuity
// set, and discontinuous otherwise. By default everything is continuous.
// The static row may be also continuous or not.
// Partition tombstone is always continuous.
//
// Continuity is ignored by instance equality. It's also transient, not
// preserved by serialization.
//
// Continuity is represented internally using flags on row entries. The key
// range between two consecutive entries (both ends exclusive) is continuous
// if and only if rows_entry::continuous() is true for the later entry. The
// range starting after the last entry is assumed to be continuous. The range
// corresponding to the key of the entry is continuous if and only if
// rows_entry::dummy() is false.
//
// Adding two fully-continuous instances gives a fully-continuous instance.
// Continuity doesn't affect how the write part is added.
//
// Addition of continuity is not commutative in general, but is associative.
// The default continuity merging rules are those required by MVCC to
// preserve its invariants. For details, refer to "Continuity merging rules" section
// in the doc in partition_version.hh.
class mutation_partition final {
public:
    using rows_type = rows_entry::container_type;
    friend class size_calculator;
private:
    tombstone _tombstone;
    lazy_row _static_row;
    bool _static_row_continuous = true;
    rows_type _rows;
    // Contains only strict prefixes so that we don't have to lookup full keys
    // in both _row_tombstones and _rows.
    range_tombstone_list _row_tombstones;
#ifdef SEASTAR_DEBUG
    table_schema_version _schema_version;
#endif
    friend class converting_mutation_partition_applier;
public:
    struct copy_comparators_only {};
    struct incomplete_tag {};
    // Constructs an empty instance which is fully discontinuous except for the partition tombstone.
    static mutation_partition make_incomplete(const schema& s, tombstone t = {}) ;
    mutation_partition(schema_ptr s)
        : _rows()
        , _row_tombstones(*s)
#ifdef SEASTAR_DEBUG
        , _schema_version(s->version())
#endif
    { }
    mutation_partition(mutation_partition& other, copy_comparators_only)
        : _rows()
        , _row_tombstones(other._row_tombstones, range_tombstone_list::copy_comparator_only())
#ifdef SEASTAR_DEBUG
        , _schema_version(other._schema_version)
#endif
    { }
    mutation_partition(mutation_partition&&) = default;
    mutation_partition(const schema& s, const mutation_partition&);
    mutation_partition(const mutation_partition&, const schema&, query::clustering_key_filter_ranges);
    
    ;
    static mutation_partition& container_of(rows_type&);
    mutation_partition& operator=(mutation_partition&& x) noexcept;
    
    
    
    // Consistent with equal()
     ;
    class printer {
        const schema& _schema;
        const mutation_partition& _mutation_partition;
    public:
        printer(const schema& s, const mutation_partition& mp)  ;
        printer(const printer&) = delete;
        
        
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);
public:
    // Makes sure there is a dummy entry after all clustered rows. Doesn't affect continuity.
    // Doesn't invalidate iterators.
    void ensure_last_dummy(const schema&);
    bool static_row_continuous() const ;
    void set_static_row_continuous(bool value) ;
    
    
    // Sets or clears continuity of clustering ranges between existing rows.
    // Returns clustering row ranges which have continuity matching the is_continuous argument.
    // Returns true iff all keys from given range are marked as continuous, or range is empty.
    // Returns true iff all keys from given range are marked as not continuous and range is not empty.
    // Returns true iff all keys from given range have continuity membership as specified by is_continuous.
    
    // Frees elements of the partition in batches.
    // Returns stop_iteration::yes iff there are no more elements to free.
    // Continuity is unspecified after this.
    
    // Applies mutation_fragment.
    // The fragment must be goverened by the same schema as this object.
    void apply(const schema& s, const mutation_fragment&);
    void apply(tombstone t) { _tombstone.apply(t); }
    void apply_delete(const schema& schema, const clustering_key_prefix& prefix, tombstone t);
    void apply_delete(const schema& schema, range_tombstone rt);
    void apply_delete(const schema& schema, clustering_key_prefix&& prefix, tombstone t);
    
    // Equivalent to applying a mutation with an empty row, created with given timestamp
    
    void apply_insert(const schema& s, clustering_key_view, api::timestamp_type created_at,
                      gc_clock::duration ttl, gc_clock::time_point expiry);
    // prefix must not be full
    void apply_row_tombstone(const schema& schema, clustering_key_prefix prefix, tombstone t);
    void apply_row_tombstone(const schema& schema, range_tombstone rt);
    //
    // Applies p to current object.
    //
    // Commutative when this_schema == p_schema. If schemas differ, data in p which
    // is not representable in this_schema is dropped, thus apply() loses commutativity.
    //
    // Weak exception guarantees.
    void apply(const schema& this_schema, const mutation_partition& p, const schema& p_schema,
            mutation_application_stats& app_stats);
    // Use in case this instance and p share the same schema.
    // Same guarantees as apply(const schema&, mutation_partition&&, const schema&);
    // Same guarantees and constraints as for apply(const schema&, const mutation_partition&, const schema&).
    // Applies p to this instance.
    //
    // Monotonic exception guarantees. In case of exception the sum of p and this remains the same as before the exception.
    // This instance and p are governed by the same schema.
    //
    // Must be provided with a pointer to the cache_tracker, which owns both this and p.
    //
    // Returns stop_iteration::no if the operation was preempted before finished, and stop_iteration::yes otherwise.
    // On preemption the sum of this and p stays the same (represents the same set of writes), and the state of this
    // object contains at least all the writes it contained before the call (monotonicity). It may contain partial writes.
    // Also, some progress is always guaranteed (liveness).
    //
    // If returns stop_iteration::yes, then the sum of this and p is NO LONGER the same as before the call,
    // the state of p is undefined and should not be used for reading.
    //
    // The operation can be driven to completion like this:
    //
    //   apply_resume res;
    //   while (apply_monotonically(..., is_preemtable::yes, &res) == stop_iteration::no) { }
    //
    // If is_preemptible::no is passed as argument then stop_iteration::no is never returned.
    //
    // If is_preemptible::yes is passed, apply_resume must also be passed,
    // same instance each time until stop_iteration::yes is returned.
    // Weak exception guarantees.
    // Assumes this and p are not owned by a cache_tracker.
    // Converts partition to the new schema. When succeeds the partition should only be accessed
    // using the new schema.
    //
    // Strong exception guarantees.
    void upgrade(const schema& old_schema, const schema& new_schema);
private:
    void insert_row(const schema& s, const clustering_key& key, deletable_row&& row);
    void insert_row(const schema& s, const clustering_key& key, const deletable_row& row);
    uint32_t do_compact(const schema& s,
        const dht::decorated_key& dk,
        gc_clock::time_point now,
        const std::vector<query::clustering_range>& row_ranges,
        bool always_return_static_content,
        bool reverse,
        uint64_t row_limit,
        can_gc_fn&,
        bool drop_tombstones_unconditionally,
        const tombstone_gc_state& gc_state);
    // Calls func for each row entry inside row_ranges until func returns stop_iteration::yes.
    // Removes all entries for which func didn't return stop_iteration::no or wasn't called at all.
    // Removes all entries that are empty, check rows_entry::empty().
    // If reversed is true, func will be called on entries in reverse order. In that case row_ranges
    // must be already in reverse order.
    template<bool reversed, typename Func>
    requires std::is_invocable_r_v<stop_iteration, Func, rows_entry&>
    void trim_rows(const schema& s,
        const std::vector<query::clustering_range>& row_ranges,
        Func&& func);
public:
    // Performs the following:
    //   - throws out data which doesn't belong to row_ranges
    //   - expires cells and tombstones based on query_time
    //   - drops cells covered by higher-level tombstones (compaction)
    //   - leaves at most row_limit live rows
    //
    // Note: a partition with a static row which has any cell live but no
    // clustered rows still counts as one row, according to the CQL row
    // counting rules.
    //
    // Returns the count of CQL rows which remained. If the returned number is
    // smaller than the row_limit it means that there was no more data
    // satisfying the query left.
    //
    // The row_limit parameter must be > 0.
    //
    uint64_t compact_for_query(const schema& s, const dht::decorated_key& dk, gc_clock::time_point query_time,
        const std::vector<query::clustering_range>& row_ranges, bool always_return_static_content,
        bool reversed, uint64_t row_limit);
    // Performs the following:
    //   - expires cells based on compaction_time
    //   - drops cells covered by higher-level tombstones
    //   - drops expired tombstones which timestamp is before max_purgeable
    void compact_for_compaction(const schema& s, can_gc_fn&,
        const dht::decorated_key& dk,
        gc_clock::time_point compaction_time,
        const tombstone_gc_state& gc_state);
    // Like compact_for_compaction but drop tombstones unconditionally
    void compact_for_compaction_drop_tombstones_unconditionally(const schema& s,
            const dht::decorated_key& dk);
    // Returns the minimal mutation_partition that when applied to "other" will
    // create a mutation_partition equal to the sum of other and this one.
    // This and other must both be governed by the same schema s.
    mutation_partition difference(schema_ptr s, const mutation_partition& other) const;
    // Returns a subset of this mutation holding only information relevant for given clustering ranges.
    // Range tombstones will be trimmed to the boundaries of the clustering ranges.
    mutation_partition sliced(const schema& s, const query::clustering_row_ranges&) const;
    // Returns true if the mutation_partition represents no writes.
    bool empty() const;
public:
    deletable_row& clustered_row(const schema& s, const clustering_key& key);
    deletable_row& clustered_row(const schema& s, clustering_key&& key);
    // Throws if the row already exists or if the row was not inserted to the
    // last position (one or more greater row already exists).
    // Weak exception guarantees.
public:
    tombstone partition_tombstone() const ;
    lazy_row& static_row() { return _static_row; }
    const lazy_row& static_row() const ;
    // return a set of rows_entry where each entry represents a CQL row sharing the same clustering key.
    
    
    tombstone range_tombstone_for_row(const schema& schema, const clustering_key& key) const;
    row_tombstone tombstone_for_row(const schema& schema, const clustering_key& key) const;
    // Can be called only for non-dummy entries
    
    
    
    
    rows_type::iterator lower_bound(const schema& schema, const query::clustering_range& r);
    rows_type::iterator upper_bound(const schema& schema, const query::clustering_range& r);
    boost::iterator_range<rows_type::iterator> range(const schema& schema, const query::clustering_range& r);
    // Returns an iterator range of rows_entry, with only non-dummy entries.
    auto non_dummy_rows() const {
        return boost::make_iterator_range(_rows.begin(), _rows.end())
            | boost::adaptors::filtered([] (const rows_entry& e) { return bool(!e.dummy()); });
    }
    void accept(const schema&, mutation_partition_visitor&) const;
    // Returns the number of live CQL rows in this partition.
    //
    // Note: If no regular rows are live, but there's something live in the
    // static row, the static row counts as one row. If there is at least one
    // regular row live, static row doesn't count.
    //
    uint64_t live_row_count(const schema&,
        gc_clock::time_point query_time = gc_clock::time_point::min()) const;
    bool is_static_row_live(const schema&,
        gc_clock::time_point query_time = gc_clock::time_point::min()) const;
    uint64_t row_count() const;
    
private:
    ;
    friend class counter_write_query_result_builder;
    void check_schema(const schema& s) const {
#ifdef SEASTAR_DEBUG
        assert(s.version() == _schema_version);
#endif
    }
};
bool has_any_live_data(const schema& s, column_kind kind, const row& cells, tombstone tomb = tombstone(),
                       gc_clock::time_point now = gc_clock::time_point::min());
namespace seastar {
    class file;
} // namespace seastar
struct reader_resources {
    int count = 0;
    ssize_t memory = 0;
    static reader_resources with_memory(ssize_t memory) ;
    
    
};
class reader_concurrency_semaphore;
/// A permit for a specific read.
///
/// Used to track the read's resource consumption. Use `consume_memory()` to
/// register memory usage, which returns a `resource_units` RAII object that
/// should be held onto while the respective resources are in use.
class reader_permit {
    friend class reader_concurrency_semaphore;
    friend class tracking_allocator_base;
public:
    class resource_units;
    class need_cpu_guard;
    class awaits_guard;
    enum class state {
        waiting_for_admission,
        waiting_for_memory,
        waiting_for_execution,
        active,
        active_need_cpu,
        active_await,
        inactive,
        evicted,
    };
    class impl;
private:
    shared_ptr<impl> _impl;
private:
    
    
    friend class optimized_optional<reader_permit>;
public:
    
    
    // Call only when needs_readmission() = true.
    resource_units consume_memory(size_t memory = 0);
    resource_units consume_resources(reader_resources res);
    // If the read was aborted, throw the exception the read was aborted with.
    // Otherwise no-op.
};
using reader_permit_opt = optimized_optional<reader_permit>;
class reader_permit::resource_units {
    reader_permit _permit;
    reader_resources _resources;
    friend class reader_permit;
    friend class reader_concurrency_semaphore;
private:
    class already_consumed_tag {};
public:
    
    
    void add(resource_units&& o);
    void reset_to(reader_resources res);
    void reset_to_zero() noexcept;
    reader_permit permit() const ;
    reader_resources resources() const ;
};
std::ostream& operator<<(std::ostream& os, reader_permit::state s);
/// Mark a permit as needing CPU.
///
/// Conceptually, a permit is considered as needing CPU, when at least one reader
/// associated with it has an ongoing foreground operation initiated by
/// its consumer. E.g. a pending `fill_buffer()` call.
/// This class is an RAII need_cpu marker meant to be used by keeping it alive
/// while the reader is in need of CPU.
class reader_permit::need_cpu_guard {
    reader_permit_opt _permit;
public:
    
    
};
/// Mark a permit as awaiting I/O or an operation running on a remote shard.
///
/// Conceptually, a permit is considered awaiting, when at least one reader
/// associated with it is waiting on I/O or a remote shard as part of a
/// foreground operation initiated by its consumer. E.g. an sstable reader
/// waiting on a disk read as part of its `fill_buffer()` call.
/// This class is an RAII awaits marker meant to be used by keeping it alive
/// until said awaited event completes.
class reader_permit::awaits_guard {
    reader_permit_opt _permit;
public:
};
 ;
 
file make_tracked_file(file f, reader_permit p);
class tracking_allocator_base {
    reader_permit _permit;
protected:
    tracking_allocator_base(reader_permit permit) noexcept : _permit(std::move(permit)) { }
    void consume(size_t memory) ;
    void signal(size_t memory) ;
};
template <typename T>
class tracking_allocator : public tracking_allocator_base {
public:
    using value_type = T;
    using propagate_on_container_move_assignment = std::true_type;
    using is_always_equal = std::false_type;
private:
    std::allocator<T> _alloc;
public:
    tracking_allocator(reader_permit permit) noexcept : tracking_allocator_base(std::move(permit)) { }
    T* allocate(size_t n) ;
    void deallocate(T* p, size_t n) ;
    template <typename U>
    friend bool operator==(const tracking_allocator<U>& a, const tracking_allocator<U>& b);
};
 ;
using namespace seastar;
class mutation_fragment;
class mutation_fragment_v2;
using mutation_fragment_opt = optimized_optional<mutation_fragment>;
using mutation_fragment_v2_opt = optimized_optional<mutation_fragment_v2>;
// mutation_fragments are the objects that streamed_mutation are going to
// stream. They can represent:
//  - a static row
//  - a clustering row
//  - a range tombstone
//
// There exists an ordering (implemented in position_in_partition class) between
// mutation_fragment objects. It reflects the order in which content of
// partition appears in the sstables.
class clustering_row {
    clustering_key_prefix _ck;
    deletable_row _row;
public:
    
    
    
    clustering_row(const schema& s, const clustering_row& other)
        : _ck(other._ck), _row(s, other._row) { }
    clustering_row(const schema& s, const rows_entry& re)
        : _ck(re.key()), _row(s, re.row()) { }
    
    
    const clustering_key_prefix& key() const ;
    void remove_tombstone() ;
    row_tombstone tomb() const ;
    const row_marker& marker() const ;
    row_marker& marker() ;
    const row& cells() const ;
    
    
    
    
    void apply(const schema& s, const clustering_row& cr) ;
    void apply(const schema& s, const rows_entry& r) ;
    void apply(const schema& s, const deletable_row& r) ;
    position_in_partition_view position() const;
    
    
    size_t memory_usage(const schema& s) const ;
    bool equal(const schema& s, const clustering_row& other) const ;
    class printer {
        const schema& _schema;
        const clustering_row& _clustering_row;
    public:
        printer(const schema& s, const clustering_row& r)  ;
        printer(const printer&) = delete;
        printer(printer&&) = delete;
        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);
    deletable_row as_deletable_row() && ;
    
};
class static_row {
    row _cells;
public:
    
    static_row(const schema& s, const static_row& other)  ;
    explicit static_row(const schema& s, const row& r) : _cells(s, column_kind::static_column, r) { }
    explicit static_row(row&& r) : _cells(std::move(r)) { }
    row& cells() ;
    const row& cells() const ;
    bool empty() const ;
    
    
    void apply(const schema& s, static_row&& sr) ;
    void set_cell(const column_definition& def, atomic_cell_or_collection&& value) ;
    position_in_partition_view position() const;
    size_t external_memory_usage(const schema& s) const ;
    size_t memory_usage(const schema& s) const ;
    bool equal(const schema& s, const static_row& other) const ;
    class printer {
        const schema& _schema;
        const static_row& _static_row;
    public:
        printer(const schema& s, const static_row& r)  ;
        printer(const printer&) = delete;
        printer(printer&&) = delete;
        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);
};
class partition_start final {
    dht::decorated_key _key;
    tombstone _partition_tombstone;
public:
    partition_start(dht::decorated_key pk, tombstone pt)
        : _key(std::move(pk))
        , _partition_tombstone(std::move(pt))
    { }
    dht::decorated_key& key() ;
    const dht::decorated_key& key() const ;
    const tombstone& partition_tombstone() const ;
    tombstone& partition_tombstone() ;
    position_in_partition_view position() const;
    size_t external_memory_usage(const schema&) const ;
    size_t memory_usage(const schema& s) const ;
    bool equal(const schema& s, const partition_start& other) const ;
    friend std::ostream& operator<<(std::ostream& is, const partition_start& row);
};
class partition_end final {
public:
    position_in_partition_view position() const;
    
    
    bool equal(const schema& s, const partition_end& other) const ;
    friend std::ostream& operator<<(std::ostream& is, const partition_end& row);
};
template<typename T, typename ReturnType>
concept MutationFragmentConsumer =
    requires(T& t, static_row sr, clustering_row cr, range_tombstone rt, partition_start ph, partition_end pe) {
        { t.consume(std::move(sr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(cr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(rt)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(ph)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(pe)) } -> std::same_as<ReturnType>;
    };
template<typename T, typename ReturnType>
concept FragmentConsumerReturning =
    requires(T t, static_row sr, clustering_row cr, range_tombstone rt, tombstone tomb) {
        { t.consume(std::move(sr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(cr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(rt)) } -> std::same_as<ReturnType>;
    };
template<typename T>
concept FragmentConsumer =
    FragmentConsumerReturning<T, stop_iteration> || FragmentConsumerReturning<T, future<stop_iteration>>;
template<typename T>
concept StreamedMutationConsumer =
    FragmentConsumer<T> && requires(T t, static_row sr, clustering_row cr, range_tombstone rt, tombstone tomb) {
        t.consume(tomb);
        t.consume_end_of_stream();
    };
template<typename T, typename ReturnType>
concept MutationFragmentVisitor =
    requires(T t, const static_row& sr, const clustering_row& cr, const range_tombstone& rt, const partition_start& ph, const partition_end& eop) {
        { t(sr) } -> std::same_as<ReturnType>;
        { t(cr) } -> std::same_as<ReturnType>;
        { t(rt) } -> std::same_as<ReturnType>;
        { t(ph) } -> std::same_as<ReturnType>;
        { t(eop) } -> std::same_as<ReturnType>;
    };
class mutation_fragment {
public:
    enum class kind {
        static_row,
        clustering_row,
        range_tombstone,
        partition_start,
        partition_end,
    };
private:
    struct data {
        data(reader_permit permit) :  _memory(permit.consume_memory()) { }
        ~data() ;
        reader_permit::resource_units _memory;
        union {
            static_row _static_row;
            clustering_row _clustering_row;
            range_tombstone _range_tombstone;
            partition_start _partition_start;
            partition_end _partition_end;
        };
    };
private:
    kind _kind;
    std::unique_ptr<data> _data;
    mutation_fragment() = default;
    explicit operator bool() const noexcept ;
    void destroy_data() noexcept;
    void reset_memory(const schema& s, std::optional<reader_resources> res = {});
    friend class optimized_optional<mutation_fragment>;
    friend class position_in_partition;
public:
    struct clustering_row_tag_t { };
    template<typename... Args>
    mutation_fragment(clustering_row_tag_t, const schema& s, reader_permit permit, Args&&... args)
        : _kind(kind::clustering_row)
        , _data(std::make_unique<data>(std::move(permit)))
    {
        new (&_data->_clustering_row) clustering_row(std::forward<Args>(args)...);
        reset_memory(s);
    }
    mutation_fragment(const schema& s, reader_permit permit, static_row&& r);
    mutation_fragment(const schema& s, reader_permit permit, clustering_row&& r);
    mutation_fragment(const schema& s, reader_permit permit, range_tombstone&& r);
    mutation_fragment(const schema& s, reader_permit permit, partition_start&& r);
    mutation_fragment(const schema& s, reader_permit permit, partition_end&& r);
    mutation_fragment(const schema& s, reader_permit permit, const mutation_fragment& o)
        : _kind(o._kind), _data(std::make_unique<data>(std::move(permit))) {
        switch (_kind) {
            case kind::static_row:
                new (&_data->_static_row) static_row(s, o._data->_static_row);
                break;
            case kind::clustering_row:
                new (&_data->_clustering_row) clustering_row(s, o._data->_clustering_row);
                break;
            case kind::range_tombstone:
                new (&_data->_range_tombstone) range_tombstone(o._data->_range_tombstone);
                break;
            case kind::partition_start:
                new (&_data->_partition_start) partition_start(o._data->_partition_start);
                break;
            case kind::partition_end:
                new (&_data->_partition_end) partition_end(o._data->_partition_end);
                break;
        }
        reset_memory(s, o._data->_memory.resources());
    }
    mutation_fragment(mutation_fragment&& other) = default;
    mutation_fragment& operator=(mutation_fragment&& other) noexcept ;
    [[gnu::always_inline]]
    ~mutation_fragment() {
        if (_data) {
            destroy_data();
        }
    }
    position_in_partition_view position() const;
    // Returns the range of positions for which this fragment holds relevant information.
    position_range range(const schema& s) const;
    // Checks if this fragment may be relevant for any range starting at given position.
    bool relevant_for_range(const schema& s, position_in_partition_view pos) const;
    // Like relevant_for_range() but makes use of assumption that pos is greater
    // than the starting position of this fragment.
    bool relevant_for_range_assuming_after(const schema& s, position_in_partition_view pos) const;
    bool has_key() const ;
    // Requirements: has_key() == true
    const clustering_key_prefix& key() const;
    
    
    bool is_clustering_row() const ;
    bool is_range_tombstone() const ;
    bool is_partition_start() const ;
    const static_row& as_static_row() const & ;
    const clustering_row& as_clustering_row() const & ;
    const range_tombstone& as_range_tombstone() const & ;
    const partition_start& as_partition_start() const & ;
    const partition_end& as_end_of_partition() const & ;
    // Requirements: mergeable_with(mf)
    void apply(const schema& s, mutation_fragment&& mf);
    template<typename Consumer>
    requires MutationFragmentConsumer<Consumer, decltype(std::declval<Consumer>().consume(std::declval<range_tombstone>()))>
    decltype(auto) consume(Consumer& consumer) && {
        _data->_memory.reset_to_zero();
        switch (_kind) {
        case kind::static_row:
            return consumer.consume(std::move(_data->_static_row));
        case kind::clustering_row:
            return consumer.consume(std::move(_data->_clustering_row));
        case kind::range_tombstone:
            return consumer.consume(std::move(_data->_range_tombstone));
        case kind::partition_start:
            return consumer.consume(std::move(_data->_partition_start));
        case kind::partition_end:
            return consumer.consume(std::move(_data->_partition_end));
        }
        abort();
    }
    template<typename Visitor>
    requires MutationFragmentVisitor<Visitor, decltype(std::declval<Visitor>()(std::declval<static_row&>()))>
    decltype(auto) visit(Visitor&& visitor) const {
        switch (_kind) {
        case kind::static_row:
            return visitor(as_static_row());
        case kind::clustering_row:
            return visitor(as_clustering_row());
        case kind::range_tombstone:
            return visitor(as_range_tombstone());
        case kind::partition_start:
            return visitor(as_partition_start());
        case kind::partition_end:
            return visitor(as_end_of_partition());
        }
        abort();
    }
    size_t memory_usage() const {
        return _data->_memory.resources().memory;
    }
    reader_permit permit() const {
        return _data->_memory.permit();
    }
    bool equal(const schema& s, const mutation_fragment& other) const {
        if (other._kind != _kind) {
            return false;
        }
        switch (_kind) {
        case kind::static_row:
            return as_static_row().equal(s, other.as_static_row());
        case kind::clustering_row:
            return as_clustering_row().equal(s, other.as_clustering_row());
        case kind::range_tombstone:
            return as_range_tombstone().equal(s, other.as_range_tombstone());
        case kind::partition_start:
            return as_partition_start().equal(s, other.as_partition_start());
        case kind::partition_end:
            return as_end_of_partition().equal(s, other.as_end_of_partition());
        }
        abort();
    }
    // Fragments which have the same position() and are mergeable can be
    // merged into one fragment with apply() which represents the sum of
    // writes represented by each of the fragments.
    // Fragments which have the same position() but are not mergeable
    // can be emitted one after the other in the stream.
    bool mergeable_with(const mutation_fragment& mf) const {
        return _kind == mf._kind && _kind != kind::range_tombstone;
    }
    class printer {
        const schema& _schema;
        const mutation_fragment& _mutation_fragment;
    public:
        printer(const schema& s, const mutation_fragment& mf) : _schema(s), _mutation_fragment(mf) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;
        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);
private:
    size_t calculate_memory_usage(const schema& s) const ;
};
std::ostream& operator<<(std::ostream&, mutation_fragment::kind);
// range_tombstone_stream is a helper object that simplifies producing a stream
// of range tombstones and merging it with a stream of clustering rows.
// Tombstones are added using apply() and retrieved using get_next().
//
// get_next(const rows_entry&) and get_next(const mutation_fragment&) allow
// merging the stream of tombstones with a stream of clustering rows. If these
// overloads return disengaged optional it means that there is no tombstone
// in the stream that should be emitted before the object given as an argument.
// (And, consequently, if the optional is engaged that tombstone should be
// emitted first). After calling any of these overloads with a mutation_fragment
// which is at some position in partition P no range tombstone can be added to
// the stream which start bound is before that position.
//
// get_next() overload which doesn't take any arguments is used to return the
// remaining tombstones. After it was called no new tombstones can be added
// to the stream.
class range_tombstone_stream {
    const schema& _schema;
    reader_permit _permit;
    position_in_partition::less_compare _cmp;
    range_tombstone_list _list;
private:
    mutation_fragment_opt do_get_next();
public:
    range_tombstone_stream(const schema& s, reader_permit permit) : _schema(s), _permit(std::move(permit)), _cmp(s), _list(s) { }
    mutation_fragment_opt get_next(const rows_entry&);
    mutation_fragment_opt get_next(const mutation_fragment&);
    // Returns next fragment with position before upper_bound or disengaged optional if no such fragments are left.
    mutation_fragment_opt get_next(position_in_partition_view upper_bound);
    mutation_fragment_opt get_next();
    // Precondition: !empty()
    const range_tombstone& peek_next() const;
    // Forgets all tombstones which are not relevant for any range starting at given position.
    void forward_to(position_in_partition_view);
    void apply(range_tombstone&& rt) ;
    void reset();
    bool empty() const;
    friend std::ostream& operator<<(std::ostream& out, const range_tombstone_stream&);
};
// F gets a stream element as an argument and returns the new value which replaces that element
// in the transformed stream.
template<typename F>
concept StreamedMutationTranformer =
    requires(F f, mutation_fragment mf, schema_ptr s) {
        { f(std::move(mf)) } -> std::same_as<mutation_fragment>;
        { f(s) } -> std::same_as<schema_ptr>;
    };
class xx_hasher;
template<>
struct appending_hash<mutation_fragment> {
    template<typename Hasher>
    void operator()(Hasher& h, const mutation_fragment& mf, const schema& s) const;
};
// Mutation fragment which represents a range tombstone boundary.
//
// The range_tombstone_change::tombstone() method returns the tombstone which takes effect
// for positions >= range_tombstone_change::position() in the stream, until the next
// range_tombstone_change is encountered.
//
// Note, a range_tombstone_change with an empty tombstone() ends the range tombstone.
// An empty tombstone naturally does not cover any timestamp.
class range_tombstone_change {
    position_in_partition _pos;
    ::tombstone _tomb;
public:
    range_tombstone_change(position_in_partition pos, tombstone tomb)
        : _pos(std::move(pos))
        , _tomb(tomb)
    { }
    range_tombstone_change(position_in_partition_view pos, tombstone tomb)
        : _pos(pos)
        , _tomb(tomb)
    { }
    const position_in_partition& position() const & ;
    position_in_partition position() && ;
    void set_position(position_in_partition pos) ;
    ::tombstone tombstone() const ;
    void set_tombstone(::tombstone tomb) ;
    
    
    
    
    bool equal(const schema& s, const range_tombstone_change& other) const ;
};
template<>
struct fmt::formatter<range_tombstone_change> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const range_tombstone_change& rt, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{{range_tombstone_change: pos={}, {}}}", rt.position(), rt.tombstone());
    }
};
template<typename T, typename ReturnType>
concept MutationFragmentConsumerV2 =
    requires(T& t,
            static_row sr,
            clustering_row cr,
            range_tombstone_change rt_chg,
            partition_start ph,
            partition_end pe) {
        { t.consume(std::move(sr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(cr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(rt_chg)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(ph)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(pe)) } -> std::same_as<ReturnType>;
    };
template<typename T, typename ReturnType>
concept MutationFragmentVisitorV2 =
    requires(T t,
            const static_row& sr,
            const clustering_row& cr,
            const range_tombstone_change& rt,
            const partition_start& ph,
            const partition_end& eop) {
        { t(sr) } -> std::same_as<ReturnType>;
        { t(cr) } -> std::same_as<ReturnType>;
        { t(rt) } -> std::same_as<ReturnType>;
        { t(ph) } -> std::same_as<ReturnType>;
        { t(eop) } -> std::same_as<ReturnType>;
    };
template<typename T, typename ReturnType>
concept FragmentConsumerReturningV2 =
requires(T t, static_row sr, clustering_row cr, range_tombstone_change rt, tombstone tomb) {
    { t.consume(std::move(sr)) } -> std::same_as<ReturnType>;
    { t.consume(std::move(cr)) } -> std::same_as<ReturnType>;
    { t.consume(std::move(rt)) } -> std::same_as<ReturnType>;
};
template<typename T>
concept FragmentConsumerV2 =
FragmentConsumerReturningV2<T, stop_iteration> || FragmentConsumerReturningV2<T, future<stop_iteration>>;
template<typename T>
concept StreamedMutationConsumerV2 =
FragmentConsumerV2<T> && requires(T t, tombstone tomb) {
    t.consume(tomb);
    t.consume_end_of_stream();
};
class mutation_fragment_v2 {
public:
    enum class kind {
        static_row,
        clustering_row,
        range_tombstone_change,
        partition_start,
        partition_end,
    };
private:
    struct data {
        data(reader_permit permit) :  _memory(permit.consume_memory()) { }
        ~data() ;
        reader_permit::resource_units _memory;
        union {
            static_row _static_row;
            clustering_row _clustering_row;
            range_tombstone_change _range_tombstone_chg;
            partition_start _partition_start;
            partition_end _partition_end;
        };
    };
private:
    kind _kind;
    std::unique_ptr<data> _data;
    mutation_fragment_v2() = default;
    explicit operator bool() const noexcept ;
    void destroy_data() noexcept;
    void reset_memory(const schema& s, std::optional<reader_resources> res = {});
    friend class optimized_optional<mutation_fragment_v2>;
    friend class position_in_partition;
public:
    struct clustering_row_tag_t { };
    template<typename... Args>
    mutation_fragment_v2(clustering_row_tag_t, const schema& s, reader_permit permit, Args&&... args)
        : _kind(kind::clustering_row)
        , _data(std::make_unique<data>(std::move(permit)))
    {
        new (&_data->_clustering_row) clustering_row(std::forward<Args>(args)...);
        _data->_memory.reset_to(reader_resources::with_memory(calculate_memory_usage(s)));
    }
    mutation_fragment_v2(const schema& s, reader_permit permit, static_row&& r);
    mutation_fragment_v2(const schema& s, reader_permit permit, clustering_row&& r);
    mutation_fragment_v2(const schema& s, reader_permit permit, range_tombstone_change&& r);
    mutation_fragment_v2(const schema& s, reader_permit permit, partition_start&& r);
    mutation_fragment_v2(const schema& s, reader_permit permit, partition_end&& r);
    mutation_fragment_v2(const schema& s, reader_permit permit, const mutation_fragment_v2& o)
        : _kind(o._kind), _data(std::make_unique<data>(std::move(permit))) {
        switch (_kind) {
            case kind::static_row:
                new (&_data->_static_row) static_row(s, o._data->_static_row);
                break;
            case kind::clustering_row:
                new (&_data->_clustering_row) clustering_row(s, o._data->_clustering_row);
                break;
            case kind::range_tombstone_change:
                new (&_data->_range_tombstone_chg) range_tombstone_change(o._data->_range_tombstone_chg);
                break;
            case kind::partition_start:
                new (&_data->_partition_start) partition_start(o._data->_partition_start);
                break;
            case kind::partition_end:
                new (&_data->_partition_end) partition_end(o._data->_partition_end);
                break;
        }
        _data->_memory.reset_to(o._data->_memory.resources());
    }
    mutation_fragment_v2(mutation_fragment_v2&& other) = default;
    mutation_fragment_v2& operator=(mutation_fragment_v2&& other) noexcept ;
    [[gnu::always_inline]]
    ~mutation_fragment_v2() {
        if (_data) {
            destroy_data();
        }
    }
    position_in_partition_view position() const;
    // Checks if this fragment may be relevant for any range starting at given position.
    bool relevant_for_range(const schema& s, position_in_partition_view pos) const;
    bool has_key() const ;
    // Requirements: has_key() == true
    const clustering_key_prefix& key() const;
    kind mutation_fragment_kind() const ;
    
    
    bool is_range_tombstone_change() const ;
    bool is_partition_start() const ;
    
    
    
    
    partition_end&& as_end_of_partition() && ;
    const static_row& as_static_row() const & ;
    const clustering_row& as_clustering_row() const & ;
    const range_tombstone_change& as_range_tombstone_change() const & ;
    const partition_start& as_partition_start() const & ;
    const partition_end& as_end_of_partition() const & ;
    // Requirements: mergeable_with(mf)
    void apply(const schema& s, mutation_fragment_v2&& mf);
    template<typename Consumer>
    requires MutationFragmentConsumerV2<Consumer, decltype(std::declval<Consumer>().consume(std::declval<range_tombstone_change>()))>
    decltype(auto) consume(Consumer& consumer) && {
        _data->_memory.reset_to_zero();
        switch (_kind) {
        case kind::static_row:
            return consumer.consume(std::move(_data->_static_row));
        case kind::clustering_row:
            return consumer.consume(std::move(_data->_clustering_row));
        case kind::range_tombstone_change:
            return consumer.consume(std::move(_data->_range_tombstone_chg));
        case kind::partition_start:
            return consumer.consume(std::move(_data->_partition_start));
        case kind::partition_end:
            return consumer.consume(std::move(_data->_partition_end));
        }
        abort();
    }
    template<typename Visitor>
    requires MutationFragmentVisitorV2<Visitor, decltype(std::declval<Visitor>()(std::declval<static_row&>()))>
    decltype(auto) visit(Visitor&& visitor) const {
        switch (_kind) {
        case kind::static_row:
            return visitor(as_static_row());
        case kind::clustering_row:
            return visitor(as_clustering_row());
        case kind::range_tombstone_change:
            return visitor(as_range_tombstone_change());
        case kind::partition_start:
            return visitor(as_partition_start());
        case kind::partition_end:
            return visitor(as_end_of_partition());
        }
        abort();
    }
    size_t memory_usage() const {
        return _data->_memory.resources().memory;
    }
    reader_permit permit() const {
        return _data->_memory.permit();
    }
    bool equal(const schema& s, const mutation_fragment_v2& other) const {
        if (other._kind != _kind) {
            return false;
        }
        switch (_kind) {
        case kind::static_row:
            return as_static_row().equal(s, other.as_static_row());
        case kind::clustering_row:
            return as_clustering_row().equal(s, other.as_clustering_row());
        case kind::range_tombstone_change:
            return as_range_tombstone_change().equal(s, other.as_range_tombstone_change());
        case kind::partition_start:
            return as_partition_start().equal(s, other.as_partition_start());
        case kind::partition_end:
            return as_end_of_partition().equal(s, other.as_end_of_partition());
        }
        abort();
    }
    // Fragments which have the same position() and are mergeable can be
    // merged into one fragment with apply() which represents the sum of
    // writes represented by each of the fragments.
    // Fragments which have the same position() but are not mergeable
    // and at least one of them is not a range_tombstone_change can be emitted one after the other in the stream.
    //
    // Undefined for range_tombstone_change.
    // Merging range tombstones requires a more complicated handling
    // because range_tombstone_change doesn't represent a write on its own, only
    // with a matching change for the end bound. It's not enough to chose one fragment over another,
    // the upper bound of the winning tombstone needs to be taken into account when merging
    // later range_tombstone_change fragments in the stream.
    bool mergeable_with(const mutation_fragment_v2& mf) const {
        return _kind == mf._kind && _kind != kind::range_tombstone_change;
    }
    class printer {
        const schema& _schema;
        const mutation_fragment_v2& _mutation_fragment;
    public:
        printer(const schema& s, const mutation_fragment_v2& mf) : _schema(s), _mutation_fragment(mf) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;
        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);
private:
    size_t calculate_memory_usage(const schema& s) const ;
};
std::ostream& operator<<(std::ostream&, mutation_fragment_v2::kind);
// F gets a stream element as an argument and returns the new value which replaces that element
// in the transformed stream.
template<typename F>
concept StreamedMutationTranformerV2 =
requires(F f, mutation_fragment_v2 mf, schema_ptr s) {
    { f(std::move(mf)) } -> std::same_as<mutation_fragment_v2>;
    { f(s) } -> std::same_as<schema_ptr>;
};
template<typename Consumer>
concept FlatMutationReaderConsumer =
    requires(Consumer c, mutation_fragment mf) {
        { c(std::move(mf)) } -> std::same_as<stop_iteration>;
    } || requires(Consumer c, mutation_fragment mf) {
        { c(std::move(mf)) } -> std::same_as<future<stop_iteration>>;
    };
template<typename T>
concept FlattenedConsumer =
    StreamedMutationConsumer<T> && requires(T obj, const dht::decorated_key& dk) {
        { obj.consume_new_partition(dk) };
        { obj.consume_end_of_partition() };
    };
template<typename T>
concept FlattenedConsumerFilter =
    requires(T filter, const dht::decorated_key& dk, const mutation_fragment& mf) {
        { filter(dk) } -> std::same_as<bool>;
        { filter(mf) } -> std::same_as<bool>;
        { filter.on_end_of_stream() } -> std::same_as<void>;
    };
template<typename Consumer>
concept FlatMutationReaderConsumerV2 =
    requires(Consumer c, mutation_fragment_v2 mf) {
        { c(std::move(mf)) } -> std::same_as<stop_iteration>;
    } || requires(Consumer c, mutation_fragment_v2 mf) {
        { c(std::move(mf)) } -> std::same_as<future<stop_iteration>>;
    };
template<typename Consumer>
concept MutationConsumer =
    requires(Consumer c, mutation m) {
        { c(std::move(m)) } -> std::same_as<stop_iteration>;
    } || requires(Consumer c, mutation m) {
        { c(std::move(m)) } -> std::same_as<future<stop_iteration>>;
    };
template<typename T>
concept FlattenedConsumerV2 =
    StreamedMutationConsumerV2<T> && requires(T obj, const dht::decorated_key& dk) {
        { obj.consume_new_partition(dk) };
        { obj.consume_end_of_partition() };
    };
template<typename T>
concept FlattenedConsumerFilterV2 =
    requires(T filter, const dht::decorated_key& dk, const mutation_fragment_v2& mf) {
        { filter(dk) } -> std::same_as<bool>;
        { filter(mf) } -> std::same_as<bool>;
        { filter.on_end_of_stream() } -> std::same_as<void>;
    };
enum class consume_in_reverse {
    no = 0,
    yes,
};
template<typename T>
concept RangeTombstoneChangeConsumer = std::invocable<T, range_tombstone_change>;
/// Generates range_tombstone_change fragments for a stream of range_tombstone fragments.
///
/// The input range_tombstones passed to consume() may be overlapping, but must be weakly ordered by position().
/// It's ok to pass consecutive range_tombstone objects with the same position.
///
/// Generated range_tombstone_change fragments will have strictly monotonic positions.
///
/// Example usage:
///
///   consume(range_tombstone(1, +inf, t));
///   flush(2, consumer);
///   consume(range_tombstone(2, +inf, t));
///   flush(3, consumer);
///   consume(range_tombstone(4, +inf, t));
///   consume(range_tombstone(4, 7, t));
///   flush(5, consumer);
///   flush(6, consumer);
///
class range_tombstone_change_generator {
    range_tombstone_list _range_tombstones;
    // All range_tombstone_change fragments with positions < than this have been emitted.
    position_in_partition _lower_bound = position_in_partition::before_all_clustered_rows();
    const schema& _schema;
public:
    range_tombstone_change_generator(const schema& s) 
    ;
    // Discards deletion information for positions < lower_bound.
    // After this, the lowest position of emitted range_tombstone_change will be before_key(lower_bound).
    void trim(const position_in_partition& lower_bound) ;
    // Emits range_tombstone_change fragments with positions smaller than upper_bound
    // for accumulated range tombstones. If end_of_range = true, range tombstone
    // change fragments with position equal to upper_bound may also be emitted.
    // After this, only range_tombstones with positions >= upper_bound may be added,
    // which guarantees that they won't affect the output of this flush.
    //
    // If upper_bound == position_in_partition::after_all_clustered_rows(),
    // emits all remaining range_tombstone_changes.
    // No range_tombstones may be added after this.
    //
    // FIXME: respect preemption
     ;
    
    void reset() ;
    bool discardable() const ;
};
struct mutation_consume_cookie {
    using crs_iterator_type = mutation_partition::rows_type::iterator;
    using rts_iterator_type = range_tombstone_list::iterator;
    struct clustering_iterators {
        crs_iterator_type crs_begin;
        crs_iterator_type crs_end;
        rts_iterator_type rts_begin;
        rts_iterator_type rts_end;
        range_tombstone_change_generator rt_gen;
        clustering_iterators(const schema& s, crs_iterator_type crs_b, crs_iterator_type crs_e, rts_iterator_type rts_b, rts_iterator_type rts_e)
            : crs_begin(std::move(crs_b)), crs_end(std::move(crs_e)), rts_begin(std::move(rts_b)), rts_end(std::move(rts_e)), rt_gen(s) { }
    };
    schema_ptr schema;
    bool partition_start_consumed = false;
    bool static_row_consumed = false;
    // only used when reverse == consume_in_reverse::yes
    bool reversed_range_tombstone = false;
    std::unique_ptr<clustering_iterators> iterators;
};
template<typename Result>
struct mutation_consume_result {
    stop_iteration stop;
    Result result;
    mutation_consume_cookie cookie;
};
template<>
struct mutation_consume_result<void> {
    stop_iteration stop;
    mutation_consume_cookie cookie;
};
class mutation final {
private:
    struct data {
        schema_ptr _schema;
        dht::decorated_key _dk;
        mutation_partition _p;
        data(dht::decorated_key&& key, schema_ptr&& schema);
        data(partition_key&& key, schema_ptr&& schema);
        data(schema_ptr&& schema, dht::decorated_key&& key, const mutation_partition& mp);
        data(schema_ptr&& schema, dht::decorated_key&& key, mutation_partition&& mp);
    };
    std::unique_ptr<data> _ptr;
private:
    mutation() = default;
    explicit operator bool() const ;
    friend class optimized_optional<mutation>;
public:
    mutation(schema_ptr schema, dht::decorated_key key)
        : _ptr(std::make_unique<data>(std::move(key), std::move(schema)))
    { }
    mutation(schema_ptr schema, partition_key key_)
        : _ptr(std::make_unique<data>(std::move(key_), std::move(schema)))
    { }
    mutation(schema_ptr schema, dht::decorated_key key, const mutation_partition& mp)
        : _ptr(std::make_unique<data>(std::move(schema), std::move(key), mp))
    { }
    mutation(schema_ptr schema, dht::decorated_key key, mutation_partition&& mp)
        : _ptr(std::make_unique<data>(std::move(schema), std::move(key), std::move(mp)))
    { }
    mutation(const mutation& m)
    ;
    mutation(mutation&&) = default;
    mutation& operator=(mutation&& x) = default;
    mutation& operator=(const mutation& m);
    void set_static_cell(const column_definition& def, atomic_cell_or_collection&& value);
    void set_static_cell(const bytes& name, const data_value& value, api::timestamp_type timestamp, ttl_opt ttl = {});
    void set_clustered_cell(const clustering_key& key, const bytes& name, const data_value& value, api::timestamp_type timestamp, ttl_opt ttl = {});
    void set_clustered_cell(const clustering_key& key, const column_definition& def, atomic_cell_or_collection&& value);
    void set_cell(const clustering_key_prefix& prefix, const bytes& name, const data_value& value, api::timestamp_type timestamp, ttl_opt ttl = {});
    void set_cell(const clustering_key_prefix& prefix, const column_definition& def, atomic_cell_or_collection&& value);
    // Upgrades this mutation to a newer schema. The new schema must
    // be obtained using only valid schema transformation:
    //  * primary key column count must not change
    //  * column types may only change to those with compatible representations
    //
    // After upgrade, mutation's partition should only be accessed using the new schema. User must
    // ensure proper isolation of accesses.
    //
    // Strong exception guarantees.
    //
    // Note that the conversion may lose information, it's possible that m1 != m2 after:
    //
    //   auto m2 = m1;
    //   m2.upgrade(s2);
    //   m2.upgrade(m1.schema());
    //
    void upgrade(const schema_ptr&);
    const partition_key& key() const ;;
    const dht::decorated_key& decorated_key() const ;;
    
    
    const schema_ptr& schema() const { return _ptr->_schema; }
    const mutation_partition& partition() const ;
    mutation_partition& partition() { return _ptr->_p; }
    
    // Consistent with hash<canonical_mutation>
    
public:
    // Consumes the mutation's content.
    //
    // The mutation is in a moved-from alike state after consumption.
    // There are tree ways to consume the mutation:
    // * consume_in_reverse::no - consume in forward order, as defined by the
    //   schema.
    // * consume_in_reverse::yes - consume in reverse order, as if the schema
    //   had the opposite clustering order. This effectively reverses the
    //   mutation's content, according to the native reverse order[1].
    //
    // For definition of [1] and [2] see docs/dev/reverse-reads.md.
    //
    // The consume operation is pausable and resumable:
    // * To pause return stop_iteration::yes from one of the consume() methods;
    // * The consume will now stop and return;
    // * To resume call consume again and pass the cookie member of the returned
    //   mutation_consume_result as the cookie parameter;
    //
    // Note that `consume_end_of_partition()` and `consume_end_of_stream()`
    // will be called each time the consume is stopping, regardless of whether
    // you are pausing or the consumption is ending for good.
    template<FlattenedConsumerV2 Consumer>
    auto consume(Consumer& consumer, consume_in_reverse reverse, mutation_consume_cookie cookie = {}) && -> mutation_consume_result<decltype(consumer.consume_end_of_stream())>;
    template<FlattenedConsumerV2 Consumer>
    auto consume_gently(Consumer& consumer, consume_in_reverse reverse, mutation_consume_cookie cookie = {}) && -> future<mutation_consume_result<decltype(consumer.consume_end_of_stream())>>;
    // See mutation_partition::live_row_count()
    uint64_t live_row_count(gc_clock::time_point query_time = gc_clock::time_point::min()) const;
    void apply(mutation&&);
    void apply(const mutation&);
    void apply(const mutation_fragment&);
    mutation operator+(const mutation& other) const;
    mutation& operator+=(const mutation& other);
    mutation& operator+=(mutation&& other);
    // Returns a subset of this mutation holding only information relevant for given clustering ranges.
    // Range tombstones will be trimmed to the boundaries of the clustering ranges.
    mutation sliced(const query::clustering_row_ranges&) const;
    unsigned shard_of() const ;
    // Returns a mutation which contains the same writes but in a minimal form.
    // Drops data covered by tombstones.
    // Does not drop expired tombstones.
    // Does not expire TTLed data.
    mutation compacted() const;
private:
    friend std::ostream& operator<<(std::ostream& os, const mutation& m);
};
namespace {
template<consume_in_reverse reverse, FlattenedConsumerV2 Consumer>
std::optional<stop_iteration> consume_clustering_fragments(schema_ptr s, mutation_partition& partition, Consumer& consumer, mutation_consume_cookie& cookie, is_preemptible preempt = is_preemptible::no) ;
} // anonymous namespace
template<FlattenedConsumerV2 Consumer>
auto mutation::consume(Consumer& consumer, consume_in_reverse reverse, mutation_consume_cookie cookie) &&
        -> mutation_consume_result<decltype(consumer.consume_end_of_stream())> {
    auto& partition = _ptr->_p;
    if (!cookie.partition_start_consumed) {
        consumer.consume_new_partition(_ptr->_dk);
        if (partition.partition_tombstone()) {
            consumer.consume(partition.partition_tombstone());
        }
        cookie.partition_start_consumed = true;
    }
    stop_iteration stop = stop_iteration::no;
    if (!cookie.static_row_consumed && !partition.static_row().empty()) {
        stop = consumer.consume(static_row(std::move(partition.static_row().get_existing())));
    }
    cookie.static_row_consumed = true;
    if (reverse == consume_in_reverse::yes) {
        stop = *consume_clustering_fragments<consume_in_reverse::yes>(_ptr->_schema, partition, consumer, cookie);
    } else {
        stop = *consume_clustering_fragments<consume_in_reverse::no>(_ptr->_schema, partition, consumer, cookie);
    }
    const auto stop_consuming = consumer.consume_end_of_partition();
    using consume_res_type = decltype(consumer.consume_end_of_stream());
    if constexpr (std::is_same_v<consume_res_type, void>) {
        consumer.consume_end_of_stream();
        return mutation_consume_result<void>{stop_consuming, std::move(cookie)};
    } else {
        return mutation_consume_result<consume_res_type>{stop_consuming, consumer.consume_end_of_stream(), std::move(cookie)};
    }
}
template<FlattenedConsumerV2 Consumer>
auto mutation::consume_gently(Consumer& consumer, consume_in_reverse reverse, mutation_consume_cookie cookie) &&
        -> future<mutation_consume_result<decltype(consumer.consume_end_of_stream())>> {
    auto& partition = _ptr->_p;
    if (!cookie.partition_start_consumed) {
        consumer.consume_new_partition(_ptr->_dk);
        if (partition.partition_tombstone()) {
            consumer.consume(partition.partition_tombstone());
        }
        cookie.partition_start_consumed = true;
    }
    stop_iteration stop = stop_iteration::no;
    if (!cookie.static_row_consumed && !partition.static_row().empty()) {
        stop = consumer.consume(static_row(std::move(partition.static_row().get_existing())));
    }
    cookie.static_row_consumed = true;
    std::optional<stop_iteration> stop_opt;
    if (reverse == consume_in_reverse::yes) {
        while (!(stop_opt = consume_clustering_fragments<consume_in_reverse::yes>(_ptr->_schema, partition, consumer, cookie, is_preemptible::yes))) {
            co_await yield();
        }
    } else {
        while (!(stop_opt = consume_clustering_fragments<consume_in_reverse::no>(_ptr->_schema, partition, consumer, cookie, is_preemptible::yes))) {
            co_await yield();
        }
    }
    stop = *stop_opt;
    const auto stop_consuming = consumer.consume_end_of_partition();
    using consume_res_type = decltype(consumer.consume_end_of_stream());
    if constexpr (std::is_same_v<consume_res_type, void>) {
        consumer.consume_end_of_stream();
        co_return mutation_consume_result<void>{stop_consuming, std::move(cookie)};
    } else {
        co_return mutation_consume_result<consume_res_type>{stop_consuming, consumer.consume_end_of_stream(), std::move(cookie)};
    }
}
struct mutation_equals_by_key {
    
};
struct mutation_hash_by_key {
    
};
struct mutation_decorated_key_less_comparator {
    bool operator()(const mutation& m1, const mutation& m2) const;
};
using mutation_opt = optimized_optional<mutation>;
// Consistent with operator==()
// Consistent across the cluster, so should not rely on particular
// serialization format, only on actual data stored.
template<>
struct appending_hash<mutation> {
    template<typename Hasher>
    void operator()(Hasher& h, const mutation& m) const ;
};
// Returns a range into partitions containing mutations covered by the range.
// partitions must be sorted according to decorated key.
// range must not wrap around.
boost::iterator_range<std::vector<mutation>::const_iterator> slice(
    const std::vector<mutation>& partitions,
    const dht::partition_range&);
// Reverses the mutation as if it was created with a schema with reverse
// clustering order. The resulting mutation will contain a reverse schema too.
namespace service::broadcast_tables {
    class update_query;
}
namespace cql3 {
class constants {
public:
#if 0
    private static final Logger logger = LoggerFactory.getLogger(Constants.class);
#endif
public:
    class setter : public operation_skip_if_unset {
    public:
        using operation_skip_if_unset::operation_skip_if_unset;
    };
    struct adder final : operation_skip_if_unset {
        using operation_skip_if_unset::operation_skip_if_unset;
    };
    struct subtracter final : operation_skip_if_unset {
        using operation_skip_if_unset::operation_skip_if_unset;
    };
    class deleter : public operation_no_unset_support {
    public:
    };
};
}
namespace cql3 {
class maps {
private:
public:
    class setter : public operation_skip_if_unset {
    public:
    };
    class setter_by_key : public operation_skip_if_unset {
        expr::expression _k;
    public:
    };
    class putter : public operation_skip_if_unset {
    public:
    };
    class discarder_by_key : public operation_no_unset_support {
    public:
    };
};
}
namespace cql3 {
class sets {
public:
    class setter : public operation_skip_if_unset {
    public:
    };
    class adder : public operation_skip_if_unset {
    public:
    };
    // Note that this is reused for Map subtraction too (we subtract a set from a map)
    class discarder : public operation_skip_if_unset {
    public:
    };
    class element_discarder : public operation_no_unset_support {
    public:
    };
};
}
namespace cql3 {
}
namespace cql3 {
}
namespace cql3 {
class keyspace_element_name {
    std::optional<sstring> _ks_name = std::nullopt;
public:
protected:
};
}
namespace cql3 {
class cf_name final : public keyspace_element_name {
    sstring _cf_name = "";
public:
};
}
class schema;
namespace cql3 {
class column_identifier;
class column_specification;
namespace functions { class function_call; }
class prepare_context final {
private:
    std::vector<shared_ptr<column_identifier>> _variable_names;
    std::vector<lw_shared_ptr<column_specification>> _specs;
    std::vector<lw_shared_ptr<column_specification>> _target_columns;
    // A list of pointers to prepared `function_call` cache ids, that
    // participate in partition key ranges computation within an LWT statement.
    std::vector<::shared_ptr<std::optional<uint8_t>>> _pk_function_calls_cache_ids;
    // The flag denoting whether the context is currently in partition key
    // processing mode (inside query restrictions AST nodes). If set to true,
    // then every `function_call` instance will be recorded in the context and
    // will be assigned an identifier, which will then be used for caching
    // the function call results.
    bool _processing_pk_restrictions = false;
public:
    // Record a new function call, which evaluates a partition key constraint.
    // Also automatically assigns an id to the AST node for caching purposes.
    // Inform the context object that it has started or ended processing the
    // partition key part of statement restrictions.
};
}
namespace cql3 {
class column_identifier;
class cql_stats;
namespace statements {
class prepared_statement;
namespace raw {
class parsed_statement {
protected:
    prepare_context _prepare_ctx;
public:
};
}
}
}
namespace service { class client_state; }
namespace cql3 {
namespace statements {
namespace raw {
class cf_statement : public parsed_statement {
protected:
    std::optional<cf_name> _cf_name;
public:
    // Only for internal calls, use the version with ClientState for user queries
};
}
}
}
namespace cql3 {
class prepare_context;
class column_specification;
class cql_statement;
namespace statements {
struct invalidated_prepared_usage_attempt {
};
class prepared_statement : public seastar::weakly_referencable<prepared_statement> {
public:
    typedef seastar::checked_ptr<seastar::weak_ptr<prepared_statement>> checked_weak_ptr;
public:
    const seastar::shared_ptr<cql_statement> statement;
    const std::vector<seastar::lw_shared_ptr<column_specification>> bound_names;
    std::vector<uint16_t> partition_key_bind_indices;
    std::vector<sstring> warnings;
};
}
}
namespace cql3 {
class query_options;
class prepare_context;
class attributes final {
private:
    expr::unset_bind_variable_guard _timestamp_unset_guard;
    std::optional<cql3::expr::expression> _timestamp;
    expr::unset_bind_variable_guard _time_to_live_unset_guard;
    std::optional<cql3::expr::expression> _time_to_live;
    std::optional<cql3::expr::expression> _timeout;
public:
private:
public:
    class raw final {
    public:
        std::optional<cql3::expr::expression> timestamp;
        std::optional<cql3::expr::expression> time_to_live;
        std::optional<cql3::expr::expression> timeout;
    private:
    };
};
}
template<typename T>
concept HasMapInterface = requires(T t) {
    typename std::remove_reference<T>::type::mapped_type;
    typename std::remove_reference<T>::type::key_type;
    typename std::remove_reference<T>::type::value_type;
    t.find(typename std::remove_reference<T>::type::key_type());
    t.begin();
    t.end();
    t.cbegin();
    t.cend();
};
/// A Boost program option holding an enum value.
///
/// The options parser will parse enum values with the help of the Mapper class, which provides a mapping
/// between some parsable form (eg, string) and the enum value.  For example, it may map the word "January" to
/// the enum value JANUARY.
///
/// Mapper must have a static method `map()` that returns a map from a streamable key type (eg, string) to the
/// enum in question.  In fact, enum_option knows which enum it represents only by referencing
/// Mapper::map().mapped_type.
///
/// \note one enum_option holds only one enum value.  When multiple choices are allowed, use
/// vector<enum_option>.
///
/// Example:
///
/// struct Type {
///   enum class ty { a1, a2, b1 };
///   static unordered_map<string, ty> map();
/// };
/// unordered_map<string, Type::ty> Type::map() {
///   return {{"a1", Type::ty::a1}, {"a2", Type::ty::a2}, {"b1", Type::ty::b1}};
/// }
/// int main(int ac, char* av[]) {
///   namespace po = boost::program_options;
///   po::options_description desc("Allowed options");
///   desc.add_options()
///     ("val", po::value<enum_option<Type>>(), "Single Type")
///     ("vec", po::value<vector<enum_option<Type>>>()->multitoken(), "Type vector");
/// }
template<typename Mapper>
requires HasMapInterface<decltype(Mapper::map())>
class enum_option {
    using map_t = typename std::remove_reference<decltype(Mapper::map())>::type;
    typename map_t::mapped_type _value;
    map_t _map;
  public:
    // For smooth conversion from enum values:
    enum_option(const typename map_t::mapped_type& v) : _value(v), _map(Mapper::map()) {}
    // So values can be default-constructed before streaming into them:
    enum_option() : _map(Mapper::map()) {}
    bool operator==(const enum_option<Mapper>& that) const {
        return _value == that._value;
    }
    // For comparison with enum values using if or switch:
    bool operator==(typename map_t::mapped_type value) const {
        return _value == value;
    }
    operator typename map_t::mapped_type() const {
        return _value;
    }
    // For program_options parser:
    friend std::istream& operator>>(std::istream& s, enum_option<Mapper>& opt) {
        typename map_t::key_type key;
        s >> key;
        const auto found = opt._map.find(key);
        if (found == opt._map.end()) {
            std::string text;
            if (s.rdstate() & s.failbit) {
                // key wasn't read successfully.
                s >> text;
            } else {
                // Turn key into text.
                std::ostringstream temp;
                temp << key;
                text = temp.str();
            }
            throw boost::program_options::invalid_option_value(text);
        }
        opt._value = found->second;
        return s;
    }
    // For various printers and formatters:
    friend std::ostream& operator<<(std::ostream& s, const enum_option<Mapper>& opt) {
        auto found = find_if(opt._map.cbegin(), opt._map.cend(),
                             [&opt](const typename map_t::value_type& e) { return e.second == opt._value; });
        if (found == opt._map.cend()) {
            return s << "?unknown";
        } else {
            return s << found->first;
        }
    }
};
namespace gms {
    class inet_address;
} // namespace gms
namespace locator { class topology; }
namespace db {
namespace hints {
// host_filter tells hints_manager towards which endpoints it is allowed to generate hints.
class host_filter final {
private:
    enum class enabled_kind {
        enabled_for_all,
        enabled_selectively,
        disabled_for_all,
    };
    enabled_kind _enabled_kind;
    std::unordered_set<sstring> _dcs;
public:
    struct enabled_for_all_tag {};
    struct disabled_for_all_tag {};
    // Creates a filter that allows hints to all endpoints (default)
    // Creates a filter that does not allow any hints.
    // Creates a filter that allows sending hints to specified DCs.
    // Parses hint filtering configuration from the hinted_handoff_enabled option.
    // Parses hint filtering configuration from a list of DCs.
};
class hints_configuration_parse_error : public std::runtime_error {
public:
    using std::runtime_error::runtime_error;
};
}
}
namespace s3 {
struct endpoint_config {
    unsigned port;
    bool use_https;
    struct aws_config {
        std::string key;
        std::string secret;
        std::string region;
    };
    std::optional<aws_config> aws;
};
using endpoint_config_ptr = seastar::lw_shared_ptr<endpoint_config>;
} // s3 namespace
namespace seastar {
class file;
struct logging_settings;
namespace tls {
class credentials_builder;
}
namespace log_cli {
class options;
}
}
namespace db {
namespace fs = std::filesystem;
class extensions;
struct seed_provider_type {
    sstring class_name;
    std::unordered_map<sstring, sstring> parameters;
};
// Describes a single error injection that should be enabled at startup.
struct error_injection_at_startup {
    sstring name;
    bool one_shot = false;
};
}
namespace utils {
}
namespace db {
/// Enumeration of all valid values for the `experimental` config entry.
struct experimental_features_t {
    // NOTE: RAFT and BROADCAST_TABLES features are not enabled via `experimental` umbrella flag.
    // These options should be enabled explicitly.
    // RAFT feature has to be enabled if BROADCAST_TABLES or TABLETS is enabled.
    enum class feature {
        UNUSED,
        UDF,
        ALTERNATOR_STREAMS,
        RAFT,
        BROADCAST_TABLES,
        KEYSPACE_STORAGE_OPTIONS,
        TABLETS,
    };
    static std::map<sstring, feature> map(); // See enum_option.
    static std::vector<enum_option<experimental_features_t>> all();
};
/// A restriction that can be in three modes: true (the operation is disabled),
/// false (the operation is allowed), or warn (the operation is allowed but
/// produces a warning in the log).
struct tri_mode_restriction_t {
    enum class mode { FALSE, TRUE, WARN };
    static std::unordered_map<sstring, mode> map(); // for enum_option<>
};
using tri_mode_restriction = enum_option<tri_mode_restriction_t>;
constexpr unsigned default_murmur3_partitioner_ignore_msb_bits = 12;
class config : public utils::config_file {
public:
    config();
    
    // For testing only
    /// True iff the feature is enabled.
    using string_map = std::unordered_map<sstring, sstring>;
                    //program_options::string_map;
    using string_list = std::vector<sstring>;
    using seed_provider_type = db::seed_provider_type;
    using hinted_handoff_enabled_type = db::hints::host_filter;
    using error_injection_at_startup = db::error_injection_at_startup;
    named_value<double> background_writer_scheduling_quota;
    named_value<bool> auto_adjust_flush_quota;
    named_value<float> memtable_flush_static_shares;
    named_value<float> compaction_static_shares;
    named_value<bool> compaction_enforce_min_threshold;
    named_value<sstring> cluster_name;
    named_value<sstring> listen_address;
    named_value<sstring> listen_interface;
    named_value<bool> listen_interface_prefer_ipv6;
    named_value<sstring> work_directory;
    named_value<sstring> commitlog_directory;
    named_value<sstring> schema_commitlog_directory;
    named_value<string_list> data_file_directories;
    named_value<sstring> hints_directory;
    named_value<sstring> view_hints_directory;
    named_value<sstring> saved_caches_directory;
    named_value<sstring> commit_failure_policy;
    named_value<sstring> disk_failure_policy;
    named_value<sstring> endpoint_snitch;
    named_value<sstring> rpc_address;
    named_value<sstring> rpc_interface;
    named_value<bool> rpc_interface_prefer_ipv6;
    named_value<seed_provider_type> seed_provider;
    named_value<uint32_t> compaction_throughput_mb_per_sec;
    named_value<uint32_t> compaction_large_partition_warning_threshold_mb;
    named_value<uint32_t> compaction_large_row_warning_threshold_mb;
    named_value<uint32_t> compaction_large_cell_warning_threshold_mb;
    named_value<uint32_t> compaction_rows_count_warning_threshold;
    named_value<uint32_t> compaction_collection_elements_count_warning_threshold;
    named_value<uint32_t> memtable_total_space_in_mb;
    named_value<uint32_t> concurrent_reads;
    named_value<uint32_t> concurrent_writes;
    named_value<uint32_t> concurrent_counter_writes;
    named_value<bool> incremental_backups;
    named_value<bool> snapshot_before_compaction;
    named_value<uint32_t> phi_convict_threshold;
    named_value<uint32_t> failure_detector_timeout_in_ms;
    named_value<sstring> commitlog_sync;
    named_value<uint32_t> commitlog_segment_size_in_mb;
    named_value<uint32_t> commitlog_sync_period_in_ms;
    named_value<uint32_t> commitlog_sync_batch_window_in_ms;
    named_value<int64_t> commitlog_total_space_in_mb;
    named_value<bool> commitlog_reuse_segments; // unused. retained for upgrade compat
    named_value<int64_t> commitlog_flush_threshold_in_mb;
    named_value<bool> commitlog_use_o_dsync;
    named_value<bool> commitlog_use_hard_size_limit;
    named_value<bool> compaction_preheat_key_cache;
    named_value<uint32_t> concurrent_compactors;
    named_value<uint32_t> in_memory_compaction_limit_in_mb;
    named_value<bool> preheat_kernel_page_cache;
    named_value<uint32_t> sstable_preemptive_open_interval_in_mb;
    named_value<bool> defragment_memory_on_idle;
    named_value<sstring> memtable_allocation_type;
    named_value<double> memtable_cleanup_threshold;
    named_value<uint32_t> file_cache_size_in_mb;
    named_value<uint32_t> memtable_flush_queue_size;
    named_value<uint32_t> memtable_flush_writers;
    named_value<uint32_t> memtable_heap_space_in_mb;
    named_value<uint32_t> memtable_offheap_space_in_mb;
    named_value<uint32_t> column_index_size_in_kb;
    named_value<uint32_t> column_index_auto_scale_threshold_in_kb;
    named_value<uint32_t> index_summary_capacity_in_mb;
    named_value<uint32_t> index_summary_resize_interval_in_minutes;
    named_value<double> reduce_cache_capacity_to;
    named_value<double> reduce_cache_sizes_at;
    named_value<uint32_t> stream_throughput_outbound_megabits_per_sec;
    named_value<uint32_t> inter_dc_stream_throughput_outbound_megabits_per_sec;
    named_value<uint32_t> stream_io_throughput_mb_per_sec;
    named_value<bool> trickle_fsync;
    named_value<uint32_t> trickle_fsync_interval_in_kb;
    named_value<bool> auto_bootstrap;
    named_value<uint32_t> batch_size_warn_threshold_in_kb;
    named_value<uint32_t> batch_size_fail_threshold_in_kb;
    named_value<sstring> broadcast_address;
    named_value<bool> listen_on_broadcast_address;
    named_value<sstring> initial_token;
    named_value<uint32_t> num_tokens;
    named_value<sstring> partitioner;
    named_value<uint16_t> storage_port;
    named_value<bool> auto_snapshot;
    named_value<uint32_t> key_cache_keys_to_save;
    named_value<uint32_t> key_cache_save_period;
    named_value<uint32_t> key_cache_size_in_mb;
    named_value<uint32_t> row_cache_keys_to_save;
    named_value<uint32_t> row_cache_size_in_mb;
    named_value<uint32_t> row_cache_save_period;
    named_value<sstring> memory_allocator;
    named_value<uint32_t> counter_cache_size_in_mb;
    named_value<uint32_t> counter_cache_save_period;
    named_value<uint32_t> counter_cache_keys_to_save;
    named_value<uint32_t> tombstone_warn_threshold;
    named_value<uint32_t> tombstone_failure_threshold;
    named_value<uint64_t> query_tombstone_page_limit;
    named_value<uint32_t> range_request_timeout_in_ms;
    named_value<uint32_t> read_request_timeout_in_ms;
    named_value<uint32_t> counter_write_request_timeout_in_ms;
    named_value<uint32_t> cas_contention_timeout_in_ms;
    named_value<uint32_t> truncate_request_timeout_in_ms;
    named_value<uint32_t> write_request_timeout_in_ms;
    named_value<uint32_t> request_timeout_in_ms;
    named_value<bool> cross_node_timeout;
    named_value<uint32_t> internode_send_buff_size_in_bytes;
    named_value<uint32_t> internode_recv_buff_size_in_bytes;
    named_value<sstring> internode_compression;
    named_value<bool> inter_dc_tcp_nodelay;
    named_value<uint32_t> streaming_socket_timeout_in_ms;
    named_value<bool> start_native_transport;
    named_value<uint16_t> native_transport_port;
    named_value<uint16_t> native_transport_port_ssl;
    named_value<uint16_t> native_shard_aware_transport_port;
    named_value<uint16_t> native_shard_aware_transport_port_ssl;
    named_value<uint32_t> native_transport_max_threads;
    named_value<uint32_t> native_transport_max_frame_size_in_mb;
    named_value<sstring> broadcast_rpc_address;
    named_value<uint16_t> rpc_port;
    named_value<bool> start_rpc;
    named_value<bool> rpc_keepalive;
    named_value<uint32_t> rpc_max_threads;
    named_value<uint32_t> rpc_min_threads;
    named_value<uint32_t> rpc_recv_buff_size_in_bytes;
    named_value<uint32_t> rpc_send_buff_size_in_bytes;
    named_value<sstring> rpc_server_type;
    named_value<bool> cache_hit_rate_read_balancing;
    named_value<double> dynamic_snitch_badness_threshold;
    named_value<uint32_t> dynamic_snitch_reset_interval_in_ms;
    named_value<uint32_t> dynamic_snitch_update_interval_in_ms;
    named_value<hinted_handoff_enabled_type> hinted_handoff_enabled;
    named_value<uint32_t> max_hinted_handoff_concurrency;
    named_value<uint32_t> hinted_handoff_throttle_in_kb;
    named_value<uint32_t> max_hint_window_in_ms;
    named_value<uint32_t> max_hints_delivery_threads;
    named_value<uint32_t> batchlog_replay_throttle_in_kb;
    named_value<sstring> request_scheduler;
    named_value<sstring> request_scheduler_id;
    named_value<string_map> request_scheduler_options;
    named_value<uint32_t> thrift_framed_transport_size_in_mb;
    named_value<uint32_t> thrift_max_message_length_in_mb;
    named_value<sstring> authenticator;
    named_value<sstring> internode_authenticator;
    named_value<sstring> authorizer;
    named_value<sstring> role_manager;
    named_value<uint32_t> permissions_validity_in_ms;
    named_value<uint32_t> permissions_update_interval_in_ms;
    named_value<uint32_t> permissions_cache_max_entries;
    named_value<string_map> server_encryption_options;
    named_value<string_map> client_encryption_options;
    named_value<string_map> alternator_encryption_options;
    named_value<uint32_t> ssl_storage_port;
    named_value<bool> enable_in_memory_data_store;
    named_value<bool> enable_cache;
    named_value<bool> enable_commitlog;
    named_value<bool> volatile_system_keyspace_for_testing;
    named_value<uint16_t> api_port;
    named_value<sstring> api_address;
    named_value<sstring> api_ui_dir;
    named_value<sstring> api_doc_dir;
    named_value<sstring> load_balance;
    named_value<bool> consistent_rangemovement;
    named_value<bool> join_ring;
    named_value<bool> load_ring_state;
    named_value<sstring> replace_node_first_boot;
    named_value<sstring> replace_address;
    named_value<sstring> replace_address_first_boot;
    named_value<sstring> ignore_dead_nodes_for_replace;
    named_value<bool> override_decommission;
    named_value<bool> enable_repair_based_node_ops;
    named_value<sstring> allowed_repair_based_node_ops;
    named_value<uint32_t> ring_delay_ms;
    named_value<uint32_t> shadow_round_ms;
    named_value<uint32_t> fd_max_interval_ms;
    named_value<uint32_t> fd_initial_value_ms;
    named_value<uint32_t> shutdown_announce_in_ms;
    named_value<bool> developer_mode;
    named_value<int32_t> skip_wait_for_gossip_to_settle;
    named_value<int32_t> force_gossip_generation;
    named_value<bool> experimental;
    named_value<std::vector<enum_option<experimental_features_t>>> experimental_features;
    named_value<size_t> lsa_reclamation_step;
    named_value<uint16_t> prometheus_port;
    named_value<sstring> prometheus_address;
    named_value<sstring> prometheus_prefix;
    named_value<bool> abort_on_lsa_bad_alloc;
    named_value<unsigned> murmur3_partitioner_ignore_msb_bits;
    named_value<double> unspooled_dirty_soft_limit;
    named_value<double> sstable_summary_ratio;
    named_value<size_t> large_memory_allocation_warning_threshold;
    named_value<bool> enable_deprecated_partitioners;
    named_value<bool> enable_keyspace_column_family_metrics;
    named_value<bool> enable_sstable_data_integrity_check;
    named_value<bool> enable_sstable_key_validation;
    named_value<bool> cpu_scheduler;
    named_value<bool> view_building;
    named_value<bool> enable_sstables_mc_format;
    named_value<bool> enable_sstables_md_format;
    named_value<sstring> sstable_format;
    named_value<bool> enable_dangerous_direct_import_of_cassandra_counters;
    named_value<bool> enable_shard_aware_drivers;
    named_value<bool> enable_ipv6_dns_lookup;
    named_value<bool> abort_on_internal_error;
    named_value<uint32_t> max_partition_key_restrictions_per_query;
    named_value<uint32_t> max_clustering_key_restrictions_per_query;
    named_value<uint64_t> max_memory_for_unlimited_query_soft_limit;
    named_value<uint64_t> max_memory_for_unlimited_query_hard_limit;
    named_value<uint32_t> reader_concurrency_semaphore_serialize_limit_multiplier;
    named_value<uint32_t> reader_concurrency_semaphore_kill_limit_multiplier;
    named_value<uint32_t> twcs_max_window_count;
    named_value<unsigned> initial_sstable_loading_concurrency;
    named_value<bool> enable_3_1_0_compatibility_mode;
    named_value<bool> enable_user_defined_functions;
    named_value<unsigned> user_defined_function_time_limit_ms;
    named_value<unsigned> user_defined_function_allocation_limit_bytes;
    named_value<unsigned> user_defined_function_contiguous_allocation_limit_bytes;
    named_value<uint32_t> schema_registry_grace_period;
    named_value<uint32_t> max_concurrent_requests_per_shard;
    named_value<bool> cdc_dont_rewrite_streams;
    named_value<tri_mode_restriction> strict_allow_filtering;
    named_value<bool> reversed_reads_auto_bypass_cache;
    named_value<bool> enable_optimized_reversed_reads;
    named_value<bool> enable_cql_config_updates;
    named_value<bool> enable_parallelized_aggregation;
    named_value<uint16_t> alternator_port;
    named_value<uint16_t> alternator_https_port;
    named_value<sstring> alternator_address;
    named_value<bool> alternator_enforce_authorization;
    named_value<sstring> alternator_write_isolation;
    named_value<uint32_t> alternator_streams_time_window_s;
    named_value<uint32_t> alternator_timeout_in_ms;
    named_value<double> alternator_ttl_period_in_seconds;
    named_value<bool> abort_on_ebadf;
    named_value<uint16_t> redis_port;
    named_value<uint16_t> redis_ssl_port;
    named_value<sstring> redis_read_consistency_level;
    named_value<sstring> redis_write_consistency_level;
    named_value<uint16_t> redis_database_count;
    named_value<string_map> redis_keyspace_replication_strategy_options;
    named_value<bool> sanitizer_report_backtrace;
    named_value<bool> flush_schema_tables_after_modification;
    // Options to restrict (forbid, warn or somehow limit) certain operations
    // or options which non-expert users are more likely to regret than to
    // enjoy:
    named_value<tri_mode_restriction> restrict_replication_simplestrategy;
    named_value<tri_mode_restriction> restrict_dtcs;
    named_value<tri_mode_restriction> restrict_twcs_without_default_ttl;
    named_value<bool> restrict_future_timestamp;
    named_value<bool> ignore_truncation_record;
    named_value<bool> force_schema_commit_log;
    named_value<uint32_t> task_ttl_seconds;
    named_value<uint32_t> nodeops_watchdog_timeout_seconds;
    named_value<uint32_t> nodeops_heartbeat_interval_seconds;
    named_value<bool> cache_index_pages;
    named_value<unsigned> x_log2_compaction_groups;
    named_value<bool> consistent_cluster_management;
    named_value<double> wasm_cache_memory_fraction;
    named_value<uint32_t> wasm_cache_timeout_in_ms;
    named_value<size_t> wasm_cache_instance_size_limit;
    named_value<uint64_t> wasm_udf_yield_fuel;
    named_value<uint64_t> wasm_udf_total_fuel;
    named_value<size_t> wasm_udf_memory_limit;
    named_value<sstring> relabel_config_file;
    named_value<sstring> object_storage_config_file;
    // wasm_udf_reserved_memory is static because the options in db::config
    // are parsed using seastar::app_template, while this option is used for
    // configuring the Seastar memory subsystem.
    static constexpr size_t wasm_udf_reserved_memory = 50 * 1024 * 1024;
    named_value<unsigned> minimum_keyspace_rf;
    locator::host_id host_id;
    utils::updateable_value<std::unordered_map<sstring, s3::endpoint_config>> object_storage_config;
    named_value<std::vector<error_injection_at_startup>> error_injections_at_startup;
    static const sstring default_tls_priority;
private:
    template<typename T>
    struct log_legacy_value : public named_value<T> {
        using MyBase = named_value<T>;
        using MyBase::MyBase;
        // do not add to boost::options. We only care about yaml config
    };
    log_legacy_value<seastar::log_level> default_log_level;
    log_legacy_value<std::unordered_map<sstring, seastar::log_level>> logger_log_level;
    log_legacy_value<bool> log_to_stdout, log_to_syslog;
    std::shared_ptr<db::extensions> _extensions;
};
}
namespace utils {
 ;
}
namespace cql3 {
namespace selection {
    class selection;
    class raw_selector;
} // namespace selection
namespace restrictions {
    class statement_restrictions;
} // namespace restrictions
namespace statements {
namespace raw {
class select_statement : public cf_statement
{
public:
    // Ordering of selected values as defined by the basic comparison order.
    // Even for a column that by default has ordering 4, 3, 2, 1 ordering it in ascending order will result in 1, 2, 3, 4.
    enum class ordering {
        ascending,
        descending
    };
    class parameters final {
    public:
        using orderings_type = std::vector<std::pair<shared_ptr<column_identifier::raw>, ordering>>;
        enum class statement_subtype { REGULAR, JSON, PRUNE_MATERIALIZED_VIEW };
    private:
        const orderings_type _orderings;
        const bool _is_distinct;
        const bool _allow_filtering;
        const statement_subtype _statement_subtype;
        bool _bypass_cache = false;
    public:
    };
    template<typename T>
    using compare_fn = std::function<bool(const T&, const T&)>;
    using result_row_type = std::vector<managed_bytes_opt>;
    using ordering_comparator_type = compare_fn<result_row_type>;
private:
    using prepared_orderings_type = std::vector<std::pair<const column_definition*, ordering>>;
private:
    lw_shared_ptr<const parameters> _parameters;
    std::vector<::shared_ptr<selection::raw_selector>> _select_clause;
    expr::expression _where_clause;
    std::optional<expr::expression> _limit;
    std::optional<expr::expression> _per_partition_limit;
    std::vector<::shared_ptr<cql3::column_identifier::raw>> _group_by_columns;
    std::unique_ptr<cql3::attributes::raw> _attrs;
public:
private:
    // Checks whether it is legal to have ORDER BY in this statement
    // Processes ORDER BY column orderings, converts column_identifiers to column_defintions
    // Checks whether this ordering reverses all results.
    // We only allow leaving select results unchanged or reversing them.
    /// Returns indices of GROUP BY cells in fetched rows.
#if 0
    public:
        virtual sstring to_string() override {
            return sstring("raw_statement(")
                + "name=" + cf_name->to_string()
                + ", selectClause=" + to_string(_select_clause)
                + ", whereClause=" + to_string(_where_clause)
                + ", isDistinct=" + to_string(_parameters->is_distinct())
                + ", isJson=" + to_string(_parameters->is_json())
                + ")";
        }
    };
#endif
};
}
}
}
namespace cql3 {
namespace util {

/// build a CQL "select" statement with the desired parameters.
/// If select_all_columns==true, all columns are selected and the value of
/// selected_columns is ignored.
std::unique_ptr<cql3::statements::raw::select_statement> build_select_statement(
        const sstring_view& cf_name,
        const sstring_view& where_clause,
        bool select_all_columns,
        const std::vector<column_definition>& selected_columns);
/// maybe_quote() takes an identifier - the name of a column, table or
/// keyspace name - and transforms it to a string which can be used in CQL
/// commands. Namely, if the identifier is not entirely lower-case (including
/// digits and underscores), it needs to be quoted to be represented in CQL.
/// Without this quoting, CQL folds uppercase letters to lower case, and
/// forbids non-alpha-numeric characters in identifier names.
/// Quoting involves wrapping the string in double-quotes ("). A double-quote
/// character itself is quoted by doubling it.
/// maybe_quote() also quotes reserved CQL keywords (e.g., "to", "where")
/// but doesn't quote *unreserved* keywords (like ttl, int or as).
/// Note that this means that if new reserved keywords are added to the
/// parser, a saved output of maybe_quote() may no longer be parsable by
/// parser. To avoid this forward-compatibility issue, use quote() instead
/// of maybe_quote() - to unconditionally quote an identifier even if it is
/// lowercase and not (yet) a keyword.
 sstring maybe_quote(const sstring& s) ;
/// quote() takes an identifier - the name of a column, table or keyspace -
/// and transforms it to a string which can be safely used in CQL commands.
/// Quoting involves wrapping the name in double-quotes ("). A double-quote
/// character itself is quoted by doubling it.
/// Quoting is necessary when the identifier contains non-alpha-numeric
/// characters, when it contains uppercase letters (which will be folded to
/// lowercase if not quoted), or when the identifier is one of many CQL
/// keywords. But it's allowed - and easier - to just unconditionally
/// quote the identifier name in CQL, so that is what this function does does.

/// single_quote() takes a string and transforms it to a string 
/// which can be safely used in CQL commands.
/// Single quoting involves wrapping the name in single-quotes ('). A sigle-quote
/// character itself is quoted by doubling it.
/// Single quoting is necessary for dates, IP addresses or string literals.
 
// Check whether timestamp is not too far in the future as this probably
// indicates its incorrectness (for example using other units than microseconds).
} // namespace util
} // namespace cql3
extern std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_0;
extern std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_8;
extern std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_16;
extern std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_24;
template <>
struct fmt::formatter<db_clock::time_point> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const db_clock::time_point& tp, FormatContext& ctx) const {
        auto t = db_clock::to_time_t(tp);
        return fmt::format_to(ctx.out(), "{:%Y/%m/%d %T}", fmt::gmtime(t));
    }
};
namespace db {
namespace marshal {
class type_parser {
    sstring _str;
    size_t _idx;
#if 0
    // A cache of parsed string, specially useful for DynamicCompositeType
    private static final Map<String, AbstractType<?>> cache = new HashMap<String, AbstractType<?>>();
    public static final TypeParser EMPTY_PARSER = new TypeParser("", 0);
#endif
public:
#if 0
    public static AbstractType<?> parse(CharSequence compareWith) throws SyntaxException, ConfigurationException
    {
        return parse(compareWith == null ? null : compareWith.toString());
    }
    public static String getShortName(AbstractType<?> type)
    {
        return type.getClass().getSimpleName();
    }
#endif
#if 0
    public Map<String, String> getKeyValueParameters() throws SyntaxException
    {
        if (isEOS())
            return Collections.emptyMap();
        if (str.charAt(idx) != '(')
            throw new IllegalStateException();
        Map<String, String> map = new HashMap<String, String>();
        ++idx; // skipping '('
        while (skipBlankAndComma())
        {
            if (str.charAt(idx) == ')')
            {
                ++idx;
                return map;
            }
            String k = readNextIdentifier();
            String v = "";
            skipBlank();
            if (str.charAt(idx) == '=')
            {
                ++idx;
                skipBlank();
                v = readNextIdentifier();
            }
            else if (str.charAt(idx) != ',' && str.charAt(idx) != ')')
            {
                throwSyntaxError("unexpected character '" + str.charAt(idx) + "'");
            }
            map.put(k, v);
        }
        throw new SyntaxException(String.format("Syntax error parsing '%s' at char %d: unexpected end of string", str, idx));
    }
#endif
    std::tuple<sstring, bytes, std::vector<bytes>, std::vector<data_type>> get_user_type_parameters();
#if 0
    public Map<Byte, AbstractType<?>> getAliasParameters() throws SyntaxException, ConfigurationException
    {
        Map<Byte, AbstractType<?>> map = new HashMap<Byte, AbstractType<?>>();
        if (isEOS())
            return map;
        if (str.charAt(idx) != '(')
            throw new IllegalStateException();
        ++idx; // skipping '('
        while (skipBlankAndComma())
        {
            if (str.charAt(idx) == ')')
            {
                ++idx;
                return map;
            }
            String alias = readNextIdentifier();
            if (alias.length() != 1)
                throwSyntaxError("An alias should be a single character");
            char aliasChar = alias.charAt(0);
            if (aliasChar < 33 || aliasChar > 127)
                throwSyntaxError("An alias should be a single character in [0..9a..bA..B-+._&]");
            skipBlank();
            if (!(str.charAt(idx) == '=' && str.charAt(idx+1) == '>'))
                throwSyntaxError("expecting '=>' token");
            idx += 2;
            skipBlank();
            try
            {
                map.put((byte)aliasChar, parse());
            }
            catch (SyntaxException e)
            {
                SyntaxException ex = new SyntaxException(String.format("Exception while parsing '%s' around char %d", str, idx));
                ex.initCause(e);
                throw ex;
            }
        }
        throw new SyntaxException(String.format("Syntax error parsing '%s' at char %d: unexpected end of string", str, idx));
    }
    public Map<ByteBuffer, CollectionType> getCollectionsParameters() throws SyntaxException, ConfigurationException
    {
        Map<ByteBuffer, CollectionType> map = new HashMap<>();
        if (isEOS())
            return map;
        if (str.charAt(idx) != '(')
            throw new IllegalStateException();
        ++idx; // skipping '('
        while (skipBlankAndComma())
        {
            if (str.charAt(idx) == ')')
            {
                ++idx;
                return map;
            }
            ByteBuffer bb = fromHex(readNextIdentifier());
            skipBlank();
            if (str.charAt(idx) != ':')
                throwSyntaxError("expecting ':' token");
            ++idx;
            skipBlank();
            try
            {
                AbstractType<?> type = parse();
                if (!(type instanceof CollectionType))
                    throw new SyntaxException(type + " is not a collection type");
                map.put(bb, (CollectionType)type);
            }
            catch (SyntaxException e)
            {
                SyntaxException ex = new SyntaxException(String.format("Exception while parsing '%s' around char %d", str, idx));
                ex.initCause(e);
                throw ex;
            }
        }
        throw new SyntaxException(String.format("Syntax error parsing '%s' at char %d: unexpected end of string", str, idx));
    }
    private ByteBuffer fromHex(String hex) throws SyntaxException
    {
        try
        {
            return ByteBufferUtil.hexToBytes(hex);
        }
        catch (NumberFormatException e)
        {
            throwSyntaxError(e.getMessage());
            return null;
        }
    }
    public Pair<Pair<String, ByteBuffer>, List<Pair<ByteBuffer, AbstractType>>> getUserTypeParameters() throws SyntaxException, ConfigurationException
    {
        if (isEOS() || str.charAt(idx) != '(')
            throw new IllegalStateException();
        ++idx; // skipping '('
        skipBlankAndComma();
        String keyspace = readNextIdentifier();
        skipBlankAndComma();
        ByteBuffer typeName = fromHex(readNextIdentifier());
        List<Pair<ByteBuffer, AbstractType>> defs = new ArrayList<>();
        while (skipBlankAndComma())
        {
            if (str.charAt(idx) == ')')
            {
                ++idx;
                return Pair.create(Pair.create(keyspace, typeName), defs);
            }
            ByteBuffer name = fromHex(readNextIdentifier());
            skipBlank();
            if (str.charAt(idx) != ':')
                throwSyntaxError("expecting ':' token");
            ++idx;
            skipBlank();
            try
            {
                AbstractType type = parse();
                defs.add(Pair.create(name, type));
            }
            catch (SyntaxException e)
            {
                SyntaxException ex = new SyntaxException(String.format("Exception while parsing '%s' around char %d", str, idx));
                ex.initCause(e);
                throw ex;
            }
        }
        throw new SyntaxException(String.format("Syntax error parsing '%s' at char %d: unexpected end of string", str, idx));
    }
#endif
#if 0
    private static AbstractType<?> getRawAbstractType(Class<? extends AbstractType<?>> typeClass) throws ConfigurationException
    {
        try
        {
            Field field = typeClass.getDeclaredField("instance");
            return (AbstractType<?>) field.get(null);
        }
        catch (NoSuchFieldException e)
        {
            throw new ConfigurationException("Invalid comparator class " + typeClass.getName() + ": must define a public static instance field or a public static method getInstance(TypeParser).");
        }
        catch (IllegalAccessException e)
        {
            throw new ConfigurationException("Invalid comparator class " + typeClass.getName() + ": must define a public static instance field or a public static method getInstance(TypeParser).");
        }
    }
    private static AbstractType<?> getRawAbstractType(Class<? extends AbstractType<?>> typeClass, TypeParser parser) throws ConfigurationException
    {
        try
        {
            Method method = typeClass.getDeclaredMethod("getInstance", TypeParser.class);
            return (AbstractType<?>) method.invoke(null, parser);
        }
        catch (NoSuchMethodException e)
        {
            throw new ConfigurationException("Invalid comparator class " + typeClass.getName() + ": must define a public static instance field or a public static method getInstance(TypeParser).");
        }
        catch (IllegalAccessException e)
        {
            throw new ConfigurationException("Invalid comparator class " + typeClass.getName() + ": must define a public static instance field or a public static method getInstance(TypeParser).");
        }
        catch (InvocationTargetException e)
        {
            ConfigurationException ex = new ConfigurationException("Invalid definition for comparator " + typeClass.getName() + ".");
            ex.initCause(e.getTargetException());
            throw ex;
        }
    }
    private void throwSyntaxError(String msg) throws SyntaxException
    {
        throw new SyntaxException(String.format("Syntax error parsing '%s' at char %d: %s", str, idx, msg));
    }
#endif
    // skip all blank and at best one comma, return true if there not EOS
    // left idx positioned on the character stopping the read
#if 0
    public char readNextChar()
    {
        skipBlank();
        return str.charAt(idx++);
    }
    public static String stringifyAliasesParameters(Map<Byte, AbstractType<?>> aliases)
    {
        StringBuilder sb = new StringBuilder();
        sb.append('(');
        Iterator<Map.Entry<Byte, AbstractType<?>>> iter = aliases.entrySet().iterator();
        if (iter.hasNext())
        {
            Map.Entry<Byte, AbstractType<?>> entry = iter.next();
            sb.append((char)(byte)entry.getKey()).append("=>").append(entry.getValue());
        }
        while (iter.hasNext())
        {
            Map.Entry<Byte, AbstractType<?>> entry = iter.next();
            sb.append(',').append((char)(byte)entry.getKey()).append("=>").append(entry.getValue());
        }
        sb.append(')');
        return sb.toString();
    }
    public static String stringifyTypeParameters(List<AbstractType<?>> types)
    {
        return stringifyTypeParameters(types, false);
    }
    public static String stringifyTypeParameters(List<AbstractType<?>> types, boolean ignoreFreezing)
    {
        StringBuilder sb = new StringBuilder("(");
        for (int i = 0; i < types.size(); i++)
        {
            if (i > 0)
                sb.append(",");
            sb.append(types.get(i).toString(ignoreFreezing));
        }
        return sb.append(')').toString();
    }
    public static String stringifyCollectionsParameters(Map<ByteBuffer, ? extends CollectionType> collections)
    {
        StringBuilder sb = new StringBuilder();
        sb.append('(');
        boolean first = true;
        for (Map.Entry<ByteBuffer, ? extends CollectionType> entry : collections.entrySet())
        {
            if (!first)
                sb.append(',');
            first = false;
            sb.append(ByteBufferUtil.bytesToHex(entry.getKey())).append(":");
            sb.append(entry.getValue());
        }
        sb.append(')');
        return sb.toString();
    }
    public static String stringifyUserTypeParameters(String keysace, ByteBuffer typeName, List<ByteBuffer> columnNames, List<AbstractType<?>> columnTypes)
    {
        StringBuilder sb = new StringBuilder();
        sb.append('(').append(keysace).append(",").append(ByteBufferUtil.bytesToHex(typeName));
        for (int i = 0; i < columnNames.size(); i++)
        {
            sb.append(',');
            sb.append(ByteBufferUtil.bytesToHex(columnNames.get(i))).append(":");
            // omit FrozenType(...) from fields because it is currently implicit
            sb.append(columnTypes.get(i).toString(true));
        }
        sb.append(')');
        return sb.toString();
    }
#endif
};
}
}
extern logging::logger dblog;
namespace db {
class paxos_grace_seconds_extension : public schema_extension {
    int32_t _paxos_gc_sec;
public:
    static constexpr auto NAME = "paxos_grace_seconds";
    int32_t get_paxos_grace_seconds() const {
        return _paxos_gc_sec;
    }
};
} // namespace db
namespace db {
class per_partition_rate_limit_extension : public schema_extension {
    per_partition_rate_limit_options _options;
public:
    static constexpr auto NAME = "per_partition_rate_limit";
    per_partition_rate_limit_extension() = default;
    
    
    const per_partition_rate_limit_options& get_options() const {
        return _options;
    }
};
}
namespace db {
using segment_id_type = uint64_t;
using position_type = uint32_t;
struct replay_position {
    static constexpr size_t max_cpu_bits = 10; // 1024 cpus. should be enough for anyone
    static constexpr size_t max_ts_bits = 8 * sizeof(segment_id_type) - max_cpu_bits;
    static constexpr segment_id_type ts_mask = (segment_id_type(1) << max_ts_bits) - 1;
    static constexpr segment_id_type cpu_mask = ~ts_mask;
    segment_id_type id;
    position_type pos;
    replay_position(segment_id_type i = 0, position_type p = 0)
        : id(i), pos(p)
    {}
     ;
};
class commitlog;
class cf_holder;
using cf_id_type = table_id;
class rp_handle {
public:
private:
    friend class commitlog;
    ::shared_ptr<cf_holder> _h;
    cf_id_type _cf;
    replay_position _rp;
};
}
namespace std {
template <>
struct hash<db::replay_position> {
    size_t operator()(const db::replay_position& v) const {
        return utils::tuple_hash()(v.id, v.pos);
    }
};
}
namespace db {
enum class schema_feature {
    VIEW_VIRTUAL_COLUMNS,
    // When set, the schema digest is calcualted in a way such that it doesn't change after all
    // tombstones in an empty partition expire.
    // See https://github.com/scylladb/scylla/issues/4485
    DIGEST_INSENSITIVE_TO_EXPIRY,
    COMPUTED_COLUMNS,
    CDC_OPTIONS,
    PER_TABLE_PARTITIONERS,
    SCYLLA_KEYSPACES,
    SCYLLA_AGGREGATES,
};
using schema_features = enum_set<super_enum<schema_feature,
    schema_feature::VIEW_VIRTUAL_COLUMNS,
    schema_feature::DIGEST_INSENSITIVE_TO_EXPIRY,
    schema_feature::COMPUTED_COLUMNS,
    schema_feature::CDC_OPTIONS,
    schema_feature::PER_TABLE_PARTITIONERS,
    schema_feature::SCYLLA_KEYSPACES,
    schema_feature::SCYLLA_AGGREGATES
    >>;
}
// Immutable mutation form which can be read using any schema version of the same table.
// Safe to access from other shards via const&.
// Safe to pass serialized across nodes.
class canonical_mutation {
    bytes_ostream _data;
public:
    // Create a mutation object interpreting this canonical mutation using
    // given schema.
    //
    // Data which is not representable in the target schema is dropped. If this
    // is not intended, user should sync the schema first.
};
// Commutative representation of table schema
// Equality ignores tombstones.
class schema_mutations {
    mutation _columnfamilies;
    mutation _columns;
    mutation_opt _view_virtual_columns;
    mutation_opt _computed_columns;
    mutation_opt _indices;
    mutation_opt _dropped_columns;
    mutation_opt _scylla_tables;
public:
    
    
    bool is_view() const;
    table_schema_version digest() const;
    std::optional<sstring> partitioner() const;
    // Returns true iff any mutations contain any live cells
};
class mutation;
namespace query {
class result;
class no_value : public std::runtime_error {
public:
    using runtime_error::runtime_error;
};
class non_null_data_value {
    data_value _v;
public:
    operator const data_value&() const ;
};
 
// Result set row is a set of cells that are associated with a row
// including regular column cells, partition keys, as well as static values.
class result_set_row {
    schema_ptr _schema;
    const std::unordered_map<sstring, non_null_data_value> _cells;
public:
    
    // Look up a deserialized row cell value by column name
    const data_value*
    get_data_value(const sstring& column_name) const ;
    // Look up a deserialized row cell value by column name
    template<typename T>
    std::optional<T>
    get(const sstring& column_name) const ;
     ;
    // throws no_value on error
     ;
};
// Result set is an in-memory representation of query results in
// deserialized format. To obtain a result set, use the result_set_builder
// class as a visitor to query_result::consume() function.
class result_set {
    schema_ptr _schema;
    std::vector<result_set_row> _rows;
public:
    // throws std::out_of_range on error
};
}
namespace data_dictionary {
class keyspace_metadata;
class user_types_storage;
}
using keyspace_metadata = data_dictionary::keyspace_metadata;
namespace replica {
class database;
}
namespace query {
class result_set;
}
namespace service {
class storage_service;
class storage_proxy;
}
namespace gms {
class feature_service;
}
namespace cql3::functions {
class user_function;
class user_aggregate;
}
namespace db {
class system_keyspace;
class extensions;
class config;
class schema_ctxt {
public:
    schema_ctxt(const replica::database&);
    schema_ctxt(distributed<replica::database>&);
    schema_ctxt(distributed<service::storage_proxy>&);
    const db::extensions& extensions() const ;
    const unsigned murmur3_partitioner_ignore_msb_bits() const ;
    uint32_t schema_registry_grace_period() const {
        return _schema_registry_grace_period;
    }
private:
    const db::extensions& _extensions;
    const unsigned _murmur3_partitioner_ignore_msb_bits;
    const uint32_t _schema_registry_grace_period;
    const std::shared_ptr<data_dictionary::user_types_storage> _user_types;
};
namespace schema_tables {
using schema_result = std::map<sstring, lw_shared_ptr<query::result_set>>;
using schema_result_value_type = std::pair<sstring, lw_shared_ptr<query::result_set>>;
const std::string COMMITLOG_FILENAME_PREFIX("SchemaLog-");
namespace v3 {
static constexpr auto NAME = "system_schema";
static constexpr auto KEYSPACES = "keyspaces";
static constexpr auto SCYLLA_KEYSPACES = "scylla_keyspaces";
static constexpr auto TABLES = "tables";
static constexpr auto SCYLLA_TABLES = "scylla_tables";
static constexpr auto COLUMNS = "columns";
static constexpr auto DROPPED_COLUMNS = "dropped_columns";
static constexpr auto TRIGGERS = "triggers";
static constexpr auto VIEWS = "views";
static constexpr auto TYPES = "types";
static constexpr auto FUNCTIONS = "functions";
static constexpr auto AGGREGATES = "aggregates";
static constexpr auto SCYLLA_AGGREGATES = "scylla_aggregates";
static constexpr auto INDEXES = "indexes";
static constexpr auto VIEW_VIRTUAL_COLUMNS = "view_virtual_columns"; // Scylla specific
static constexpr auto COMPUTED_COLUMNS = "computed_columns"; // Scylla specific
static constexpr auto SCYLLA_TABLE_SCHEMA_HISTORY = "scylla_table_schema_history"; // Scylla specific;
// Belongs to the "system" keyspace
}
namespace legacy {
class schema_mutations {
    mutation _columnfamilies;
    mutation _columns;
public:
};
}
using namespace v3;
// Change on non-backwards compatible changes of schema mutations.
// Replication of schema between nodes with different version is inhibited.
extern const sstring version;
// Returns schema_ptrs for all schema tables supported by given schema_features.
// Like all_tables(), but returns schema::cf_name() of each table.
// saves/creates "ks" + all tables etc, while first deleting all old schema entries (will be rewritten)
// saves/creates "system_schema" keyspace
// Calculates schema digest for all non-system keyspaces
future<std::vector<canonical_mutation>> convert_schema_to_mutations(distributed<service::storage_proxy>& proxy, schema_features);
std::vector<mutation> adjust_schema_for_schema_features(std::vector<mutation> schema, schema_features features);
// Must be called on shard 0.
// Recalculates the local schema version.
//
// It is safe to call concurrently with recalculate_schema_version() and merge_schema() in which case it
// is guaranteed that the schema version we end up with after all calls will reflect the most recent state
// of feature_service and schema tables.
future<std::set<sstring>> merge_keyspaces(distributed<service::storage_proxy>& proxy, schema_result&& before, schema_result&& after);
seastar::future<std::vector<shared_ptr<cql3::functions::user_function>>> create_functions_from_schema_partition(replica::database& db, lw_shared_ptr<query::result_set> result);
future<std::map<sstring, schema_ptr>> create_tables_from_tables_partition(distributed<service::storage_proxy>& proxy, const schema_result::mapped_type& result);
std::vector<mutation> make_drop_table_mutations(lw_shared_ptr<keyspace_metadata> keyspace, schema_ptr table, api::timestamp_type timestamp);
schema_ptr create_table_from_mutations(const schema_ctxt&, schema_mutations, std::optional<table_schema_version> version = {});
view_ptr create_view_from_mutations(const schema_ctxt&, schema_mutations, std::optional<table_schema_version> version = {});
future<std::vector<view_ptr>> create_views_from_schema_partition(distributed<service::storage_proxy>& proxy, const schema_result::mapped_type& result);
schema_mutations make_schema_mutations(schema_ptr s, api::timestamp_type timestamp, bool with_columns);


std::vector<mutation> make_create_view_mutations(lw_shared_ptr<keyspace_metadata> keyspace, view_ptr view, api::timestamp_type timestamp);
class preserve_version_tag {};
using preserve_version = bool_class<preserve_version_tag>;
template<typename K, typename V>
std::optional<std::map<K, V>> get_map(const query::result_set_row& row, const sstring& name) {
    if (auto values = row.get<map_type_impl::native_type>(name)) {
        std::map<K, V> map;
        for (auto&& entry : *values) {
            map.emplace(value_cast<K>(entry.first), value_cast<V>(entry.second));
        };
        return map;
    }
    return std::nullopt;
}
/// Stores the column mapping for the table being created or altered in the system table
/// which holds a history of schema versions alongside with their column mappings.
/// Can be used to insert entries with TTL (equal to DEFAULT_GC_GRACE_SECONDS) in case we are
/// overwriting an existing column mapping to garbage collect obsolete entries.
future<> store_column_mapping(distributed<service::storage_proxy>& proxy, schema_ptr s, bool with_ttl);
/// Query column mapping for a given version of the table locally.
/// Check that column mapping exists for a given version of the table
/// Delete matching column mapping entries from the `system.scylla_table_schema_history` table
} // namespace schema_tables
} // namespace db
namespace ser {
template <>
struct serializer<utils::UUID> {
  ;
  ;
  ;
};
template <>
struct serializer<const utils::UUID> : public serializer<utils::UUID>
{};
template <>
struct serializer<tasks::task_id> {
  template <typename Output>
  static void write(Output& buf, const tasks::task_id& v);
  template <typename Input>
  static tasks::task_id read(Input& buf);
  template <typename Input>
  static void skip(Input& buf);
};
template <>
struct serializer<const tasks::task_id> : public serializer<tasks::task_id>
{};
template <>
struct serializer<table_id> {
  ;
  ;
  ;
};
template <>
struct serializer<const table_id> : public serializer<table_id>
{};
template <>
struct serializer<table_schema_version> {
  ;
  template <typename Input>
  static table_schema_version read(Input& buf);
  ;
};
template <>
struct serializer<const table_schema_version> : public serializer<table_schema_version>
{};
template <>
struct serializer<query_id> {
  ;
  ;
  ;
};
template <>
struct serializer<const query_id> : public serializer<query_id>
{};
template <>
struct serializer<locator::host_id> {
  ;
  ;
  ;
};
template <>
struct serializer<const locator::host_id> : public serializer<locator::host_id>
{};
} // ser
namespace ser {
template <>
struct serializer<counter_id> {
  ;
  ;
  ;
};
template <>
struct serializer<const counter_id> : public serializer<counter_id>
{};
template <>
struct serializer<counter_shard> {
  ;
  ;
  ;
};
template <>
struct serializer<const counter_shard> : public serializer<counter_shard>
{};
template <>
struct serializer<tombstone> {
  ;
  ;
  template <typename Input>
  static void skip(Input& buf);
};
template <>
struct serializer<const tombstone> : public serializer<tombstone>
{};
template <>
struct serializer<bound_kind> {
  template <typename Output>
  static void write(Output& buf, const bound_kind& v);
  template <typename Input>
  static bound_kind read(Input& buf);
  template <typename Input>
  static void skip(Input& buf);
};
template <>
struct serializer<const bound_kind> : public serializer<bound_kind>
{};
template <>
struct serializer<range_tombstone> {
  template <typename Output>
  static void write(Output& buf, const range_tombstone& v);
  template <typename Input>
  static range_tombstone read(Input& buf);
  template <typename Input>
  static void skip(Input& buf);
};
template <>
struct serializer<const range_tombstone> : public serializer<range_tombstone>
{};
template <>
struct serializer<column_mapping_entry> {
  template <typename Output>
  static void write(Output& buf, const column_mapping_entry& v);
  template <typename Input>
  static column_mapping_entry read(Input& buf);
  template <typename Input>
  static void skip(Input& buf);
};
template <>
struct serializer<const column_mapping_entry> : public serializer<column_mapping_entry>
{};
template <>
struct serializer<column_mapping> {
  template <typename Output>
  static void write(Output& buf, const column_mapping& v);
  template <typename Input>
  static column_mapping read(Input& buf);
  template <typename Input>
  static void skip(Input& buf);
};
template <>
struct serializer<const column_mapping> : public serializer<column_mapping>
{};
template <>
struct serializer<partition_end> {
  template <typename Output>
  static void write(Output& buf, const partition_end& v);
  template <typename Input>
  static partition_end read(Input& buf);
  template <typename Input>
  static void skip(Input& buf);
};
template <>
struct serializer<const partition_end> : public serializer<partition_end>
{};
} // ser
using seastar::future;
/// \brief Represents a stream of mutation fragments.
///
/// Mutation fragments represent writes to the database.
///
/// Each fragment has an implicit position in the database,
/// which also determines its position in the stream relative to other fragments.
/// The global position of a fragment is a tuple ordered lexicographically:
///
///    (ring_position of a partition key, position_in_partition)
///
/// The stream has a hierarchical form. All fragments which occur
/// between partition_start and partition_end represent writes to the partition
/// identified by the partition_start::key(). The partition key is not repeated
/// with inner fragments.
///
/// The stream of mutation fragments conforms to the following form:
///
///   stream ::= partition*
///   partition ::= partition_start static_row? clustered* partition_end
///   clustered ::= clustering_row | range_tombstone_change
///
/// Deletions of ranges of rows within a given partition are represented with range_tombstone_change fragments.
/// At any point in the stream there is a single active clustered tombstone.
/// It is initially equal to the neutral tombstone when the stream of each partition starts.
/// range_tombstone_change fragments signify changes of the active clustered tombstone.
/// All fragments emitted while a given clustered tombstone is active are affected by that tombstone.
/// The clustered tombstone is independent from the partition tombstone carried in partition_start.
/// The partition tombstone takes effect for all fragments within the partition.
///
/// The stream guarantees that each partition ends with a neutral active clustered tombstone
/// by closing active tombstones with a range_tombstone_change.
/// In fast-forwarding mode, each sub-stream ends with a neutral active clustered tombstone.
///
/// All fragments within a partition have weakly monotonically increasing position().
/// Consecutive range_tombstone_change fragments may share the position.
/// All clustering row fragments within a partition have strictly monotonically increasing position().
///
/// \section Clustering restrictions
///
/// A stream may produce writes relevant to only some clustering ranges, for
/// example by specifying clustering ranges in a partition_slice passed to
/// mutation_source::make_reader(). This will make the stream return information
/// for a subset of writes that it would normally return should the stream be
/// unrestricted.
///
/// The restricted stream obeys the following rules:
///
///   0) The stream must contain fragments corresponding to all writes
///      which are relevant to the requested ranges.
///
///   1) The ranges of non-neutral clustered tombstones must be enclosed in requested
///      ranges. In other words, range tombstones don't extend beyond boundaries of requested ranges.
///
///   2) The stream will not return writes which are absent in the unrestricted stream,
///      both for the requested clustering ranges and not requested ranges.
///      This means that it's safe to populate cache with all the returned information.
///      Even though it may be incomplete for non-requested ranges, it won't contain
///      incorrect information.
///
///   3) All clustered fragments have position() which is within the requested
///      ranges or, in case of range_tombstone_change fragments, equal to the end bound.
///
///   4) Streams may produce redundant range_tombstone_change fragments
///      which do not change the current clustered tombstone, or have the same position.
///
/// \section Intra-partition fast-forwarding mode
///
/// The stream can operate in an alternative mode when streamed_mutation::forwarding::yes
/// is passed to the stream constructor (see mutation_source).
///
/// In this mode, the original stream is not produced at once, but divided into sub-streams, where
/// each is produced at a time, ending with the end-of-stream condition (is_end_of_stream()).
/// The user needs to explicitly advance the stream to the next sub-stream by calling
/// fast_forward_to() or next_partition().
///
/// The original stream is divided like this:
///
///    1) For every partition, the first sub-stream will contain
///       partition_start and the static_row
///
///    2) Calling fast_forward_to() moves to the next sub-stream within the
///       current partition. The stream will contain all fragments relevant to
///       the position_range passed to fast_forward_to().
///
///    3) The position_range passed to fast_forward_to() is a clustering key restriction.
///       Same rules apply as with clustering restrictions described above.
///
///    4) The sub-stream will not end with a non-neutral active clustered tombstone. All range tombstones are closed.
///
///    5) partition_end is never emitted, the user needs to call next_partition()
///       to move to the next partition in the original stream, which will open
///       the initial sub-stream of the next partition.
///       An empty sub-stream after next_partition() indicates global end-of-stream (no next partition).
///
/// \section Consuming
///
/// The best way to consume those mutation_fragments is to call
/// flat_mutation_reader::consume with a consumer that receives the fragments.
class flat_mutation_reader_v2 final {
public:
    using tracked_buffer = circular_buffer<mutation_fragment_v2, tracking_allocator<mutation_fragment_v2>>;
    class impl {
    private:
        tracked_buffer _buffer;
        size_t _buffer_size = 0;
        bool _close_required = false;
    protected:
        size_t max_buffer_size_in_bytes = default_max_buffer_size_in_bytes();
        // The stream producer should set this to indicate that there are no
        // more fragments to produce.
        // Calling fill_buffer() will not add any new fragments
        // unless the reader is fast-forwarded to a new range.
        bool _end_of_stream = false;
        schema_ptr _schema;
        reader_permit _permit;
        friend class flat_mutation_reader_v2;
    protected:
        template<typename... Args>
        void push_mutation_fragment(Args&&... args) ;
        void clear_buffer() ;
        void reserve_additional(size_t n) ;
        void clear_buffer_to_next_partition();
        template<typename Source>
        future<bool> fill_buffer_from(Source&);
        const tracked_buffer& buffer() const ;
    public:
        impl(schema_ptr s, reader_permit permit) : _buffer(permit), _schema(std::move(s)), _permit(std::move(permit)) { }
        virtual ~impl() ;
        virtual future<> fill_buffer() = 0;
        virtual future<> next_partition() = 0;
        bool is_end_of_stream() const ;
        bool is_buffer_empty() const ;
        bool is_buffer_full() const ;
        bool is_close_required() const ;
        void set_close_required() ;
        static constexpr size_t default_max_buffer_size_in_bytes() ;
        mutation_fragment_v2 pop_mutation_fragment() ;
        void unpop_mutation_fragment(mutation_fragment_v2 mf) ;
        future<mutation_fragment_v2_opt> operator()() ;
        template<typename Consumer>
        requires FlatMutationReaderConsumerV2<Consumer>
        // Stops when consumer returns stop_iteration::yes or end of stream is reached.
        // Next call will start from the next mutation_fragment_v2 in the stream.
        future<> consume_pausable(Consumer consumer) {
            return repeat([this, consumer = std::move(consumer)] () mutable {
                if (is_buffer_empty()) {
                    if (is_end_of_stream()) {
                        return make_ready_future<stop_iteration>(stop_iteration::yes);
                    }
                    return fill_buffer().then([] {
                        return make_ready_future<stop_iteration>(stop_iteration::no);
                    });
                }
                if constexpr (std::is_same_v<future<stop_iteration>, decltype(consumer(pop_mutation_fragment()))>) {
                    return consumer(pop_mutation_fragment());
                } else {
                    auto result = stop_iteration::no;
                    while ((result = consumer(pop_mutation_fragment())) != stop_iteration::yes && !is_buffer_empty() && !need_preempt()) {}
                    return make_ready_future<stop_iteration>(result);
                }
            });
        }
        template<typename Consumer, typename Filter>
        requires FlatMutationReaderConsumerV2<Consumer> && FlattenedConsumerFilterV2<Filter>
        // A variant of consume_pausable() that expects to be run in
        // a seastar::thread.
        // Partitions for which filter(decorated_key) returns false are skipped
        // entirely and never reach the consumer.
        void consume_pausable_in_thread(Consumer consumer, Filter filter) {
            while (true) {
                thread::maybe_yield();
                if (is_buffer_empty()) {
                    if (is_end_of_stream()) {
                        return;
                    }
                    fill_buffer().get();
                    continue;
                }
                auto mf = pop_mutation_fragment();
                if (mf.is_partition_start() && !filter(mf.as_partition_start().key())) {
                    next_partition().get();
                    continue;
                }
                if (!filter(mf)) {
                    continue;
                }
                auto do_stop = futurize_invoke([&consumer, mf = std::move(mf)] () mutable {
                    return consumer(std::move(mf));
                });
                if (do_stop.get0()) {
                    return;
                }
            }
        };
    private:
        template<typename Consumer>
        struct consumer_adapter {
            flat_mutation_reader_v2::impl& _reader;
            std::optional<dht::decorated_key> _decorated_key;
            Consumer _consumer;
            consumer_adapter(flat_mutation_reader_v2::impl& reader, Consumer c)
                    : _reader(reader)
                    , _consumer(std::move(c))
            { }
            future<stop_iteration> operator()(mutation_fragment_v2&& mf) ;
            future<stop_iteration> consume(static_row&& sr) ;
        private:
        };
    public:
        template<typename Consumer>
        requires FlattenedConsumerV2<Consumer>
        // Stops when consumer returns stop_iteration::yes from consume_end_of_partition or end of stream is reached.
        // Next call will receive fragments from the next partition.
        // When consumer returns stop_iteration::yes from methods other than consume_end_of_partition then the read
        // of the current partition is ended, consume_end_of_partition is called and if it returns stop_iteration::no
        // then the read moves to the next partition.
        // Reference to the decorated key that is passed to consume_new_partition() remains valid until after
        // the call to consume_end_of_partition().
        //
        // This method is useful because most of current consumers use this semantic.
        //
        //
        // This method returns whatever is returned from Consumer::consume_end_of_stream().S
        auto consume(Consumer consumer) {
            return do_with(consumer_adapter<Consumer>(*this, std::move(consumer)), [this] (consumer_adapter<Consumer>& adapter) {
                return consume_pausable(std::ref(adapter)).then([&adapter] {
                    return adapter._consumer.consume_end_of_stream();
                });
            });
        }
        template<typename Consumer, typename Filter>
        requires FlattenedConsumerV2<Consumer> && FlattenedConsumerFilterV2<Filter>
        // A variant of consumee() that expects to be run in a seastar::thread.
        // Partitions for which filter(decorated_key) returns false are skipped
        // entirely and never reach the consumer.
        auto consume_in_thread(Consumer consumer, Filter filter) {
            auto adapter = consumer_adapter<Consumer>(*this, std::move(consumer));
            consume_pausable_in_thread(std::ref(adapter), std::move(filter));
            filter.on_end_of_stream();
            return adapter._consumer.consume_end_of_stream();
        };
        virtual future<> fast_forward_to(const dht::partition_range&) = 0;
        virtual future<> fast_forward_to(position_range) = 0;
        // close should cancel any outstanding background operations,
        // if possible, and wait on them to complete.
        // It should also transitively close underlying resources
        // and wait on them too.
        //
        // Once closed, the reader should be unusable.
        //
        // Similar to destructors, close must never fail.
        virtual future<> close() noexcept = 0;
        size_t buffer_size() const ;
        tracked_buffer detach_buffer() noexcept ;
        void move_buffer_content_to(impl& other) ;
        void check_abort() ;
        db::timeout_clock::time_point timeout() const noexcept ;
        void set_timeout(db::timeout_clock::time_point timeout) noexcept ;
    };
private:
    std::unique_ptr<impl> _impl;
    flat_mutation_reader_v2() = default;
    explicit operator bool() const noexcept ;
    friend class optimized_optional<flat_mutation_reader_v2>;
    void do_upgrade_schema(const schema_ptr&);
    static void on_close_error(std::unique_ptr<impl>, std::exception_ptr ep) noexcept;
public:
    
    
    flat_mutation_reader_v2(flat_mutation_reader_v2&&) = default;
    flat_mutation_reader_v2& operator=(const flat_mutation_reader_v2&) = delete;
    flat_mutation_reader_v2& operator=(flat_mutation_reader_v2&& o) noexcept;
    ~flat_mutation_reader_v2();
    future<mutation_fragment_v2_opt> operator()() ;
    template <typename Consumer>
    requires FlatMutationReaderConsumerV2<Consumer>
    auto consume_pausable(Consumer consumer) {
        return _impl->consume_pausable(std::move(consumer));
    }
    template <typename Consumer>
    requires FlattenedConsumerV2<Consumer>
    auto consume(Consumer consumer) {
        return _impl->consume(std::move(consumer));
    }
    class filter {
    private:
        std::function<bool (const dht::decorated_key&)> _partition_filter = [] (const dht::decorated_key&) { return true; };
        std::function<bool (const mutation_fragment_v2&)> _mutation_fragment_filter = [] (const mutation_fragment_v2&) { return true; };
    public:
        filter() = default;
        filter(std::function<bool (const dht::decorated_key&)>&& pf)
                : _partition_filter(std::move(pf))
        { }
        filter(std::function<bool (const dht::decorated_key&)>&& pf,
               std::function<bool (const mutation_fragment_v2&)>&& mf)
                : _partition_filter(std::move(pf))
                , _mutation_fragment_filter(std::move(mf))
        { }
        template <typename Functor>
        filter(Functor&& f)
                : _partition_filter(std::forward<Functor>(f))
        { }
        bool operator()(const dht::decorated_key& dk) const {
            return _partition_filter(dk);
        }
        bool operator()(const mutation_fragment_v2& mf) const {
            return _mutation_fragment_filter(mf);
        }
        void on_end_of_stream() const { }
    };
    struct no_filter {
        bool operator()(const dht::decorated_key& dk) const ;
    };
    template<typename Consumer, typename Filter>
    requires FlattenedConsumerV2<Consumer> && FlattenedConsumerFilterV2<Filter>
    auto consume_in_thread(Consumer consumer, Filter filter) {
        return _impl->consume_in_thread(std::move(consumer), std::move(filter));
    }
    template<typename Consumer>
    requires FlattenedConsumerV2<Consumer>
    auto consume_in_thread(Consumer consumer) {
        return consume_in_thread(std::move(consumer), no_filter{});
    }
    // Skips to the next partition.
    //
    // Skips over the remaining fragments of the current partitions. If the
    // reader is currently positioned at a partition start nothing is done.
    //
    // If the last produced fragment comes from partition `P`, then the reader
    // is considered to still be in partition `P`, which means that `next_partition`
    // will move the reader to the partition immediately following `P`.
    // This case happens in particular when the last produced fragment was
    // `partition_end` for `P`.
    //
    // Only skips within the current partition range, i.e. if the current
    // partition is the last in the range the reader will be at EOS.
    //
    // Can be used to skip over entire partitions if interleaved with
    // `operator()()` calls.
    future<> next_partition() {
        _impl->set_close_required();
        return _impl->next_partition();
    }
    future<> fill_buffer() {
        _impl->set_close_required();
        return _impl->fill_buffer();
    }
    // Changes the range of partitions to pr. The range can only be moved
    // forwards. pr.begin() needs to be larger than pr.end() of the previousl
    // used range (i.e. either the initial one passed to the constructor or a
    // previous fast forward target).
    // pr needs to be valid until the reader is destroyed or fast_forward_to()
    // is called again.
    future<> fast_forward_to(const dht::partition_range& pr) {
        _impl->set_close_required();
        return _impl->fast_forward_to(pr);
    }
    // Skips to a later range of rows.
    // The new range must not overlap with the current range.
    //
    // In forwarding mode the stream does not return all fragments right away,
    // but only those belonging to the current clustering range. Initially
    // current range only covers the static row. The stream can be forwarded
    // (even before end-of- stream) to a later range with fast_forward_to().
    // Forwarding doesn't change initial restrictions of the stream, it can
    // only be used to skip over data.
    //
    // Monotonicity of positions is preserved by forwarding. That is fragments
    // emitted after forwarding will have greater positions than any fragments
    // emitted before forwarding.
    //
    // For any range, all range tombstones relevant for that range which are
    // present in the original stream will be emitted. Range tombstones
    // emitted before forwarding which overlap with the new range are not
    // necessarily re-emitted.
    //
    // When forwarding mode is not enabled, fast_forward_to()
    // cannot be used.
    //
    // `fast_forward_to` can be called only when the reader is within a partition
    // and it affects the set of fragments returned from that partition.
    // In particular one must first enter a partition by fetching a `partition_start`
    // fragment before calling `fast_forward_to`.
    future<> fast_forward_to(position_range cr) {
        _impl->set_close_required();
        return _impl->fast_forward_to(std::move(cr));
    }
    // Closes the reader.
    //
    // Note: The reader object can can be safely destroyed after close returns.
    // since close makes sure to keep the underlying impl object alive until
    // the latter's close call is resolved.
    future<> close() noexcept {
        if (auto i = std::move(_impl)) {
            auto f = i->close();
            // most close implementations are expexcted to return a ready future
            // so expedite prcessing it.
            if (f.available() && !f.failed()) {
                return f;
            }
            // close must not fail
            return f.handle_exception([i = std::move(i)] (std::exception_ptr ep) mutable {
                on_close_error(std::move(i), std::move(ep));
            });
        }
        return make_ready_future<>();
    }
    // Returns true iff the stream reached the end.
    // There are no more fragments in the buffer and calling
    // fill_buffer() will not add any.
    bool is_end_of_stream() const { return _impl->is_end_of_stream() && is_buffer_empty(); }
    bool is_buffer_empty() const { return _impl->is_buffer_empty(); }
    bool is_buffer_full() const { return _impl->is_buffer_full(); }
    static constexpr size_t default_max_buffer_size_in_bytes() {
        return impl::default_max_buffer_size_in_bytes();
    }
    mutation_fragment_v2 pop_mutation_fragment() { return _impl->pop_mutation_fragment(); }
    void unpop_mutation_fragment(mutation_fragment_v2 mf) { _impl->unpop_mutation_fragment(std::move(mf)); }
    const schema_ptr& schema() const { return _impl->_schema; }
    const reader_permit& permit() const { return _impl->_permit; }
    reader_permit& permit() { return _impl->_permit; }
    db::timeout_clock::time_point timeout() const noexcept { return _impl->timeout(); }
    void set_timeout(db::timeout_clock::time_point timeout) noexcept { _impl->set_timeout(timeout); }
    void set_max_buffer_size(size_t size) {
        _impl->max_buffer_size_in_bytes = size;
    }
    // Resolves with a pointer to the next fragment in the stream without consuming it from the stream,
    // or nullptr if there are no more fragments.
    // The returned pointer is invalidated by any other non-const call to this object.
    future<mutation_fragment_v2*> peek() {
        if (!is_buffer_empty()) {
            return make_ready_future<mutation_fragment_v2*>(&_impl->_buffer.front());
        }
        if (is_end_of_stream()) {
            return make_ready_future<mutation_fragment_v2*>(nullptr);
        }
        return fill_buffer().then([this] {
            return peek();
        });
    }
    // A peek at the next fragment in the buffer.
    // Cannot be called if is_buffer_empty() returns true.
    const mutation_fragment_v2& peek_buffer() const { return _impl->_buffer.front(); }
    // The actual buffer size of the reader.
    // Altough we consistently refer to this as buffer size throught the code
    // we really use "buffer size" as the size of the collective memory
    // used by all the mutation fragments stored in the buffer of the reader.
    size_t buffer_size() const {
        return _impl->buffer_size();
    }
    const tracked_buffer& buffer() const {
        return _impl->buffer();
    }
    // Detach the internal buffer of the reader.
    // Roughly equivalent to depleting it by calling pop_mutation_fragment()
    // until is_buffer_empty() returns true.
    // The reader will need to allocate a new buffer on the next fill_buffer()
    // call.
    tracked_buffer detach_buffer() noexcept {
        return _impl->detach_buffer();
    }
    // Moves the buffer content to `other`.
    //
    // If the buffer of `other` is empty this is very efficient as the buffers
    // are simply swapped. Otherwise the content of the buffer is moved
    // fragmuent-by-fragment.
    // Allows efficient implementation of wrapping readers that do no
    // transformation to the fragment stream.
    void move_buffer_content_to(impl& other) {
        _impl->move_buffer_content_to(other);
    }
    // Causes this reader to conform to s.
    // Multiple calls of upgrade_schema() compose, effects of prior calls on the stream are preserved.
    void upgrade_schema(const schema_ptr& s) {
        if (__builtin_expect(s != schema(), false)) {
            do_upgrade_schema(s);
        }
    }
};
using flat_mutation_reader_v2_opt = optimized_optional<flat_mutation_reader_v2>;
 ;
// Consumes mutation fragments until StopCondition is true.
// The consumer will stop iff StopCondition returns true, in particular
// reaching the end of stream alone won't stop the reader.
template<typename StopCondition, typename ConsumeMutationFragment, typename ConsumeEndOfStream>
requires requires(StopCondition stop, ConsumeMutationFragment consume_mf, ConsumeEndOfStream consume_eos, mutation_fragment_v2 mf) {
    { stop() } -> std::same_as<bool>;
    { consume_mf(std::move(mf)) } -> std::same_as<void>;
    { consume_eos() } -> std::same_as<future<>>;
}
future<> consume_mutation_fragments_until(
        flat_mutation_reader_v2& r,
        StopCondition&& stop,
        ConsumeMutationFragment&& consume_mf,
        ConsumeEndOfStream&& consume_eos)
{
    return do_until([stop] { return stop(); }, [&r, stop, consume_mf, consume_eos] {
        while (!r.is_buffer_empty()) {
            consume_mf(r.pop_mutation_fragment());
            if (stop() || need_preempt()) {
                return make_ready_future<>();
            }
        }
        if (r.is_end_of_stream()) {
            return consume_eos();
        }
        return r.fill_buffer();
    });
}
// Creates a stream which is like r but with transformation applied to the elements.
template<typename T>
requires StreamedMutationTranformerV2<T>
flat_mutation_reader_v2 transform(flat_mutation_reader_v2 r, T t) {
    class transforming_reader : public flat_mutation_reader_v2::impl {
        flat_mutation_reader_v2 _reader;
        T _t;
        struct consumer {
            transforming_reader* _owner;
            stop_iteration operator()(mutation_fragment_v2&& mf) {
                _owner->push_mutation_fragment(_owner->_t(std::move(mf)));
                return stop_iteration(_owner->is_buffer_full());
            }
        };
    public:
        transforming_reader(flat_mutation_reader_v2&& r, T&& t)
                : impl(t(r.schema()), r.permit())
                , _reader(std::move(r))
                , _t(std::move(t))
        {}
        virtual future<> fill_buffer() override {
            if (_end_of_stream) {
                return make_ready_future<>();
            }
            return _reader.consume_pausable(consumer{this}).then([this] {
                if (_reader.is_end_of_stream()) {
                    _end_of_stream = true;
                }
            });
        }
        virtual future<> next_partition() override {
            clear_buffer_to_next_partition();
            if (is_buffer_empty()) {
                return _reader.next_partition();
            }
            return make_ready_future<>();
        }
        virtual future<> fast_forward_to(const dht::partition_range& pr) override {
            clear_buffer();
            _end_of_stream = false;
            return _reader.fast_forward_to(pr);
        }
        virtual future<> fast_forward_to(position_range pr) override {
            clear_buffer();
            _end_of_stream = false;
            return _reader.fast_forward_to(std::move(pr));
        }
        virtual future<> close() noexcept override {
            return _reader.close();
        }
    };
    return make_flat_mutation_reader_v2<transforming_reader>(std::move(r), std::move(t));
}
// Reads a single partition from a reader. Returns empty optional if there are no more partitions to be read.
future<mutation_opt> read_mutation_from_flat_mutation_reader(flat_mutation_reader_v2&);
// Calls the consumer for each element of the reader's stream until end of stream
// is reached or the consumer requests iteration to stop by returning stop_iteration::yes.
// The consumer should accept mutation as the argument and return stop_iteration.
// The returned future<> resolves when consumption ends.
template <typename Consumer>
requires MutationConsumer<Consumer>
inline
future<> consume_partitions(flat_mutation_reader_v2& reader, Consumer consumer) {
    return do_with(std::move(consumer), [&reader] (Consumer& c) -> future<> {
        return repeat([&reader, &c] () {
            return read_mutation_from_flat_mutation_reader(reader).then([&c] (mutation_opt&& mo) -> future<stop_iteration> {
                if (!mo) {
                    return make_ready_future<stop_iteration>(stop_iteration::yes);
                }
                return futurize_invoke(c, std::move(*mo));
            });
        });
    });
}
/// A cosumer function that is passed a flat_mutation_reader to be consumed from
/// and returns a future<> resolved when the reader is fully consumed, and closed.
/// Note: the function assumes ownership of the reader and must close it in all cases.
using reader_consumer_v2 = noncopyable_function<future<> (flat_mutation_reader_v2)>;
namespace ser {
template <typename Output>
void serializer<bound_kind>::write(Output& buf, const bound_kind& v) {
  serialize(buf, static_cast<uint8_t>(v));
}
template<typename Input>
bound_kind serializer<bound_kind>::read(Input& buf) {
  return static_cast<bound_kind>(deserialize(buf, boost::type<uint8_t>()));
}
template <typename Output>
void serializer<range_tombstone>::write(Output& buf, const range_tombstone& obj) {
  set_size(buf, obj);
  static_assert(is_equivalent<decltype(obj.start), clustering_key_prefix>::value, "member value has a wrong type");
  serialize(buf, obj.start);
  static_assert(is_equivalent<decltype(obj.tomb), tombstone>::value, "member value has a wrong type");
  serialize(buf, obj.tomb);
  static_assert(is_equivalent<decltype(obj.start_kind), bound_kind>::value, "member value has a wrong type");
  serialize(buf, obj.start_kind);
  static_assert(is_equivalent<decltype(obj.end), clustering_key_prefix>::value, "member value has a wrong type");
  serialize(buf, obj.end);
  static_assert(is_equivalent<decltype(obj.end_kind), bound_kind>::value, "member value has a wrong type");
  serialize(buf, obj.end_kind);
}
template <typename Input>
range_tombstone serializer<range_tombstone>::read(Input& buf) {
 return seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  auto in = buf.read_substream(size - sizeof(size_type));
  auto __local_0 = deserialize(in, boost::type<clustering_key_prefix>());
  auto __local_1 = deserialize(in, boost::type<tombstone>());
  auto __local_2 = (in.size()>0) ?
    deserialize(in, boost::type<bound_kind>()) : bound_kind::incl_start;
  auto __local_3 = (in.size()>0) ?
    deserialize(in, boost::type<clustering_key_prefix>()) : __local_0;
  auto __local_4 = (in.size()>0) ?
    deserialize(in, boost::type<bound_kind>()) : bound_kind::incl_end;
  range_tombstone res {std::move(__local_0), std::move(__local_1), std::move(__local_2), std::move(__local_3), std::move(__local_4)};
  return res;
 });
}
template <typename Input>
void serializer<range_tombstone>::skip(Input& buf) {
 seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  buf.skip(size - sizeof(size_type));
 });
}
template <typename Output>
void serializer<column_mapping_entry>::write(Output& buf, const column_mapping_entry& obj) {
  set_size(buf, obj);
  static_assert(is_equivalent<decltype(obj.name()), bytes>::value, "member value has a wrong type");
  serialize(buf, obj.name());
  static_assert(is_equivalent<decltype(obj.type_name()), sstring>::value, "member value has a wrong type");
  serialize(buf, obj.type_name());
}
template <typename Input>
column_mapping_entry serializer<column_mapping_entry>::read(Input& buf) {
 return seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  auto in = buf.read_substream(size - sizeof(size_type));
  auto __local_0 = deserialize(in, boost::type<bytes>());
  auto __local_1 = deserialize(in, boost::type<sstring>());
  column_mapping_entry res {std::move(__local_0), std::move(__local_1)};
  return res;
 });
}
template <typename Input>
void serializer<column_mapping_entry>::skip(Input& buf) {
 seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  buf.skip(size - sizeof(size_type));
 });
}
template <typename Output>
void serializer<column_mapping>::write(Output& buf, const column_mapping& obj) {
  set_size(buf, obj);
  static_assert(is_equivalent<decltype(obj.columns()), std::vector<column_mapping_entry>>::value, "member value has a wrong type");
  serialize(buf, obj.columns());
  static_assert(is_equivalent<decltype(obj.n_static()), uint32_t>::value, "member value has a wrong type");
  serialize(buf, obj.n_static());
}
template <typename Input>
column_mapping serializer<column_mapping>::read(Input& buf) {
 return seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  auto in = buf.read_substream(size - sizeof(size_type));
  auto __local_0 = deserialize(in, boost::type<std::vector<column_mapping_entry>>());
  auto __local_1 = deserialize(in, boost::type<uint32_t>());
  column_mapping res {std::move(__local_0), std::move(__local_1)};
  return res;
 });
}
template <typename Input>
void serializer<column_mapping>::skip(Input& buf) {
 seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  buf.skip(size - sizeof(size_type));
 });
}
template <typename Output>
void serializer<partition_end>::write(Output& buf, const partition_end& obj) {
  set_size(buf, obj);
}
template <typename Input>
partition_end serializer<partition_end>::read(Input& buf) {
 return seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  buf.skip(size - sizeof(size_type));
  partition_end res {};
  return res;
 });
}
template <typename Input>
void serializer<partition_end>::skip(Input& buf) {
 seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  buf.skip(size - sizeof(size_type));
 });
}
struct counter_cell_full_view {
    utils::input_stream v;
    auto shards() const {
      return seastar::with_serialized_stream(v, [] (auto& v) {
       auto in = v;
       return vector_deserializer<counter_shard>(in);
      });
    }
};
template<>
struct serializer<counter_cell_full_view> {
    template<typename Input>
    static counter_cell_full_view read(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        auto v_start = v;
        auto start_size = v.size();
        skip(v);
        return counter_cell_full_view{v_start.read_substream(start_size - v.size())};
      });
    }
    template<typename Output>
    static void write(Output& out, counter_cell_full_view v) {
        v.v.copy_to(out);
    }
    template<typename Input>
    static void skip(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        auto& in = v;
       ser::skip(in, boost::type<std::vector<counter_shard>>());
      });
    }
};
struct counter_cell_update_view {
    utils::input_stream v;
};
template<>
struct serializer<counter_cell_update_view> {
     ;
     ;
     ;
};
struct live_cell_view {
    utils::input_stream v;
};
template<>
struct serializer<live_cell_view> {
     ;
     ;
     ;
};
struct live_marker_view {
    utils::input_stream v;
    auto created_at() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<api::timestamp_type>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       return deserialize(in, boost::type<api::timestamp_type>());
      });
    }
};
template<>
struct serializer<live_marker_view> {
    template<typename Input>
    static live_marker_view read(Input& v) ;
     ;
     ;
};
struct no_marker_view {
    utils::input_stream v;
};
template<>
struct serializer<no_marker_view> {
     ;
     ;
     ;
};
struct tombstone_view {
    utils::input_stream v;
    operator tombstone() const ;
    auto timestamp() const ;
    auto deletion_time() const ;
};
template<>
struct serializer<tombstone_view> {
    template<typename Input>
    static tombstone_view read(Input& v) ;
     ;
     ;
};
 ;
 ;
struct counter_cell_view {
    utils::input_stream v;
};
template<>
struct serializer<counter_cell_view> {
     ;
     ;
     ;
};
struct dead_cell_view {
    utils::input_stream v;
};
template<>
struct serializer<dead_cell_view> {
     ;
     ;
    template<typename Input>
    static void skip(Input& v) ;
};
struct dead_marker_view {
    utils::input_stream v;
    auto tomb() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<tombstone_view>())) {
       std::ignore = this;
       auto in = v;
       return deserialize(in, boost::type<tombstone_view>());
      });
    }
};
template<>
struct serializer<dead_marker_view> {
     ;
     ;
     ;
};
struct expiring_cell_view {
    utils::input_stream v;
};
template<>
struct serializer<expiring_cell_view> {
     ;
     ;
     ;
};
struct expiring_marker_view {
    utils::input_stream v;
    auto lm() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<live_marker_view>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       return deserialize(in, boost::type<live_marker_view>());
      });
    }
    auto ttl() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<gc_clock::duration>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       ser::skip(in, boost::type<live_marker_view>());
       return deserialize(in, boost::type<gc_clock::duration>());
      });
    }
    auto expiry() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<gc_clock::time_point>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       ser::skip(in, boost::type<live_marker_view>());
       ser::skip(in, boost::type<gc_clock::duration>());
       return deserialize(in, boost::type<gc_clock::time_point>());
      });
    }
};
template<>
struct serializer<expiring_marker_view> {
    template<typename Input>
    static expiring_marker_view read(Input& v) ;
     ;
     ;
};
struct partition_start_view {
    utils::input_stream v;
};
template<>
struct serializer<partition_start_view> {
     ;
     ;
     ;
};
struct range_tombstone_view {
    utils::input_stream v;
};
template<>
struct serializer<range_tombstone_view> {
     ;
     ;
     ;
};
 ;
 ;
struct collection_element_view {
    utils::input_stream v;
};
template<>
struct serializer<collection_element_view> {
     ;
     ;
     ;
};
struct collection_cell_view {
    utils::input_stream v;
    
    
};
template<>
struct serializer<collection_cell_view> {
    template<typename Input>
    static collection_cell_view read(Input& v) ;
    template<typename Output>
    static void write(Output& out, collection_cell_view v) ;
     ;
};
 ;
template<typename Input>
boost::variant<live_cell_view, expiring_cell_view, dead_cell_view, counter_cell_view, unknown_variant_type> deserialize(Input& v, boost::type<boost::variant<live_cell_view, expiring_cell_view, dead_cell_view, counter_cell_view, unknown_variant_type>>) ;
 ;
template<typename Input>
boost::variant<boost::variant<live_cell_view, expiring_cell_view, dead_cell_view, counter_cell_view, unknown_variant_type>, collection_cell_view, unknown_variant_type> deserialize(Input& v, boost::type<boost::variant<boost::variant<live_cell_view, expiring_cell_view, dead_cell_view, counter_cell_view, unknown_variant_type>, collection_cell_view, unknown_variant_type>>) {
  return seastar::with_serialized_stream(v, [] (auto& v) {
    auto in = v;
    deserialize(in, boost::type<size_type>());
    size_type o = deserialize(in, boost::type<size_type>());
    if (o == 0) {
        v.skip(sizeof(size_type)*2);
        return boost::variant<boost::variant<live_cell_view, expiring_cell_view, dead_cell_view, counter_cell_view, unknown_variant_type>, collection_cell_view, unknown_variant_type>(deserialize(v, boost::type<boost::variant<live_cell_view, expiring_cell_view, dead_cell_view, counter_cell_view, unknown_variant_type>>()));
    }
    if (o == 1) {
        v.skip(sizeof(size_type)*2);
        return boost::variant<boost::variant<live_cell_view, expiring_cell_view, dead_cell_view, counter_cell_view, unknown_variant_type>, collection_cell_view, unknown_variant_type>(deserialize(v, boost::type<collection_cell_view>()));
    }
    return boost::variant<boost::variant<live_cell_view, expiring_cell_view, dead_cell_view, counter_cell_view, unknown_variant_type>, collection_cell_view, unknown_variant_type>(deserialize(v, boost::type<unknown_variant_type>()));
  });
}
struct column_view {
    utils::input_stream v;
    auto id() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<uint32_t>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       return deserialize(in, boost::type<uint32_t>());
      });
    }
    auto c() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<boost::variant<boost::variant<live_cell_view, expiring_cell_view, dead_cell_view, counter_cell_view, unknown_variant_type>, collection_cell_view, unknown_variant_type>>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       ser::skip(in, boost::type<uint32_t>());
       return deserialize(in, boost::type<boost::variant<boost::variant<live_cell_view, expiring_cell_view, dead_cell_view, counter_cell_view, unknown_variant_type>, collection_cell_view, unknown_variant_type>>());
      });
    }
};
template<>
struct serializer<column_view> {
    template<typename Input>
    static column_view read(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        auto v_start = v;
        auto start_size = v.size();
        skip(v);
        return column_view{v_start.read_substream(start_size - v.size())};
      });
    }
    template<typename Output>
    static void write(Output& out, column_view v) {
        v.v.copy_to(out);
    }
    template<typename Input>
    static void skip(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        v.skip(read_frame_size(v));
      });
    }
};
struct row_view {
    utils::input_stream v;
};
template<>
struct serializer<row_view> {
     ;
     ;
     ;
};
 ;
 ;
struct deletable_row_view {
    utils::input_stream v;
};
template<>
struct serializer<deletable_row_view> {
     ;
     ;
     ;
};
struct static_row_view {
    utils::input_stream v;
};
template<>
struct serializer<static_row_view> {
     ;
     ;
     ;
};
struct clustering_row_view {
    utils::input_stream v;
};
template<>
struct serializer<clustering_row_view> {
     ;
     ;
     ;
};
struct mutation_partition_view {
    utils::input_stream v;
};
template<>
struct serializer<mutation_partition_view> {
     ;
     ;
     ;
};
struct canonical_mutation_view {
    utils::input_stream v;
};
template<>
struct serializer<canonical_mutation_view> {
     ;
     ;
     ;
};
struct mutation_view {
    utils::input_stream v;
};
template<>
struct serializer<mutation_view> {
     ;
     ;
     ;
};
 ;
 ;
struct mutation_fragment_view {
    utils::input_stream v;
};
template<>
struct serializer<mutation_fragment_view> {
     ;
     ;
     ;
};
////// State holders
template<typename Output>
struct state_of_counter_cell_full {
    empty_frame<Output> f;
};
template<typename Output>
struct state_of_counter_cell_update {
    empty_frame<Output> f;
};
template<typename Output>
struct state_of_counter_cell {
    frame<Output> f;
};
template<typename Output>
struct state_of_counter_cell__value {
    frame<Output> f;
    state_of_counter_cell<Output> _parent;
};
template<typename Output>
struct state_of_counter_cell__value__counter_cell_full {
    empty_frame<Output> f;
    state_of_counter_cell__value<Output> _parent;
};
template<typename Output>
struct state_of_counter_cell__value__counter_cell_update {
    empty_frame<Output> f;
    state_of_counter_cell__value<Output> _parent;
};
template<typename Output>
struct state_of_tombstone {
    frame<Output> f;
};
template<typename Output>
struct state_of_live_cell {
    frame<Output> f;
};
template<typename Output>
struct state_of_expiring_cell {
    frame<Output> f;
};
template<typename Output>
struct state_of_expiring_cell__c {
    frame<Output> f;
    state_of_expiring_cell<Output> _parent;
};
template<typename Output>
struct state_of_dead_cell {
    empty_frame<Output> f;
};
template<typename Output>
struct state_of_dead_cell__tomb {
    frame<Output> f;
    state_of_dead_cell<Output> _parent;
};
template<typename Output>
struct state_of_collection_element {
    frame<Output> f;
};
template<typename Output>
struct state_of_collection_element__value {
    frame<Output> f;
    state_of_collection_element<Output> _parent;
};
template<typename Output>
struct state_of_collection_element__value__live_cell {
    frame<Output> f;
    state_of_collection_element__value<Output> _parent;
};
template<typename Output>
struct state_of_collection_element__value__expiring_cell {
    frame<Output> f;
    state_of_collection_element__value<Output> _parent;
};
template<typename Output>
struct state_of_collection_element__value__expiring_cell__c {
    frame<Output> f;
    state_of_collection_element__value__expiring_cell<Output> _parent;
};
template<typename Output>
struct state_of_collection_element__value__dead_cell {
    empty_frame<Output> f;
    state_of_collection_element__value<Output> _parent;
};
template<typename Output>
struct state_of_collection_element__value__dead_cell__tomb {
    frame<Output> f;
    state_of_collection_element__value__dead_cell<Output> _parent;
};
template<typename Output>
struct state_of_collection_cell {
    frame<Output> f;
};
template<typename Output>
struct state_of_collection_cell__tomb {
    frame<Output> f;
    state_of_collection_cell<Output> _parent;
};
template<typename Output>
struct state_of_column {
    frame<Output> f;
};
template<typename Output>
struct state_of_column__c {
    frame<Output> f;
    state_of_column<Output> _parent;
};
template<typename Output>
struct state_of_column__c__variant {
    frame<Output> f;
    state_of_column__c<Output> _parent;
};
template<typename Output>
struct state_of_column__c__variant__live_cell {
    frame<Output> f;
    state_of_column__c__variant<Output> _parent;
};
template<typename Output>
struct state_of_column__c__variant__expiring_cell {
    frame<Output> f;
    state_of_column__c__variant<Output> _parent;
};
template<typename Output>
struct state_of_column__c__variant__expiring_cell__c {
    frame<Output> f;
    state_of_column__c__variant__expiring_cell<Output> _parent;
};
template<typename Output>
struct state_of_column__c__variant__dead_cell {
    empty_frame<Output> f;
    state_of_column__c__variant<Output> _parent;
};
template<typename Output>
struct state_of_column__c__variant__dead_cell__tomb {
    frame<Output> f;
    state_of_column__c__variant__dead_cell<Output> _parent;
};
template<typename Output>
struct state_of_column__c__variant__counter_cell {
    frame<Output> f;
    state_of_column__c__variant<Output> _parent;
};
template<typename Output>
struct state_of_column__c__variant__counter_cell__value {
    frame<Output> f;
    state_of_column__c__variant__counter_cell<Output> _parent;
};
template<typename Output>
struct state_of_column__c__variant__counter_cell__value__counter_cell_full {
    empty_frame<Output> f;
    state_of_column__c__variant__counter_cell__value<Output> _parent;
};
template<typename Output>
struct state_of_column__c__variant__counter_cell__value__counter_cell_update {
    empty_frame<Output> f;
    state_of_column__c__variant__counter_cell__value<Output> _parent;
};
template<typename Output>
struct state_of_column__c__collection_cell {
    frame<Output> f;
    state_of_column__c<Output> _parent;
};
template<typename Output>
struct state_of_column__c__collection_cell__tomb {
    frame<Output> f;
    state_of_column__c__collection_cell<Output> _parent;
};
template<typename Output>
struct state_of_row {
    frame<Output> f;
};
template<typename Output>
struct state_of_no_marker {
    empty_frame<Output> f;
};
template<typename Output>
struct state_of_live_marker {
    frame<Output> f;
};
template<typename Output>
struct state_of_expiring_marker {
    frame<Output> f;
};
template<typename Output>
struct state_of_expiring_marker__lm {
    frame<Output> f;
    state_of_expiring_marker<Output> _parent;
};
template<typename Output>
struct state_of_dead_marker {
    empty_frame<Output> f;
};
template<typename Output>
struct state_of_dead_marker__tomb {
    frame<Output> f;
    state_of_dead_marker<Output> _parent;
};
template<typename Output>
struct state_of_deletable_row {
    frame<Output> f;
};
template<typename Output>
struct state_of_deletable_row__marker {
    frame<Output> f;
    state_of_deletable_row<Output> _parent;
};
template<typename Output>
struct state_of_deletable_row__marker__live_marker {
    frame<Output> f;
    state_of_deletable_row__marker<Output> _parent;
};
template<typename Output>
struct state_of_deletable_row__marker__expiring_marker {
    frame<Output> f;
    state_of_deletable_row__marker<Output> _parent;
};
template<typename Output>
struct state_of_deletable_row__marker__expiring_marker__lm {
    frame<Output> f;
    state_of_deletable_row__marker__expiring_marker<Output> _parent;
};
template<typename Output>
struct state_of_deletable_row__marker__dead_marker {
    empty_frame<Output> f;
    state_of_deletable_row__marker<Output> _parent;
};
template<typename Output>
struct state_of_deletable_row__marker__dead_marker__tomb {
    frame<Output> f;
    state_of_deletable_row__marker__dead_marker<Output> _parent;
};
template<typename Output>
struct state_of_deletable_row__marker__no_marker {
    empty_frame<Output> f;
    state_of_deletable_row__marker<Output> _parent;
};
template<typename Output>
struct state_of_deletable_row__deleted_at {
    frame<Output> f;
    state_of_deletable_row<Output> _parent;
};
template<typename Output>
struct state_of_deletable_row__cells {
    frame<Output> f;
    state_of_deletable_row<Output> _parent;
};
template<typename Output>
struct state_of_deletable_row__shadowable_deleted_at {
    frame<Output> f;
    state_of_deletable_row<Output> _parent;
};
template<typename Output>
struct state_of_range_tombstone {
    frame<Output> f;
};
template<typename Output>
struct state_of_range_tombstone__tomb {
    frame<Output> f;
    state_of_range_tombstone<Output> _parent;
};
template<typename Output>
struct state_of_mutation_partition {
    frame<Output> f;
};
template<typename Output>
struct state_of_mutation_partition__tomb {
    frame<Output> f;
    state_of_mutation_partition<Output> _parent;
};
template<typename Output>
struct state_of_mutation_partition__static_row {
    frame<Output> f;
    state_of_mutation_partition<Output> _parent;
};
template<typename Output>
struct state_of_mutation {
    frame<Output> f;
};
template<typename Output>
struct state_of_mutation__partition {
    frame<Output> f;
    state_of_mutation<Output> _parent;
};
template<typename Output>
struct state_of_mutation__partition__tomb {
    frame<Output> f;
    state_of_mutation__partition<Output> _parent;
};
template<typename Output>
struct state_of_mutation__partition__static_row {
    frame<Output> f;
    state_of_mutation__partition<Output> _parent;
};
template<typename Output>
struct state_of_canonical_mutation {
    frame<Output> f;
};
template<typename Output>
struct state_of_canonical_mutation__partition {
    frame<Output> f;
    state_of_canonical_mutation<Output> _parent;
};
template<typename Output>
struct state_of_canonical_mutation__partition__tomb {
    frame<Output> f;
    state_of_canonical_mutation__partition<Output> _parent;
};
template<typename Output>
struct state_of_canonical_mutation__partition__static_row {
    frame<Output> f;
    state_of_canonical_mutation__partition<Output> _parent;
};
template<typename Output>
struct state_of_clustering_row {
    frame<Output> f;
};
template<typename Output>
struct state_of_clustering_row__row {
    frame<Output> f;
    state_of_clustering_row<Output> _parent;
};
template<typename Output>
struct state_of_clustering_row__row__marker {
    frame<Output> f;
    state_of_clustering_row__row<Output> _parent;
};
template<typename Output>
struct state_of_clustering_row__row__marker__live_marker {
    frame<Output> f;
    state_of_clustering_row__row__marker<Output> _parent;
};
template<typename Output>
struct state_of_clustering_row__row__marker__expiring_marker {
    frame<Output> f;
    state_of_clustering_row__row__marker<Output> _parent;
};
template<typename Output>
struct state_of_clustering_row__row__marker__expiring_marker__lm {
    frame<Output> f;
    state_of_clustering_row__row__marker__expiring_marker<Output> _parent;
};
template<typename Output>
struct state_of_clustering_row__row__marker__dead_marker {
    empty_frame<Output> f;
    state_of_clustering_row__row__marker<Output> _parent;
};
template<typename Output>
struct state_of_clustering_row__row__marker__dead_marker__tomb {
    frame<Output> f;
    state_of_clustering_row__row__marker__dead_marker<Output> _parent;
};
template<typename Output>
struct state_of_clustering_row__row__marker__no_marker {
    empty_frame<Output> f;
    state_of_clustering_row__row__marker<Output> _parent;
};
template<typename Output>
struct state_of_clustering_row__row__deleted_at {
    frame<Output> f;
    state_of_clustering_row__row<Output> _parent;
};
template<typename Output>
struct state_of_clustering_row__row__cells {
    frame<Output> f;
    state_of_clustering_row__row<Output> _parent;
};
template<typename Output>
struct state_of_clustering_row__row__shadowable_deleted_at {
    frame<Output> f;
    state_of_clustering_row__row<Output> _parent;
};
template<typename Output>
struct state_of_static_row {
    frame<Output> f;
};
template<typename Output>
struct state_of_static_row__cells {
    frame<Output> f;
    state_of_static_row<Output> _parent;
};
template<typename Output>
struct state_of_partition_start {
    frame<Output> f;
};
template<typename Output>
struct state_of_partition_start__partition_tombstone {
    frame<Output> f;
    state_of_partition_start<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment {
    frame<Output> f;
};
template<typename Output>
struct state_of_mutation_fragment__fragment {
    frame<Output> f;
    state_of_mutation_fragment<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row {
    frame<Output> f;
    state_of_mutation_fragment__fragment<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row__row {
    frame<Output> f;
    state_of_mutation_fragment__fragment__clustering_row<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row__row__marker {
    frame<Output> f;
    state_of_mutation_fragment__fragment__clustering_row__row<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row__row__marker__live_marker {
    frame<Output> f;
    state_of_mutation_fragment__fragment__clustering_row__row__marker<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker {
    frame<Output> f;
    state_of_mutation_fragment__fragment__clustering_row__row__marker<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker__lm {
    frame<Output> f;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row__row__marker__dead_marker {
    empty_frame<Output> f;
    state_of_mutation_fragment__fragment__clustering_row__row__marker<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row__row__marker__dead_marker__tomb {
    frame<Output> f;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__dead_marker<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row__row__marker__no_marker {
    empty_frame<Output> f;
    state_of_mutation_fragment__fragment__clustering_row__row__marker<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row__row__deleted_at {
    frame<Output> f;
    state_of_mutation_fragment__fragment__clustering_row__row<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row__row__cells {
    frame<Output> f;
    state_of_mutation_fragment__fragment__clustering_row__row<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__clustering_row__row__shadowable_deleted_at {
    frame<Output> f;
    state_of_mutation_fragment__fragment__clustering_row__row<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__static_row {
    frame<Output> f;
    state_of_mutation_fragment__fragment<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__static_row__cells {
    frame<Output> f;
    state_of_mutation_fragment__fragment__static_row<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__range_tombstone {
    frame<Output> f;
    state_of_mutation_fragment__fragment<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__range_tombstone__tomb {
    frame<Output> f;
    state_of_mutation_fragment__fragment__range_tombstone<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__partition_start {
    frame<Output> f;
    state_of_mutation_fragment__fragment<Output> _parent;
};
template<typename Output>
struct state_of_mutation_fragment__fragment__partition_start__partition_tombstone {
    frame<Output> f;
    state_of_mutation_fragment__fragment__partition_start<Output> _parent;
};
////// Nodes
template<typename Output>
struct after_counter_cell_full__shards {
    Output& _out;
    state_of_counter_cell_full<Output> _state;
};
template<typename Output>
struct counter_cell_full__shards {
    Output& _out;
    state_of_counter_cell_full<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct writer_of_counter_cell_full {
    Output& _out;
    state_of_counter_cell_full<Output> _state;
};
template<typename Output>
struct after_counter_cell_update__delta {
    Output& _out;
    state_of_counter_cell_update<Output> _state;
};
template<typename Output>
struct writer_of_counter_cell_update {
    Output& _out;
    state_of_counter_cell_update<Output> _state;
};
template<typename Output>
struct after_live_cell__value {
    Output& _out;
    state_of_live_cell<Output> _state;
};
template<typename Output>
struct after_live_cell__created_at {
    Output& _out;
    state_of_live_cell<Output> _state;
     ;
};
template<typename Output>
struct writer_of_live_cell {
    Output& _out;
    state_of_live_cell<Output> _state;
};
template<typename Output>
struct after_live_marker__created_at {
    Output& _out;
    state_of_live_marker<Output> _state;
};
template<typename Output>
struct writer_of_live_marker {
    Output& _out;
    state_of_live_marker<Output> _state;
};
template<typename Output>
struct writer_of_no_marker {
    Output& _out;
    state_of_no_marker<Output> _state;
};
template<typename Output>
struct after_tombstone__deletion_time {
    Output& _out;
    state_of_tombstone<Output> _state;
};
template<typename Output>
struct after_tombstone__timestamp {
    Output& _out;
    state_of_tombstone<Output> _state;
};
template<typename Output>
struct writer_of_tombstone {
    Output& _out;
    state_of_tombstone<Output> _state;
};
template<typename Output>
struct after_counter_cell__value {
    Output& _out;
    state_of_counter_cell<Output> _state;
};
template<typename Output>
struct after_counter_cell__value__counter_cell_full__shards {
    Output& _out;
    state_of_counter_cell__value__counter_cell_full<Output> _state;
};
template<typename Output>
struct counter_cell__value__counter_cell_full__shards {
    Output& _out;
    state_of_counter_cell__value__counter_cell_full<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct counter_cell__value__counter_cell_full {
    Output& _out;
    state_of_counter_cell__value__counter_cell_full<Output> _state;
};
template<typename Output>
struct after_counter_cell__value__counter_cell_update__delta {
    Output& _out;
    state_of_counter_cell__value__counter_cell_update<Output> _state;
};
template<typename Output>
struct counter_cell__value__counter_cell_update {
    Output& _out;
    state_of_counter_cell__value__counter_cell_update<Output> _state;
};
template<typename Output>
struct after_counter_cell__created_at {
    Output& _out;
    state_of_counter_cell__value<Output> _state;
     ;
     ;
};
template<typename Output>
struct writer_of_counter_cell {
    Output& _out;
    state_of_counter_cell<Output> _state;
};
template<typename Output>
struct after_dead_cell__tomb {
    Output& _out;
    state_of_dead_cell<Output> _state;
};
template<typename Output>
struct after_dead_cell__tomb__deletion_time {
    Output& _out;
    state_of_dead_cell__tomb<Output> _state;
};
template<typename Output>
struct after_dead_cell__tomb__timestamp {
    Output& _out;
    state_of_dead_cell__tomb<Output> _state;
};
template<typename Output>
struct dead_cell__tomb {
    Output& _out;
    state_of_dead_cell__tomb<Output> _state;
};
template<typename Output>
struct writer_of_dead_cell {
    Output& _out;
    state_of_dead_cell<Output> _state;
};
template<typename Output>
struct after_dead_marker__tomb {
    Output& _out;
    state_of_dead_marker<Output> _state;
};
template<typename Output>
struct after_dead_marker__tomb__deletion_time {
    Output& _out;
    state_of_dead_marker__tomb<Output> _state;
};
template<typename Output>
struct after_dead_marker__tomb__timestamp {
    Output& _out;
    state_of_dead_marker__tomb<Output> _state;
};
template<typename Output>
struct dead_marker__tomb {
    Output& _out;
    state_of_dead_marker__tomb<Output> _state;
};
template<typename Output>
struct writer_of_dead_marker {
    Output& _out;
    state_of_dead_marker<Output> _state;
};
template<typename Output>
struct after_expiring_cell__c {
    Output& _out;
    state_of_expiring_cell<Output> _state;
};
template<typename Output>
struct after_expiring_cell__c__value {
    Output& _out;
    state_of_expiring_cell__c<Output> _state;
};
template<typename Output>
struct after_expiring_cell__c__created_at {
    Output& _out;
    state_of_expiring_cell__c<Output> _state;
     ;
};
template<typename Output>
struct expiring_cell__c {
    Output& _out;
    state_of_expiring_cell__c<Output> _state;
};
template<typename Output>
struct after_expiring_cell__expiry {
    Output& _out;
    state_of_expiring_cell<Output> _state;
     ;
};
template<typename Output>
struct after_expiring_cell__ttl {
    Output& _out;
    state_of_expiring_cell<Output> _state;
};
template<typename Output>
struct writer_of_expiring_cell {
    Output& _out;
    state_of_expiring_cell<Output> _state;
};
template<typename Output>
struct after_expiring_marker__expiry {
    Output& _out;
    state_of_expiring_marker<Output> _state;
};
template<typename Output>
struct after_expiring_marker__ttl {
    Output& _out;
    state_of_expiring_marker<Output> _state;
};
template<typename Output>
struct after_expiring_marker__lm {
    Output& _out;
    state_of_expiring_marker<Output> _state;
};
template<typename Output>
struct after_expiring_marker__lm__created_at {
    Output& _out;
    state_of_expiring_marker__lm<Output> _state;
};
template<typename Output>
struct expiring_marker__lm {
    Output& _out;
    state_of_expiring_marker__lm<Output> _state;
};
template<typename Output>
struct writer_of_expiring_marker {
    Output& _out;
    state_of_expiring_marker<Output> _state;
     ;
};
template<typename Output>
struct after_partition_start__partition_tombstone {
    Output& _out;
    state_of_partition_start<Output> _state;
};
template<typename Output>
struct after_partition_start__partition_tombstone__deletion_time {
    Output& _out;
    state_of_partition_start__partition_tombstone<Output> _state;
};
template<typename Output>
struct after_partition_start__partition_tombstone__timestamp {
    Output& _out;
    state_of_partition_start__partition_tombstone<Output> _state;
};
template<typename Output>
struct partition_start__partition_tombstone {
    Output& _out;
    state_of_partition_start__partition_tombstone<Output> _state;
};
template<typename Output>
struct after_partition_start__key {
    Output& _out;
    state_of_partition_start<Output> _state;
};
template<typename Output>
struct writer_of_partition_start {
    Output& _out;
    state_of_partition_start<Output> _state;
};
template<typename Output>
struct after_range_tombstone__end_kind {
    Output& _out;
    state_of_range_tombstone<Output> _state;
};
template<typename Output>
struct after_range_tombstone__end {
    Output& _out;
    state_of_range_tombstone<Output> _state;
};
template<typename Output>
struct after_range_tombstone__start_kind {
    Output& _out;
    state_of_range_tombstone<Output> _state;
};
template<typename Output>
struct after_range_tombstone__tomb {
    Output& _out;
    state_of_range_tombstone<Output> _state;
};
template<typename Output>
struct after_range_tombstone__tomb__deletion_time {
    Output& _out;
    state_of_range_tombstone__tomb<Output> _state;
};
template<typename Output>
struct after_range_tombstone__tomb__timestamp {
    Output& _out;
    state_of_range_tombstone__tomb<Output> _state;
};
template<typename Output>
struct range_tombstone__tomb {
    Output& _out;
    state_of_range_tombstone__tomb<Output> _state;
};
template<typename Output>
struct after_range_tombstone__start {
    Output& _out;
    state_of_range_tombstone<Output> _state;
};
template<typename Output>
struct writer_of_range_tombstone {
    Output& _out;
    state_of_range_tombstone<Output> _state;
};
template<typename Output>
struct after_collection_element__value {
    Output& _out;
    state_of_collection_element<Output> _state;
};
template<typename Output>
struct after_collection_element__value__live_cell__value {
    Output& _out;
    state_of_collection_element__value__live_cell<Output> _state;
};
template<typename Output>
struct after_collection_element__value__live_cell__created_at {
    Output& _out;
    state_of_collection_element__value__live_cell<Output> _state;
     ;
};
template<typename Output>
struct collection_element__value__live_cell {
    Output& _out;
    state_of_collection_element__value__live_cell<Output> _state;
};
template<typename Output>
struct after_collection_element__value__expiring_cell__c {
    Output& _out;
    state_of_collection_element__value__expiring_cell<Output> _state;
};
template<typename Output>
struct after_collection_element__value__expiring_cell__c__value {
    Output& _out;
    state_of_collection_element__value__expiring_cell__c<Output> _state;
};
template<typename Output>
struct after_collection_element__value__expiring_cell__c__created_at {
    Output& _out;
    state_of_collection_element__value__expiring_cell__c<Output> _state;
     ;
};
template<typename Output>
struct collection_element__value__expiring_cell__c {
    Output& _out;
    state_of_collection_element__value__expiring_cell__c<Output> _state;
};
template<typename Output>
struct after_collection_element__value__expiring_cell__expiry {
    Output& _out;
    state_of_collection_element__value__expiring_cell<Output> _state;
     ;
};
template<typename Output>
struct after_collection_element__value__expiring_cell__ttl {
    Output& _out;
    state_of_collection_element__value__expiring_cell<Output> _state;
};
template<typename Output>
struct collection_element__value__expiring_cell {
    Output& _out;
    state_of_collection_element__value__expiring_cell<Output> _state;
};
template<typename Output>
struct after_collection_element__value__dead_cell__tomb {
    Output& _out;
    state_of_collection_element__value__dead_cell<Output> _state;
};
template<typename Output>
struct after_collection_element__value__dead_cell__tomb__deletion_time {
    Output& _out;
    state_of_collection_element__value__dead_cell__tomb<Output> _state;
};
template<typename Output>
struct after_collection_element__value__dead_cell__tomb__timestamp {
    Output& _out;
    state_of_collection_element__value__dead_cell__tomb<Output> _state;
};
template<typename Output>
struct collection_element__value__dead_cell__tomb {
    Output& _out;
    state_of_collection_element__value__dead_cell__tomb<Output> _state;
};
template<typename Output>
struct collection_element__value__dead_cell {
    Output& _out;
    state_of_collection_element__value__dead_cell<Output> _state;
};
template<typename Output>
struct after_collection_element__key {
    Output& _out;
    state_of_collection_element__value<Output> _state;
     ;
     ;
     ;
};
template<typename Output>
struct writer_of_collection_element {
    Output& _out;
    state_of_collection_element<Output> _state;
     ;
};
template<typename Output>
struct after_collection_cell__elements {
    Output& _out;
    state_of_collection_cell<Output> _state;
};
template<typename Output>
struct collection_cell__elements {
    Output& _out;
    state_of_collection_cell<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct after_collection_cell__tomb {
    Output& _out;
    state_of_collection_cell<Output> _state;
};
template<typename Output>
struct after_collection_cell__tomb__deletion_time {
    Output& _out;
    state_of_collection_cell__tomb<Output> _state;
};
template<typename Output>
struct after_collection_cell__tomb__timestamp {
    Output& _out;
    state_of_collection_cell__tomb<Output> _state;
};
template<typename Output>
struct collection_cell__tomb {
    Output& _out;
    state_of_collection_cell__tomb<Output> _state;
};
template<typename Output>
struct writer_of_collection_cell {
    Output& _out;
    state_of_collection_cell<Output> _state;
};
template<typename Output>
struct after_column__c {
    Output& _out;
    state_of_column<Output> _state;
};
template<typename Output>
struct after_column__c__variant {
    Output& _out;
    state_of_column__c<Output> _state;
};
template<typename Output>
struct after_column__c__variant__live_cell__value {
    Output& _out;
    state_of_column__c__variant__live_cell<Output> _state;
};
template<typename Output>
struct after_column__c__variant__live_cell__created_at {
    Output& _out;
    state_of_column__c__variant__live_cell<Output> _state;
     ;
};
template<typename Output>
struct column__c__variant__live_cell {
    Output& _out;
    state_of_column__c__variant__live_cell<Output> _state;
};
template<typename Output>
struct after_column__c__variant__expiring_cell__c {
    Output& _out;
    state_of_column__c__variant__expiring_cell<Output> _state;
};
template<typename Output>
struct after_column__c__variant__expiring_cell__c__value {
    Output& _out;
    state_of_column__c__variant__expiring_cell__c<Output> _state;
};
template<typename Output>
struct after_column__c__variant__expiring_cell__c__created_at {
    Output& _out;
    state_of_column__c__variant__expiring_cell__c<Output> _state;
     ;
};
template<typename Output>
struct column__c__variant__expiring_cell__c {
    Output& _out;
    state_of_column__c__variant__expiring_cell__c<Output> _state;
};
template<typename Output>
struct after_column__c__variant__expiring_cell__expiry {
    Output& _out;
    state_of_column__c__variant__expiring_cell<Output> _state;
     ;
};
template<typename Output>
struct after_column__c__variant__expiring_cell__ttl {
    Output& _out;
    state_of_column__c__variant__expiring_cell<Output> _state;
};
template<typename Output>
struct column__c__variant__expiring_cell {
    Output& _out;
    state_of_column__c__variant__expiring_cell<Output> _state;
};
template<typename Output>
struct after_column__c__variant__dead_cell__tomb {
    Output& _out;
    state_of_column__c__variant__dead_cell<Output> _state;
};
template<typename Output>
struct after_column__c__variant__dead_cell__tomb__deletion_time {
    Output& _out;
    state_of_column__c__variant__dead_cell__tomb<Output> _state;
};
template<typename Output>
struct after_column__c__variant__dead_cell__tomb__timestamp {
    Output& _out;
    state_of_column__c__variant__dead_cell__tomb<Output> _state;
};
template<typename Output>
struct column__c__variant__dead_cell__tomb {
    Output& _out;
    state_of_column__c__variant__dead_cell__tomb<Output> _state;
};
template<typename Output>
struct column__c__variant__dead_cell {
    Output& _out;
    state_of_column__c__variant__dead_cell<Output> _state;
};
template<typename Output>
struct after_column__c__variant__counter_cell__value {
    Output& _out;
    state_of_column__c__variant__counter_cell<Output> _state;
};
template<typename Output>
struct after_column__c__variant__counter_cell__value__counter_cell_full__shards {
    Output& _out;
    state_of_column__c__variant__counter_cell__value__counter_cell_full<Output> _state;
};
template<typename Output>
struct column__c__variant__counter_cell__value__counter_cell_full__shards {
    Output& _out;
    state_of_column__c__variant__counter_cell__value__counter_cell_full<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct column__c__variant__counter_cell__value__counter_cell_full {
    Output& _out;
    state_of_column__c__variant__counter_cell__value__counter_cell_full<Output> _state;
};
template<typename Output>
struct after_column__c__variant__counter_cell__value__counter_cell_update__delta {
    Output& _out;
    state_of_column__c__variant__counter_cell__value__counter_cell_update<Output> _state;
};
template<typename Output>
struct column__c__variant__counter_cell__value__counter_cell_update {
    Output& _out;
    state_of_column__c__variant__counter_cell__value__counter_cell_update<Output> _state;
};
template<typename Output>
struct after_column__c__variant__counter_cell__created_at {
    Output& _out;
    state_of_column__c__variant__counter_cell__value<Output> _state;
     ;
     ;
};
template<typename Output>
struct column__c__variant__counter_cell {
    Output& _out;
    state_of_column__c__variant__counter_cell<Output> _state;
};
template<typename Output>
struct column__c__variant {
    Output& _out;
    state_of_column__c__variant<Output> _state;
     ;
     ;
     ;
     ;
};
template<typename Output>
struct after_column__c__collection_cell__elements {
    Output& _out;
    state_of_column__c__collection_cell<Output> _state;
};
template<typename Output>
struct column__c__collection_cell__elements {
    Output& _out;
    state_of_column__c__collection_cell<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct after_column__c__collection_cell__tomb {
    Output& _out;
    state_of_column__c__collection_cell<Output> _state;
};
template<typename Output>
struct after_column__c__collection_cell__tomb__deletion_time {
    Output& _out;
    state_of_column__c__collection_cell__tomb<Output> _state;
};
template<typename Output>
struct after_column__c__collection_cell__tomb__timestamp {
    Output& _out;
    state_of_column__c__collection_cell__tomb<Output> _state;
};
template<typename Output>
struct column__c__collection_cell__tomb {
    Output& _out;
    state_of_column__c__collection_cell__tomb<Output> _state;
};
template<typename Output>
struct column__c__collection_cell {
    Output& _out;
    state_of_column__c__collection_cell<Output> _state;
};
template<typename Output>
struct after_column__id {
    Output& _out;
    state_of_column__c<Output> _state;
     ;
};
template<typename Output>
struct writer_of_column {
    Output& _out;
    state_of_column<Output> _state;
};
template<typename Output>
struct after_row__columns {
    Output& _out;
    state_of_row<Output> _state;
};
template<typename Output>
struct row__columns {
    Output& _out;
    state_of_row<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct writer_of_row {
    Output& _out;
    state_of_row<Output> _state;
};
template<typename Output>
struct after_deletable_row__shadowable_deleted_at {
    Output& _out;
    state_of_deletable_row<Output> _state;
};
template<typename Output>
struct after_deletable_row__shadowable_deleted_at__deletion_time {
    Output& _out;
    state_of_deletable_row__shadowable_deleted_at<Output> _state;
};
template<typename Output>
struct after_deletable_row__shadowable_deleted_at__timestamp {
    Output& _out;
    state_of_deletable_row__shadowable_deleted_at<Output> _state;
};
template<typename Output>
struct deletable_row__shadowable_deleted_at {
    Output& _out;
    state_of_deletable_row__shadowable_deleted_at<Output> _state;
};
template<typename Output>
struct after_deletable_row__cells {
    Output& _out;
    state_of_deletable_row<Output> _state;
};
template<typename Output>
struct after_deletable_row__cells__columns {
    Output& _out;
    state_of_deletable_row__cells<Output> _state;
};
template<typename Output>
struct deletable_row__cells__columns {
    Output& _out;
    state_of_deletable_row__cells<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct deletable_row__cells {
    Output& _out;
    state_of_deletable_row__cells<Output> _state;
};
template<typename Output>
struct after_deletable_row__deleted_at {
    Output& _out;
    state_of_deletable_row<Output> _state;
     ;
};
template<typename Output>
struct after_deletable_row__deleted_at__deletion_time {
    Output& _out;
    state_of_deletable_row__deleted_at<Output> _state;
};
template<typename Output>
struct after_deletable_row__deleted_at__timestamp {
    Output& _out;
    state_of_deletable_row__deleted_at<Output> _state;
};
template<typename Output>
struct deletable_row__deleted_at {
    Output& _out;
    state_of_deletable_row__deleted_at<Output> _state;
};
template<typename Output>
struct after_deletable_row__marker {
    Output& _out;
    state_of_deletable_row<Output> _state;
};
template<typename Output>
struct after_deletable_row__marker__live_marker__created_at {
    Output& _out;
    state_of_deletable_row__marker__live_marker<Output> _state;
};
template<typename Output>
struct deletable_row__marker__live_marker {
    Output& _out;
    state_of_deletable_row__marker__live_marker<Output> _state;
};
template<typename Output>
struct after_deletable_row__marker__expiring_marker__expiry {
    Output& _out;
    state_of_deletable_row__marker__expiring_marker<Output> _state;
};
template<typename Output>
struct after_deletable_row__marker__expiring_marker__ttl {
    Output& _out;
    state_of_deletable_row__marker__expiring_marker<Output> _state;
};
template<typename Output>
struct after_deletable_row__marker__expiring_marker__lm {
    Output& _out;
    state_of_deletable_row__marker__expiring_marker<Output> _state;
};
template<typename Output>
struct after_deletable_row__marker__expiring_marker__lm__created_at {
    Output& _out;
    state_of_deletable_row__marker__expiring_marker__lm<Output> _state;
};
template<typename Output>
struct deletable_row__marker__expiring_marker__lm {
    Output& _out;
    state_of_deletable_row__marker__expiring_marker__lm<Output> _state;
};
template<typename Output>
struct deletable_row__marker__expiring_marker {
    Output& _out;
    state_of_deletable_row__marker__expiring_marker<Output> _state;
     ;
};
template<typename Output>
struct after_deletable_row__marker__dead_marker__tomb {
    Output& _out;
    state_of_deletable_row__marker__dead_marker<Output> _state;
};
template<typename Output>
struct after_deletable_row__marker__dead_marker__tomb__deletion_time {
    Output& _out;
    state_of_deletable_row__marker__dead_marker__tomb<Output> _state;
};
template<typename Output>
struct after_deletable_row__marker__dead_marker__tomb__timestamp {
    Output& _out;
    state_of_deletable_row__marker__dead_marker__tomb<Output> _state;
};
template<typename Output>
struct deletable_row__marker__dead_marker__tomb {
    Output& _out;
    state_of_deletable_row__marker__dead_marker__tomb<Output> _state;
};
template<typename Output>
struct deletable_row__marker__dead_marker {
    Output& _out;
    state_of_deletable_row__marker__dead_marker<Output> _state;
};
template<typename Output>
struct deletable_row__marker__no_marker {
    Output& _out;
    state_of_deletable_row__marker__no_marker<Output> _state;
};
template<typename Output>
struct after_deletable_row__key {
    Output& _out;
    state_of_deletable_row__marker<Output> _state;
     ;
     ;
     ;
     ;
};
template<typename Output>
struct writer_of_deletable_row {
    Output& _out;
    state_of_deletable_row<Output> _state;
};
template<typename Output>
struct after_static_row__cells {
    Output& _out;
    state_of_static_row<Output> _state;
};
template<typename Output>
struct after_static_row__cells__columns {
    Output& _out;
    state_of_static_row__cells<Output> _state;
};
template<typename Output>
struct static_row__cells__columns {
    Output& _out;
    state_of_static_row__cells<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct static_row__cells {
    Output& _out;
    state_of_static_row__cells<Output> _state;
};
template<typename Output>
struct writer_of_static_row {
    Output& _out;
    state_of_static_row<Output> _state;
     ;
};
template<typename Output>
struct after_clustering_row__row {
    Output& _out;
    state_of_clustering_row<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__shadowable_deleted_at {
    Output& _out;
    state_of_clustering_row__row<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__shadowable_deleted_at__deletion_time {
    Output& _out;
    state_of_clustering_row__row__shadowable_deleted_at<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__shadowable_deleted_at__timestamp {
    Output& _out;
    state_of_clustering_row__row__shadowable_deleted_at<Output> _state;
};
template<typename Output>
struct clustering_row__row__shadowable_deleted_at {
    Output& _out;
    state_of_clustering_row__row__shadowable_deleted_at<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__cells {
    Output& _out;
    state_of_clustering_row__row<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__cells__columns {
    Output& _out;
    state_of_clustering_row__row__cells<Output> _state;
};
template<typename Output>
struct clustering_row__row__cells__columns {
    Output& _out;
    state_of_clustering_row__row__cells<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct clustering_row__row__cells {
    Output& _out;
    state_of_clustering_row__row__cells<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__deleted_at {
    Output& _out;
    state_of_clustering_row__row<Output> _state;
     ;
};
template<typename Output>
struct after_clustering_row__row__deleted_at__deletion_time {
    Output& _out;
    state_of_clustering_row__row__deleted_at<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__deleted_at__timestamp {
    Output& _out;
    state_of_clustering_row__row__deleted_at<Output> _state;
};
template<typename Output>
struct clustering_row__row__deleted_at {
    Output& _out;
    state_of_clustering_row__row__deleted_at<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__marker {
    Output& _out;
    state_of_clustering_row__row<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__marker__live_marker__created_at {
    Output& _out;
    state_of_clustering_row__row__marker__live_marker<Output> _state;
};
template<typename Output>
struct clustering_row__row__marker__live_marker {
    Output& _out;
    state_of_clustering_row__row__marker__live_marker<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__marker__expiring_marker__expiry {
    Output& _out;
    state_of_clustering_row__row__marker__expiring_marker<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__marker__expiring_marker__ttl {
    Output& _out;
    state_of_clustering_row__row__marker__expiring_marker<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__marker__expiring_marker__lm {
    Output& _out;
    state_of_clustering_row__row__marker__expiring_marker<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__marker__expiring_marker__lm__created_at {
    Output& _out;
    state_of_clustering_row__row__marker__expiring_marker__lm<Output> _state;
};
template<typename Output>
struct clustering_row__row__marker__expiring_marker__lm {
    Output& _out;
    state_of_clustering_row__row__marker__expiring_marker__lm<Output> _state;
};
template<typename Output>
struct clustering_row__row__marker__expiring_marker {
    Output& _out;
    state_of_clustering_row__row__marker__expiring_marker<Output> _state;
     ;
};
template<typename Output>
struct after_clustering_row__row__marker__dead_marker__tomb {
    Output& _out;
    state_of_clustering_row__row__marker__dead_marker<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__marker__dead_marker__tomb__deletion_time {
    Output& _out;
    state_of_clustering_row__row__marker__dead_marker__tomb<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__marker__dead_marker__tomb__timestamp {
    Output& _out;
    state_of_clustering_row__row__marker__dead_marker__tomb<Output> _state;
};
template<typename Output>
struct clustering_row__row__marker__dead_marker__tomb {
    Output& _out;
    state_of_clustering_row__row__marker__dead_marker__tomb<Output> _state;
};
template<typename Output>
struct clustering_row__row__marker__dead_marker {
    Output& _out;
    state_of_clustering_row__row__marker__dead_marker<Output> _state;
};
template<typename Output>
struct clustering_row__row__marker__no_marker {
    Output& _out;
    state_of_clustering_row__row__marker__no_marker<Output> _state;
};
template<typename Output>
struct after_clustering_row__row__key {
    Output& _out;
    state_of_clustering_row__row__marker<Output> _state;
     ;
     ;
     ;
     ;
};
template<typename Output>
struct clustering_row__row {
    Output& _out;
    state_of_clustering_row__row<Output> _state;
};
template<typename Output>
struct writer_of_clustering_row {
    Output& _out;
    state_of_clustering_row<Output> _state;
     ;
};
template<typename Output>
struct after_mutation_partition__rows {
    Output& _out;
    state_of_mutation_partition<Output> _state;
};
template<typename Output>
struct mutation_partition__rows {
    Output& _out;
    state_of_mutation_partition<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct after_mutation_partition__range_tombstones {
    Output& _out;
    state_of_mutation_partition<Output> _state;
};
template<typename Output>
struct mutation_partition__range_tombstones {
    Output& _out;
    state_of_mutation_partition<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct after_mutation_partition__static_row {
    Output& _out;
    state_of_mutation_partition<Output> _state;
};
template<typename Output>
struct after_mutation_partition__static_row__columns {
    Output& _out;
    state_of_mutation_partition__static_row<Output> _state;
};
template<typename Output>
struct mutation_partition__static_row__columns {
    Output& _out;
    state_of_mutation_partition__static_row<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct mutation_partition__static_row {
    Output& _out;
    state_of_mutation_partition__static_row<Output> _state;
};
template<typename Output>
struct after_mutation_partition__tomb {
    Output& _out;
    state_of_mutation_partition<Output> _state;
     ;
};
template<typename Output>
struct after_mutation_partition__tomb__deletion_time {
    Output& _out;
    state_of_mutation_partition__tomb<Output> _state;
};
template<typename Output>
struct after_mutation_partition__tomb__timestamp {
    Output& _out;
    state_of_mutation_partition__tomb<Output> _state;
};
template<typename Output>
struct mutation_partition__tomb {
    Output& _out;
    state_of_mutation_partition__tomb<Output> _state;
};
template<typename Output>
struct writer_of_mutation_partition {
    Output& _out;
    state_of_mutation_partition<Output> _state;
};
template<typename Output>
struct after_canonical_mutation__partition {
    Output& _out;
    state_of_canonical_mutation<Output> _state;
};
template<typename Output>
struct after_canonical_mutation__partition__rows {
    Output& _out;
    state_of_canonical_mutation__partition<Output> _state;
};
template<typename Output>
struct canonical_mutation__partition__rows {
    Output& _out;
    state_of_canonical_mutation__partition<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct after_canonical_mutation__partition__range_tombstones {
    Output& _out;
    state_of_canonical_mutation__partition<Output> _state;
};
template<typename Output>
struct canonical_mutation__partition__range_tombstones {
    Output& _out;
    state_of_canonical_mutation__partition<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct after_canonical_mutation__partition__static_row {
    Output& _out;
    state_of_canonical_mutation__partition<Output> _state;
};
template<typename Output>
struct after_canonical_mutation__partition__static_row__columns {
    Output& _out;
    state_of_canonical_mutation__partition__static_row<Output> _state;
};
template<typename Output>
struct canonical_mutation__partition__static_row__columns {
    Output& _out;
    state_of_canonical_mutation__partition__static_row<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct canonical_mutation__partition__static_row {
    Output& _out;
    state_of_canonical_mutation__partition__static_row<Output> _state;
};
template<typename Output>
struct after_canonical_mutation__partition__tomb {
    Output& _out;
    state_of_canonical_mutation__partition<Output> _state;
     ;
};
template<typename Output>
struct after_canonical_mutation__partition__tomb__deletion_time {
    Output& _out;
    state_of_canonical_mutation__partition__tomb<Output> _state;
};
template<typename Output>
struct after_canonical_mutation__partition__tomb__timestamp {
    Output& _out;
    state_of_canonical_mutation__partition__tomb<Output> _state;
};
template<typename Output>
struct canonical_mutation__partition__tomb {
    Output& _out;
    state_of_canonical_mutation__partition__tomb<Output> _state;
};
template<typename Output>
struct canonical_mutation__partition {
    Output& _out;
    state_of_canonical_mutation__partition<Output> _state;
};
template<typename Output>
struct after_canonical_mutation__mapping {
    Output& _out;
    state_of_canonical_mutation<Output> _state;
     ;
};
template<typename Output>
struct after_canonical_mutation__key {
    Output& _out;
    state_of_canonical_mutation<Output> _state;
};
template<typename Output>
struct after_canonical_mutation__schema_version {
    Output& _out;
    state_of_canonical_mutation<Output> _state;
};
template<typename Output>
struct after_canonical_mutation__table_id {
    Output& _out;
    state_of_canonical_mutation<Output> _state;
};
template<typename Output>
struct writer_of_canonical_mutation {
    Output& _out;
    state_of_canonical_mutation<Output> _state;
};
template<typename Output>
struct after_mutation__partition {
    Output& _out;
    state_of_mutation<Output> _state;
};
template<typename Output>
struct after_mutation__partition__rows {
    Output& _out;
    state_of_mutation__partition<Output> _state;
};
template<typename Output>
struct mutation__partition__rows {
    Output& _out;
    state_of_mutation__partition<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct after_mutation__partition__range_tombstones {
    Output& _out;
    state_of_mutation__partition<Output> _state;
};
template<typename Output>
struct mutation__partition__range_tombstones {
    Output& _out;
    state_of_mutation__partition<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct after_mutation__partition__static_row {
    Output& _out;
    state_of_mutation__partition<Output> _state;
};
template<typename Output>
struct after_mutation__partition__static_row__columns {
    Output& _out;
    state_of_mutation__partition__static_row<Output> _state;
};
template<typename Output>
struct mutation__partition__static_row__columns {
    Output& _out;
    state_of_mutation__partition__static_row<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct mutation__partition__static_row {
    Output& _out;
    state_of_mutation__partition__static_row<Output> _state;
};
template<typename Output>
struct after_mutation__partition__tomb {
    Output& _out;
    state_of_mutation__partition<Output> _state;
     ;
};
template<typename Output>
struct after_mutation__partition__tomb__deletion_time {
    Output& _out;
    state_of_mutation__partition__tomb<Output> _state;
};
template<typename Output>
struct after_mutation__partition__tomb__timestamp {
    Output& _out;
    state_of_mutation__partition__tomb<Output> _state;
};
template<typename Output>
struct mutation__partition__tomb {
    Output& _out;
    state_of_mutation__partition__tomb<Output> _state;
};
template<typename Output>
struct mutation__partition {
    Output& _out;
    state_of_mutation__partition<Output> _state;
};
template<typename Output>
struct after_mutation__key {
    Output& _out;
    state_of_mutation<Output> _state;
     ;
};
template<typename Output>
struct after_mutation__schema_version {
    Output& _out;
    state_of_mutation<Output> _state;
};
template<typename Output>
struct after_mutation__table_id {
    Output& _out;
    state_of_mutation<Output> _state;
};
template<typename Output>
struct writer_of_mutation {
    Output& _out;
    state_of_mutation<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment {
    Output& _out;
    state_of_mutation_fragment<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__shadowable_deleted_at {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__shadowable_deleted_at__deletion_time {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__shadowable_deleted_at<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__shadowable_deleted_at__timestamp {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__shadowable_deleted_at<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row__row__shadowable_deleted_at {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__shadowable_deleted_at<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__cells {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__cells__columns {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__cells<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row__row__cells__columns {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__cells<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row__row__cells {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__cells<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__deleted_at {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row<Output> _state;
     ;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__deleted_at__deletion_time {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__deleted_at<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__deleted_at__timestamp {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__deleted_at<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row__row__deleted_at {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__deleted_at<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__marker {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__marker__live_marker__created_at {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__live_marker<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row__row__marker__live_marker {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__live_marker<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker__expiry {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker__ttl {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker__lm {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker__lm__created_at {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker__lm<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row__row__marker__expiring_marker__lm {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker__lm<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row__row__marker__expiring_marker {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__expiring_marker<Output> _state;
     ;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__marker__dead_marker__tomb {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__dead_marker<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__marker__dead_marker__tomb__deletion_time {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__dead_marker__tomb<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__marker__dead_marker__tomb__timestamp {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__dead_marker__tomb<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row__row__marker__dead_marker__tomb {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__dead_marker__tomb<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row__row__marker__dead_marker {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__dead_marker<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row__row__marker__no_marker {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker__no_marker<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__clustering_row__row__key {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row__marker<Output> _state;
     ;
     ;
     ;
     ;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row__row {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row__row<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__clustering_row {
    Output& _out;
    state_of_mutation_fragment__fragment__clustering_row<Output> _state;
     ;
};
template<typename Output>
struct after_mutation_fragment__fragment__static_row__cells {
    Output& _out;
    state_of_mutation_fragment__fragment__static_row<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__static_row__cells__columns {
    Output& _out;
    state_of_mutation_fragment__fragment__static_row__cells<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__static_row__cells__columns {
    Output& _out;
    state_of_mutation_fragment__fragment__static_row__cells<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
};
template<typename Output>
struct mutation_fragment__fragment__static_row__cells {
    Output& _out;
    state_of_mutation_fragment__fragment__static_row__cells<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__static_row {
    Output& _out;
    state_of_mutation_fragment__fragment__static_row<Output> _state;
     ;
};
template<typename Output>
struct after_mutation_fragment__fragment__range_tombstone__end_kind {
    Output& _out;
    state_of_mutation_fragment__fragment__range_tombstone<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__range_tombstone__end {
    Output& _out;
    state_of_mutation_fragment__fragment__range_tombstone<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__range_tombstone__start_kind {
    Output& _out;
    state_of_mutation_fragment__fragment__range_tombstone<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__range_tombstone__tomb {
    Output& _out;
    state_of_mutation_fragment__fragment__range_tombstone<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__range_tombstone__tomb__deletion_time {
    Output& _out;
    state_of_mutation_fragment__fragment__range_tombstone__tomb<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__range_tombstone__tomb__timestamp {
    Output& _out;
    state_of_mutation_fragment__fragment__range_tombstone__tomb<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__range_tombstone__tomb {
    Output& _out;
    state_of_mutation_fragment__fragment__range_tombstone__tomb<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__range_tombstone__start {
    Output& _out;
    state_of_mutation_fragment__fragment__range_tombstone<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__range_tombstone {
    Output& _out;
    state_of_mutation_fragment__fragment__range_tombstone<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__partition_start__partition_tombstone {
    Output& _out;
    state_of_mutation_fragment__fragment__partition_start<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__partition_start__partition_tombstone__deletion_time {
    Output& _out;
    state_of_mutation_fragment__fragment__partition_start__partition_tombstone<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__partition_start__partition_tombstone__timestamp {
    Output& _out;
    state_of_mutation_fragment__fragment__partition_start__partition_tombstone<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__partition_start__partition_tombstone {
    Output& _out;
    state_of_mutation_fragment__fragment__partition_start__partition_tombstone<Output> _state;
};
template<typename Output>
struct after_mutation_fragment__fragment__partition_start__key {
    Output& _out;
    state_of_mutation_fragment__fragment__partition_start<Output> _state;
};
template<typename Output>
struct mutation_fragment__fragment__partition_start {
    Output& _out;
    state_of_mutation_fragment__fragment__partition_start<Output> _state;
};
template<typename Output>
struct writer_of_mutation_fragment {
    Output& _out;
    state_of_mutation_fragment<Output> _state;
     ;
     ;
     ;
};
} // ser
namespace ser {
class mutation_partition_view;
}
class partition_builder;
class converting_mutation_partition_applier;
template<typename T>
concept MutationViewVisitor = requires (T& visitor, tombstone t, atomic_cell ac,
                                             collection_mutation_view cmv, range_tombstone rt,
                                             position_in_partition_view pipv, row_tombstone row_tomb,
                                             row_marker rm) {
    visitor.accept_partition_tombstone(t);
    visitor.accept_static_cell(column_id(), std::move(ac));
    visitor.accept_static_cell(column_id(), cmv);
    visitor.accept_row_tombstone(rt);
    visitor.accept_row(pipv, row_tomb, rm,
            is_dummy::no, is_continuous::yes);
    visitor.accept_row_cell(column_id(), std::move(ac));
    visitor.accept_row_cell(column_id(), cmv);
};
class mutation_partition_view_virtual_visitor {
public:
    virtual ~mutation_partition_view_virtual_visitor();
    virtual void accept_partition_tombstone(tombstone t) = 0;
    virtual void accept_static_cell(column_id, atomic_cell ac) = 0;
    virtual void accept_static_cell(column_id, collection_mutation_view cmv) = 0;
    virtual stop_iteration accept_row_tombstone(range_tombstone rt) = 0;
    virtual stop_iteration accept_row(position_in_partition_view pipv, row_tombstone rt, row_marker rm, is_dummy, is_continuous) = 0;
    virtual void accept_row_cell(column_id, atomic_cell ac) = 0;
    virtual void accept_row_cell(column_id, collection_mutation_view cmv) = 0;
};
// View on serialized mutation partition. See mutation_partition_serializer.
class mutation_partition_view {
    utils::input_stream _in;
private:
    mutation_partition_view(utils::input_stream v)
        : _in(v)
    { }
    ;
    ;
    struct accept_ordered_cookie {
        bool accepted_partition_tombstone = false;
        bool accepted_static_row = false;
        struct rts_crs_iterators {
            ser::vector_deserializer<ser::range_tombstone_view>::const_iterator rts_begin;
            ser::vector_deserializer<ser::range_tombstone_view>::const_iterator rts_end;
            ser::vector_deserializer<ser::deletable_row_view>::const_iterator crs_begin;
            ser::vector_deserializer<ser::deletable_row_view>::const_iterator crs_end;
        };
        std::optional<rts_crs_iterators> iterators;
    };
    struct accept_ordered_result {
        stop_iteration stop = stop_iteration::no;
        accept_ordered_cookie cookie;
    };
    ;
public:
    
    
};
class mutation;
class flat_mutation_reader_v2;
namespace ser {
class mutation_view;
}
template<typename Result>
struct frozen_mutation_consume_result {
    stop_iteration stop;
    Result result;
};
template<>
struct frozen_mutation_consume_result<void> {
    stop_iteration stop;
};
// mutation_partition_view visitor which consumes a frozen_mutation.
template<FlattenedConsumerV2 Consumer>
class frozen_mutation_consumer_adaptor final : public mutation_partition_view_virtual_visitor {
private:
    const schema& _schema;
    std::optional<dht::decorated_key> _dk;
    lazy_row _static_row;
    range_tombstone_change_generator _rt_gen;
    alloc_strategy_unique_ptr<rows_entry> _current_row_entry;
    deletable_row* _current_row = nullptr;
    Consumer& _consumer;
    stop_iteration _stop_consuming = stop_iteration::no;
    stop_iteration flush_rows_and_tombstones(position_in_partition_view pos) ;
public:
    frozen_mutation_consumer_adaptor(schema_ptr s, Consumer& consumer)
        : _schema(*s)
        , _rt_gen(_schema)
        , _consumer(consumer)
    {
    }
    Consumer& consumer() ;
    void on_new_partition(const partition_key& key) ;
    virtual void accept_partition_tombstone(tombstone t) override ;
    virtual void accept_static_cell(column_id id, atomic_cell cell) override ;
    virtual void accept_static_cell(column_id id, collection_mutation_view collection) override ;
    virtual stop_iteration accept_row_tombstone(range_tombstone rt) override ;
    virtual stop_iteration accept_row(position_in_partition_view key, row_tombstone deleted_at, row_marker rm, is_dummy dummy, is_continuous continuous) override ;
    void accept_row_cell(column_id id, atomic_cell cell) override ;
    virtual void accept_row_cell(column_id id, collection_mutation_view collection) override ;
    auto on_end_of_partition() ;
};
// Immutable, compact form of mutation.
//
// This form is primarily destined to be sent over the network channel.
// Regular mutation can't be deserialized because its complex data structures
// need schema reference at the time object is constructed. We can't lookup
// schema before we deserialize column family ID. Another problem is that even
// if we had the ID somehow, low level RPC layer doesn't know how to lookup
// the schema. Data can be wrapped in frozen_mutation without schema
// information, the schema is only needed to access some of the fields.
//
class frozen_mutation final {
private:
    bytes_ostream _bytes;
    partition_key _pk;
private:
    partition_key deserialize_key() const;
    ser::mutation_view mutation_view() const;
public:
     // FIXME: Should replace column_family_id()
    // The supplied schema must be of the same version as the schema of
    // the mutation which was used to create this instance.
    // throws schema_mismatch_error otherwise.
    future<mutation> unfreeze_gently(schema_ptr s) const;
    // Automatically upgrades the stored mutation to the supplied schema with custom column mapping.
    mutation unfreeze_upgrading(schema_ptr schema, const column_mapping& cm) const;
    // Consumes the frozen mutation's content.
    //
    // The consume operation is stoppable:
    // * To stop, return stop_iteration::yes from one of the consume() methods;
    // * The consume will now stop and return;
    //
    // Note that `consume_end_of_partition()` and `consume_end_of_stream()`
    // will be called each time the consume is stopping, regardless of whether
    // you are pausing or the consumption is ending for good.
    ;
    ;
    // Consumes the frozen mutation's content.
    //
    // The consume operation is stoppable:
    // * To stop, return stop_iteration::yes from one of the consume() methods;
    // * The consume will now stop and return;
    //
    // Note that `consume_end_of_partition()` and `consume_end_of_stream()`
    // will be called each time the consume is stopping, regardless of whether
    // you are pausing or the consumption is ending for good.
    template<FlattenedConsumerV2 Consumer>
    auto consume_gently(schema_ptr s, Consumer& consumer) const -> future<frozen_mutation_consume_result<decltype(consumer.consume_end_of_stream())>>;
    template<FlattenedConsumerV2 Consumer>
    auto consume_gently(schema_ptr s, frozen_mutation_consumer_adaptor<Consumer>& adaptor) const -> future<frozen_mutation_consume_result<decltype(adaptor.consumer().consume_end_of_stream())>>;
    
    struct printer {
        const frozen_mutation& self;
        schema_ptr schema;
        
    };
    // Same requirements about the schema as unfreeze().
};
struct frozen_mutation_and_schema {
    frozen_mutation fm;
    schema_ptr s;
};
// Can receive streamed_mutation in reversed order.
class streamed_mutation_freezer {
    const schema& _schema;
    partition_key _key;
    bool _reversed;
    tombstone _partition_tombstone;
    std::optional<static_row> _sr;
    std::deque<clustering_row> _crs;
    range_tombstone_list _rts;
public:
    streamed_mutation_freezer(const schema& s, const partition_key& key, bool reversed = false)
        : _schema(s), _key(key), _reversed(reversed), _rts(s) { }
    stop_iteration consume(tombstone pt);
    stop_iteration consume(static_row&& sr);
    stop_iteration consume(clustering_row&& cr);
    stop_iteration consume(range_tombstone&& rt);
    frozen_mutation consume_end_of_stream();
};
static constexpr size_t default_frozen_fragment_size = 128 * 1024;
using frozen_mutation_consumer_fn = std::function<future<stop_iteration>(frozen_mutation, bool)>;

class reader_permit;
class frozen_mutation_fragment {
    bytes_ostream _bytes;
public:
    
    const bytes_ostream& representation() const ;
};
class frozen_mutation_and_schema;
namespace replica {
struct cf_stats;
}
namespace db {
namespace view {
class stats;
// Part of the view description which depends on the base schema version.
//
// This structure may change even though the view schema doesn't change, so
// it needs to live outside view_ptr.
struct base_dependent_view_info {
private:
    schema_ptr _base_schema;
    // Id of a regular base table column included in the view's PK, if any.
    // Scylla views only allow one such column, alternator can have up to two.
    std::vector<column_id> _base_regular_columns_in_view_pk;
    std::vector<column_id> _base_static_columns_in_view_pk;
    // For tracing purposes, if the view is out of sync with its base table
    // and there exists a column which is not in base, its name is stored
    // and added to debug messages.
    std::optional<bytes> _column_missing_in_base = {};
public:
    const schema_ptr& base_schema() const;
    // Indicates if the view hase pk columns which are not part of the base
    // pk, it seems that !base_non_pk_columns_in_view_pk.empty() is the same,
    // but actually there are cases where we can compute this boolean without
    // succeeding to reliably build the former.
    const bool has_base_non_pk_columns_in_view_pk;
    // If base_non_pk_columns_in_view_pk couldn't reliably be built, this base
    // info can't be used for computing view updates, only for reading the materialized
    // view.
    const bool use_only_for_reads;
    // A constructor for a base info that can facilitate reads and writes from the materialized view.
    
    // A constructor for a base info that can facilitate only reads from the materialized view.
    
};
// Immutable snapshot of view's base-schema-dependent part.
using base_info_ptr = lw_shared_ptr<const base_dependent_view_info>;
// Snapshot of the view schema and its base-schema-dependent part.
struct view_and_base {
    view_ptr view;
    base_info_ptr base;
};
// An immutable representation of a clustering or static row of the base table.
struct clustering_or_static_row {
private:
    std::optional<clustering_key_prefix> _key;
    deletable_row _row;
public:
    const std::optional<clustering_key_prefix>& key() const ;
    row_tombstone tomb() const ;
    const row_marker& marker() const ;
    const row& cells() const ;
};
struct view_key_and_action {
    struct no_action {};
    struct shadowable_tombstone_tag {
        api::timestamp_type ts;
    };
    using action = std::variant<no_action, row_marker, shadowable_tombstone_tag>;
    bytes _key_bytes;
    action _action = no_action{};
};
class view_updates final {
    view_ptr _view;
    const view_info& _view_info;
    schema_ptr _base;
    base_info_ptr _base_info;
    std::unordered_map<partition_key, mutation_partition, partition_key::hashing, partition_key::equality> _updates;
    mutable size_t _op_count = 0;
    const bool _backing_secondary_index;
public:
    void generate_update(data_dictionary::database db, const partition_key& base_key, const clustering_or_static_row& update, const std::optional<clustering_or_static_row>& existing, gc_clock::time_point now);
private:
    struct view_row_entry {
        deletable_row* _row;
        view_key_and_action::action _action;
    };
};
class view_update_builder {
    data_dictionary::database _db;
    const replica::table& _base;
    schema_ptr _schema; // The base schema
    std::vector<view_updates> _view_updates;
    flat_mutation_reader_v2 _updates;
    flat_mutation_reader_v2_opt _existings;
    tombstone _update_partition_tombstone;
    tombstone _update_current_tombstone;
    tombstone _existing_partition_tombstone;
    tombstone _existing_current_tombstone;
    mutation_fragment_v2_opt _update;
    mutation_fragment_v2_opt _existing;
    gc_clock::time_point _now;
    partition_key _key = partition_key::make_empty();
public:
    // build_some() works on batches of 100 (max_rows_for_view_updates)
    // updated rows, but can_skip_view_updates() can decide that some of
    // these rows do not effect the view, and as a result build_some() can
    // fewer than 100 rows - in extreme cases even zero (see issue #12297).
    // So we can't use an empty returned vector to signify that the view
    // update building is done - and we wrap the return value in an
    // std::optional, which is disengaged when the iteration is done.
    future<std::optional<utils::chunked_vector<frozen_mutation_and_schema>>> build_some();
private:
};
std::vector<view_and_base> with_base_info_snapshot(std::vector<view_ptr>);
}
}
namespace locator {
/// Generates split points which divide the ring into ranges which share the same replica set.
///
/// Initially the ring space the splitter works with is set to the whole ring.
/// The space can be changed using reset().
class token_range_splitter {
public:
    /// Resets the splitter to work with the ring range [pos, +inf).
    /// Each token t returned by next_token() means that keys in the range:
    ///
    ///   [prev_pos, dht::ring_position_view::ending_at(t))
    ///
    /// share the same replica set.
    ///
    /// If this is the first call to next_token() after construction or reset() then prev_pos is the
    /// beginning of the ring space. Otherwise, it is dht::ring_position_view::ending_at(prev_t)
    /// where prev_t is the token returned by the previous call to next_token().
    /// If std::nullopt is returned it means that the ring space was exhausted.
};
}
using namespace seastar;
namespace locator {
using inet_address = gms::inet_address;
// Endpoint Data Center and Rack names
struct endpoint_dc_rack {
    sstring dc;
    sstring rack;
    static thread_local const endpoint_dc_rack default_location;
};
using dc_rack_fn = seastar::noncopyable_function<endpoint_dc_rack(inet_address)>;
} // namespace locator
using namespace seastar;
namespace locator {
class topology;
}
namespace std {
}
namespace locator {
class node;
using node_holder = std::unique_ptr<node>;
class node {
public:
    using this_node = bool_class<struct this_node_tag>;
    using idx_type = int;
    enum class state {
        none = 0,
        joining,    // while bootstrapping, replacing
        normal,
        leaving,    // while decommissioned, removed, replaced
        left        // after decommissioned, removed, replaced
    };
private:
    const locator::topology* _topology;
    locator::host_id _host_id;
    inet_address _endpoint;
    endpoint_dc_rack _dc_rack;
    state _state;
    // Is this node the `localhost` instance
    this_node _is_this_node;
    idx_type _idx = -1;
public:
    const locator::topology* topology() const noexcept ;
    const locator::host_id& host_id() const noexcept ;
    const inet_address& endpoint() const noexcept ;
    const endpoint_dc_rack& dc_rack() const noexcept ;
    // Is this "localhost"?
    this_node is_this_node() const noexcept ;
    // idx < 0 means "unassigned"
    
    
    static std::string to_string(state);
private:
    static node_holder make(const locator::topology* topology, locator::host_id id, inet_address endpoint, endpoint_dc_rack dc_rack, state state, node::this_node is_this_node = this_node::no, idx_type idx = -1);
    
    
    friend class topology;
};
class topology {
public:
    struct config {
        host_id this_host_id;
        inet_address this_endpoint;
        endpoint_dc_rack local_dc_rack;
        bool disable_proximity_sorting = false;
    };
public:
    // Adds a node with given host_id, endpoint, and DC/rack.
    // Optionally updates node's current host_id, endpoint, or DC/rack.
    // Note: the host_id may be updated from null to non-null after a new node gets a new, random host_id,
    // or a peer node host_id may be updated when the node is replaced with another node using the same ip address.
    
    // Removes a node using its host_id
    // Returns true iff the node was found and removed.
    
    // Looks up a node by its host_id.
    // Returns a pointer to the node if found, or nullptr otherwise.
    const node* find_node(host_id id) const noexcept;
    // Looks up a node by its inet_address.
    // Returns a pointer to the node if found, or nullptr otherwise.
    const node* find_node(const inet_address& ep) const noexcept;
    // Finds a node by its index
    // Returns a pointer to the node if found, or nullptr otherwise.
    // Returns true if a node with given host_id is found
    // Legacy entry point from token_metadata::update_topology
    const std::unordered_map<sstring,
                           std::unordered_set<inet_address>>&
    get_datacenter_endpoints() const {
        return _dc_endpoints;
    }
    const std::unordered_map<sstring,
                       std::unordered_map<sstring,
                                          std::unordered_set<inet_address>>>&
    get_datacenter_racks() const {
        return _dc_racks;
    }
    const std::unordered_set<sstring>& get_datacenters() const noexcept {
        return _datacenters;
    }
    // Get dc/rack location of this node
    const endpoint_dc_rack& get_location() const noexcept {
        return _this_node ? _this_node->dc_rack() : _cfg.local_dc_rack;
    }
    // Get dc/rack location of a node identified by host_id
    // The specified node must exist.
    const endpoint_dc_rack& get_location(host_id id) const {
        return find_node(id)->dc_rack();
    }
    // Get dc/rack location of a node identified by endpoint
    // The specified node must exist.
    const endpoint_dc_rack& get_location(const inet_address& ep) const;
    // Get datacenter of this node
    // Get datacenter of a node identified by host_id
    // The specified node must exist.
    // Get datacenter of a node identified by endpoint
    // The specified node must exist.
    // Get rack of this node
    // Get rack of a node identified by host_id
    // The specified node must exist.
    // Get rack of a node identified by endpoint
    // The specified node must exist.
    ;
     ;
    void sort_by_proximity(inet_address address, inet_address_vector_replica_set& addresses) const;
    void for_each_node(std::function<void(const node*)> func) const;
private:
    unsigned _shard;
    config _cfg;
    const node* _this_node = nullptr;
    std::vector<node_holder> _nodes;
    std::unordered_map<host_id, const node*> _nodes_by_host_id;
    std::unordered_map<inet_address, const node*> _nodes_by_endpoint;
    std::unordered_map<sstring, std::unordered_set<const node*>> _dc_nodes;
    std::unordered_map<sstring, std::unordered_map<sstring, std::unordered_set<const node*>>> _dc_rack_nodes;
    std::unordered_map<sstring,
                       std::unordered_set<inet_address>>
        _dc_endpoints;
    std::unordered_map<sstring,
                       std::unordered_map<sstring,
                                          std::unordered_set<inet_address>>>
        _dc_racks;
    bool _sort_by_proximity = true;
    // pre-calculated
    std::unordered_set<sstring> _datacenters;
    ;
    friend class token_metadata_impl;
public:
};
} // namespace locator
namespace std {
} // namespace std
template <>
struct fmt::formatter<locator::node> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const locator::node& node, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}/{}", node.host_id(), node.endpoint());
    }
};
template <>
struct fmt::formatter<locator::node::state> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const locator::node::state& state, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", locator::node::to_string(state));
    }
};
namespace cdc {
struct generation_id_v1 {
    db_clock::time_point ts;
};
struct generation_id_v2 {
    db_clock::time_point ts;
    utils::UUID id;
};
using generation_id = std::variant<generation_id_v1, generation_id_v2>;
} // namespace cdc
template <>
struct fmt::formatter<cdc::generation_id_v1> {
    constexpr auto parse(format_parse_context& ctx) { return ctx.begin(); }
    template <typename FormatContext>
    auto format(const cdc::generation_id_v1& gen_id, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", gen_id.ts);
    }
};
template <>
struct fmt::formatter<cdc::generation_id_v2> {
    constexpr auto parse(format_parse_context& ctx) { return ctx.begin(); }
    template <typename FormatContext>
    auto format(const cdc::generation_id_v2& gen_id, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "({}, {})", gen_id.ts, gen_id.id);
    }
};
template <>
struct fmt::formatter<cdc::generation_id> {
    constexpr auto parse(format_parse_context& ctx) { return ctx.begin(); }
    template <typename FormatContext>
    auto format(const cdc::generation_id& gen_id, FormatContext& ctx) const {
        return std::visit([&ctx] (auto& id) {
            return fmt::format_to(ctx.out(), "{}", id);
        }, gen_id);
    }
};
namespace utils {
// Note: do not use directly, use utils::tagged_integer instead.
// The reason this double-tagged template exist
// is to distinguish between utils::tagged_integer
// and raft::internal::tagged_uint64 that have incompatible
// idl types and therefore must not be convertible to each other.
template <typename Final, typename Tag, std::integral ValueType>
class tagged_tagged_integer {
public:
    using value_type = ValueType;
private:
    value_type _value;
public:
    explicit tagged_tagged_integer(value_type v) noexcept : _value(v) {}
    operator value_type() const noexcept { return _value; }
    explicit operator bool() const { return _value != 0; }
    auto operator<=>(const tagged_tagged_integer& o) const = default;
    tagged_tagged_integer& operator-=(const tagged_tagged_integer& o) ;
};
template <typename Tag, std::integral ValueType>
using tagged_integer = tagged_tagged_integer<struct final, Tag, ValueType>;
} // namespace utils
namespace std {
template <typename Final, typename Tag, std::integral ValueType>
struct hash<utils::tagged_tagged_integer<Final, Tag, ValueType>> {
    size_t operator()(const utils::tagged_tagged_integer<Final, Tag, ValueType>& x) const noexcept {
        return hash<ValueType>{}(x.value());
    }
};
template <typename Final, typename Tag, std::integral ValueType>
[[maybe_unused]] ostream& operator<<(ostream& s, const utils::tagged_tagged_integer<Final, Tag, ValueType>& x) ;
template <typename Final, typename Tag, std::integral ValueType>
struct numeric_limits<utils::tagged_tagged_integer<Final, Tag, ValueType>> : public numeric_limits<ValueType> {
    using tagged_tagged_integer_t = utils::tagged_tagged_integer<Final, Tag, ValueType>;
    using value_limits = numeric_limits<ValueType>;
    static_assert(numeric_limits<ValueType>::is_specialized && numeric_limits<ValueType>::is_bounded);
    static constexpr tagged_tagged_integer_t min() ;
    
};
} // namespace std
namespace raft {
namespace internal {
template<typename Tag>
using tagged_id = utils::tagged_uuid<Tag>;
template<typename Tag>
using tagged_uint64 = utils::tagged_tagged_integer<struct non_final, Tag, uint64_t>;
} // end of namespace internal
} // end of namespace raft
namespace raft {
// Raft protocol state machine clock ticks at different speeds
// depending on the environment. A typical clock tick for
// a production system is 100ms, while a test system can
// tick it at the speed of the hardware clock.
//
// Every state machine has an own instance of logical clock,
// this enables tests when different state machines run at
// different clock speeds.
class logical_clock final {
public:
    using rep = int64_t;
    // There is no realistic period for a logical clock,
    // just use the smallest period possible.
    using period = std::chrono::nanoseconds::period;
    using duration = std::chrono::duration<rep, period>;
    using time_point = std::chrono::time_point<logical_clock, duration>;
    static constexpr bool is_steady = true;
    
    time_point now() const noexcept ;
    static constexpr time_point min() ;
private:
    time_point _now = min();
};
} // end of namespace raft
namespace std {
} // end of namespace std
#if defined(__clang_major__) && __clang_major__ <= 14
namespace std {
    using source_location = std::experimental::source_location;
}
#endif
namespace raft {
// Keeps user defined command. A user is responsible to serialize
// a state machine operation into it before passing to raft and
// deserialize in apply() before applying.
using command = bytes_ostream;
using command_cref = std::reference_wrapper<const command>;
extern seastar::logger logger;
// This is user provided id for a snapshot
using snapshot_id = internal::tagged_id<struct snapshot_id_tag>;
// Unique identifier of a server in a Raft group
using server_id = internal::tagged_id<struct server_id_tag>;
// Unique identifier of a Raft group
using group_id = raft::internal::tagged_id<struct group_id_tag>;
// This type represents the raft term
using term_t = raft::internal::tagged_uint64<struct term_tag>;
// This type represensts the index into the raft log
using index_t = raft::internal::tagged_uint64<struct index_tag>;
// Identifier for a read barrier request
using read_id = raft::internal::tagged_uint64<struct read_id_tag>;
// Opaque connection properties. May contain ip:port pair for instance.
// This value is disseminated between cluster member
// through regular log replication as part of a configuration
// log entry. Upon receiving it a server passes it down to
// RPC module through on_configuration_change() call where it is deserialized
// and used to obtain connection info for the node `id`. After a server
// is added to the RPC module RPC's send functions can be used to communicate
// with it using its `id`.
using server_info = bytes;
struct server_address {
    server_id id;
    server_info info;
};
struct config_member {
    server_address addr;
    bool can_vote;
};
struct server_address_hash {
    using is_transparent = void;
};
struct config_member_hash {
    using is_transparent = void;
};
using server_address_set = std::unordered_set<server_address, server_address_hash, std::equal_to<>>;
using config_member_set = std::unordered_set<config_member, config_member_hash, std::equal_to<>>;
// A configuration change decomposed to joining and leaving
// servers. Helps validate the configuration and update RPC.
struct configuration_diff {
    config_member_set joining, leaving;
};
struct configuration {
    // Contains the current configuration. When configuration
    // change is in progress, contains the new configuration.
    config_member_set current;
    // Used during the transitioning period of configuration
    // changes.
    config_member_set previous;
    // Return true if the previous configuration is still
    // in use
    // Count the number of voters in a configuration
    // Check if transitioning to a proposed configuration is safe.
    // Compute a diff between a proposed configuration and the current one.
    // True if the current or previous configuration contains
    // this server.
    // Same as contains() but true only if the member can vote.
    // Enter a joint configuration given a new set of servers.
    // Transition from C_old + C_new to C_new.
};
struct log_entry {
    // Dummy entry is used when a leader needs to commit an entry
    // (after leadership change for instance) but there is nothing
    // else to commit.
    struct dummy {};
    term_t term;
    index_t idx;
    std::variant<command, configuration, dummy> data;
};
using log_entry_ptr = seastar::lw_shared_ptr<const log_entry>;
struct error : public std::runtime_error {
    using std::runtime_error::runtime_error;
};
struct not_a_leader : public error {
    server_id leader;
};
struct not_a_member : public error {
};
struct dropped_entry : public error {
};
struct commit_status_unknown : public error {
};
struct stopped_error : public error {
};
struct conf_change_in_progress : public error {
};
struct config_error : public error {
    using error::error;
};
struct timeout_error : public error {
    using error::error;
};
struct state_machine_error: public error {
};
// Should be thrown by the rpc implementation to signal that the connection to the peer has been lost.
// It's unspecified if any actions caused by rpc were actually performed on the target node.
struct transport_error: public error {
    using error::error;
};
struct command_is_too_big_error: public error {
    size_t command_size;
    size_t limit;
};
struct no_other_voting_member : public error {
};
struct request_aborted : public error {
};
struct snapshot_descriptor {
    // Index and term of last entry in the snapshot
    index_t idx = index_t(0);
    term_t term = term_t(0);
    // The committed configuration in the snapshot
    configuration config{};
    // Id of the snapshot.
    snapshot_id id;
};
struct append_request {
    // The leader's term.
    term_t current_term;
    // Index of the log entry immediately preceding new ones
    index_t prev_log_idx;
    // Term of prev_log_idx entry.
    term_t prev_log_term;
    // The leader's commit_idx.
    index_t leader_commit_idx;
    // Log entries to store (empty vector for heartbeat; may send more
    // than one entry for efficiency).
    std::vector<log_entry_ptr> entries;
};
struct append_reply {
    struct rejected {
        // Index of non matching entry that caused the request
        // to be rejected.
        index_t non_matching_idx;
        // Last index in the follower's log, can be used to find next
        // matching index more efficiently.
        index_t last_idx;
    };
    struct accepted {
        // Last entry that was appended (may be smaller than max log index
        // in case follower's log is longer and appended entries match).
        index_t last_new_idx;
    };
    // Current term, for leader to update itself.
    term_t current_term;
    // Contains an index of the last committed entry on the follower
    // It is used by a leader to know if a follower is behind and issuing
    // empty append entry with updates commit_idx if it is
    // Regular RAFT handles this by always sending enoty append requests
    // as a heartbeat.
    index_t commit_idx;
    std::variant<rejected, accepted> result;
};
struct vote_request {
    // The candidate’s term.
    term_t current_term;
    // The index of the candidate's last log entry.
    index_t last_log_idx;
    // The term of the candidate's last log entry.
    term_t last_log_term;
    // True if this is prevote request
    bool is_prevote;
    // If the flag is set the request will not be ignored even
    // if there is an active leader. Used during leadership transfer.
    bool force;
};
struct vote_reply {
    // Current term, for the candidate to update itself.
    term_t current_term;
    // True means the candidate received a vote.
    bool vote_granted;
    // True if it is a reply to prevote request
    bool is_prevote;
};
struct install_snapshot {
    // Current term on a leader
    term_t current_term;
    // A snapshot to install
    snapshot_descriptor snp;
};
struct snapshot_reply {
    // Follower current term
    term_t current_term;
    // True if the snapshot was applied, false otherwise.
    bool success;
};
// 3.10 section from PhD Leadership transfer extension
struct timeout_now {
    // Current term on a leader
    term_t current_term;
};
struct read_quorum {
    // The leader's term.
    term_t current_term;
    // The leader's commit_idx. Has the same semantics
    // as in append_entries.
    index_t leader_commit_idx;
    // The id of the read barrier. Only valid within this term.
    read_id id;
};
struct read_quorum_reply {
    // The leader's term, as sent in the read_quorum request.
    // read_id is only valid (and unique) within a given term.
    term_t current_term;
    // Piggy-back follower's commit_idx, for the same purposes
    // as in append_reply::commit_idx
    index_t commit_idx;
    // Copy of the id from a read_quorum request
    read_id id;
};
struct entry_id {
    // Added entry term
    term_t term;
    // Added entry log index
    index_t idx;
};
// The execute_add_entry/execute_modify_config methods can return this error to signal
// that the request should be retried.
// The exception is only used internally for entry/config forwarding and should not be leaked to a user.
struct transient_error: public error {
    // for IDL serialization
    // A leader that the client should use for retrying.
    // Could be empty, if the new leader is not known.
    // Client should wait for a new leader in this case.
    server_id leader;
};
// Response to add_entry or modify_config RPC.
// Carries either entry id (the entry is not committed yet),
// transient_error (the entry is not added to Raft log), or, for
// modify_config, commit_status_unknown (commit status is
// unknown).
using add_entry_reply = std::variant<entry_id, transient_error, commit_status_unknown, not_a_member>;
// std::monostate {} if the leader cannot execute the barrier because
// it did not commit any entries yet
// raft::not_a_leader if the node is not a leader
// index_t index that is safe to read without breaking linearizability
using read_barrier_reply = std::variant<std::monostate, index_t, raft::not_a_leader>;
using rpc_message = std::variant<append_request,
      append_reply,
      vote_request,
      vote_reply,
      install_snapshot,
      snapshot_reply,
      timeout_now,
      read_quorum,
      read_quorum_reply>;
// we need something that can be truncated from both sides.
// std::deque move constructor is not nothrow hence cannot be used
// also, boost::deque deallocates blocks when items are removed,
// we don't want to hold on to memory we don't use.
using log_entries = boost::container::deque<log_entry_ptr>;
// 3.4 Leader election
// If a follower receives no communication over a period of
// time called the election timeout, then it assumes there is
// no viable leader and begins an election to choose a new
// leader.
static constexpr logical_clock::duration ELECTION_TIMEOUT = logical_clock::duration{10};
// rpc, persistence and state_machine classes will have to be implemented by the
// raft user to provide network, persistency and busyness logic support
// repectively.
class rpc;
class persistence;
// Any of the functions may return an error, but it will kill the
// raft instance that uses it. Depending on what state the failure
// leaves the state is the raft instance will either have to be recreated
// with the same state machine and rejoined the cluster with the same server_id
// or it new raft instance will have to be created with empty state machine and
// it will have to rejoin to the cluster with different server_id through
// configuration change.
class state_machine {
public:
    // This is called after entries are committed (replicated to
    // at least quorum of servers). If a provided vector contains
    // more than one entry all of them will be committed simultaneously.
    // Will be eventually called on all replicas, for all committed commands.
    // Raft owns the data since it may be still replicating.
    // Raft will not call another apply until the retuned future
    // will not become ready.
    // The function suppose to take a snapshot of a state machine
    // To be called during log compaction or when a leader brings
    // a lagging follower up-to-date
    // The function drops a snapshot with a provided id
    // reload state machine from a snapshot id
    // To be used by a restarting server or by a follower that
    // catches up to a leader
    // stops the state machine instance by aborting the work
    // that can be aborted and waiting for all the rest to complete
    // any unfinished apply/snapshot operation may return an error after
    // this function is called
};
class rpc_server;
// It is safe for for rpc implementation to drop any message.
// Error returned by send function will be ignored. All send_()
// functions can be called concurrently, returned future should be
// waited only for back pressure purposes (unless specified otherwise in
// the function's comment). Values passed by reference may be freed as soon
// as function returns.
class rpc {
protected:
    // Pointer to Raft server. Needed for passing RPC messages.
    rpc_server* _client = nullptr;
public:
    // Send a snapshot snap to a server server_id.
    //
    // Unlike other RPC, this is a synchronous call:
    //
    // A returned future is resolved when snapshot is sent and
    // successfully applied by a receiver. Will be waited to
    // know if a snapshot transfer succeeded.
    // Send provided append_request to the supplied server, does
    // not wait for reply. The returned future resolves when
    // message is sent. It does not mean it was received.
    // Send a reply to an append_request.
    // Send a vote request.
    // Sends a reply to a vote request.
    // Send a request to start leader election.
    // Send a read barrier request.
    // Send a reply to read barrier request.
    // Forward a read barrier request to the leader.
    // Should throw a raft::transport_error if the target host is unreachable.
    // In this case, the call will be retried after some time,
    // possibly with a different server_id if the leader has changed by then.
    // Two-way RPC for adding an entry on the leader
    // @param id the leader
    // @param cmd raft::command to be added to the leader's log
    // @retval either term and index of the committed entry or
    // not_a_leader exception.
    // Send a configuration change request to the leader. Block until the
    // leader replies.
    // Should throw a raft::transport_error if the target host is unreachable.
    // When a configuration is changed this function is called with the
    // info about the changes. It is also called when a new server
    // starts and its configuration is loaded from raft storage.
    //
    // In fact, today we always call this function first, just
    // with the added and only then with the removed servers, to
    // simplify RPC's job of delivering a batch of messages
    // addressing both  added and removed servers. Passing the
    // added servers first, then passing a batch, and then passing
    // the removed servers makes it easier for RPC to deliver all
    // messages in the batch.
    // Stop the RPC instance by aborting the work that can be
    // aborted and waiting for all the rest to complete any
    // unfinished send operation may return an error after this
    // function is called.
    //
    // The implementation must ensure that `_client->apply_snapshot`, `_client->execute_add_entry`,
    // `_client->execute_modify_config` and `_client->execute_read_barrier` are not called
    // after `abort()` is called (even before `abort()` future resolves).
private:
    friend rpc_server;
};
// Each Raft server is a receiver of RPC messages.
// Defines the API specific to receiving RPC input.
class rpc_server {
public:
    ;
    // This function is called by append_entries RPC
    // This function is called by append_entries_reply RPC
    // This function is called to handle RequestVote RPC.
    // Handle response to RequestVote RPC
    // Apply incoming snapshot, future resolves when application is complete
    // Try to execute read barrier, future resolves when the barrier is completed or error happens
    // An endpoint on the leader to add an entry to the raft log,
    // as requested by a remote follower.
    // An endpoint on the leader to change configuration,
    // as requested by a remote follower.
    // If the future resolves successfully, a dummy entry was committed after the configuration change.
    // Update RPC implementation with this client as
    // the receiver of RPC input.
};
// This class represents persistent storage state for the internal fsm. If any of the
// function returns an error the Raft instance will be aborted.
class persistence {
public:
    // Persist given term and vote.
    // Can be called concurrently with other save-* functions in
    // the persistence and with itself but an implementation has to
    // make sure that the result is returned back in the calling order.
    // Load persisted term and vote.
    // Called during Raft server initialization only, is not run
    // in parallel with store.
    virtual future<std::pair<term_t, server_id>> load_term_and_vote() = 0;
    // Persist given commit index.
    // Cannot be called conccurrently with itself.
    // Persisting a commit index is optional.
    // Load persisted commit index.
    // Called during Raft server initialization only, is not run
    // in parallel with store. If no commit index was storred zero
    // will be returned.
    // Persist given snapshot and drop all but 'preserve_log_entries'
    // entries from the Raft log starting from the beginning.
    // This can overwrite a previously persisted snapshot.
    // Is called only after the previous invocation completes.
    // In other words, it's the caller's responsibility to serialize
    // calls to this function. Can be called in parallel with
    // store_log_entries() but snap.index should belong to an already
    // persisted entry.
    // Load a saved snapshot.
    // This only loads it into memory, but does not apply yet. To
    // apply call 'state_machine::load_snapshot(snapshot::id)'
    // Called during Raft server initialization only, should not
    // run in parallel with store.
    // Persist given log entries.
    // Can be called without waiting for previous call to resolve,
    // but internally all writes should be serialized into forming
    // one contiguous log that holds entries in order of the
    // function invocation.
    // Load saved Raft log. Called during Raft server
    // initialization only, should not run in parallel with store.
    // Truncate all entries with an index greater or equal than
    // the given index in the log and persist the truncation. Can be
    // called in parallel with store_log_entries() but internally
    // should be linearized vs store_log_entries():
    // store_log_entries() called after truncate_log() should wait
    // for truncation to complete internally before persisting its
    // entries.
    // Stop the persistence instance by aborting the work that can be
    // aborted and waiting for all the rest to complete. Any
    // unfinished store/load operation may return an error after
    // this function is called.
};
// To support many Raft groups per server, Seastar Raft
// extends original Raft with a shared failure detector.
// It is used instead of empty AppendEntries PRCs in idle
// cluster.
// This allows multiple Raft groups to share heartbeat traffic.
class failure_detector {
public:
    // Called by each server on each tick, which defaults to 10
    // per second. Should return true if the server is
    // alive. False results may impact liveness.
};
} // namespace raft
namespace service {
enum class node_state: uint8_t {
    none,                // the new node joined group0 but did not bootstraped yet (has no tokens and data to serve)
    bootstrapping,       // the node is currently in the process of streaming its part of the ring
    decommissioning,     // the node is being decomissioned and stream its data to nodes that took over
    removing,            // the node is being removed and its data is streamed to nodes that took over from still alive owners
    replacing,           // the node replaces another dead node in the cluster and it data is being streamed to it
    rebuilding,          // the node is being rebuild and is streaming data from other replicas
    normal,              // the node does not do any streaming and serves the slice of the ring that belongs to it
    left                 // the node left the cluster and group0
};
enum class topology_request: uint8_t {
    join,
    leave,
    remove,
    replace,
    rebuild
};
using request_param = std::variant<raft::server_id, sstring, uint32_t>;
struct ring_slice {
    std::unordered_set<dht::token> tokens;
    // When a new node joins the cluster, always a new CDC generation is created.
    // This is the UUID used to access the data of the CDC generation introduced
    // when the node owning this ring_slice joined (it's the partition key in CDC_GENERATIONS_V3 table).
    utils::UUID new_cdc_generation_data_uuid;
};
struct replica_state {
    node_state state;
    seastar::sstring datacenter;
    seastar::sstring rack;
    seastar::sstring release_version;
    std::optional<ring_slice> ring; // if engaged contain the set of tokens the node owns together with their state
    size_t shard_count;
    uint8_t ignore_msb;
};
struct topology {
    enum class transition_state: uint8_t {
        commit_cdc_generation,
        write_both_read_old,
        write_both_read_new,
    };
    std::optional<transition_state> tstate;
    // Nodes that are normal members of the ring
    std::unordered_map<raft::server_id, replica_state> normal_nodes;
    // Nodes that are left
    std::unordered_set<raft::server_id> left_nodes;
    // Nodes that are waiting to be joined by the topology coordinator
    std::unordered_map<raft::server_id, replica_state> new_nodes;
    // Nodes that are in the process to be added to the ring
    // Currently only at most one node at a time will be here
    std::unordered_map<raft::server_id, replica_state> transition_nodes;
    // Pending topology requests
    std::unordered_map<raft::server_id, topology_request> requests;
    // Holds parameters for a request per node and valid during entire
    // operation untill the node becomes normal
    std::unordered_map<raft::server_id, request_param> req_param;
    std::optional<cdc::generation_id_v2> current_cdc_generation_id;
    // Find only nodes in non 'left' state
    // Return true if node exists in any state including 'left' one
};
struct raft_topology_snapshot {
    // Mutations for the system.topology table.
    std::vector<canonical_mutation> topology_mutations;
    // Mutation for system.cdc_generations_v3, contains the current CDC generation data.
    std::optional<canonical_mutation> cdc_generation_mutation;
};
struct raft_topology_pull_params {
};
// State machine that is responsible for topology change
struct topology_state_machine {
    using topology_type = topology;
    topology_type _topology;
    condition_variable event;
};
// Raft leader uses this command to drive bootstrap process on other nodes
struct raft_topology_cmd {
      enum class command: uint8_t {
          barrier,         // request to wait for the latest topology
          stream_ranges,   // reqeust to stream data, return when streaming is
                           // done
          fence_old_reads  // wait for all reads started before to complete
      };
      command cmd;
};
// returned as a result of raft_bootstrap_cmd
struct raft_topology_cmd_result {
    enum class command_status: uint8_t {
        fail,
        success
    };
    command_status status = command_status::fail;
};
}
// forward declaration since replica/database.hh includes this file
namespace replica {
class keyspace;
}
namespace locator {
class abstract_replication_strategy;
using token = dht::token;
class token_metadata;
class tablet_metadata;
struct host_id_or_endpoint {
    host_id id;
    gms::inet_address endpoint;
    enum class param_type {
        host_id,
        endpoint,
        auto_detect
    };
    // Map the host_id to endpoint based on whichever of them is set,
    // using the token_metadata
};
class token_metadata_impl;
class token_metadata final {
    std::unique_ptr<token_metadata_impl> _impl;
public:
    struct config {
        topology::config topo_cfg;
    };
    using inet_address = gms::inet_address;
private:
    friend class token_metadata_ring_splitter;
    class tokens_iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = token;
        using difference_type = std::ptrdiff_t;
        using pointer = token*;
        using reference = token&;
    public:
        tokens_iterator(const token& start, const token_metadata_impl* token_metadata);
    private:
        std::vector<token>::const_iterator _cur_it;
        size_t _remaining = 0;
        const token_metadata_impl* _token_metadata = nullptr;
        friend class token_metadata_impl;
    };
public:
    token_metadata(token_metadata&&) noexcept; // Can't use "= default;" - hits some static_assert in unique_ptr
    token_metadata& operator=(token_metadata&&) noexcept;
    ~token_metadata();
    
    
    void set_tablets(tablet_metadata);
    // Update token->endpoint mappings for a given \c endpoint.
    // \c tokens are all the tokens that are now owned by \c endpoint.
    //
    // Note: the function is not exception safe!
    // It must be called only on a temporary copy of the token_metadata
    future<> update_normal_tokens(std::unordered_set<token> tokens, inet_address endpoint);
    const std::unordered_map<token, inet_address>& get_token_to_endpoint() const;
    const std::unordered_set<inet_address>& get_leaving_endpoints() const;
    const std::unordered_map<token, inet_address>& get_bootstrap_tokens() const;
    void update_topology(inet_address ep, endpoint_dc_rack dr, std::optional<node::state> opt_st = std::nullopt);
    
    
    topology& get_topology();
    const topology& get_topology() const;
    void debug_show() const;
    /// Return the unique host ID for an end-point or nullopt if not found.
    /// Parses the \c host_id_string either as a host uuid or as an ip address and returns the mapping.
    /// Throws std::invalid_argument on parse error or std::runtime_error if the host_id wasn't found.
    /// Returns host_id of the local node.
    // Checks if the node is part of the token ring. If yes, the node is one of
    // the nodes that owns the tokens and inside the set _normal_token_owners.
    // Is this node being replaced by another node
    // Is any node being replaced by another node
    future<token_metadata> clone_only_token_map() const noexcept;
    future<token_metadata> clone_after_all_left() const noexcept;
    future<> clear_gently() noexcept;
    static boost::icl::interval<token>::interval_type range_to_interval(range<dht::token> r);
    // returns empty vector if keyspace_name not found.
    // This function returns a list of nodes to which a read request should be directed.
    // Returns not null only during topology changes, if _topology_change_stage == read_new and
    // new set of replicas differs from the old one.
    // updates the current topology_transition_state of this instance,
    // this value is preserved in all clone functions,
    // by default it's not set
    friend class token_metadata_impl;
};
using token_metadata_ptr = lw_shared_ptr<const token_metadata>;
using mutable_token_metadata_ptr = lw_shared_ptr<token_metadata>;
using token_metadata_lock = semaphore_units<>;
using token_metadata_lock_func = noncopyable_function<future<token_metadata_lock>() noexcept>;
 ;
class shared_token_metadata {
    mutable_token_metadata_ptr _shared;
    token_metadata_lock_func _lock_func;
public:
    // used to construct the shared object as a sharded<> instance
    // lock_func returns semaphore_units<>
    // Token metadata changes are serialized
    // using the schema_tables merge_lock.
    //
    // Must be called on shard 0.
    // mutate_token_metadata_on_all_shards acquires the shared_token_metadata lock,
    // clones the token_metadata (using clone_async)
    // and calls an asynchronous functor on
    // the cloned copy of the token_metadata to mutate it.
    //
    // If the functor is successful, the mutated clone
    // is set back to to the shared_token_metadata,
    // otherwise, the clone is destroyed.
    // mutate_token_metadata_on_all_shards acquires the shared_token_metadata lock,
    // clones the token_metadata (using clone_async)
    // and calls an asynchronous functor on
    // the cloned copy of the token_metadata to mutate it.
    //
    // If the functor is successful, the mutated clone
    // is set back to to the shared_token_metadata on all shards,
    // otherwise, the clone is destroyed.
    //
    // Must be called on shard 0.
};
}
namespace streaming {
enum class stream_reason : uint8_t {
    unspecified,
    bootstrap,
    decommission,
    removenode,
    rebuild,
    repair,
    replace,
};
}
template <>
struct fmt::formatter<streaming::stream_reason> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const streaming::stream_reason& r, FormatContext& ctx) const {
        using enum streaming::stream_reason;
        switch (r) {
        case unspecified:
            return formatter<std::string_view>::format("unspecified", ctx);
        case bootstrap:
            return formatter<std::string_view>::format("bootstrap", ctx);
        case decommission:
            return formatter<std::string_view>::format("decommission", ctx);
        case removenode:
            return formatter<std::string_view>::format("removenode", ctx);
        case rebuild:
            return formatter<std::string_view>::format("rebuild", ctx);
        case repair:
            return formatter<std::string_view>::format("repair", ctx);
        case replace:
            return formatter<std::string_view>::format("replace", ctx);
        }
        std::abort();
    }
};
namespace streaming { class stream_manager; }
namespace gms { class gossiper; }
namespace db { class config; }
namespace dht {
using check_token_endpoint = bool_class<struct check_token_endpoint_tag>;
class boot_strapper {
    using inet_address = gms::inet_address;
    using token_metadata = locator::token_metadata;
    using token_metadata_ptr = locator::token_metadata_ptr;
    using token = dht::token;
    distributed<replica::database>& _db;
    sharded<streaming::stream_manager>& _stream_manager;
    abort_source& _abort_source;
    inet_address _address;
    locator::endpoint_dc_rack _dr;
    std::unordered_set<token> _tokens;
    const token_metadata_ptr _token_metadata_ptr;
public:
#if 0
    public static class StringSerializer implements IVersionedSerializer<String>
    {
        public static final StringSerializer instance = new StringSerializer();
        public void serialize(String s, DataOutputPlus out, int version) throws IOException
        {
            out.writeUTF(s);
        }
        public String deserialize(DataInput in, int version) throws IOException
        {
            return in.readUTF();
        }
        public long serializedSize(String s, int version)
        {
            return TypeSizes.NATIVE.sizeof(s);
        }
    }
#endif
private:
};
} // namespace dht
namespace dht {
class incremental_owned_ranges_checker {
    const dht::token_range_vector& _sorted_owned_ranges;
    mutable dht::token_range_vector::const_iterator _it;
public:
    // Must be called with increasing token values.
};
} // dht
using namespace seastar;
namespace bs2 = boost::signals2;
namespace gms {
class feature_service;
class feature final {
    using signal_type = bs2::signal_type<void (), bs2::keywords::mutex_type<bs2::dummy_mutex>>::type;
    feature_service* _service = nullptr;
    sstring _name;
    bool _enabled = false;
    mutable signal_type _s;
public:
    using listener_registration = std::any;
    class listener {
        friend class feature;
        bs2::scoped_connection _conn;
        signal_type::slot_type _slot;
    protected:
        bool _started = false;
    public:
        
        // Has to run inside seastar::async context
        
    };
    explicit feature(feature_service& service, std::string_view name, bool enabled = false);
    // Has to run inside seastar::async context
    const sstring& name() const ;
    // Will call the callback functor when this feature is enabled, unless
    // the returned listener_registration is destroyed earlier.
};
} // namespace gms
namespace db { class config; }
namespace service { class storage_service; }
namespace gms {
class feature_service;
struct feature_config {
private:
    std::set<sstring> _disabled_features;
    friend class feature_service;
};
using namespace std::literals;
class feature_service final {
    friend class feature;
    std::unordered_map<sstring, std::reference_wrapper<feature>> _registered_features;
    feature_config _config;
public:
    // Key in the 'system.scylla_local' table, that is used to
    // persist enabled features
    static constexpr const char* ENABLED_FEATURES_KEY = "enabled_features";
public:
    gms::feature user_defined_functions { *this, "UDF"sv };
    gms::feature md_sstable { *this, "MD_SSTABLE_FORMAT"sv };
    gms::feature me_sstable { *this, "ME_SSTABLE_FORMAT"sv };
    gms::feature view_virtual_columns { *this, "VIEW_VIRTUAL_COLUMNS"sv };
    gms::feature digest_insensitive_to_expiry { *this, "DIGEST_INSENSITIVE_TO_EXPIRY"sv };
    gms::feature computed_columns { *this, "COMPUTED_COLUMNS"sv };
    gms::feature cdc { *this, "CDC"sv };
    gms::feature nonfrozen_udts { *this, "NONFROZEN_UDTS"sv };
    gms::feature hinted_handoff_separate_connection { *this, "HINTED_HANDOFF_SEPARATE_CONNECTION"sv };
    gms::feature lwt { *this, "LWT"sv };
    gms::feature per_table_partitioners { *this, "PER_TABLE_PARTITIONERS"sv };
    gms::feature per_table_caching { *this, "PER_TABLE_CACHING"sv };
    gms::feature digest_for_null_values { *this, "DIGEST_FOR_NULL_VALUES"sv };
    gms::feature correct_idx_token_in_secondary_index { *this, "CORRECT_IDX_TOKEN_IN_SECONDARY_INDEX"sv };
    gms::feature alternator_streams { *this, "ALTERNATOR_STREAMS"sv };
    gms::feature alternator_ttl { *this, "ALTERNATOR_TTL"sv };
    gms::feature range_scan_data_variant { *this, "RANGE_SCAN_DATA_VARIANT"sv };
    gms::feature cdc_generations_v2 { *this, "CDC_GENERATIONS_V2"sv };
    gms::feature user_defined_aggregates { *this, "UDA"sv };
    // Historically max_result_size contained only two fields: soft_limit and
    // hard_limit. It was somehow obscure because for normal paged queries both
    // fields were equal and meant page size. For unpaged queries and reversed
    // queries soft_limit was used to warn when the size of the result exceeded
    // the soft_limit and hard_limit was used to throw when the result was
    // bigger than this hard_limit. To clean things up, we introduced the third
    // field into max_result_size. It's name is page_size. Now page_size always
    // means the size of the page while soft and hard limits are just what their
    // names suggest. They are no longer interepreted as page size. This is not
    // a backwards compatible change so this new cluster feature is used to make
    // sure the whole cluster supports the new page_size field and we can safely
    // send it to replicas.
    gms::feature separate_page_size_and_safety_limit { *this, "SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT"sv };
    // Replica is allowed to send back empty pages to coordinator on queries.
    gms::feature empty_replica_pages { *this, "EMPTY_REPLICA_PAGES"sv };
    gms::feature supports_raft_cluster_mgmt { *this, "SUPPORTS_RAFT_CLUSTER_MANAGEMENT"sv };
    gms::feature tombstone_gc_options { *this, "TOMBSTONE_GC_OPTIONS"sv };
    gms::feature parallelized_aggregation { *this, "PARALLELIZED_AGGREGATION"sv };
    gms::feature keyspace_storage_options { *this, "KEYSPACE_STORAGE_OPTIONS"sv };
    gms::feature typed_errors_in_read_rpc { *this, "TYPED_ERRORS_IN_READ_RPC"sv };
    gms::feature schema_commitlog { *this, "SCHEMA_COMMITLOG"sv };
    gms::feature uda_native_parallelized_aggregation { *this, "UDA_NATIVE_PARALLELIZED_AGGREGATION"sv };
    gms::feature aggregate_storage_options { *this, "AGGREGATE_STORAGE_OPTIONS"sv };
    gms::feature collection_indexing { *this, "COLLECTION_INDEXING"sv };
    gms::feature large_collection_detection { *this, "LARGE_COLLECTION_DETECTION"sv };
    gms::feature secondary_indexes_on_static_columns { *this, "SECONDARY_INDEXES_ON_STATIC_COLUMNS"sv };
    gms::feature tablets { *this, "TABLETS"sv };
public:
    const std::unordered_map<sstring, std::reference_wrapper<feature>>& registered_features() const;
    // Persist enabled feature in the `system.scylla_local` table under the "enabled_features" key.
    // The key itself is maintained as an `unordered_set<string>` and serialized via `to_string`
    // function to preserve readability.
};
} // namespace gms
namespace gms {
using version_type = utils::tagged_integer<struct version_type_tag, int32_t>;
namespace version_generator
{
}
} // namespace gms
namespace version {
class version {
    std::tuple<uint16_t, uint16_t, uint16_t> _version;
public:
};
}
namespace gms {
class versioned_value {
    version_type _version;
    sstring _value;
public:
    // this must be a char that cannot be present in any token
    static constexpr char DELIMITER = ',';
    static constexpr const char DELIMITER_STR[] = { DELIMITER, 0 };
    // values for ApplicationState.STATUS
    static constexpr const char* STATUS_UNKNOWN = "UNKNOWN";
    static constexpr const char* STATUS_BOOTSTRAPPING = "BOOT";
    static constexpr const char* STATUS_NORMAL = "NORMAL";
    static constexpr const char* STATUS_LEAVING = "LEAVING";
    static constexpr const char* STATUS_LEFT = "LEFT";
    static constexpr const char* STATUS_MOVING = "MOVING";
    static constexpr const char* REMOVING_TOKEN = "removing";
    static constexpr const char* REMOVED_TOKEN = "removed";
    static constexpr const char* HIBERNATE = "hibernate";
    static constexpr const char* SHUTDOWN = "shutdown";
    // values for ApplicationState.REMOVAL_COORDINATOR
    static constexpr const char* REMOVAL_COORDINATOR = "REMOVER";
    ;
    ;
public:
public:
    // Reverse of `make_full_token_string`.
    // Reverse of `make_cdc_generation_id_string`.
    ;
}; // class versioned_value
} // namespace gms
class no_such_class : public std::runtime_error {
public:
    using runtime_error::runtime_error;
};
 
// BaseType is a base type of a type hierarchy that this registry will hold
// Args... are parameters for object's constructor
template<typename BaseType, typename... Args>
class nonstatic_class_registry {
    template<typename T, typename ResultType = typename T::ptr_type>
    requires requires (typename T::ptr_type ptr) {
        { ptr.get() } -> std::same_as<T*>;
    }
    struct result_for;
    template<typename T>
    struct result_for<T, std::unique_ptr<T>> {
        typedef std::unique_ptr<T> type;
        template<typename Impl>
        static inline type make(Args&& ...args) {
            return std::make_unique<Impl>(std::forward<Args>(args)...);
        }
    };
    template<typename T>
    struct result_for<T, seastar::shared_ptr<T>> {
        typedef seastar::shared_ptr<T> type;
        template<typename Impl>
        static type make(Args&& ...args) ;
    };
    template<typename T>
    struct result_for<T, seastar::lw_shared_ptr<T>> {
        typedef seastar::lw_shared_ptr<T> type;
        // lw_shared is not (yet?) polymorph, thus having automatic
        // instantiation of it makes no sense. This way we get a nice
        // compilation error if someone messes up.
    };
    template<typename T>
    struct result_for<T, std::shared_ptr<T>> {
        typedef std::shared_ptr<T> type;
        template<typename Impl>
        static inline type make(Args&& ...args) {
            return std::make_shared<Impl>(std::forward<Args>(args)...);
        }
   };
    template<typename T, typename D>
    struct result_for<T, std::unique_ptr<T, D>> {
        typedef std::unique_ptr<T, D> type;
        template<typename Impl>
        static inline type make(Args&& ...args) {
            return std::make_unique<Impl, D>(std::forward<Args>(args)...);
        }
    };
public:
    using result_type = typename BaseType::ptr_type;
    using creator_type = std::function<result_type(Args...)>;
private:
    std::unordered_map<sstring, creator_type> _classes;
public:
    void register_class(sstring name, creator_type creator);
    template<typename T>
    void register_class(sstring name);
    result_type create(const sstring& name, Args&&...);
    std::unordered_map<sstring, creator_type>& classes() {
        return _classes;
    }
    const std::unordered_map<sstring, creator_type>& classes() const ;
    sstring to_qualified_class_name(std::string_view class_name) const;
};
template<typename BaseType, typename... Args>
void nonstatic_class_registry<BaseType, Args...>::register_class(sstring name, typename nonstatic_class_registry<BaseType, Args...>::creator_type creator) {
    classes().emplace(name, std::move(creator));
}
template<typename BaseType, typename... Args>
template<typename T>
void nonstatic_class_registry<BaseType, Args...>::register_class(sstring name) {
    register_class(name, &result_for<BaseType>::template make<T>);
}
// BaseType is a base type of a type hierarchy that this registry will hold
// Args... are parameters for object's constructor
template<typename BaseType, typename... Args>
class class_registry {
    using base_registry = nonstatic_class_registry<BaseType, Args...>;
    static base_registry& registry() {
        static base_registry the_registry;
        return the_registry;
    }
public:
    using result_type = typename base_registry::result_type;
    using creator_type = std::function<result_type(Args...)>;
public:
    static void register_class(sstring name, creator_type creator) ;
    template<typename T>
    static void register_class(sstring name) {
        registry().template register_class<T>(std::move(name));
    }
    template <typename... U>
    static result_type create(const sstring& name, U&&... a) {
        return registry().create(name, std::forward<U>(a)...);
    }
    static std::unordered_map<sstring, creator_type>& classes() {
        return registry().classes();
    }
    static sstring to_qualified_class_name(std::string_view class_name) ;
};
template<typename BaseType, typename T, typename... Args>
struct class_registrator {
    class_registrator(const sstring& name) {
        class_registry<BaseType, Args...>::template register_class<T>(name);
    }
    class_registrator(const sstring& name, typename class_registry<BaseType, Args...>::creator_type creator) ;
};
template<typename BaseType, typename... Args>
typename nonstatic_class_registry<BaseType, Args...>::result_type nonstatic_class_registry<BaseType, Args...>::create(const sstring& name, Args&&... args) {
    auto it = classes().find(name);
    if (it == classes().end()) {
        throw no_such_class(sstring("unable to find class '") + name + sstring("'"));
    }
    return it->second(std::forward<Args>(args)...);
}
template<typename BaseType, typename... Args>
typename class_registry<BaseType, Args...>::result_type  create_object(const sstring& name, Args&&... args) {
    return class_registry<BaseType, Args...>::create(name, std::forward<Args>(args)...);
}
class qualified_name {
    sstring _qname;
public:
    operator const sstring&() const {
        return _qname;
    }
};
class unqualified_name {
    sstring _qname;
public:
    operator const sstring&() const {
        return _qname;
    }
};
namespace gms {
class gossiper;
enum class application_state;
}
namespace locator {
using snitch_signal_t = boost::signals2::signal_type<void (), boost::signals2::keywords::mutex_type<boost::signals2::dummy_mutex>>::type;
using snitch_signal_slot_t = std::function<future<>()>;
using snitch_signal_connection_t = boost::signals2::scoped_connection;
struct snitch_ptr;
typedef gms::inet_address inet_address;
struct snitch_config {
    sstring name = "SimpleSnitch";
    sstring properties_file_name = "";
    unsigned io_cpu_id = 0;
    bool broadcast_rpc_address_specified_by_user = false;
    // Gossiping-property-file specific
    gms::inet_address listen_address;
    // GCE-specific
    sstring gce_meta_server_url = "";
};
struct i_endpoint_snitch {
public:
    using ptr_type = std::unique_ptr<i_endpoint_snitch>;
    virtual std::list<std::pair<gms::application_state, gms::versioned_value>> get_app_states() const = 0;
    ;
    // noop by default
    // noop by default
    ;
    // noop by default
    ;
    // noop by default
    // noop by default
    ;
    ;
    ;
    // should be called for production snitches before calling start()
    // tells wheter the INTERNAL_IP address should be preferred over endpoint address
protected:
protected:
    enum class snitch_state {
        initializing,
        running,
        io_pausing,
        io_paused,
        stopping,
        stopped
    } _state = snitch_state::initializing;
};
struct snitch_ptr : public peering_sharded_service<snitch_ptr> {
    using ptr_type = i_endpoint_snitch::ptr_type;
private:
    ptr_type _ptr;
};
class snitch_base : public i_endpoint_snitch {
public:
    //
    // Sons have to implement:
    // virtual sstring get_rack()        = 0;
    // virtual sstring get_datacenter()  = 0;
    //
    virtual std::list<std::pair<gms::application_state, gms::versioned_value>> get_app_states() const override;
protected:
    sstring _my_dc;
    sstring _my_rack;
};
} // namespace locator
namespace utils {
template<typename T, typename VectorType>
class basic_sequenced_set {
public:
    using value_type = T;
    using size_type = size_t;
    using iterator = typename VectorType::iterator;
    using const_iterator = typename VectorType::const_iterator;
    ;
    // The implementation is not exception safe
    // so mark the method noexcept to terminate in case anything throws
private:
    std::unordered_set<T> _set;
    VectorType _vec;
};
template <typename T>
using sequenced_set = basic_sequenced_set<T, std::vector<T>>;
} // namespace utils
namespace std {
 ;
} // namespace std
// forward declaration since replica/database.hh includes this file
namespace replica {
class database;
class keyspace;
}
namespace locator {
extern logging::logger rslogger;
using inet_address = gms::inet_address;
using token = dht::token;
enum class replication_strategy_type {
    simple,
    local,
    network_topology,
    everywhere_topology,
};
using can_yield = utils::can_yield;
using replication_strategy_config_options = std::map<sstring, sstring>;
using replication_map = std::unordered_map<token, inet_address_vector_replica_set>;
using endpoint_set = utils::basic_sequenced_set<inet_address, inet_address_vector_replica_set>;
class vnode_effective_replication_map;
class effective_replication_map_factory;
class per_table_replication_strategy;
class tablet_aware_replication_strategy;
class abstract_replication_strategy : public seastar::enable_shared_from_this<abstract_replication_strategy> {
    friend class vnode_effective_replication_map;
    friend class per_table_replication_strategy;
    friend class tablet_aware_replication_strategy;
protected:
    replication_strategy_config_options _config_options;
    replication_strategy_type _my_type;
    bool _per_table = false;
    bool _uses_tablets = false;
     ;
     ;
     ;
public:
    using ptr_type = seastar::shared_ptr<abstract_replication_strategy>;
    // Evaluates to true iff calculate_natural_endpoints
    // returns different results for different tokens.
    // The returned vector has size O(number of normal token owners), which is O(number of nodes in the cluster).
    // Note: it is not guaranteed that the function will actually yield. If the complexity of a particular implementation
    // is small, that implementation may not yield since by itself it won't cause a reactor stall (assuming practical
    // cluster sizes and number of tokens per node). The caller is responsible for yielding if they call this function
    // in a loop.
    // Returns the last stop_iteration result of the called func
    virtual std::optional<std::unordered_set<sstring>> recognized_options(const topology&) const = 0;
    // Decide if the replication strategy allow removing the node being
    // replaced from the natural endpoints when a node is being replaced in the
    // cluster. LocalStrategy is the not allowed to do so because it always
    // returns the node itself as the natural_endpoints and the node will not
    // appear in the pending_endpoints.
    // If returns true then tables governed by this replication strategy have separate
    // effective_replication_maps.
    // If returns false, they share the same effective_replication_map, which is per keyspace.
    // If returns true, then this replication strategy extends per_table_replication_strategy.
    // Note, a replication strategy may extend per_table_replication_strategy while !is_per_table(),
    // depending on actual strategy options.
    // Returns true iff this replication strategy is based on vnodes.
    // If this is the case, all tables governed by this replication strategy share the effective replication map.
    // Use the token_metadata provided by the caller instead of _token_metadata
    // Note: must be called with initialized, non-empty token_metadata.
    // Caller must ensure that token_metadata will not change throughout the call.
    future<std::unordered_map<dht::token_range, inet_address_vector_replica_set>> get_range_addresses(const token_metadata& tm) const;
    future<dht::token_range_vector> get_pending_address_ranges(const token_metadata_ptr tmptr, std::unordered_set<token> pending_tokens, inet_address pending_address, locator::endpoint_dc_rack dr) const;
};
using replication_strategy_ptr = seastar::shared_ptr<const abstract_replication_strategy>;
using mutable_replication_strategy_ptr = seastar::shared_ptr<abstract_replication_strategy>;
/// \brief Represents effective replication (assignment of replicas to keys).
///
/// It's a result of application of a given replication strategy instance
/// over a particular token metadata version, for a given table.
/// Can be shared by multiple tables if they have the same replication.
///
/// Immutable, users can assume that it doesn't change.
///
/// Holding to this object keeps the associated token_metadata_ptr alive,
/// keeping the token metadata version alive and seen as in use.
class effective_replication_map {
protected:
    replication_strategy_ptr _rs;
    token_metadata_ptr _tmptr;
    size_t _replication_factor;
public:
    effective_replication_map(replication_strategy_ptr, token_metadata_ptr, size_t replication_factor) noexcept;
    virtual ~effective_replication_map() = default;
    const abstract_replication_strategy& get_replication_strategy() const noexcept ;
    
    
    const topology& get_topology() const noexcept ;
    size_t get_replication_factor() const noexcept ;
    /// Returns addresses of replicas for a given token.
    /// Does not include pending replicas except for a pending replica which
    /// has the same address as one of the old replicas. This can be the case during "nodetool replace"
    /// operation which adds a replica which has the same address as the replaced replica.
    /// Use get_natural_endpoints_without_node_being_replaced() to get replicas without any pending replicas.
    /// This won't be necessary after we implement https://github.com/scylladb/scylladb/issues/6403.
    /// Returns addresses of replicas for a given token.
    /// Does not include pending replicas.
    /// Returns the set of pending replicas for a given token.
    /// Pending replica is a replica which gains ownership of data.
    /// Non-empty only during topology change.
    /// Returns a token_range_splitter which is line with the replica assignment of this replication map.
    /// The splitter can live longer than this instance.
};
using effective_replication_map_ptr = seastar::shared_ptr<const effective_replication_map>;
using mutable_effective_replication_map_ptr = seastar::shared_ptr<effective_replication_map>;
/// Replication strategies which support per-table replication extend this trait.
///
/// It will be accessed only if the replication strategy actually works in per-table mode,
/// that is after mark_as_per_table() is called, and as a result
/// abstract_replication_strategy::is_per_table() returns true.
class per_table_replication_strategy {
protected:
public:
};
// Holds the full replication_map resulting from applying the
// effective replication strategy over the given token_metadata
// and replication_strategy_config_options.
// Used for token-based replication strategies.
class vnode_effective_replication_map : public enable_shared_from_this<vnode_effective_replication_map>
                                      , public effective_replication_map {
public:
    struct factory_key {
        replication_strategy_type rs_type;
        long ring_version;
        replication_strategy_config_options rs_config_options;
    };
private:
    replication_map _replication_map;
    std::optional<factory_key> _factory_key = std::nullopt;
    effective_replication_map_factory* _factory = nullptr;
    friend class abstract_replication_strategy;
    friend class effective_replication_map_factory;
public: // effective_replication_map
public:
    // get_ranges() returns the list of ranges held by the given endpoint.
    // The list is sorted, and its elements are non overlapping and non wrap-around.
    // It the analogue of Origin's getAddressRanges().get(endpoint).
    // This function is not efficient, and not meant for the fast path.
    //
    // Note: must be called after token_metadata has been initialized.
    // get_primary_ranges() returns the list of "primary ranges" for the given
    // endpoint. "Primary ranges" are the ranges that the node is responsible
    // for storing replica primarily, which means this is the first node
    // returned calculate_natural_endpoints().
    // This function is the analogue of Origin's
    // StorageService.getPrimaryRangesForEndpoint().
    //
    // Note: must be called after token_metadata has been initialized.
    // get_primary_ranges_within_dc() is similar to get_primary_ranges()
    // except it assigns a primary node for each range within each dc,
    // instead of one node globally.
    //
    // Note: must be called after token_metadata has been initialized.
    future<std::unordered_map<dht::token_range, inet_address_vector_replica_set>>
    get_range_addresses() const;
private:
public:
};
using vnode_effective_replication_map_ptr = shared_ptr<const vnode_effective_replication_map>;
using mutable_vnode_effective_replication_map_ptr = shared_ptr<vnode_effective_replication_map>;
using vnode_erm_ptr = vnode_effective_replication_map_ptr;
using mutable_vnode_erm_ptr = mutable_vnode_effective_replication_map_ptr;
// Apply the replication strategy over the current configuration and the given token_metadata.
// Class to hold a coherent view of a keyspace
// effective replication map on all shards
class global_vnode_effective_replication_map {
    std::vector<foreign_ptr<vnode_erm_ptr>> _erms;
public:
};
} // namespace locator
std::ostream& operator<<(std::ostream& os, locator::replication_strategy_type);
std::ostream& operator<<(std::ostream& os, const locator::vnode_effective_replication_map::factory_key& key);
template <>
struct fmt::formatter<locator::vnode_effective_replication_map::factory_key> {
    constexpr auto parse(format_parse_context& ctx) {
        return ctx.end();
    }
    template <typename FormatContext>
    auto format(const locator::vnode_effective_replication_map::factory_key& key, FormatContext& ctx) {
        std::ostringstream os;
        os << key;
        return fmt::format_to(ctx.out(), "{}", os.str());
    }
};
template<>
struct appending_hash<locator::vnode_effective_replication_map::factory_key> {
    template<typename Hasher>
    void operator()(Hasher& h, const locator::vnode_effective_replication_map::factory_key& key) const ;
};
namespace std {
template <>
struct hash<locator::vnode_effective_replication_map::factory_key> {
    size_t operator()(const locator::vnode_effective_replication_map::factory_key& key) const {
        simple_xx_hasher h;
        appending_hash<locator::vnode_effective_replication_map::factory_key>{}(h, key);
        return h.finalize();
    }
};
} // namespace std
namespace locator {
class effective_replication_map_factory : public peering_sharded_service<effective_replication_map_factory> {
    std::unordered_map<vnode_effective_replication_map::factory_key, vnode_effective_replication_map*> _effective_replication_maps;
    future<> _background_work = make_ready_future<>();
    bool _stopped = false;
public:
    // looks up the vnode_effective_replication_map on the local shard.
    // If not found, tries to look one up for reference on shard 0
    // so its replication map can be cloned.  Otherwise, calculates the
    // vnode_effective_replication_map for the local shard.
    //
    // Therefore create should be called first on shard 0, then on all other shards.
private:
    friend class vnode_effective_replication_map;
};
}
namespace streaming {
class stream_event_handler;
class stream_manager;
class stream_result_future;
class stream_session;
class stream_state;
using plan_id = utils::tagged_uuid<struct plan_id_tag>;
} // namespace streaming
namespace gms {
using generation_type = utils::tagged_integer<struct generation_type_tag, int32_t>;
}
namespace gms {
class heart_beat_state {
private:
    generation_type _generation;
    version_type _version;
public:
};
} // gms
namespace gms {
enum class application_state {
    STATUS = 0,
    LOAD,
    SCHEMA,
    DC,
    RACK,
    RELEASE_VERSION,
    REMOVAL_COORDINATOR,
    INTERNAL_IP,
    RPC_ADDRESS,
    X_11_PADDING, // padding specifically for 1.1
    SEVERITY,
    NET_VERSION,
    HOST_ID,
    TOKENS,
    SUPPORTED_FEATURES,
    CACHE_HITRATES,
    SCHEMA_TABLES_VERSION,
    RPC_READY,
    VIEW_BACKLOG,
    SHARD_COUNT,
    IGNORE_MSB_BITS,
    CDC_GENERATION_ID,
    SNITCH_NAME,
};
}
namespace gms {
class endpoint_state {
public:
    using clk = seastar::lowres_system_clock;
private:
    heart_beat_state _heart_beat_state;
    std::map<application_state, versioned_value> _application_state;
    clk::time_point _update_timestamp;
    bool _is_alive;
    bool _is_normal = false;
public:
    // Valid only on shard 0
    // Valid only on shard 0
    // @Deprecated
};
} // gms
namespace gms {
class i_endpoint_state_change_subscriber {
public:
};
} // namespace gms
namespace gms { class inet_address; }
namespace netw {
struct msg_addr;
enum class messaging_verb;
class messaging_service;
using connection_drop_signal_t = boost::signals2::signal_type<void (gms::inet_address), boost::signals2::keywords::mutex_type<boost::signals2::dummy_mutex>>::type;
using connection_drop_slot_t = std::function<void(gms::inet_address)>;
using connection_drop_registration_t = boost::signals2::scoped_connection;
}
namespace streaming {
enum class stream_session_state {
    INITIALIZED,
    PREPARING,
    STREAMING,
    WAIT_COMPLETE,
    COMPLETE,
    FAILED,
};
} // namespace
namespace streaming {
class stream_summary {
public:
    table_id cf_id;
    int files;
    long total_size;
};
} // namespace streaming
namespace streaming {
class stream_session;
class stream_task {
public:
    shared_ptr<stream_session> session;
    table_id cf_id;
public:
    virtual void abort() = 0;
    
};
} // namespace streaming
namespace streaming {
struct stream_detail {
    table_id cf_id;
    
};
} // namespace streaming
namespace streaming {
class send_info;
class stream_transfer_task : public stream_task {
private:
    // A stream_transfer_task always contains the same range to stream
    dht::token_range_vector _ranges;
    std::map<unsigned, dht::partition_range_vector> _shard_ranges;
    long _total_size;
    bool _mutation_done_sent = false;
public:
public:
};
} // namespace streaming
namespace streaming {
class stream_receive_task : public stream_task {
private:
    // number of files to receive
    int total_files;
    // total size of files to receive
    long total_size;
public:
};
} // namespace streaming
namespace compat {
using wrapping_partition_range = wrapping_range<dht::ring_position>;
// unwraps a vector of wrapping ranges into a vector of nonwrapping ranges
// if the vector happens to be sorted by the left bound, it remains sorted
template <typename T, typename Comparator>
std::vector<nonwrapping_range<T>>
unwrap(std::vector<wrapping_range<T>>&& v, Comparator&& cmp) {
    std::vector<nonwrapping_range<T>> ret;
    ret.reserve(v.size() + 1);
    for (auto&& wr : v) {
        if (wr.is_wrap_around(cmp)) {
            auto&& p = std::move(wr).unwrap();
            ret.insert(ret.begin(), nonwrapping_range<T>(std::move(p.first)));
            ret.emplace_back(std::move(p.second));
        } else {
            ret.emplace_back(std::move(wr));
        }
    }
    return ret;
}
// unwraps a vector of wrapping ranges into a vector of nonwrapping ranges
// if the vector happens to be sorted by the left bound, it remains sorted
 ;
 ;
template <typename T>
std::vector<wrapping_range<T>>
wrap(std::vector<nonwrapping_range<T>>&& v) {
    // re-wrap (-inf,x) ... (y, +inf) into (y, x):
    if (v.size() >= 2 && !v.front().start() && !v.back().end()) {
        auto ret = std::vector<wrapping_range<T>>();
        ret.reserve(v.size() - 1);
        std::move(v.begin() + 1, v.end() - 1, std::back_inserter(ret));
        ret.emplace_back(std::move(v.back()).start(), std::move(v.front()).end());
        return ret;
    }
    // want boost::adaptor::moved ...
    return boost::copy_range<std::vector<wrapping_range<T>>>(v);
}
inline
dht::token_range_vector
unwrap(std::vector<wrapping_range<dht::token>>&& v) {
    return unwrap(std::move(v), dht::token_comparator());
}
class one_or_two_partition_ranges : public std::pair<dht::partition_range, std::optional<dht::partition_range>> {
    using pair = std::pair<dht::partition_range, std::optional<dht::partition_range>>;
public:
    explicit one_or_two_partition_ranges(dht::partition_range&& f)
        : pair(std::move(f), std::nullopt) {
    }
    explicit one_or_two_partition_ranges(dht::partition_range&& f, dht::partition_range&& s)
        : pair(std::move(f), std::move(s)) {
    }
    operator dht::partition_range_vector() const & {
        auto ret = dht::partition_range_vector();
        // not reserving, since ret.size() is likely to be 1
        ret.push_back(first);
        if (second) {
            ret.push_back(*second);
        }
        return ret;
    }
    operator dht::partition_range_vector() && {
        auto ret = dht::partition_range_vector();
        // not reserving, since ret.size() is likely to be 1
        ret.push_back(std::move(first));
        if (second) {
            ret.push_back(std::move(*second));
        }
        return ret;
    }
};
// Unwraps `range` and calls `func` with its components, with an unwrapped
// range type, as a parameter (once or twice)
 ;
}
namespace streaming {
class stream_request {
public:
    using token = dht::token;
    sstring keyspace;
    dht::token_range_vector ranges;
    // For compatibility with <= 1.5, we send wrapping ranges (though they will never wrap).
    std::vector<sstring> column_families;
};
} // namespace streaming
namespace streaming {
class prepare_message {
public:
    std::vector<stream_request> requests;
    std::vector<stream_summary> summaries;
    uint32_t dst_cpu_id;
};
} // namespace streaming
namespace streaming {
class progress_info {
public:
    using inet_address = gms::inet_address;
    enum class direction { OUT, IN };
    inet_address peer;
    sstring file_name;
    direction dir;
    long current_bytes;
    long total_bytes;
};
} // namespace streaming
namespace db {
class config;
class system_distributed_keyspace;
namespace view {
class view_update_generator;
}
}
namespace service {
class migration_manager;
};
namespace netw {
class messaging_service;
};
namespace gms {
class gossiper;
}
namespace streaming {
struct stream_bytes {
    int64_t bytes_sent = 0;
    int64_t bytes_received = 0;
};
class stream_manager : public gms::i_endpoint_state_change_subscriber, public enable_shared_from_this<stream_manager>, public peering_sharded_service<stream_manager> {
    using inet_address = gms::inet_address;
    using endpoint_state = gms::endpoint_state;
    using application_state = gms::application_state;
    using versioned_value = gms::versioned_value;
private:
    sharded<replica::database>& _db;
    sharded<db::system_distributed_keyspace>& _sys_dist_ks;
    sharded<db::view::view_update_generator>& _view_update_generator;
    sharded<netw::messaging_service>& _ms;
    sharded<service::migration_manager>& _mm;
    gms::gossiper& _gossiper;
    std::unordered_map<plan_id, shared_ptr<stream_result_future>> _initiated_streams;
    std::unordered_map<plan_id, shared_ptr<stream_result_future>> _receiving_streams;
    std::unordered_map<plan_id, std::unordered_map<gms::inet_address, stream_bytes>> _stream_bytes;
    uint64_t _total_incoming_bytes{0};
    uint64_t _total_outgoing_bytes{0};
    semaphore _mutation_send_limiter{256};
    seastar::metrics::metric_groups _metrics;
    std::unordered_map<streaming::stream_reason, float> _finished_percentage;
    utils::updateable_value<uint32_t> _io_throughput_mbs;
    serialized_action _io_throughput_updater = serialized_action([this] { return update_io_throughput(_io_throughput_mbs()); });
    std::optional<utils::observer<uint32_t>> _io_throughput_option_observer;
public:
public:
private:
    future<> update_io_throughput(uint32_t value_mbs);
public:
    void update_finished_percentage(streaming::stream_reason reason, float percentage);
};
} // namespace streaming
namespace streaming {
class session_info {
public:
    using inet_address = gms::inet_address;
    inet_address peer;
    std::vector<stream_summary> receiving_summaries;
    std::vector<stream_summary> sending_summaries;
    stream_session_state state;
    std::map<sstring, progress_info> receiving_files;
    std::map<sstring, progress_info> sending_files;
private:
};
} // namespace streaming
namespace db {
class system_distributed_keyspace;
}
namespace service {
class migration_manager;
}
namespace db::view {
class view_update_generator;
}
namespace streaming {
class stream_result_future;
class stream_session : public enable_shared_from_this<stream_session> {
private:
    using messaging_verb = netw::messaging_verb;
    using messaging_service = netw::messaging_service;
    using msg_addr = netw::msg_addr;
    using inet_address = gms::inet_address;
    using token = dht::token;
    using ring_position = dht::ring_position;
public:
    inet_address peer;
    unsigned dst_cpu_id = 0;
private:
    stream_manager& _mgr;
    // should not be null when session is started
    shared_ptr<stream_result_future> _stream_result;
    // stream requests to send to the peer
    std::vector<stream_request> _requests;
    // streaming tasks are created and managed per ColumnFamily ID
    std::map<table_id, stream_transfer_task> _transfers;
    // data receivers, filled after receiving prepare message
    std::map<table_id, stream_receive_task> _receivers;
    //private final StreamingMetrics metrics;
    //private final StreamConnectionFactory factory;
    int64_t _bytes_sent = 0;
    int64_t _bytes_received = 0;
    int _retries;
    bool _is_aborted =  false;
    stream_session_state _state = stream_session_state::INITIALIZED;
    bool _complete_sent = false;
    bool _received_failed_complete_message = false;
    session_info _session_info;
    stream_reason _reason = stream_reason::unspecified;
public:
public:
public:
public:
private:
};
} // namespace streaming
namespace streaming {
class stream_coordinator {
public:
    using inet_address = gms::inet_address;
private:
    class host_streaming_data;
    std::map<inet_address, shared_ptr<stream_session>> _peer_sessions;
    bool _is_receiving;
public:
public:
public:
};
} // namespace streaming
namespace streaming {
class stream_plan {
private:
    using inet_address = gms::inet_address;
    using token = dht::token;
    stream_manager& _mgr;
    plan_id _plan_id;
    sstring _description;
    stream_reason _reason;
    std::vector<stream_event_handler*> _handlers;
    shared_ptr<stream_coordinator> _coordinator;
    bool _range_added = false;
    bool _aborted = false;
public:
    
    
    stream_plan& transfer_ranges(inet_address to, sstring keyspace, dht::token_range_vector ranges, std::vector<sstring> column_families);
public:
};
} // namespace streaming
namespace replica {
class database;
}
namespace gms { class gossiper; }
namespace locator { class topology; }
namespace dht {
class range_streamer {
public:
    using inet_address = gms::inet_address;
    using token_metadata = locator::token_metadata;
    using token_metadata_ptr = locator::token_metadata_ptr;
    using stream_plan = streaming::stream_plan;
    using stream_state = streaming::stream_state;
public:
    class i_source_filter {
    public:
        virtual bool should_include(const locator::topology&, inet_address endpoint) = 0;
    };
    class failure_detector_source_filter : public i_source_filter {
    private:
        std::set<gms::inet_address> _down_nodes;
    public:
    };
    class single_datacenter_filter : public i_source_filter {
    private:
        sstring _source_dc;
    public:
        virtual bool should_include(const locator::topology& topo, inet_address endpoint) override ;
    };
    range_streamer(distributed<replica::database>& db, sharded<streaming::stream_manager>& sm, const token_metadata_ptr tmptr, abort_source& abort_source, std::unordered_set<token> tokens,
            inet_address address, locator::endpoint_dc_rack dr, sstring description, streaming::stream_reason reason)
        : _db(db)
        , _stream_manager(sm)
        , _token_metadata_ptr(std::move(tmptr))
        , _abort_source(abort_source)
        , _tokens(std::move(tokens))
        , _address(address)
        , _dr(std::move(dr))
        , _description(std::move(description))
        , _reason(reason)
    {
        _abort_source.check();
    }
    range_streamer(distributed<replica::database>& db, sharded<streaming::stream_manager>& sm, const token_metadata_ptr tmptr, abort_source& abort_source,
            inet_address address, locator::endpoint_dc_rack dr, sstring description, streaming::stream_reason reason)
        : range_streamer(db, sm, std::move(tmptr), abort_source, std::unordered_set<token>(), address, std::move(dr), description, reason) {
    }
    void add_source_filter(std::unique_ptr<i_source_filter> filter) {
        _source_filters.emplace(std::move(filter));
    }
    future<> add_ranges(const sstring& keyspace_name, locator::vnode_effective_replication_map_ptr erm, dht::token_range_vector ranges, gms::gossiper& gossiper, bool is_replacing);
    void add_tx_ranges(const sstring& keyspace_name, std::unordered_map<inet_address, dht::token_range_vector> ranges_per_endpoint);
    
private:
    
    std::unordered_map<dht::token_range, std::vector<inet_address>>
    get_all_ranges_with_sources_for(const sstring& keyspace_name, locator::vnode_effective_replication_map_ptr erm, dht::token_range_vector desired_ranges);
    std::unordered_map<dht::token_range, std::vector<inet_address>>
    get_all_ranges_with_strict_sources_for(const sstring& keyspace_name, locator::vnode_effective_replication_map_ptr erm, dht::token_range_vector desired_ranges, gms::gossiper& gossiper);
private:
    std::unordered_map<inet_address, dht::token_range_vector>
    get_range_fetch_map(const std::unordered_map<dht::token_range, std::vector<inet_address>>& ranges_with_sources,
                        const std::unordered_set<std::unique_ptr<i_source_filter>>& source_filters,
                        const sstring& keyspace);
#if 0
    // For testing purposes
    Multimap<String, Map.Entry<InetAddress, Collection<Range<Token>>>> toFetch()
    {
        return toFetch;
    }
#endif
    const token_metadata& get_token_metadata() ;
public:
    
    
private:
    distributed<replica::database>& _db;
    sharded<streaming::stream_manager>& _stream_manager;
    const token_metadata_ptr _token_metadata_ptr;
    abort_source& _abort_source;
    std::unordered_set<token> _tokens;
    inet_address _address;
    locator::endpoint_dc_rack _dr;
    sstring _description;
    streaming::stream_reason _reason;
    std::unordered_multimap<sstring, std::unordered_map<inet_address, dht::token_range_vector>> _to_stream;
    std::unordered_set<std::unique_ptr<i_source_filter>> _source_filters;
    // Number of tx and rx ranges added
    unsigned _nr_tx_added = 0;
    unsigned _nr_rx_added = 0;
    // Limit the number of nodes to stream in parallel to reduce memory pressure with large cluster.
    seastar::semaphore _limiter{16};
    size_t _nr_total_ranges = 0;
    size_t _nr_ranges_remaining = 0;
};
} // dht
namespace fs = std::filesystem;
namespace utils {
    class file_lock {
    public:
        
        
        file_lock(file_lock&&) noexcept;
        ~file_lock();
        file_lock& operator=(file_lock&&) = default;
        static future<file_lock> acquire(fs::path);
        fs::path path() const;
        sstring to_string() const ;
    private:
        class impl;
        file_lock(fs::path);
        std::unique_ptr<impl> _impl;
    };
    std::ostream& operator<<(std::ostream& out, const file_lock& f);
}
using namespace seastar;
namespace db {
class config;
}
namespace utils {
class directories {
public:
    class set {
    public:
        void add(fs::path path);
        void add(sstring path);
        void add(std::vector<sstring> path);
        void add_sharded(sstring path);
        const std::set<fs::path> get_paths() const ;
    private:
        std::set<fs::path> _paths;
    };
    directories(bool developer_mode);
    future<> create_and_verify(set dir_set);
    static future<> verify_owner_and_mode(std::filesystem::path path);
private:
    bool _developer_mode;
    std::vector<file_lock> _locks;
};
} // namespace utils
#if defined(__GLIBCXX__) && (defined(__x86_64__) || defined(__aarch64__))
  #define OPTIMIZED_EXCEPTION_HANDLING_AVAILABLE
#endif
#if !defined(NO_OPTIMIZED_EXCEPTION_HANDLING)
  #if defined(OPTIMIZED_EXCEPTION_HANDLING_AVAILABLE)
    #define USE_OPTIMIZED_EXCEPTION_HANDLING
  #else
    #warn "Fast implementation of some of the exception handling routines is not available for this platform. Expect poor exception handling performance."
  #endif
#endif
namespace seastar { class logger; }
typedef std::function<bool (const std::system_error &)> system_error_lambda_t;


bool is_timeout_exception(std::exception_ptr e);
class storage_io_error : public std::exception {
private:
    std::error_code _code;
    std::string _what;
public:
    storage_io_error(std::system_error& e) noexcept
        : _code{e.code()}
        , _what{std::string("Storage I/O error: ") + std::to_string(e.code().value()) + ": " + e.what()}
    { }
    
};
// Rethrow exception if not null
//
// Helps with the common coroutine exception-handling idiom:
//
//  std::exception_ptr ex;
//  try {
//      ...
//  } catch (...) {
//      ex = std::current_exception();
//  }
//
//  // release resource(s)
//  maybe_rethrow_exception(std::move(ex));
//
//  return result;
//
namespace utils::internal {
#if defined(OPTIMIZED_EXCEPTION_HANDLING_AVAILABLE)
void* try_catch_dynamic(std::exception_ptr& eptr, const std::type_info* catch_type) noexcept;
template<typename Ex>
class nested_exception : public Ex, public std::nested_exception {
private:
    void set_nested_exception(std::exception_ptr nested_eptr) {
        // Hack: libstdc++'s std::nested_exception has just one field
        // which is a std::exception_ptr. It is initialized
        // to std::current_exception on its construction, but we override
        // it here.
        auto* nex = dynamic_cast<std::nested_exception*>(this);
        // std::nested_exception is virtual without any base classes,
        // so according to the ABI we just need to skip the vtable pointer
        // and align
        auto* nptr = reinterpret_cast<std::exception_ptr*>(
                seastar::align_up(
                        reinterpret_cast<char*>(nex) + sizeof(void*),
                        alignof(std::nested_exception)));
        *nptr = std::move(nested_eptr);
    }
public:
    explicit nested_exception(const Ex& ex, std::exception_ptr&& nested_eptr)
            : Ex(ex) {
        set_nested_exception(std::move(nested_eptr));
    }
    explicit nested_exception(Ex&& ex, std::exception&& nested_eptr)
            : Ex(std::move(ex)) {
        set_nested_exception(std::move(nested_eptr));
    }
};
#endif
} // utils::internal
/// If the exception_ptr holds an exception which would match on a `catch (T&)`
/// clause, returns a pointer to it. Otherwise, returns `nullptr`.
///
/// The exception behind the pointer is valid as long as the exception
/// behind the exception_ptr is valid.
 ;
/// Analogous to std::throw_with_nested, but instead of capturing the currently
/// thrown exception, takes the exception to be nested inside as an argument,
/// and does not throw the new exception but rather returns it.
 ;
namespace utils {
using inet_address = gms::inet_address;
class fb_utilities {
private:
public:
   static constexpr int32_t MAX_UNSIGNED_SHORT = 0xFFFF;
};
}
namespace gms {
class gossip_digest { // implements Comparable<GossipDigest>
private:
    using inet_address = gms::inet_address;
    inet_address _endpoint;
    generation_type _generation;
    version_type _max_version;
public:
}; // class gossip_digest
} // namespace gms
namespace gms {
class gossip_digest_syn {
private:
    sstring _cluster_id;
    sstring _partioner;
    utils::chunked_vector<gossip_digest> _digests;
public:
};
}
namespace utils {
template <typename T> class in;
template <typename T> struct is_in {
    static constexpr bool value = false;
};
template <typename T> struct is_in<in<T>> {
    static constexpr bool value = true;
};
template <typename T> struct is_in<const in<T>> {
    static constexpr bool value = true;
};
//
// Allows initializer lists of mixed rvalue/lvalues, where receive can
// use move semantics on provided value(s).
// Allows in-place conversion, _BUT_ (warning), a converted value
// is only valid until end-of-statement (;). So a call like:
//
//  void apa(initializer_list<in<std::string>>);
//  apa({ "ola", "kong" });
//
// is ok, but
//
//  initializer_list<in<std::string>> apa = { "ola", "kong" };
//  for (auto&x : apa) ...
//
// is not.
// So again. Only for calls.
template<typename T>
class in {
public:
    // Support for implicit conversion via perfect forwarding.
    //
    struct storage {
        bool created;
        typename std::aligned_storage<sizeof(T), alignof(T)>::type data;
    };
    // conversion constuctor. See warning above.
    template<typename T1,
      typename std::enable_if<std::is_convertible<T1, T>::value
          && !is_in<typename std::remove_reference<T1>::type>::value,int>::type = 0>
    in(T1&& x, storage&& s = storage())
        : _value(*new (&s.data) T (std::forward<T1>(x)))
        , _is_rvalue(true)
    {
        s.created = true;
    }
    in(T& l)
        : _value(l)
        , _is_rvalue(false)
    {} // For T1&& becoming T1&.
    // Accessors.
    //
    bool lvalue() const { return !_is_rvalue; }
    bool rvalue() const { return _is_rvalue; }
    operator const T&() const { return get(); }
    const T& get() const { return _value; }
    T&& rget() const { return std::move (const_cast<T&> (_value)); }
    // Return a copy if lvalue.
    //
    T move() const {
        if (_is_rvalue) {
            return rget();
        }
        return _value;
    }
private:
    const T& _value;
    bool _is_rvalue;
};
template<typename T, typename Container>
Container make_from_in_list(std::initializer_list<in<T>> args) {
    return boost::copy_range<Container>(args | boost::adaptors::transformed([](const in<T>& v) {
        return v.move();
    }));
}
template<typename T, typename... Args>
std::vector<T, Args...> make_vector_from_in_list(std::initializer_list<in<T>> args) {
    return make_from_in_list<T, std::vector<T, Args...>>(args);
}
}
namespace db {
class config;
class system_keyspace;
}
namespace gms {
class gossip_digest_syn;
class gossip_digest_ack;
class gossip_digest_ack2;
class gossip_digest;
class inet_address;
class i_endpoint_state_change_subscriber;
class gossip_get_endpoint_states_request;
class gossip_get_endpoint_states_response;
class feature_service;
using advertise_myself = bool_class<class advertise_myself_tag>;
struct syn_msg_pending {
    bool pending = false;
    std::optional<gossip_digest_syn> syn_msg;
};
struct ack_msg_pending {
    bool pending = false;
    std::optional<utils::chunked_vector<gossip_digest>> ack_msg_digest;
};
struct gossip_config {
    seastar::scheduling_group gossip_scheduling_group = seastar::scheduling_group();
    sstring cluster_name;
    std::set<inet_address> seeds;
    sstring partitioner;
    uint32_t ring_delay_ms = 30 * 1000;
    uint32_t shadow_round_ms = 300 * 1000;
    uint32_t shutdown_announce_ms = 2 * 1000;
    uint32_t skip_wait_for_gossip_to_settle = -1;
};
class gossiper : public seastar::async_sharded_service<gossiper>, public seastar::peering_sharded_service<gossiper> {
public:
    using clk = seastar::lowres_system_clock;
    using ignore_features_of_local_node = bool_class<class ignore_features_of_local_node_tag>;
    using generation_for_nodes = std::unordered_map<gms::inet_address, generation_type>;
private:
    using messaging_verb = netw::messaging_verb;
    using messaging_service = netw::messaging_service;
    using msg_addr = netw::msg_addr;
    static constexpr uint32_t _default_cpuid = 0;
    timer<lowres_clock> _scheduled_gossip_task;
    bool _enabled = false;
    semaphore _callback_running{1};
    semaphore _apply_state_locally_semaphore{100};
    seastar::gate _background_msg;
    std::unordered_map<gms::inet_address, syn_msg_pending> _syn_handlers;
    std::unordered_map<gms::inet_address, ack_msg_pending> _ack_handlers;
    bool _advertise_myself = true;
    // Map ip address and generation number
    generation_for_nodes _advertise_to_nodes;
    future<> _failure_detector_loop_done{make_ready_future<>()} ;
public:
    // Get current generation number for the given nodes
    // Only respond echo message listed in nodes with the generation number
public:
public:
    using endpoint_locks_map = utils::loading_shared_values<inet_address, semaphore>;
    struct endpoint_permit {
        endpoint_locks_map::entry_ptr _ptr;
        semaphore_units<> _units;
    };
private:
    std::unordered_map<inet_address, endpoint_state> _endpoint_state_map;
    // Used for serializing changes to _endpoint_state_map and running of associated change listeners.
    endpoint_locks_map _endpoint_locks;
public:
    const std::vector<sstring> DEAD_STATES = {
        versioned_value::REMOVING_TOKEN,
        versioned_value::REMOVED_TOKEN,
        versioned_value::STATUS_LEFT,
    };
    const std::vector<sstring> SILENT_SHUTDOWN_STATES = {
        versioned_value::REMOVING_TOKEN,
        versioned_value::REMOVED_TOKEN,
        versioned_value::STATUS_LEFT,
        versioned_value::HIBERNATE,
        versioned_value::STATUS_BOOTSTRAPPING,
        versioned_value::STATUS_UNKNOWN,
    };
    static constexpr std::chrono::milliseconds INTERVAL{1000};
    static constexpr std::chrono::hours A_VERY_LONG_TIME{24 * 3};
    static constexpr std::chrono::milliseconds GOSSIP_SETTLE_MIN_WAIT_MS{5000};
    // Maximimum difference between remote generation value and generation
    // value this node would get if this node were restarted that we are
    // willing to accept about a peer.
    static constexpr generation_type::value_type MAX_GENERATION_DIFFERENCE = 86400 * 365;
    std::chrono::milliseconds fat_client_timeout;
private:
    std::default_random_engine _random_engine{std::random_device{}()};
    atomic_vector<shared_ptr<i_endpoint_state_change_subscriber>> _subscribers;
    std::list<std::vector<inet_address>> _endpoints_to_talk_with;
    utils::chunked_vector<inet_address> _live_endpoints;
    uint64_t _live_endpoints_version = 0;
    std::unordered_set<inet_address> _pending_mark_alive_endpoints;
    std::unordered_map<inet_address, clk::time_point> _unreachable_endpoints;
    semaphore _endpoint_update_semaphore = semaphore(1);
    std::set<inet_address> _seeds;
    std::map<inet_address, clk::time_point> _just_removed_endpoints;
    std::map<inet_address, clk::time_point> _expire_time_endpoint_map;
    bool _in_shadow_round = false;
    std::unordered_map<inet_address, clk::time_point> _shadow_unreachable_endpoints;
    utils::chunked_vector<inet_address> _shadow_live_endpoints;
    // replicate shard 0 live endpoints across all other shards.
    // _endpoint_update_semaphore must be held for the whole duration
    // Replicates given endpoint_state to all other shards.
    // The state state doesn't have to be kept alive around until completes.
    // Replicates "states" from "src" to all other shards.
    // "src" and "states" must be kept alive until completes and must not change.
    // Replicates given value to all other shards.
    // The value must be kept alive until completes and not change.
public:
private:
public:
private:
private:
public:
public:
private:
public:
    // removes ALL endpoint states; should only be called after shadow gossip
private:
    
public:
    bool is_alive(inet_address ep) const;
    bool is_dead_state(const endpoint_state& eps) const;
    // Wait for nodes to be alive on all shards
    
    // Get live members synchronized to all shards
    future<std::set<inet_address>> get_live_members_synchronized();
    
private:
    // notify that a local application state is going to change (doesn't get triggered for remote changes)
    // notify that an application state has changed
public:
public:
public:
private:
public:
    // Needed by seastar::sharded
public:
    bool is_enabled() const;
    void finish_shadow_round();
    bool is_in_shadow_round() const;
    void goto_shadow_round();
public:
public:
public:
    // Check if a node is in NORMAL or SHUTDOWN status which means the node is
    // part of the token ring from the gossip point of view and operates in
    // normal status or was in normal status but is shutdown.
public:
public:
private:
    uint64_t _nr_run = 0;
    uint64_t _msg_processing = 0;
    bool _gossip_settled = false;
    class msg_proc_guard;
private:
    abort_source& _abort_source;
    feature_service& _feature_service;
    const locator::shared_token_metadata& _shared_token_metadata;
    netw::messaging_service& _messaging;
    sharded<db::system_keyspace>& _sys_ks;
    utils::updateable_value<uint32_t> _failure_detector_timeout_ms;
    utils::updateable_value<int32_t> _force_gossip_generation;
    gossip_config _gcfg;
    // Get features supported by a particular node
    // Get features supported by all the nodes this node knows about
public:
private:
    seastar::metrics::metric_groups _metrics;
public:
public:
private:
};
struct gossip_get_endpoint_states_request {
    // Application states the sender requested
    std::unordered_set<gms::application_state> application_states;
};
struct gossip_get_endpoint_states_response {
    std::unordered_map<gms::inet_address, gms::endpoint_state> endpoint_state_map;
};
} // namespace gms
namespace utils {
template<uint64_t Min, uint64_t Max, size_t Precision>
requires (Min >= Precision && Min < Max && log2floor(Max) == log2ceil(Max) && log2floor(Min) == log2ceil(Min) && log2floor(Precision) == log2ceil(Precision))
class approx_exponential_histogram {
public:
    static constexpr unsigned NUM_EXP_RANGES = log2floor(Max/Min);
    static constexpr size_t NUM_BUCKETS = NUM_EXP_RANGES * Precision + 1;
    static constexpr unsigned PRECISION_BITS = log2floor(Precision);
    static constexpr unsigned BASESHIFT = log2floor(Min);
    static constexpr uint64_t LOWER_BITS_MASK = Precision - 1;
private:
    std::array<uint64_t, NUM_BUCKETS> _buckets;
public:
    ;
};
 ;
class time_estimated_histogram : public approx_exponential_histogram<512, 33554432, 4> {
public:
    using clock = std::chrono::steady_clock;
    using duration = clock::duration;
};
struct estimated_histogram {
    using clock = std::chrono::steady_clock;
    using duration = clock::duration;
    std::vector<int64_t> bucket_offsets;
    // buckets is one element longer than bucketOffsets -- the last element is values greater than the last offset
    std::vector<int64_t> buckets;
    int64_t _count = 0;
    int64_t _sample_sum = 0;
    estimated_histogram(int bucket_count = 90) ;
    seastar::metrics::histogram get_histogram(size_t lower_bucket = 1, size_t max_buckets = 16) const ;
    seastar::metrics::histogram get_histogram(duration minmal_latency, size_t max_buckets = 16) const ;
private:
    void new_offsets(int size) ;
public:
    
};
}
namespace utils {
class latency_counter {
public:
    using clock = std::chrono::steady_clock;
    using time_point = clock::time_point;
    using duration = clock::duration;
private:
    time_point _start;
    time_point _stop;
public:
    
    latency_counter& check_and_stop() ;
    static time_point now() ;
};
}
namespace utils {
class moving_average {
    double _alpha = 0;
    bool _initialized = false;
    latency_counter::duration _tick_interval;
    uint64_t _count = 0;
    double _rate = 0;
public:
    moving_average(latency_counter::duration interval, latency_counter::duration tick_interval) :
        _tick_interval(tick_interval) {
        _alpha = 1 - std::exp(-std::chrono::duration_cast<std::chrono::seconds>(tick_interval).count()/
                static_cast<double>(std::chrono::duration_cast<std::chrono::seconds>(interval).count()));
    }
    void add(uint64_t val = 1) ;
    
    
    double rate() const ;
};
template <typename Unit>
class basic_ihistogram {
public:
    using duration_unit = Unit;
    // count holds all the events
    int64_t count;
    // total holds only the events we sample
    int64_t total;
    int64_t min;
    int64_t max;
    int64_t sum;
    int64_t started;
    double mean;
    double variance;
    int64_t sample_mask;
    boost::circular_buffer<int64_t> sample;
    basic_ihistogram(size_t size = 1024, int64_t _sample_mask = 0x80)
            : count(0), total(0), min(0), max(0), sum(0), started(0), mean(0), variance(0),
              sample_mask(_sample_mask), sample(
                    size) {
    }
     ;
    
    ;
};
 ;
using ihistogram = basic_ihistogram<std::chrono::microseconds>;
class meter_timer {
    std::function<void()> _fun;
    timer<> _timer;
public:
    static constexpr latency_counter::duration tick_interval() ;
    meter_timer(std::function<void()>&& fun) : _fun(std::move(fun)), _timer(_fun) {
        _timer.arm_periodic(tick_interval());
    }
};
struct rate_moving_average {
    uint64_t count = 0;
    double rates[3] = {0};
    double mean_rate = 0;
    
    
};
 
class rates_moving_average {
    latency_counter::time_point start_time;
    moving_average rates[3] = {{std::chrono::minutes(1), meter_timer::tick_interval()}, {std::chrono::minutes(5), meter_timer::tick_interval()}, {std::chrono::minutes(15), meter_timer::tick_interval()}};
public:
    // _count is public so the collectd will be able to use it.
    // for all other cases use the count() method
    uint64_t _count = 0;
    
    
    rate_moving_average rate() const {
        rate_moving_average res;
        double elapsed = std::chrono::duration_cast<std::chrono::seconds>(latency_counter::now() - start_time).count();
        // We condition also in elapsed because it can happen that the call
        // for the rate calculation was performed too early and will not yield
        // meaningful results (i.e mean_rate is infinity) so the best thing is
        // to return 0 as it best reflects the state.
        if ((_count > 0) && (elapsed >= 1.0)) [[likely]] {
            res.mean_rate = (_count / elapsed);
        } else {
            res.mean_rate = 0;
        }
        res.count = _count;
        for (int i = 0; i < 3; i++) {
            res.rates[i] = rates[i].rate();
        }
        return res;
    }
    void update() noexcept ;
    uint64_t count() const ;
};
class timed_rate_moving_average {
    rates_moving_average _rates;
    meter_timer _timer;
public:
    timed_rate_moving_average() : _timer([this]{_rates.update();}) {
    }
    
    
    
    
    rate_moving_average rate() const noexcept ;
};
class summary_calculator {
    std::vector<double> _quantiles = { 0.5, 0.95, 0.99};
    std::vector<double> _summary = { 0, 0, 0};
    time_estimated_histogram _previous_histogram;
    time_estimated_histogram _current_histogram;
public:
    void update() ;
     ;
};
struct rate_moving_average_and_histogram {
    ihistogram hist;
    rate_moving_average rate;
};
class timed_rate_moving_average_and_histogram {
public:
    ihistogram hist;
    timed_rate_moving_average met;
     ;
};
class timed_rate_moving_average_summary_and_histogram {
    meter_timer _timer;
    summary_calculator _summary;
    rates_moving_average _rates;
    size_t _match_duration = 0;
    size_t _last_update = 0;
public:
    ihistogram hist;
    timed_rate_moving_average_summary_and_histogram(size_t size) : _timer([this]{
        _rates.update();
        _last_update++;
        if (_last_update < _match_duration) {
            return;
        }
        _last_update = 0;
        _summary.update();}), hist(size, 0) {
    }
    template <typename Rep, typename Ratio>
    void mark(std::chrono::duration<Rep, Ratio> dur) noexcept ;
    
};
}
 ;

namespace db {
class schema_ctxt;
}
// Transport for schema_ptr across shards/nodes.
// It's safe to access from another shard by const&.
class frozen_schema {
    bytes_ostream _data;
public:
    explicit frozen_schema(bytes_ostream);
    frozen_schema(const schema_ptr&);
    frozen_schema(frozen_schema&&) = default;
    frozen_schema(const frozen_schema&) = default;
    frozen_schema& operator=(const frozen_schema&) = default;
    frozen_schema& operator=(frozen_schema&&) = default;
    schema_ptr unfreeze(const db::schema_ctxt&) const;
    const bytes_ostream& representation() const;
};
namespace ser {
template <>
struct serializer<canonical_mutation> {
  ;
  ;
  ;
};
template <>
struct serializer<const canonical_mutation> : public serializer<canonical_mutation>
{};
template <>
struct serializer<schema_mutations> {
  ;
  template <typename Input>
  static schema_mutations read(Input& buf);
  template <typename Input>
  static void skip(Input& buf);
};
template <>
struct serializer<const schema_mutations> : public serializer<schema_mutations>
{};
template <>
struct serializer<frozen_schema> {
  ;
  ;
  template <typename Input>
  static void skip(Input& buf);
};
template <>
struct serializer<const frozen_schema> : public serializer<frozen_schema>
{};
} // ser
namespace ser {
struct schema_view {
    utils::input_stream v;
    auto version() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<table_schema_version>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       return deserialize(in, boost::type<table_schema_version>());
      });
    }
    auto mutations() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<schema_mutations>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       ser::skip(in, boost::type<table_schema_version>());
       return deserialize(in, boost::type<schema_mutations>());
      });
    }
};
template<>
struct serializer<schema_view> {
    template<typename Input>
    static schema_view read(Input& v) ;
     ;
     ;
};
////// State holders
template<typename Output>
struct state_of_schema {
    frame<Output> f;
};
////// Nodes
template<typename Output>
struct after_schema__mutations {
    Output& _out;
    state_of_schema<Output> _state;
    void  end_schema() ;
};
template<typename Output>
struct after_schema__version {
    Output& _out;
    state_of_schema<Output> _state;
    after_schema__mutations<Output> write_mutations(const schema_mutations& t) && ;
};
template<typename Output>
struct writer_of_schema {
    Output& _out;
    state_of_schema<Output> _state;
    writer_of_schema(Output& out) 
            ;
    after_schema__version<Output> write_version(const table_schema_version& t) && ;
};
} // ser
namespace db {
class extensions;
class seed_provider_type;
class config;
namespace view {
class view_update_generator;
}
}
namespace gms {
class feature_service;
class inet_address;
}
extern logging::logger startlog;
class bad_configuration_error : public std::exception {};

class service_set {
public:
    
    ;
     ;
     ;
private:
    class impl;
    std::unique_ptr<impl> _impl;
};
class configurable {
public:
    // Hook to add command line options and/or add main config options
    ;
    // Called after command line is parsed and db/config populated.
    // Hooked config can for example take this oppurtunity to load any file(s).
    enum class system_state {
        started,
        stopped,
    };
    using notify_func = std::function<future<>(system_state)>;
    class notify_set {
    public:
    private:
        friend class configurable;
        std::vector<notify_func> _listeners;
    };
    // visible for testing
    static std::vector<std::reference_wrapper<configurable>>& configurables();
private:
};
/// Implements <code>text LIKE pattern</code>.
///
/// The pattern is a string of characters with two wildcards:
/// - '_' matches any single character
/// - '%' matches any substring (including an empty string)
/// - '\' escapes the next pattern character, so it matches verbatim
/// - any other pattern character matches itself
///
/// The whole text must match the pattern; thus <code>'abc' LIKE 'a'</code> doesn't match, but
/// <code>'abc' LIKE 'a%'</code> matches.
class like_matcher {
    class impl;
    std::unique_ptr<impl> _impl;
public:
    /// Compiles \c pattern and stores the result.
    ///
    /// \param pattern UTF-8 encoded pattern with wildcards '_' and '%'.
    like_matcher(like_matcher&&) noexcept; // Must be defined in .cc, where class impl is known.
    ~like_matcher();
    /// Runs the compiled pattern on \c text.
    ///
    /// \return true iff text matches constructor's pattern.
    
    /// Resets pattern if different from the current one.
    
};
namespace seastar {
class data_source;
}
/// \brief Creates an data_source from another data_source but returns its data in chunks not bigger than a given limit
///
/// \param src Source data_source from which data will be taken
/// \return resulting data_source that returns data in chunks not bigger than a given limit
seastar::data_source make_limiting_data_source(seastar::data_source&& src,
                                               seastar::noncopyable_function<size_t()>&& limit_generator);
namespace dht {
class murmur3_partitioner final : public i_partitioner {
public:
    murmur3_partitioner() = default;
    virtual const sstring name() const override { return "org.apache.cassandra.dht.Murmur3Partitioner"; }
    virtual token get_token(const schema& s, partition_key_view key) const override;
    virtual token get_token(const sstables::key_view& key) const override;
private:
    token get_token(bytes_view key) const;
    token get_token(uint64_t value) const;
};
}
// is_evictable::yes means that the object is part of an evictable snapshots in MVCC,
// and non-evictable one otherwise.
// See docs/dev/mvcc.md for more details.
using is_evictable = bool_class<class evictable_tag>;
// Represents a set of writes made to a single partition.
//
// Like mutation_partition, but intended to be used in cache/memtable
// so the tradeoffs are different. This representation must be memory-efficient
// and must support incremental eviction of its contents. It is used in MVCC so
// algorithms for merging must respect MVCC invariants. See docs/dev/mvcc.md.
//
// The object is schema-dependent. Each instance is governed by some
// specific schema version. Accessors require a reference to the schema object
// of that version.
//
// There is an operation of addition defined on mutation_partition objects
// (also called "apply"), which gives as a result an object representing the
// sum of writes contained in the addends. For instances governed by the same
// schema, addition is commutative and associative.
//
// In addition to representing writes, the object supports specifying a set of
// partition elements called "continuity". This set can be used to represent
// lack of information about certain parts of the partition. It can be
// specified which ranges of clustering keys belong to that set. We say that a
// key range is continuous if all keys in that range belong to the continuity
// set, and discontinuous otherwise. By default everything is continuous.
// The static row may be also continuous or not.
// Partition tombstone is always continuous.
//
// Continuity is ignored by instance equality. It's also transient, not
// preserved by serialization.
//
// Continuity is represented internally using flags on row entries. The key
// range between two consecutive entries (both ends exclusive) is continuous
// if and only if rows_entry::continuous() is true for the later entry. The
// range starting after the last entry is assumed to be continuous. The range
// corresponding to the key of the entry is continuous if and only if
// rows_entry::dummy() is false.
//
// Adding two fully-continuous instances gives a fully-continuous instance.
// Continuity doesn't affect how the write part is added.
//
// Addition of continuity is not commutative in general, but is associative.
// The default continuity merging rules are those required by MVCC to
// preserve its invariants. For details, refer to "Continuity merging rules" section
// in the doc in partition_version.hh.
class mutation_partition_v2 final {
public:
    using rows_type = rows_entry::container_type;
    friend class size_calculator;
private:
    tombstone _tombstone;
    lazy_row _static_row;
    bool _static_row_continuous = true;
    rows_type _rows;
#ifdef SEASTAR_DEBUG
    table_schema_version _schema_version;
#endif
    friend class converting_mutation_partition_applier;
public:
    struct copy_comparators_only {};
    struct incomplete_tag {};
    // Constructs an empty instance which is fully discontinuous except for the partition tombstone.
    mutation_partition_v2(incomplete_tag, const schema& s, tombstone);
    
    
    // Assumes that p is fully continuous.
    // Assumes that p is fully continuous.
    // Consistent with equal()
     ;
    class printer {
        const schema& _schema;
        const mutation_partition_v2& _mutation_partition;
    public:
    };
public:
    // Makes sure there is a dummy entry after all clustered rows. Doesn't affect continuity.
    // Doesn't invalidate iterators.
    // Sets or clears continuity of clustering ranges between existing rows.
    // Returns clustering row ranges which have continuity matching the is_continuous argument.
    // Returns true iff all keys from given range are marked as continuous, or range is empty.
    // Returns true iff all keys from given range are marked as not continuous and range is not empty.
    // Returns true iff all keys from given range have continuity membership as specified by is_continuous.
    // Frees elements of the partition in batches.
    // Returns stop_iteration::yes iff there are no more elements to free.
    // Continuity is unspecified after this.
    // Applies mutation_fragment.
    // The fragment must be goverened by the same schema as this object.
    // Equivalent to applying a mutation with an empty row, created with given timestamp
    // prefix must not be full
    //
    // Applies p to current object.
    //
    // Commutative when this_schema == p_schema. If schemas differ, data in p which
    // is not representable in this_schema is dropped, thus apply() loses commutativity.
    //
    // Weak exception guarantees.
    // Use in case this instance and p share the same schema.
    // Same guarantees as apply(const schema&, mutation_partition_v2&&, const schema&);
    // Applies p to this instance.
    //
    // Monotonic exception guarantees. In case of exception the sum of p and this remains the same as before the exception.
    // This instance and p are governed by the same schema.
    //
    // Must be provided with a pointer to the cache_tracker, which owns both this and p.
    //
    // Returns stop_iteration::no if the operation was preempted before finished, and stop_iteration::yes otherwise.
    // On preemption the sum of this and p stays the same (represents the same set of writes), and the state of this
    // object contains at least all the writes it contained before the call (monotonicity). It may contain partial writes.
    // Also, some progress is always guaranteed (liveness).
    //
    // If returns stop_iteration::yes, then the sum of this and p is NO LONGER the same as before the call,
    // the state of p is undefined and should not be used for reading.
    //
    // The operation can be driven to completion like this:
    //
    //   apply_resume res;
    //   while (apply_monotonically(..., is_preemtable::yes, &res) == stop_iteration::no) { }
    //
    // If is_preemptible::no is passed as argument then stop_iteration::no is never returned.
    //
    // If is_preemptible::yes is passed, apply_resume must also be passed,
    // same instance each time until stop_iteration::yes is returned.
    // Weak exception guarantees.
    // Assumes this and p are not owned by a cache_tracker and non-evictable.
    // Converts partition to the new schema. When succeeds the partition should only be accessed
    // using the new schema.
    //
    // Strong exception guarantees.
    // Transforms this instance into a minimal one which still represents the same set of writes.
    // Does not garbage collect expired data, so the result is clock-independent and
    // should produce the same result on all replicas.
    // has_redundant_dummies(*this) is guaranteed to be false after this.
private:
    // Erases the entry if it's safe to do so without changing the logical state of the partition.
public:
    // Returns true if the mutation_partition_v2 represents no writes.
public:
    // Throws if the row already exists or if the row was not inserted to the
    // last position (one or more greater row already exists).
    // Weak exception guarantees.
public:
    // return a set of rows_entry where each entry represents a CQL row sharing the same clustering key.
    // Returns an iterator range of rows_entry, with only non-dummy entries.
private:
    ;
    friend class counter_write_query_result_builder;
};
// Returns true iff the mutation contains dummy rows which are redundant,
// meaning that they can be removed without affecting the set of writes represented by the mutation.
template<typename T>
class anchorless_list_base_hook {
    anchorless_list_base_hook<T>* _next = nullptr;
    anchorless_list_base_hook<T>* _prev = nullptr;
public:
    template <typename ValueType>
    class iterator {
        anchorless_list_base_hook<T>* _position;
    public:
        using iterator_category = std::bidirectional_iterator_tag;
        using value_type = ValueType;
        using difference_type = ssize_t;
        using pointer = ValueType*;
        using reference = ValueType&;
    public:
    };
    template <typename ValueType>
    class reverse_iterator {
        anchorless_list_base_hook<T>* _position;
    public:
        using iterator_category = std::forward_iterator_tag;
        using value_type = ValueType;
        using difference_type = ssize_t;
        using pointer = ValueType*;
        using reference = ValueType&;
    public:
    };
    class range {
        anchorless_list_base_hook<T>* _begin;
        anchorless_list_base_hook<T>* _end;
    public:
        using iterator = anchorless_list_base_hook::iterator<T>;
        using const_iterator = anchorless_list_base_hook::iterator<const T>;
    };
    class reversed_range {
        anchorless_list_base_hook<T>* _begin;
        anchorless_list_base_hook<T>* _end;
    public:
        using iterator = anchorless_list_base_hook::reverse_iterator<T>;
        using const_iterator = anchorless_list_base_hook::reverse_iterator<const T>;
    };
public:
    // Inserts this after elem.
    // Inserts the chain starting at head after this elem.
    // Assumes !head.prev() and !next().
    // Inserts this before elem.
    T* prev() const ;
    T* last() const ;
    
    
    
    
};
//  A movable pointer-like object paired with exactly one other object of the same type. 
//  The two objects which are paired with each other point at each other.
//  The relationship is symmetrical.
//
//  A pair of such objects can be used for implementing bi-directional traversal in data structures.
//
//  Moving this object automatically updates the other reference, so the references remain
//  consistent when the containing objects are managed by LSA.
//
//                get()
//          ------------------>
//   -----------             -----------
//  | entangled |~~~~~~~~~~~| entangled |
//   -----------             -----------
//          <------------------
//                get()
//
class entangled final {
    entangled* _ref = nullptr;
private:
    struct init_tag {};
    entangled(init_tag, entangled& other) {
        assert(!other._ref);
        _ref = &other;
        other._ref = this;
    }
public:
    // Creates a new object which is paired with a given "other".
    // The other is also paired with this object afterwards.
    // The other must not be paired before the call.
    static entangled make_paired_with(entangled& other) {
        return entangled(init_tag(), other);
    }
    // Creates an unpaired object.
    entangled() = default;
    ~entangled() {
        if (_ref) {
            _ref->_ref = nullptr;
        }
    }
    entangled* get() { return _ref; }
    explicit operator bool() const { return _ref != nullptr; }
    template<typename T>
    T* get(entangled T::* paired_member) {
        if (!_ref) {
            return nullptr;
        }
        return boost::intrusive::get_parent_from_member(get(), paired_member);
    }
    template<typename T>
    const T* get(entangled T::* paired_member) const ;
};
namespace logalloc {
struct occupancy_stats;
class region;
class region_impl;
class allocating_section;
constexpr int segment_size_shift = 17; // 128K; see #151, #152
constexpr size_t segment_size = 1 << segment_size_shift;
constexpr size_t max_zone_segments = 256;
//
// Frees some amount of objects from the region to which it's attached.
//
// This should eventually stop given no new objects are added:
//
//     while (eviction_fn() == memory::reclaiming_result::reclaimed_something) ;
//
using eviction_fn = std::function<memory::reclaiming_result()>;
// Listens for events from a region
class region_listener {
public:
    virtual ~region_listener();
    virtual void add(region* r) = 0;
    virtual void del(region* r) = 0;
    virtual void moved(region* old_address, region* new_address) = 0;
    virtual void increase_usage(region* r, ssize_t delta) = 0;
    virtual void decrease_evictable_usage(region* r) = 0;
    virtual void decrease_usage(region* r, ssize_t delta) = 0;
};
// Controller for all LSA regions. There's one per shard.
class tracker {
public:
    class impl;
    struct config {
        bool defragment_on_idle;
        bool abort_on_lsa_bad_alloc;
        bool sanitizer_report_backtrace = false; // Better reports but slower
        size_t lsa_reclamation_step;
        scheduling_group background_reclaim_sched_group;
    };
    struct stats {
        size_t segments_compacted;
        size_t lsa_buffer_segments;
        uint64_t memory_allocated;
        uint64_t memory_freed;
        uint64_t memory_compacted;
        uint64_t memory_evicted;
        friend stats operator+(const stats& s1, const stats& s2) ;
        
        
        
    };
    
    future<> stop();
private:
    std::unique_ptr<impl> _impl;
    memory::reclaimer _reclaimer;
    friend class region;
    friend class region_impl;
    memory::reclaiming_result reclaim(seastar::memory::reclaimer::request);
public:
    tracker();
    ~tracker();
    stats statistics() const;
    //
    // Tries to reclaim given amount of bytes in total using all compactible
    // and evictable regions. Returns the number of bytes actually reclaimed.
    // That value may be smaller than requested when evictable pools are empty
    // and compactible pools can't compact any more.
    //
    // Invalidates references to objects in all compactible and evictable regions.
    //
    size_t reclaim(size_t bytes);
    // Compacts as much as possible. Very expensive, mainly for testing.
    // Guarantees that every live object from reclaimable regions will be moved.
    // Invalidates references to objects in all compactible and evictable regions.
    void full_compaction();
    // Returns aggregate statistics for all pools.
    // Returns statistics for all segments allocated by LSA on this shard.
    // Returns amount of allocated memory not managed by LSA
    impl& get_impl() noexcept { return *_impl; }
    // Returns the minimum number of segments reclaimed during single reclamation cycle.
};
class tracker_reclaimer_lock {
    tracker::impl& _tracker_impl;
public:
    tracker_reclaimer_lock(tracker::impl& impl) noexcept;
    ~tracker_reclaimer_lock();
};
class segment_descriptor;
/// A unique pointer to a chunk of memory allocated inside an LSA region.
///
/// The pointer can be in disengaged state in which case it doesn't point at any buffer (nullptr state).
/// When the pointer points at some buffer, it is said to be engaged.
///
/// The pointer owns the object.
/// When the pointer is destroyed or it transitions from engaged to disengaged state, the buffer is freed.
/// The buffer is never leaked when operating by the API of lsa_buffer.
/// The pointer object can be safely destroyed in any allocator context.
///
/// The pointer object is never invalidated.
/// The pointed-to buffer can be moved around by LSA, so the pointer returned by get() can be
/// invalidated, but the pointer object itself is updated automatically and get() always returns
/// a pointer which is valid at the time of the call.
///
/// Must not outlive the region.
class lsa_buffer {
    friend class region_impl;
    entangled _link;           // Paired with segment_descriptor::_buf_pointers[...]
    segment_descriptor* _desc; // Valid only when engaged
    char* _buf = nullptr;      // Valid only when engaged
    size_t _size = 0;
public:
    using char_type = char;
    ;
    /// Makes this instance point to the buffer pointed to by the other pointer.
    /// If this pointer was engaged before, the owned buffer is freed.
    /// The other pointer will be in disengaged state after this.
    /// Disengages the pointer.
    /// If the pointer was engaged before, the owned buffer is freed.
    /// Postcondition: !bool(*this)
    /// Returns a pointer to the first element of the buffer.
    /// Valid only when engaged.
    /// Returns the number of bytes in the buffer.
    /// Returns true iff the pointer is engaged.
};
// Monoid representing pool occupancy statistics.
// Naturally ordered so that sparser pools come fist.
// All sizes in bytes.
class occupancy_stats {
    size_t _free_space;
    size_t _total_space;
public:
    occupancy_stats() noexcept : _free_space(0), _total_space(0) {}
    occupancy_stats(size_t free_space, size_t total_space) noexcept
        : _free_space(free_space), _total_space(total_space) { }
    bool operator<(const occupancy_stats& other) const noexcept {
        return used_fraction() < other.used_fraction();
    }
    friend occupancy_stats operator+(const occupancy_stats& s1, const occupancy_stats& s2) noexcept ;
    friend occupancy_stats operator-(const occupancy_stats& s1, const occupancy_stats& s2) noexcept ;
    occupancy_stats& operator+=(const occupancy_stats& other) noexcept {
        _total_space += other._total_space;
        _free_space += other._free_space;
        return *this;
    }
    occupancy_stats& operator-=(const occupancy_stats& other) noexcept {
        _total_space -= other._total_space;
        _free_space -= other._free_space;
        return *this;
    }
    size_t used_space() const noexcept {
        return _total_space - _free_space;
    }
    size_t free_space() const noexcept {
        return _free_space;
    }
    size_t total_space() const noexcept {
        return _total_space;
    }
    float used_fraction() const noexcept {
        return _total_space ? float(used_space()) / total_space() : 0;
    }
    explicit operator bool() const noexcept ;
};
class basic_region_impl : public allocation_strategy {
protected:
    tracker& _tracker;
    bool _reclaiming_enabled = true;
    seastar::shard_id _cpu = this_shard_id();
public:
    basic_region_impl(tracker& tracker) 
    ;
    bool reclaiming_enabled() const noexcept {
        return _reclaiming_enabled;
    }
};
//
// Log-structured allocator region.
//
// Objects allocated using this region are said to be owned by this region.
// Objects must be freed only using the region which owns them. Ownership can
// be transferred across regions using the merge() method. Region must be live
// as long as it owns any objects.
//
// Each region has separate memory accounting and can be compacted
// independently from other regions. To reclaim memory from all regions use
// shard_tracker().
//
// Region is automatically added to the set of
// compactible regions when constructed.
//
class region {
public:
    using impl = region_impl;
private:
    shared_ptr<basic_region_impl> _impl;
private:
public:
    
    
    // Allocates a buffer of a given size.
    // The buffer's pointer will be aligned to 4KB.
    // Note: it is wasteful to allocate buffers of sizes which are not a multiple of the alignment.
    // Merges another region into this region. The other region is left empty.
    // Doesn't invalidate references to allocated objects.
    // Compacts everything. Mainly for testing.
    // Invalidates references to allocated objects.
    // Runs eviction function once. Mainly for testing.
    // Changes the reclaimability state of this region. When region is not
    // reclaimable, it won't be considered by tracker::reclaim(). By default region is
    // reclaimable after construction.
    // Returns the reclaimability state of this region.
    // Returns a value which is increased when this region is either compacted or
    // evicted from, which invalidates references into the region.
    // When the value returned by this method doesn't change, references remain valid.
    // Will cause subsequent calls to evictable_occupancy() to report empty occupancy.
    // Follows region's occupancy in the parent region group. Less fine-grained than occupancy().
    // After ground_evictable_occupancy() is called returns 0.
    // Makes this region an evictable region. Supplied function will be called
    // when data from this region needs to be evicted in order to reclaim space.
    // The function should free some space from this region.
    friend class allocating_section;
};
// Forces references into the region to remain valid as long as this guard is
// live by disabling compaction and eviction.
// Can be nested.
struct reclaim_lock {
    region& _region;
    bool _prev;
};
// Utility for running critical sections which need to lock some region and
// also allocate LSA memory. The object learns from failures how much it
// should reserve up front in order to not cause allocation failures.
class allocating_section {
    // Do not decay below these minimal values
    static constexpr size_t s_min_lsa_reserve = 1;
    static constexpr size_t s_min_std_reserve = 1024;
    static constexpr uint64_t s_bytes_per_decay = 10'000'000'000;
    static constexpr unsigned s_segments_per_decay = 100'000;
    size_t _lsa_reserve = s_min_lsa_reserve; // in segments
    size_t _std_reserve = s_min_std_reserve; // in bytes
    size_t _minimum_lsa_emergency_reserve = 0;
    int64_t _remaining_std_bytes_until_decay = s_bytes_per_decay;
    int _remaining_lsa_segments_until_decay = s_segments_per_decay;
private:
    struct guard {
        tracker::impl& _tracker;
        size_t _prev;
    };
public:
    //
    // Reserves standard allocator and LSA memory for subsequent operations that
    // have to be performed with memory reclamation disabled.
    //
    // Throws std::bad_alloc when reserves can't be increased to a sufficient level.
    //
     ;
    //
    // Invokes func with reclaim_lock on region r. If LSA allocation fails
    // inside func it is retried after increasing LSA segment reserve. The
    // memory reserves are increased with region lock off allowing for memory
    // reclamation to take place in the region.
    //
    // References in the region are invalidated when allocating section is re-entered
    // on allocation failure.
    //
    // Throws std::bad_alloc when reserves can't be increased to a sufficient level.
    //
     ;
    //
    // Reserves standard allocator and LSA memory and
    // invokes func with reclaim_lock on region r. If LSA allocation fails
    // inside func it is retried after increasing LSA segment reserve. The
    // memory reserves are increased with region lock off allowing for memory
    // reclamation to take place in the region.
    //
    // References in the region are invalidated when allocating section is re-entered
    // on allocation failure.
    //
    // Throws std::bad_alloc when reserves can't be increased to a sufficient level.
    //
     ;
};
// Use the segment pool appropriate for the standard allocator.
//
// In debug mode, this will use the release standard allocator store.
// Call once, when initializing the application, before any LSA allocation takes place.
}
namespace utils {
// Represents a deferring operation which defers cooperatively with the caller.
//
// The operation is started and resumed by calling run(), which returns
// with stop_iteration::no whenever the operation defers and is not completed yet.
// When the operation is finally complete, run() returns with stop_iteration::yes.
// After that, run() should not be invoked any more.
//
// This allows the caller to:
//   1) execute some post-defer and pre-resume actions atomically
//   2) have control over when the operation is resumed and in which context,
//      in particular the caller can cancel the operation at deferring points.
//
// One simple way to drive the operation to completion:
//
//   coroutine c;
//   while (c.run() == stop_iteartion::no) {}
//
class coroutine final {
public:
    coroutine(noncopyable_function<stop_iteration()> f) : _run(std::move(f)) {}
    stop_iteration run() { return _run(); }
    explicit operator bool() const { return bool(_run); }
private:
    noncopyable_function<stop_iteration()> _run;
};
// Makes a coroutine which does nothing.
}
class static_row;
// This is MVCC implementation for mutation_partitions.
//
// See docs/dev/mvcc.md for important design information.
//
// It is assumed that mutation_partitions are stored in some sort of LSA-managed
// container (memtable or row cache).
//
// partition_entry - the main handle to the mutation_partition, allows writes
//                   and reads.
// partition_version - mutation_partition inside a list of partition versions.
//                     mutation_partition represents just a difference against
//                     the next one in the list. To get a single
//                     mutation_partition fully representing this version one
//                     needs to merge this one and all its successors in the
//                     list.
// partition_snapshot - a handle to some particular partition_version. It allows
//                      only reads and itself is immutable the partition version
//                      it represents won't be modified as long as the snapshot
//                      is alive.
//
// pe - partition_entry
// pv - partition_version
// ps - partition_snapshot
// ps(u) - partition_snapshot marked as unique owner
// Scene I. Write-only loads
//   pv
//   ^
//   |
//   pe
// In case of write-only loads all incoming mutations are directly applied
// to the partition_version that partition_entry is pointing to. The list
// of partition_versions contains only a single element.
//
// Scene II. Read-only loads
//   pv
//   ^
//   |
//   pe <- ps
// In case of read-only scenarios there is only a single partition_snapshot
// object that points to the partition_entry. There is only a single
// partition_version.
//
// Scene III. Writes and reads
//   pv -- pv -- pv
//   ^     ^     ^
//   |     |     |
//   pe    ps    ps
// If the partition_entry that needs to be modified is currently read from (i.e.
// there exist a partition_snapshot pointing to it) instead of applying new
// mutation directly a new partition version is created and added at the front
// of the list. partition_entry points to the new version (so that it has the
// most recent view of stored data) while the partition_snapshot points to the
// same partition_version it pointed to before (so that the data it sees doesn't
// change).
// As a result the list may contain multiple partition versions used by
// different partition snapshots.
// When the partition_snapshot is destroyed partition_versions are squashed
// together to minimize the amount of elements on the list.
//
// Scene IV. Schema upgrade
//   pv    pv --- pv
//   ^     ^      ^
//   |     |      |
//   pe    ps(u)  ps
// When there is a schema upgrade the list of partition versions pointed to
// by partition_entry is replaced by a new single partition_version that is a
// result of squashing and upgrading the old versions.
// Old versions not used by any partition snapshot are removed. The first
// partition snapshot on the list is marked as unique which means that upon
// its destruction it won't attempt to squash versions but instead remove
// the unused ones and pass the "unique owner" mark the next snapshot on the
// list (if there is any).
//
// Scene V. partition_entry eviction
//   pv
//   ^
//   |
//   ps(u)
// When partition_entry is removed (e.g. because it was evicted from cache)
// the partition versions are removed in a similar manner than in the schema
// upgrade scenario. The unused ones are destroyed right away and the first
// snapshot on the list is marked as unique owner so that on its destruction
// it continues removal of the partition versions.
class partition_version_ref;
class partition_version : public anchorless_list_base_hook<partition_version> {
    partition_version_ref* _backref = nullptr;
    mutation_partition_v2 _partition;
    friend class partition_version_ref;
    friend class partition_entry;
    friend class partition_snapshot;
public:
    // Frees elements of this version in batches.
    // Returns stop_iteration::yes iff there are no more elements to free.
    const mutation_partition_v2& partition() const ;
    bool is_referenced() const ;
    // Returns true iff this version is directly referenced from a partition_entry (is its newset version).
    
    
    
};
using partition_version_range = anchorless_list_base_hook<partition_version>::range;
using partition_version_reversed_range = anchorless_list_base_hook<partition_version>::reversed_range;
class partition_version_ref {
    partition_version* _version = nullptr;
    bool _unique_owner = false;
    friend class partition_version;
public:
    
    ~partition_version_ref() ;
    partition_version& operator*() ;
    const partition_version& operator*() const ;
    partition_version* operator->() ;
    const partition_version* operator->() const ;
    
    
};
class partition_entry;
class cache_tracker;
class mutation_cleaner;
static constexpr cache_tracker* no_cache_tracker = nullptr;
static constexpr mutation_cleaner* no_cleaner = nullptr;
class partition_snapshot : public enable_lw_shared_from_this<partition_snapshot> {
public:
    // Only snapshots created with the same value of phase can point to the same version.
    using phase_type = uint64_t;
    static constexpr phase_type default_phase = 0; // For use with non-evictable snapshots
    static constexpr phase_type min_phase = 1; // Use 1 to prevent underflow on apply_to_incomplete()
    static constexpr phase_type max_phase = std::numeric_limits<phase_type>::max();
    // Ordinal number of a partition version within a snapshot. Starts with 0.
    using version_number_type = size_t;
public:
    // Used for determining reference stability.
    // References and iterators into versions owned by the snapshot
    // obtained between two equal change_mark objects were produced
    // by that snapshot are guaranteed to be still valid.
    //
    // Has a null state which is != than anything returned by get_change_mark().
    class change_mark {
        uint64_t _reclaim_count = 0;
        size_t _versions_count = 0; // merge_partition_versions() removes versions on merge
    private:
        friend class partition_snapshot;
    public:
    };
private:
    schema_ptr _schema;
    // Either _version or _entry is non-null.
    partition_version_ref _version;
    partition_entry* _entry;
    phase_type _phase;
    logalloc::region* _region;
    mutation_cleaner* _cleaner;
    cache_tracker* _tracker;
    boost::intrusive::slist_member_hook<> _cleaner_hook;
    std::optional<apply_resume> _version_merging_state;
    bool _locked = false;
    friend class partition_entry;
    friend class mutation_cleaner_impl;
public:
    // Makes the snapshot locked.
    // See is_locked() for meaning.
    // Can be called only when at_lastest_version(). The snapshot must remain latest as long as it's locked.
    // Makes the snapshot no longer locked.
    // See is_locked() for meaning.
    // Tells whether the snapshot is locked.
    // Locking the snapshot prevents it from getting detached from the partition entry.
    // It also prevents the partition entry from being evicted.
    // Returns a reference to the partition_snapshot which is attached to given non-latest partition version.
    // Assumes !v.is_referenced_from_entry() && v.is_referenced().
    // If possible, merges the version pointed to by this snapshot with
    // adjacent partition versions. Leaves the snapshot in an unspecified state.
    // Can be retried if previous merge attempt has failed.
    // Prepares the snapshot for cleaning by moving to the right-most unreferenced version.
    // Returns stop_iteration::yes if there is nothing to merge with and the snapshot
    // should be collected right away, and stop_iteration::no otherwise.
    // When returns stop_iteration::no, the snapshots is guaranteed to not be attached
    // to the latest version.
    // Brings the snapshot to the front of the LRU.
    // Must be called after snapshot's original region is merged into a different region
    // before the original region is destroyed, unless the snapshot is destroyed earlier.
    using range_tombstone_result = utils::chunked_vector<range_tombstone>;
};
class partition_snapshot_ptr {
    lw_shared_ptr<partition_snapshot> _snp;
public:
    using value_type = partition_snapshot;
    partition_snapshot_ptr(lw_shared_ptr<partition_snapshot> snp) : _snp(std::move(snp)) {}
    ~partition_snapshot_ptr();
};
class real_dirty_memory_accounter;
// Represents mutation_partition with snapshotting support a la MVCC.
//
// Internally the state is represented by an ordered list of mutation_partition
// objects called versions. The logical mutation_partition state represented
// by that chain is equal to reducing the chain using mutation_partition::apply()
// from left (latest version) to right.
//
// We distinguish evictable and non-evictable partition entries. Entries which
// are non-evictable have all their elements non-evictable and fully continuous.
// Partition snapshots inherit evictability of the entry, which remains invariant
// for a snapshot.
//
// After evictable partition_entry is linked into a cache_tracker, that cache_tracker
// must always be passed to methods which accept a pointer to a cache_tracker.
// Also, evict() must be called before the entry is unlinked from a cache_tracker.
// For non-evictable entries, no_cache_tracker should be passed to methods which accept a cache_tracker.
//
// As long as an entry is linked to a cache_tracker, it must belong to a cache_entry.
// partition_version objects may be linked with a cache_tracker and detached from a cache_entry
// if owned by a snapshot.
//
class partition_entry {
    partition_snapshot* _snapshot = nullptr;
    partition_version_ref _version;
    friend class partition_snapshot;
    friend class cache_entry;
private:
    void set_version(partition_version*);
public:
    struct evictable_tag {};
    // Constructs a non-evictable entry holding empty partition
    partition_entry() = default;
    // Constructs a non-evictable entry
    explicit partition_entry(mutation_partition_v2);
    partition_entry(const schema&, mutation_partition);
    // Returns a reference to partition_entry containing given pv,
    // assuming pv.is_referenced_from_entry().
    
    // Constructs an evictable entry
    // Strong exception guarantees for the state of mp.
    
    ~partition_entry();
    // Frees elements of this entry in batches.
    // Active snapshots are detached, data referenced by them is not cleared.
    // Returns stop_iteration::yes iff there are no more elements to free.
    stop_iteration clear_gently(cache_tracker*) noexcept;
    static partition_entry make_evictable(const schema& s, mutation_partition&& mp);
    
    
    // Removes data contained by this entry, but not owned by snapshots.
    // Snapshots will be unlinked and evicted independently by reclaimer.
    // This entry is invalid after this and can only be destroyed.
    // Tells whether this entry is locked.
    // Locked entries are undergoing an update and should not have their snapshots
    // detached from the entry.
    // Certain methods can only be called when !is_locked().
    // Strong exception guarantees.
    // Assumes this instance and mp are fully continuous.
    // Use only on non-evictable entries.
    // Must not be called when is_locked().
    // Adds mutation_partition represented by "other" to the one represented
    // by this entry.
    // This entry must be evictable.
    //
    // The argument must be fully-continuous.
    //
    // The continuity of this entry remains unchanged. Information from "other"
    // which is incomplete in this instance is dropped. In other words, this
    // performs set intersection on continuity information, drops information
    // which falls outside of the continuity range, and applies regular merging
    // rules for the rest.
    //
    // Weak exception guarantees.
    // If an exception is thrown this and pe will be left in some valid states
    // such that if the operation is retried (possibly many times) and eventually
    // succeeds the result will be as if the first attempt didn't fail.
    //
    // The schema of pe must conform to s.
    //
    // Returns a coroutine object representing the operation.
    // The coroutine must be resumed with the region being unlocked.
    //
    // The coroutine cannot run concurrently with other apply() calls.
    // If this entry is evictable, cache_tracker must be provided.
    // Must not be called when is_locked().
    // Returns a reference to existing version with an active snapshot of given phase
    // or creates a new version and returns a reference to it.
    // Doesn't affect value or continuity of the partition.
    // needs to be called with reclaiming disabled
    // Must not be called when is_locked().
    // Snapshots with different values of phase will point to different partition_version objects.
    // When is_locked(), read() can only be called with a phase which is <= the phase of the current snapshot.
    class printer {
        const schema& _schema;
        const partition_entry& _partition_entry;
    public:
    };
};
// Monotonic exception guarantees
// Double-ended chained list of partition_version objects
// utilizing partition_version's intrinsic anchorless_list_base_hook.
class partition_version_list {
    // All references have unique_owner set to true
    // so that partition_version::is_referenced_from_entry() is false
    // so that versions owned here (by mutation_cleaner) are not evicted-from.
    partition_version_ref _head;
    partition_version_ref _tail; // nullptr means _tail == _head.
public:
    // Appends v to the tail of this deque.
    // The version must not be already referenced.
    // Returns a reference to the first version in this deque.
    // Call only if !empty().
    // Returns true iff contains any versions.
    // Detaches the first version from the list.
    // Assumes !empty().
    // Appends other to the tail of this deque.
    // The other deque will be left empty.
};
class mutation_cleaner;
class mutation_cleaner_impl final {
    using snapshot_list = boost::intrusive::slist<partition_snapshot,
        boost::intrusive::member_hook<partition_snapshot, boost::intrusive::slist_member_hook<>, &partition_snapshot::_cleaner_hook>,
                boost::intrusive::cache_last<true>>;
    struct worker {
        condition_variable cv;
        snapshot_list snapshots;
        logalloc::allocating_section alloc_section;
        bool done = false; // true means the worker was abandoned and cannot access the mutation_cleaner_impl instance.
    };
private:
    logalloc::region& _region;
    cache_tracker* _tracker;
    mutation_cleaner* _cleaner;
    partition_version_list _versions;
    lw_shared_ptr<worker> _worker_state;
    mutation_application_stats& _app_stats;
    seastar::scheduling_group _scheduling_group;
    std::function<void(size_t)> _on_space_freed;
private:
public:
};
// Container for garbage partition_version objects, used for freeing them incrementally.
//
// Mutation cleaner extends the lifetime of mutation_partition without doing
// the same for its schema. This means that the destruction of mutation_partition
// as well as any LSA migrators it may use cannot depend on the schema. Moreover,
// all used LSA migrators need remain alive and registered as long as
// mutation_cleaner is alive. In particular, this means that the instances of
// mutation_cleaner should not be thread local objects (or members of thread
// local objects).
class mutation_cleaner final {
    lw_shared_ptr<mutation_cleaner_impl> _impl;
public:
    // Frees some of the data. Returns stop_iteration::yes iff all was freed.
    // Must be invoked under owning allocator.
    // Must be invoked under owning allocator.
    // Must be invoked under owning allocator.
    // Returns a guard object which when freed calls the on_space_freed callback with the amount
    // of memory freed in the region in terms of total_space() during the time the guard was
    // alive.
    // The guard must not outlive the cleaner.
    // Enqueues v for destruction.
    // The object must not be part of any list, and must not be accessed externally any more.
    // In particular, it must not be attached, even indirectly, to any snapshot or partition_entry,
    // and must not be evicted from.
    // Must be invoked under owning allocator.
    // Destroys v now or later.
    // Same requirements as destroy_later().
    // Must be invoked under owning allocator.
    // Transfers objects from other to this.
    // This and other must belong to the same logalloc::region, and the same cache_tracker.
    // After the call other will refer to this cleaner.
    // Returns true iff contains no unfreed objects
    // Forces cleaning and returns a future which resolves when there is nothing to clean.
    // Will merge given snapshot using partition_snapshot::merge_partition_versions() and then destroys it
    // using destroy_from_this(), possibly deferring in between.
    // This instance becomes the sole owner of the partition_snapshot object, the caller should not destroy it
    // nor access it after calling this.
};
class atomic_cell;
class row_marker;
struct collection_mutation_description;
class compaction_garbage_collector {
public:
    virtual ~compaction_garbage_collector() = default;
    virtual void collect(column_id id, atomic_cell) = 0;
    virtual void collect(column_id id, collection_mutation_description) = 0;
    virtual void collect(row_marker) = 0;
};
enum class mutation_fragment_stream_validation_level {
    none, // disable validation altogether
    partition_region, // fragment kind
    token,
    partition_key,
    clustering_key,
};
/// Low level fragment stream validator.
///
/// Tracks and validates the monotonicity of the passed in fragment kinds,
/// position in partition, token or partition keys.
class mutation_fragment_stream_validator {
public:
    class validation_result {
        sstring _what;
    private:
        explicit validation_result() = default;
    public:
        const sstring& what() const ;
        explicit operator bool() const ;
        
    };
private:
    const ::schema& _schema;
    mutation_fragment_v2::kind _prev_kind;
    position_in_partition _prev_pos;
    dht::decorated_key _prev_partition_key;
    tombstone _current_tombstone;
private:
    
public:
    /// Validate the monotonicity of the fragment kind.
    ///
    /// Should be used when the full, more heavy-weight position-in-partition
    /// monotonicity validation provided by
    /// `operator()(const mutation_fragment&)` is not desired.
    /// Using both overloads for the same stream is not supported.
    /// Advances the previous fragment kind, but only if the validation passes.
    /// `new_current_tombstone` should be engaged only when the fragment changes
    /// the current tombstone (range tombstone change fragments).
    ///
    /// \returns true if the fragment kind is valid.
    /// Validates the monotonicity of the mutation fragment kind and position.
    ///
    /// Validates the mutation fragment kind monotonicity and
    /// position-in-partition.
    /// A more complete version of `operator()(mutation_fragment::kind)`.
    /// Using both overloads for the same stream is not supported.
    /// Advances the previous fragment kind and position-in-partition, but only
    /// if the validation passes.
    /// `new_current_tombstone` should be engaged only when the fragment changes
    /// the current tombstone (range tombstone change fragments).
    ///
    /// \returns true if the mutation fragment kind is valid.
    /// Validates the monotonicity of the mutation fragment.
    ///
    /// Equivalent to calling `operator()(mf.kind(), mf.position())`.
    /// See said overload for more details.
    ///
    /// \returns true if the mutation fragment kind is valid.
    /// Validates the monotonicity of the token.
    ///
    /// Does not check fragment level monotonicity.
    /// Advances the previous token, but only if the validation passes.
    /// Cannot be used in parallel with the `dht::decorated_key`
    /// overload.
    ///
    /// \returns true if the token is valid.
    /// Validates the monotonicity of the partition.
    ///
    /// Does not check fragment level monotonicity.
    /// Advances the previous partition-key, but only if the validation passes.
    /// Cannot be used in parallel with the `dht::token`
    /// overload.
    ///
    /// \returns true if the partition key is valid.
    /// Reset the state of the validator to the given partition
    ///
    /// Reset the state of the validator as if it has just validated a valid
    /// partition start with the provided key. This can be used t force a reset
    /// to a given partition that is normally invalid and hence wouldn't advance
    /// the internal state. This can be used by users that can correct such
    /// invalid streams and wish to continue validating it.
    /// Reset the state of the validator to the given fragment
    ///
    /// Reset the state of the validator as if it has just validated a valid
    /// fragment. This can be used t force a reset to a given fragment that is
    /// normally invalid and hence wouldn't advance the internal state. This
    /// can be used by users that can correct such invalid streams and wish to
    /// continue validating it.
    /// Validate that the stream was properly closed.
    ///
    /// \returns false if the last partition wasn't closed, i.e. the last
    /// fragment wasn't a `partition_end` fragment.
    /// The previous valid fragment kind.
    /// The previous valid position.
    ///
    /// Call only if operator()(position_in_partition_view) was used.
    /// Get the current effective tombstone
    ///
    /// Call only if operator()(mutation_fragment_v2) or
    /// operator()(mutation_fragment_v2::kind, position_in_partition_view, std::optional<tombstone>)
    /// was not used.
    /// The previous valid partition key.
    ///
    /// Call only if operator()(dht::token) or operator()(const dht::decorated_key&) was used.
    /// The previous valid partition key.
    ///
    /// Call only if operator()(const dht::decorated_key&) was used.
    const dht::decorated_key& previous_partition_key() const ;
};
struct invalid_mutation_fragment_stream : public std::runtime_error {
    explicit invalid_mutation_fragment_stream(std::runtime_error e);
};
/// Track position_in_partition transitions and validate monotonicity.
///
/// Will throw `invalid_mutation_fragment_stream` if any violation is found.
/// If the `abort_on_internal_error` configuration option is set, it will
/// abort instead.
/// Implements the FlattenedConsumerFilter concept.
class mutation_fragment_stream_validating_filter {
    mutation_fragment_stream_validator _validator;
    sstring _name_storage;
    std::string_view _name_view; // always valid
    mutation_fragment_stream_validation_level _validation_level;
private:
    
public:
    /// Constructor.
    ///
    /// \arg name is used in log messages to identify the validator, the
    ///     schema identity is added automatically
    /// \arg compare_keys enable validating clustering key monotonicity
    
    mutation_fragment_stream_validating_filter(const char* name, const schema& s, mutation_fragment_stream_validation_level level);
    mutation_fragment_stream_validating_filter(mutation_fragment_stream_validating_filter&&) = delete;
    mutation_fragment_stream_validating_filter(const mutation_fragment_stream_validating_filter&) = delete;
    sstring full_name() const;
    
    
    /// Equivalent to `operator()(mf.kind(), mf.position())`
    /// Equivalent to `operator()(partition_end{})`
    bool on_end_of_partition();
    void on_end_of_stream();
    mutation_fragment_stream_validator& validator() ;
};
static bool has_ck_selector(const query::clustering_row_ranges& ranges) ;
enum class compact_for_sstables {
    no,
    yes,
};
template<typename T>
concept CompactedFragmentsConsumerV2 = requires(T obj, tombstone t, const dht::decorated_key& dk, static_row sr,
        clustering_row cr, range_tombstone_change rtc, tombstone current_tombstone, row_tombstone current_row_tombstone, bool is_alive) {
    obj.consume_new_partition(dk);
    obj.consume(t);
    { obj.consume(std::move(sr), current_tombstone, is_alive) } -> std::same_as<stop_iteration>;
    { obj.consume(std::move(cr), current_row_tombstone, is_alive) } -> std::same_as<stop_iteration>;
    { obj.consume(std::move(rtc)) } -> std::same_as<stop_iteration>;
    { obj.consume_end_of_partition() } -> std::same_as<stop_iteration>;
    obj.consume_end_of_stream();
};
struct detached_compaction_state {
    ::partition_start partition_start;
    std::optional<::static_row> static_row;
    std::optional<range_tombstone_change> current_tombstone;
};
class noop_compacted_fragments_consumer {
public:
    void consume_new_partition(const dht::decorated_key& dk) ;
    void consume(tombstone t) ;
    stop_iteration consume(static_row&& sr, tombstone, bool) ;
    stop_iteration consume(clustering_row&& cr, row_tombstone, bool) ;
    stop_iteration consume(range_tombstone_change&& rtc) ;
    stop_iteration consume_end_of_partition() ;
    void consume_end_of_stream() ;
};
class mutation_compactor_garbage_collector : public compaction_garbage_collector {
    const schema& _schema;
    column_kind _kind;
    std::optional<clustering_key> _ckey;
    row_tombstone _tomb;
    row_marker _marker;
    row _row;
public:
    explicit mutation_compactor_garbage_collector(const schema& schema)  ;
    
    
     ;
     ;
};
struct compaction_stats {
    struct row_stats {
        uint64_t live = 0;
        uint64_t dead = 0;
    };
    uint64_t partitions = 0;
    row_stats static_rows;
    row_stats clustering_rows;
    uint64_t range_tombstones = 0;
};
template<compact_for_sstables SSTableCompaction>
class compact_mutation_state {
    const schema& _schema;
    gc_clock::time_point _query_time;
    std::function<api::timestamp_type(const dht::decorated_key&)> _get_max_purgeable;
    can_gc_fn _can_gc;
    api::timestamp_type _max_purgeable = api::missing_timestamp;
    std::optional<gc_clock::time_point> _gc_before;
    const query::partition_slice& _slice;
    uint64_t _row_limit{};
    uint32_t _partition_limit{};
    uint64_t _partition_row_limit{};
    tombstone_gc_state _tombstone_gc_state;
    tombstone _partition_tombstone;
    bool _static_row_live{};
    uint64_t _rows_in_current_partition;
    uint32_t _current_partition_limit;
    bool _empty_partition{};
    bool _empty_partition_in_gc_consumer{};
    const dht::decorated_key* _dk{};
    dht::decorated_key _last_dk;
    bool _return_static_content_on_partition_with_no_rows{};
    std::optional<static_row> _last_static_row;
    position_in_partition _last_pos;
    // Currently active tombstone, can be different than the tombstone emitted to
    // the regular consumer (_current_emitted_tombstone) because even purged
    // tombstone that are not emitted are still applied to data when compacting.
    tombstone _effective_tombstone;
    // Track last emitted tombstone to regular and gc consumers respectively.
    // Used to determine whether any active tombstones need closing at EOS.
    tombstone _current_emitted_tombstone;
    tombstone _current_emitted_gc_tombstone;
    std::unique_ptr<mutation_compactor_garbage_collector> _collector;
    compaction_stats _stats;
    mutation_fragment_stream_validating_filter _validator;
    // Remember if we requested to stop mid-partition.
    stop_iteration _stop = stop_iteration::no;
private:
     ;
    static constexpr bool sstable_compaction() {
        return SSTableCompaction == compact_for_sstables::yes;
    }
     ;
     ;
    ;
    ;
    bool can_gc(tombstone t) ;;
public:
    compact_mutation_state(compact_mutation_state&&) = delete; // Because 'this' is captured
    compact_mutation_state(const schema& s, gc_clock::time_point query_time, const query::partition_slice& slice, uint64_t limit,
              uint32_t partition_limit, mutation_fragment_stream_validation_level validation_level = mutation_fragment_stream_validation_level::token)
        : _schema(s)
        , _query_time(query_time)
        , _can_gc(always_gc)
        , _slice(slice)
        , _row_limit(limit)
        , _partition_limit(partition_limit)
        , _partition_row_limit(_slice.options.contains(query::partition_slice::option::distinct) ? 1 : slice.partition_row_limit())
        , _tombstone_gc_state(nullptr)
        , _last_dk({dht::token(), partition_key::make_empty()})
        , _last_pos(position_in_partition::for_partition_end())
        , _validator("mutation_compactor for read", _schema, validation_level)
    {
        static_assert(!sstable_compaction(), "This constructor cannot be used for sstable compaction.");
    }
    compact_mutation_state(const schema& s, gc_clock::time_point compaction_time,
            std::function<api::timestamp_type(const dht::decorated_key&)> get_max_purgeable,
            const tombstone_gc_state& gc_state)
        : _schema(s)
        , _query_time(compaction_time)
        , _get_max_purgeable(std::move(get_max_purgeable))
        , _can_gc([this] (tombstone t) { return can_gc(t); })
        , _slice(s.full_slice())
        , _tombstone_gc_state(gc_state)
        , _last_dk({dht::token(), partition_key::make_empty()})
        , _last_pos(position_in_partition::for_partition_end())
        , _collector(std::make_unique<mutation_compactor_garbage_collector>(_schema))
        // We already have a validator for compaction in the sstable writer, no need to validate twice
        , _validator("mutation_compactor for compaction", _schema, mutation_fragment_stream_validation_level::none)
    {
        static_assert(sstable_compaction(), "This constructor can only be used for sstable compaction.");
    }
    void consume_new_partition(const dht::decorated_key& dk) ;
    template <typename Consumer, typename GCConsumer>
    requires CompactedFragmentsConsumerV2<Consumer> && CompactedFragmentsConsumerV2<GCConsumer>
    void consume(tombstone t, Consumer& consumer, GCConsumer& gc_consumer) ;
    template <typename Consumer>
    requires CompactedFragmentsConsumerV2<Consumer>
    void force_partition_not_empty(Consumer& consumer) ;
    template <typename Consumer, typename GCConsumer>
    requires CompactedFragmentsConsumerV2<Consumer> && CompactedFragmentsConsumerV2<GCConsumer>
    stop_iteration consume(static_row&& sr, Consumer& consumer, GCConsumer& gc_consumer) ;
    template <typename Consumer, typename GCConsumer>
    requires CompactedFragmentsConsumerV2<Consumer> && CompactedFragmentsConsumerV2<GCConsumer>
    stop_iteration consume(clustering_row&& cr, Consumer& consumer, GCConsumer& gc_consumer) ;
    template <typename Consumer, typename GCConsumer>
    requires CompactedFragmentsConsumerV2<Consumer> && CompactedFragmentsConsumerV2<GCConsumer>
    stop_iteration consume(range_tombstone_change&& rtc, Consumer& consumer, GCConsumer& gc_consumer) ;
    template <typename Consumer, typename GCConsumer>
    requires CompactedFragmentsConsumerV2<Consumer> && CompactedFragmentsConsumerV2<GCConsumer>
    stop_iteration consume_end_of_partition(Consumer& consumer, GCConsumer& gc_consumer) ;
    template <typename Consumer, typename GCConsumer>
    requires CompactedFragmentsConsumerV2<Consumer> && CompactedFragmentsConsumerV2<GCConsumer>
    auto consume_end_of_stream(Consumer& consumer, GCConsumer& gc_consumer) {
        _validator.on_end_of_stream();
        if (_dk) {
            _last_dk = *_dk;
            _dk = &_last_dk;
        }
        if constexpr (std::is_same_v<std::result_of_t<decltype(&GCConsumer::consume_end_of_stream)(GCConsumer&)>, void>) {
            gc_consumer.consume_end_of_stream();
            return consumer.consume_end_of_stream();
        } else {
            return std::pair(consumer.consume_end_of_stream(), gc_consumer.consume_end_of_stream());
        }
    }
    /// The decorated key of the partition the compaction is positioned in.
    /// Can be null if the compaction wasn't started yet.
    const dht::decorated_key* current_partition() const ;
    // Only updated when SSTableCompaction == compact_for_sstables::no.
    // Only meaningful if compaction has started already (current_partition() != nullptr).
    position_in_partition_view current_position() const ;
    std::optional<full_position> current_full_position() const ;
    /// Reset limits and query-time to the new page's ones and re-emit the
    /// partition-header and static row if there are clustering rows or range
    /// tombstones left in the partition.
     ;
    
    /// Detach the internal state of the compactor
    ///
    /// The state is represented by the last seen partition header, static row
    /// and active range tombstones. Replaying these fragments through a new
    /// compactor will result in the new compactor being in the same state *this
    /// is (given the same outside parameters of course). Practically this
    /// allows the compaction state to be stored in the compacted reader.
    /// If the currently compacted partition is exhausted a disengaged optional
    /// is returned -- in this case there is no state to detach.
    std::optional<detached_compaction_state> detach_state() && ;
    const compaction_stats& stats() const ;
};
template<compact_for_sstables SSTableCompaction, typename Consumer, typename GCConsumer>
requires CompactedFragmentsConsumerV2<Consumer> && CompactedFragmentsConsumerV2<GCConsumer>
class compact_mutation_v2 {
    lw_shared_ptr<compact_mutation_state<SSTableCompaction>> _state;
    Consumer _consumer;
    // Garbage Collected Consumer
    GCConsumer _gc_consumer;
public:
    // Can only be used for compact_for_sstables::no
    compact_mutation_v2(const schema& s, gc_clock::time_point query_time, const query::partition_slice& slice, uint64_t limit,
              uint32_t partition_limit,
              Consumer consumer, GCConsumer gc_consumer = GCConsumer())
        : _state(make_lw_shared<compact_mutation_state<SSTableCompaction>>(s, query_time, slice, limit, partition_limit))
        , _consumer(std::move(consumer))
        , _gc_consumer(std::move(gc_consumer)) {
    }
    // Can only be used for compact_for_sstables::yes
    compact_mutation_v2(const schema& s, gc_clock::time_point compaction_time,
            std::function<api::timestamp_type(const dht::decorated_key&)> get_max_purgeable,
            const tombstone_gc_state& gc_state,
            Consumer consumer, GCConsumer gc_consumer = GCConsumer())
        : _state(make_lw_shared<compact_mutation_state<SSTableCompaction>>(s, compaction_time, get_max_purgeable, gc_state))
        , _consumer(std::move(consumer))
        , _gc_consumer(std::move(gc_consumer)) {
    }
    compact_mutation_v2(lw_shared_ptr<compact_mutation_state<SSTableCompaction>> state, Consumer consumer,
                     GCConsumer gc_consumer = GCConsumer())
        : _state(std::move(state))
        , _consumer(std::move(consumer))
        , _gc_consumer(std::move(gc_consumer)) {
    }
    void consume_new_partition(const dht::decorated_key& dk) {
        _state->consume_new_partition(dk);
    }
    void consume(tombstone t) {
        _state->consume(std::move(t), _consumer, _gc_consumer);
    }
    stop_iteration consume(static_row&& sr) {
        return _state->consume(std::move(sr), _consumer, _gc_consumer);
    }
    stop_iteration consume(clustering_row&& cr) {
        return _state->consume(std::move(cr), _consumer, _gc_consumer);
    }
    stop_iteration consume(range_tombstone_change&& rtc) {
        return _state->consume(std::move(rtc), _consumer, _gc_consumer);
    }
    stop_iteration consume_end_of_partition() {
        return _state->consume_end_of_partition(_consumer, _gc_consumer);
    }
    auto consume_end_of_stream() {
        return _state->consume_end_of_stream(_consumer, _gc_consumer);
    }
    lw_shared_ptr<compact_mutation_state<SSTableCompaction>> get_state() {
        return _state;
    }
};
template<typename Consumer>
requires CompactedFragmentsConsumerV2<Consumer>
struct compact_for_query_v2 : compact_mutation_v2<compact_for_sstables::no, Consumer, noop_compacted_fragments_consumer> {
    using compact_mutation_v2<compact_for_sstables::no, Consumer, noop_compacted_fragments_consumer>::compact_mutation_v2;
};
using compact_for_query_state_v2 = compact_mutation_state<compact_for_sstables::no>;
template<typename Consumer, typename GCConsumer = noop_compacted_fragments_consumer>
requires CompactedFragmentsConsumerV2<Consumer> && CompactedFragmentsConsumerV2<GCConsumer>
struct compact_for_compaction_v2 : compact_mutation_v2<compact_for_sstables::yes, Consumer, GCConsumer> {
    using compact_mutation_v2<compact_for_sstables::yes, Consumer, GCConsumer>::compact_mutation_v2;
};
/// Converts a stream of range_tombstone_change fragments to an equivalent stream of range_tombstone objects.
/// The input fragments must be ordered by their position().
/// The produced range_tombstone objects are non-overlapping and ordered by their position().
///
/// on_end_of_stream() must be called after consuming all fragments to produce the final fragment.
///
/// Example usage:
///
///   range_tombstone_assembler rta;
///   if (auto rt_opt = rta.consume(range_tombstone_change(...))) {
///       produce(*rt_opt);
///   }
///   if (auto rt_opt = rta.consume(range_tombstone_change(...))) {
///       produce(*rt_opt);
///   }
///   if (auto rt_opt = rta.flush(position_in_partition(...)) {
///       produce(*rt_opt);
///   }
///   rta.on_end_of_stream();
///
class range_tombstone_assembler {
    std::optional<range_tombstone_change> _prev_rt;
private:
    
public:
    
    std::optional<range_tombstone_change> get_range_tombstone_change() && ;
    void reset() ;
    std::optional<range_tombstone> consume(const schema& s, range_tombstone_change&& rt) ;
    void on_end_of_stream() ;
    // Returns true if and only if flush() may return something.
    // Returns false if flush() won't return anything for sure.
    bool needs_flush() const ;
    std::optional<range_tombstone> flush(const schema& s, position_in_partition_view pos) ;
    bool discardable() const ;
};
class mutation_rebuilder {
    schema_ptr _s;
    mutation_opt _m;
public:
    explicit mutation_rebuilder(schema_ptr s) : _s(std::move(s)) { }
    
    
};
// Builds the mutation corresponding to the next partition in the mutation fragment stream.
// Implements FlattenedConsumerV2, MutationFragmentConsumerV2 and FlatMutationReaderConsumerV2.
// Does not work with streams in streamed_mutation::forwarding::yes mode.
class mutation_rebuilder_v2 {
    schema_ptr _s;
    mutation_rebuilder _builder;
    range_tombstone_assembler _rt_assembler;
public:
    mutation_rebuilder_v2(schema_ptr s) : _s(std::move(s)), _builder(_s) { }
public:
    stop_iteration consume(partition_start mf) ;
    
    
public:
    void consume_new_partition(const dht::decorated_key& dk) ;
    stop_iteration consume(tombstone t) ;
    stop_iteration consume(range_tombstone_change&& rt) ;
    stop_iteration consume(static_row&& sr) ;
    stop_iteration consume(clustering_row&& cr) ;
    stop_iteration consume_end_of_partition() ;
    mutation_opt consume_end_of_stream() ;
};
namespace ser {
template<typename Output>
class writer_of_mutation_partition;
}
class mutation_partition_serializer {
    static size_t size(const schema&, const mutation_partition&);
public:
    using size_type = uint32_t;
private:
    const schema& _schema;
    const mutation_partition& _p;
private:
    ;
public:
    using count_type = uint32_t;
public:
};
class reconcilable_result;
class frozen_reconcilable_result;
class mutation_source;
// Can be read by other cores after publishing.
struct partition {
    uint32_t _row_count_low_bits;
    frozen_mutation _m; // FIXME: We don't need cf UUID, which frozen_mutation includes.
    uint32_t _row_count_high_bits;
    
    
    
    uint64_t row_count() const ;
    const frozen_mutation& mut() const ;
};
// The partitions held by this object are ordered according to dht::decorated_key ordering and non-overlapping.
// Each mutation must have different key.
//
// Can be read by other cores after publishing.
class reconcilable_result {
    uint32_t _row_count_low_bits;
    query::short_read _short_read;
    query::result_memory_tracker _memory_tracker;
    utils::chunked_vector<partition> _partitions;
    uint32_t _row_count_high_bits;
public:
    
    
    reconcilable_result(uint64_t row_count, utils::chunked_vector<partition> partitions, query::short_read short_read,
                        query::result_memory_tracker memory_tracker = { });
    const utils::chunked_vector<partition>& partitions() const;
    query::short_read is_short_read() const ;
    size_t memory_usage() const ;
    
    struct printer {
        const reconcilable_result& self;
        schema_ptr schema;
        
    };
    printer pretty_printer(schema_ptr) const;
};
class reconcilable_result_builder {
    const schema& _schema;
    const query::partition_slice& _slice;
    bool _reversed;
    bool _return_static_content_on_partition_with_no_rows{};
    bool _static_row_is_alive{};
    uint64_t _total_live_rows = 0;
    query::result_memory_accounter _memory_accounter;
    stop_iteration _stop;
    std::optional<streamed_mutation_freezer> _mutation_consumer;
    range_tombstone_assembler _rt_assembler;
    uint64_t _live_rows{};
    // make this the last member so it is destroyed first. #7240
    utils::chunked_vector<partition> _result;
private:
    stop_iteration consume(range_tombstone&& rt);
public:
    // Expects table schema (non-reversed) and half-reversed (legacy) slice when building results for reverse query.
    reconcilable_result_builder(const schema& s, const query::partition_slice& slice,
                                query::result_memory_accounter&& accounter) noexcept
        : _schema(s), _slice(slice), _reversed(_slice.options.contains(query::partition_slice::option::reversed))
        , _memory_accounter(std::move(accounter))
    { }
    void consume_new_partition(const dht::decorated_key& dk);
    void consume(tombstone t);
    stop_iteration consume(static_row&& sr, tombstone, bool is_alive);
    stop_iteration consume(clustering_row&& cr, row_tombstone, bool is_alive);
    stop_iteration consume(range_tombstone_change&& rtc);
    stop_iteration consume_end_of_partition();
    reconcilable_result consume_end_of_stream();
};

// Query the content of the mutation.
//
// The mutation is destroyed in the process, see `mutation::consume()`.

// Performs a query for counter updates.
// Partition visitor which builds mutation_partition corresponding to the data its fed with.
class partition_builder final : public mutation_partition_visitor {
private:
    const schema& _schema;
    mutation_partition& _partition;
    deletable_row* _current_row;
public:
    // @p will hold the result of building.
    // @p must be empty.
    void accept_row_cell(column_id id, atomic_cell&& cell) ;
    
};
//
// Fluent builder for query::partition_slice.
//
// Selects everything by default, unless restricted. Each property can be
// restricted separately. For example, by default all static columns are
// selected, but if with_static_column() is called then only that column will
// be included. Still, all regular columns and the whole clustering range will
// be selected (unless restricted).
//
class partition_slice_builder {
    std::optional<query::column_id_vector> _regular_columns;
    std::optional<query::column_id_vector> _static_columns;
    std::optional<std::vector<query::clustering_range>> _row_ranges;
    std::unique_ptr<query::specific_ranges> _specific_ranges;
    const schema& _schema;
    query::partition_slice::option_set _options;
    uint64_t _partition_row_limit = query::partition_max_rows;
public:
    partition_slice_builder(const schema& schema);
    
    partition_slice_builder& with_static_column(bytes name);
    partition_slice_builder& with_no_static_columns();
    partition_slice_builder& with_regular_column(bytes name);
    partition_slice_builder& with_no_regular_columns();
    partition_slice_builder& with_range(query::clustering_range range);
    partition_slice_builder& with_ranges(std::vector<query::clustering_range>);
    // noop if no ranges have been set yet
    partition_slice_builder& mutate_ranges(std::function<void(std::vector<query::clustering_range>&)>);
    // noop if no specific ranges have been set yet
    partition_slice_builder& mutate_specific_ranges(std::function<void(query::specific_ranges&)>);
    partition_slice_builder& without_partition_key_columns();
    partition_slice_builder& without_clustering_key_columns();
    partition_slice_builder& reversed();
    template <query::partition_slice::option OPTION>
    partition_slice_builder& with_option() ;
    template <query::partition_slice::option OPTION>
    partition_slice_builder& with_option_toggled() ;
    partition_slice_builder& with_partition_row_limit(uint64_t limit);
    query::partition_slice build();
};
namespace utils {
// Synchronizer which allows to track and wait for asynchronous operations
// which were in progress at the time of wait initiation.
class phased_barrier {
public:
    using phase_type = uint64_t;
private:
    using gate = seastar::gate;
    lw_shared_ptr<gate> _gate;
    phase_type _phase;
public:
    class operation {
        lw_shared_ptr<gate> _gate;
    public:
        operation(lw_shared_ptr<gate> g) : _gate(std::move(g)) {}
        operation(const operation&) = delete;
    };
    // Starts new operation. The operation ends when the "operation" object is destroyed.
    // The operation may last longer than the life time of the phased_barrier.
    // Starts a new phase and waits for all operations started in any of the earlier phases.
    // It is fine to start multiple awaits in parallel.
    // Cannot fail.
    // Returns current phase number. The smallest value returned is 0.
    // Number of operations in current phase.
};
}
namespace bplus {
enum class with_debug { no, yes };
enum class key_search { linear, binary, both };
template <typename Key, typename Less>
concept SimpleLessCompare = requires (Less l, Key k) {
    { l.simplify_key(k) } noexcept -> std::same_as<int64_t>;
};
template <typename Value, typename Less>
union maybe_key {
    Value v;
    template <typename L = Less>
    requires (!SimpleLessCompare<Value, L>)
    maybe_key() noexcept {}
    template <typename L = Less>
    requires (!SimpleLessCompare<Value, L>)
    void reset() noexcept { v.~Value(); }
    template <typename L = Less>
    requires (SimpleLessCompare<Value, L>)
    maybe_key() noexcept : v(utils::simple_key_unused_value) {}
    template <typename L = Less>
    requires (SimpleLessCompare<Value, L>)
    void reset() noexcept { v = utils::simple_key_unused_value; }
    ~maybe_key() {}
    maybe_key(const maybe_key&) = delete;
     ;
    
    template <typename... Args>
    void replace(Args&&... args) noexcept ;
    void replace(maybe_key&& other) = delete; // not to be called by chance
};
// For .{do_something_with_data}_and_dispose methods below
template <typename T>
void default_dispose(T* value) noexcept ;
template <typename Key>
requires std::is_nothrow_copy_constructible_v<Key>
Key copy_key(const Key& other) noexcept {
    return Key(other);
}
constexpr bool strict_separation_key = true;
template <typename Key, typename T, typename Less, size_t NodeSize> class validator;
template <with_debug Debug> class statistics;
template <typename Key, typename T, typename Less, size_t NodeSize, key_search Search, with_debug Debug> class node;
template <typename Key, typename T, typename Less, size_t NodeSize, key_search Search, with_debug Debug> class data;
template <typename T, typename Key>
concept CanGetKeyFromValue = requires (T val) {
    { val.key() } -> std::same_as<Key>;
};
struct stats {
    unsigned long nodes;
    std::vector<unsigned long> nodes_filled;
    unsigned long leaves;
    std::vector<unsigned long> leaves_filled;
    unsigned long datas;
};
template <typename Key, typename T, typename Less, size_t NodeSize,
            key_search Search = key_search::binary, with_debug Debug = with_debug::no>
requires LessNothrowComparable<Key, Key, Less> &&
        std::is_nothrow_move_constructible_v<Key> &&
        std::is_nothrow_move_constructible_v<T>
class tree {
public:
    class iterator;
    class const_iterator;
    friend class validator<Key, T, Less, NodeSize>;
    friend class node<Key, T, Less, NodeSize, Search, Debug>;
    // Sanity not to allow slow key-search in non-debug mode
    static_assert(Debug == with_debug::yes || Search != key_search::both);
    using node = class node<Key, T, Less, NodeSize, Search, Debug>;
    using data = class data<Key, T, Less, NodeSize, Search, Debug>;
    using kid_index = typename node::kid_index;
private:
    node* _root = nullptr;
    node* _left = nullptr;
    node* _right = nullptr;
    [[no_unique_address]] Less _less;
    template <typename K>
    node& find_leaf_for(const K& k) const noexcept {
        node* cur = _root;
        while (!cur->is_leaf()) {
            kid_index i = cur->index_for(k, _less);
            cur = cur->_kids[i].n;
        }
        return *cur;
    }
    void maybe_init_empty_tree() {
        if (_root != nullptr) {
            return;
        }
        node* n = node::create();
        n->_flags |= node::NODE_LEAF | node::NODE_ROOT | node::NODE_RIGHTMOST | node::NODE_LEFTMOST;
        do_set_root(n);
        do_set_left(n);
        do_set_right(n);
    }
    node* left_leaf_slow() const noexcept {
        node* cur = _root;
        while (!cur->is_leaf()) {
            cur = cur->_kids[0].n;
        }
        return cur;
    }
    node* right_leaf_slow() const noexcept {
        node* cur = _root;
        while (!cur->is_leaf()) {
            cur = cur->_kids[cur->_num_keys].n;
        }
        return cur;
    }
    template <typename K>
    requires LessNothrowComparable<K, Key, Less>
    const_iterator get_bound(const K& k, bool upper, bool& match) const noexcept {
        match = false;
        if (empty()) {
            return end();
        }
        node& n = find_leaf_for(k);
        kid_index i = n.index_for(k, _less);
        if (i == 0) {
            assert(n.is_leftmost());
            return begin();
        } else if (i <= n._num_keys) {
            const_iterator cur = const_iterator(n._kids[i].d, i);
            if (upper || _less(n._keys[i - 1].v, k)) {
                cur++;
            } else {
                match = true;
            }
            return cur;
        } else {
            assert(n.is_rightmost());
            return end();
        }
    }
    template <typename K>
    iterator get_bound(const K& k, bool upper, bool& match) noexcept {
        return iterator(const_cast<const tree*>(this)->get_bound(k, upper, match));
    }
public:
    tree(const tree& other) = delete;
    const tree& operator=(const tree& other) = delete;
    tree& operator=(tree&& other) = delete;
    explicit tree(Less less) noexcept : _less(less) { }
    ~tree() { clear(); }
    Less less() const noexcept { return _less; }
    tree(tree&& other) noexcept : _less(std::move(other._less)) {
        if (other._root) {
            do_set_root(other._root);
            do_set_left(other._left);
            do_set_right(other._right);
            other._root = nullptr;
            other._left = nullptr;
            other._right = nullptr;
        }
    }
    // XXX -- this uses linear scan over the leaf nodes
    size_t size_slow() const noexcept {
        if (_root == nullptr) {
            return 0;
        }
        size_t ret = 0;
        const node* leaf = _left;
        while (1) {
            assert(leaf->is_leaf());
            ret += leaf->_num_keys;
            if (leaf == _right) {
                break;
            }
            leaf = leaf->get_next();
        }
        return ret;
    }
    // Returns result that is equal (both not less than each other)
    template <typename K = Key>
    requires LessNothrowComparable<K, Key, Less>
    const_iterator find(const K& k) const noexcept {
        if (empty()) {
            return end();
        }
        node& n = find_leaf_for(k);
        kid_index i = n.index_for(k, _less);
        if (i >= 1 && !_less(n._keys[i - 1].v, k)) {
            return const_iterator(n._kids[i].d, i);
        } else {
            return end();
        }
    }
    template <typename K = Key>
    requires LessNothrowComparable<K, Key, Less>
    iterator find(const K& k) noexcept {
        return iterator(const_cast<const tree*>(this)->find(k));
    }
    // Returns the least x out of those !less(x, k)
    template <typename K = Key>
    iterator lower_bound(const K& k) noexcept {
        bool match;
        return get_bound(k, false, match);
    }
    template <typename K = Key>
    const_iterator lower_bound(const K& k) const noexcept {
        bool match;
        return get_bound(k, false, match);
    }
    template <typename K = Key>
    iterator lower_bound(const K& k, bool& match) noexcept {
        return get_bound(k, false, match);
    }
    template <typename K = Key>
    const_iterator lower_bound(const K& k, bool& match) const noexcept {
        return get_bound(k, false, match);
    }
    // Returns the least x out of those less(k, x)
    template <typename K = Key>
    iterator upper_bound(const K& k) noexcept {
        bool match;
        return get_bound(k, true, match);
    }
    template <typename K = Key>
    const_iterator upper_bound(const K& k) const noexcept {
        bool match;
        return get_bound(k, true, match);
    }
    template <typename... Args>
    std::pair<iterator, bool> emplace(Key k, Args&&... args) {
        maybe_init_empty_tree();
        node& n = find_leaf_for(k);
        kid_index i = n.index_for(k, _less);
        if (i >= 1 && !_less(n._keys[i - 1].v, k)) {
            // Direct hit
            return std::pair(iterator(n._kids[i].d, i), false);
        }
        data* d = data::create(std::forward<Args>(args)...);
        auto x = seastar::defer([&d] { data::destroy(*d, default_dispose<T>); });
        n.insert(i, std::move(k), d, _less);
        assert(d->attached());
        x.cancel();
        return std::pair(iterator(d, i + 1), true);
    }
    template <typename Func>
    requires Disposer<Func, T>
    iterator erase_and_dispose(const Key& k, Func&& disp) noexcept {
        maybe_init_empty_tree();
        node& n = find_leaf_for(k);
        data* d;
        kid_index i = n.index_for(k, _less);
        if (i == 0) {
            return end();
        }
        assert(n._num_keys > 0);
        if (_less(n._keys[i - 1].v, k)) {
            return end();
        }
        d = n._kids[i].d;
        iterator it(d, i);
        it++;
        n.remove(i, _less);
        data::destroy(*d, disp);
        return it;
    }
    template <typename Func>
    requires Disposer<Func, T>
    iterator erase_and_dispose(iterator from, iterator to, Func&& disp) noexcept {
        while (from != to) {
            from = from.erase_and_dispose(disp, _less);
        }
        return to;
    }
    template <typename... Args>
    iterator erase(Args&&... args) noexcept { return erase_and_dispose(std::forward<Args>(args)..., default_dispose<T>); }
    template <typename Func>
    requires Disposer<Func, T>
    void clear_and_dispose(Func&& disp) noexcept {
        if (_root != nullptr) {
            _root->clear(
                [&disp] (data* d) noexcept { data::destroy(*d, disp); },
                [] (node* n) noexcept { node::destroy(*n); }
            );
            node::destroy(*_root);
            _root = nullptr;
            _left = nullptr;
            _right = nullptr;
        }
    }
    void clear() noexcept { clear_and_dispose(default_dispose<T>); }
private:
    void do_set_left(node *n) noexcept {
        assert(n->is_leftmost());
        _left = n;
        n->_kids[0]._leftmost_tree = this;
    }
    void do_set_right(node *n) noexcept {
        assert(n->is_rightmost());
        _right = n;
        n->_rightmost_tree = this;
    }
    void do_set_root(node *n) noexcept {
        assert(n->is_root());
        n->_root_tree = this;
        _root = n;
    }
public:
    template <bool Const>
    class iterator_base {
    protected:
        using tree_ptr = std::conditional_t<Const, const tree*, tree*>;
        using data_ptr = std::conditional_t<Const, const data*, data*>;
        using node_ptr = std::conditional_t<Const, const node*, node*>;
        union {
            tree_ptr    _tree;
            data_ptr    _data;
        };
        kid_index _idx; // Index in leaf's _kids array pointing to _data
        static constexpr kid_index npos = 0;
        bool is_end() const noexcept { return _idx == npos; }
        explicit iterator_base(tree_ptr t) noexcept : _tree(t), _idx(npos) { }
        iterator_base(data_ptr d, kid_index idx) noexcept : _data(d), _idx(idx) {
            assert(!is_end());
        }
        iterator_base() noexcept : iterator_base(static_cast<tree_ptr>(nullptr)) {}
        node_ptr revalidate() noexcept {
            assert(!is_end());
            node_ptr leaf = _data->_leaf;
            if (_idx > leaf->_num_keys || leaf->_kids[_idx].d != _data) {
                _idx = leaf->index_for(_data);
            }
            return leaf;
        }
    public:
        using iterator_category = std::bidirectional_iterator_tag;
        using value_type = std::conditional_t<Const, const T, T>;
        using difference_type = ssize_t;
        using pointer = value_type*;
        using reference = value_type&;
        reference operator*() const noexcept { return _data->value; }
        pointer operator->() const noexcept { return &_data->value; }
        iterator_base& operator++() noexcept {
            node_ptr leaf = revalidate();
            if (_idx < leaf->_num_keys) {
                _idx++;
            } else {
                if (leaf->is_rightmost()) {
                    _idx = npos;
                    _tree = leaf->_rightmost_tree;
                    return *this;
                }
                leaf = leaf->get_next();
                _idx = 1;
            }
            _data = leaf->_kids[_idx].d;
            return *this;
        }
        iterator_base& operator--() noexcept {
            if (is_end()) {
                node* n = _tree->_right;
                assert(n->_num_keys > 0);
                _data = n->_kids[n->_num_keys].d;
                _idx = n->_num_keys;
                return *this;
            }
            node_ptr leaf = revalidate();
            if (_idx > 1) {
                _idx--;
            } else {
                leaf = leaf->get_prev();
                _idx = leaf->_num_keys;
            }
            _data = leaf->_kids[_idx].d;
            return *this;
        }
        iterator_base operator++(int) noexcept {
            iterator_base cur = *this;
            operator++();
            return cur;
        }
        iterator_base operator--(int) noexcept {
            iterator_base cur = *this;
            operator--();
            return cur;
        }
        bool operator==(const iterator_base& o) const noexcept { return is_end() ? o.is_end() : _data == o._data; }
    };
    using iterator_base_const = iterator_base<true>;
    using iterator_base_nonconst = iterator_base<false>;
    class const_iterator final : public iterator_base_const {
        friend class tree;
        using super = iterator_base_const;
        explicit const_iterator(const tree* t) noexcept : super(t) {}
        const_iterator(const data* d, kid_index idx) noexcept : super(d, idx) {}
    public:
        const_iterator() noexcept : super() {}
    };
    class iterator final : public iterator_base_nonconst {
        friend class tree;
        using super = iterator_base_nonconst;
        explicit iterator(tree* t) noexcept : super(t) {}
        iterator(data* d, kid_index idx) noexcept : super(d, idx) {}
    public:
        iterator(const const_iterator&& other) noexcept {
            if (other.is_end()) {
                super::_idx = super::npos;
                super::_tree = const_cast<tree *>(other._tree);
            } else {
                super::_idx = other._idx;
                super::_data = const_cast<data *>(other._data);
            }
        }
        iterator() noexcept : super() {}
        explicit iterator(T* value) noexcept
                : super(boost::intrusive::get_parent_from_member(value, &data::value), 1) {
            super::revalidate();
        }
        template <typename KeyFn, typename... Args>
        iterator emplace_before(KeyFn key, Less less, Args&&... args) {
            node* leaf;
            kid_index i;
            if (!super::is_end()) {
                leaf = super::revalidate();
                i = super::_idx - 1;
                if (i == 0 && !leaf->is_leftmost()) {
                    if (!strict_separation_key) {
                        assert(false && "Not implemented");
                    }
                    leaf = leaf->get_prev();
                    i = leaf->_num_keys;
                }
            } else {
                super::_tree->maybe_init_empty_tree();
                leaf = super::_tree->_right;
                i = leaf->_num_keys;
            }
            assert(i >= 0);
            data* d = data::create(std::forward<Args>(args)...);
            auto x = seastar::defer([&d] { data::destroy(*d, default_dispose<T>); });
            leaf->insert(i, std::move(key(d)), d, less);
            assert(d->attached());
            x.cancel();
            return iterator(d, i + 1);
        }
        template <typename... Args>
        iterator emplace_before(Key k, Less less, Args&&... args) {
            return emplace_before([&k] (data*) -> Key { return std::move(k); },
                    less, std::forward<Args>(args)...);
        }
        template <typename... Args>
        requires CanGetKeyFromValue<T, Key>
        iterator emplace_before(Less less, Args&&... args) {
            return emplace_before([] (data* d) -> Key { return d->value.key(); },
                    less, std::forward<Args>(args)...);
        }
    private:
        iterator next_after_erase(node* leaf) const noexcept {
            if (super::_idx < leaf->_num_keys) {
                return iterator(leaf->_kids[super::_idx + 1].d, super::_idx);
            }
            if (leaf->is_rightmost()) {
                return iterator(leaf->_rightmost_tree);
            }
            leaf = leaf->get_next();
            return iterator(leaf->_kids[1].d, 1);
        }
    public:
        template <typename Func>
        requires Disposer<Func, T>
        iterator erase_and_dispose(Func&& disp, Less less) noexcept {
            node* leaf = super::revalidate();
            iterator cur = next_after_erase(leaf);
            leaf->remove(super::_idx, less);
            data::destroy(*super::_data, disp);
            return cur;
        }
        iterator erase(Less less) { return erase_and_dispose(default_dispose<T>, less); }
        template <typename... Args>
        requires DynamicObject<T>
        void reconstruct(size_t new_payload_size, Args&&... args) {
            size_t new_size = super::_data->storage_size(new_payload_size);
            node* leaf = super::revalidate();
            auto ptr = current_allocator().alloc<data>(new_size);
            data *dat, *cur = super::_data;
            try {
                dat = new (ptr) data(std::forward<Args>(args)...);
            } catch(...) {
                current_allocator().free(ptr, new_size);
                throw;
            }
            dat->_leaf = leaf;
            cur->_leaf = nullptr;
            super::_data = dat;
            leaf->_kids[super::_idx].d = dat;
            current_allocator().destroy(cur);
        }
    };
    const_iterator begin() const noexcept {
        if (empty()) {
            return end();
        }
        assert(_left->_num_keys > 0);
        // Leaf nodes have data pointers starting from index 1
        return const_iterator(_left->_kids[1].d, 1);
    }
    const_iterator end() const noexcept { return const_iterator(this); }
    using const_reverse_iterator = std::reverse_iterator<const_iterator>;
    const_reverse_iterator rbegin() const noexcept { return std::make_reverse_iterator(end()); }
    const_reverse_iterator rend() const noexcept { return std::make_reverse_iterator(begin()); }
    iterator begin() noexcept { return iterator(const_cast<const tree*>(this)->begin()); }
    iterator end() noexcept { return iterator(this); }
    using reverse_iterator = std::reverse_iterator<iterator>;
    reverse_iterator rbegin() noexcept { return std::make_reverse_iterator(end()); }
    reverse_iterator rend() noexcept { return std::make_reverse_iterator(begin()); }
    bool empty() const noexcept { return _root == nullptr || _root->_num_keys == 0; }
    struct stats get_stats() const noexcept {
        struct stats st;
        st.nodes = 0;
        st.leaves = 0;
        st.datas = 0;
        if (_root != nullptr) {
            st.nodes_filled.resize(NodeSize + 1);
            st.leaves_filled.resize(NodeSize + 1);
            _root->fill_stats(st);
        }
        return st;
    }
};
template <typename K, typename Key, typename Less, size_t Size, key_search Search>
struct searcher { };
template <typename K, typename Key, typename Less, size_t Size>
struct searcher<K, Key, Less, Size, key_search::linear> {
    static size_t gt(const K& k, const maybe_key<Key, Less>* keys, size_t nr, Less less) noexcept ;;
};
template <typename K, typename Less, size_t Size>
requires SimpleLessCompare<K, Less>
struct searcher<K, int64_t, Less, Size, key_search::linear> {
    static_assert(sizeof(maybe_key<int64_t, Less>) == sizeof(int64_t));
    static size_t gt(const K& k, const maybe_key<int64_t, Less>* keys, size_t nr, Less less) noexcept {
        return utils::array_search_gt(less.simplify_key(k), reinterpret_cast<const int64_t*>(keys), Size, nr);
    }
};
template <typename K, typename Key, typename Less, size_t Size>
struct searcher<K, Key, Less, Size, key_search::binary> {
    
};
template <typename K, typename Key, typename Less, size_t Size>
struct searcher<K, Key, Less, Size, key_search::both> {
    
};
template <typename Key, typename T, typename Less, size_t NodeSize, key_search Search, with_debug Debug>
class node final {
    friend class validator<Key, T, Less, NodeSize>;
    friend class tree<Key, T, Less, NodeSize, Search, Debug>;
    friend class data<Key, T, Less, NodeSize, Search, Debug>;
    using tree = class tree<Key, T, Less, NodeSize, Search, Debug>;
    using data = class data<Key, T, Less, NodeSize, Search, Debug>;
    class prealloc;
    static constexpr size_t NodeHalf = ((NodeSize - 1) / 2);
    static_assert(NodeHalf >= 1);
    union node_or_data_or_tree {
        node* n;
        data* d;
        tree* _leftmost_tree; // See comment near node::__next about this
    };
    using node_or_data = node_or_data_or_tree;
    [[no_unique_address]] utils::neat_id<Debug == with_debug::yes> id;
    unsigned short _num_keys;
    unsigned short _flags;
    static const unsigned short NODE_ROOT       = 0x1;
    static const unsigned short NODE_LEAF       = 0x2;
    static const unsigned short NODE_LEFTMOST   = 0x4; // leaf with smallest keys in the tree
    static const unsigned short NODE_RIGHTMOST  = 0x8; // leaf with greatest keys in the tree
    bool is_leaf() const noexcept ;
    bool is_root() const noexcept ;
    bool is_rightmost() const noexcept ;
    bool is_leftmost() const noexcept ;
    maybe_key<Key, Less> _keys[NodeSize];
    node_or_data _kids[NodeSize + 1];
    // Type-aliases for code-reading convenience
    using key_index = size_t;
    using kid_index = size_t;
    union {
        node* _parent;
        tree* _root_tree;
    };
    union {
        node* __next;
        tree* _rightmost_tree;
    };
    node* get_next() const noexcept ;
    void set_next(node *n) noexcept ;
    node* get_prev() const noexcept ;
    void set_prev(node* n) noexcept ;
    // Links the new node n right after the current one
    void link(node& n) noexcept ;
    void unlink() noexcept ;
    node(const node& other) = delete;
     ;
    kid_index index_for(node *n) const noexcept ;
    bool need_refill() const noexcept ;
    bool can_grab_from() const noexcept ;
    
    
    void shift_right(size_t s) noexcept ;
    void shift_left(size_t s) noexcept ;
    // Helper for assert(). See comment for do_insert for details.
    template <typename DFunc, typename NFunc>
    requires Disposer<DFunc, data> && Disposer<NFunc, node>
    void clear(DFunc&& ddisp, NFunc&& ndisp) noexcept {
        if (is_leaf()) {
            _flags &= ~(node::NODE_LEFTMOST | node::NODE_RIGHTMOST);
            set_next(this);
            set_prev(this);
        } else {
            node* n = _kids[0].n;
            n->clear(ddisp, ndisp);
            ndisp(n);
        }
        for (key_index i = 0; i < _num_keys; i++) {
            _keys[i].reset();
            if (is_leaf()) {
                ddisp(_kids[i + 1].d);
            } else {
                node* n = _kids[i + 1].n;
                n->clear(ddisp, ndisp);
                ndisp(n);
            }
        }
        _num_keys = 0;
    }
    static node* create() {
        return current_allocator().construct<node>();
    }
    static void destroy(node& n) noexcept {
        current_allocator().destroy(&n);
    }
    void drop() noexcept {
        assert(!is_root());
        if (is_leaf()) {
            unlink();
        }
        destroy(*this);
    }
    void insert_into_full(kid_index idx, Key k, node_or_data nd, Less less, prealloc& nodes) noexcept {
        if (!is_root()) {
            node& p = *_parent;
            kid_index i = p.index_for(_keys[0].v, less);
            if (idx > 1 && i > 0) {
                node* left = p._kids[i - 1].n;
                if (left->can_push_to()) {
                    idx--;
                    left->grab_from_right(*this, p._keys[i - 1]);
                }
            }
            if (idx < _num_keys && i < p._num_keys) {
                node* right = p._kids[i + 1].n;
                if (right->can_push_to()) {
                    right->grab_from_left(*this, p._keys[i]);
                }
            }
            if (_num_keys < NodeSize) {
                do_insert(idx, std::move(k), nd, less);
                nodes.drain();
                return;
            }
        }
        split_and_insert(idx, std::move(k), nd, less, nodes);
    }
    void split_and_insert(kid_index idx, Key k, node_or_data nd, Less less, prealloc& nodes) noexcept {
        assert(_num_keys == NodeSize);
        node* nn = nodes.pop();
        maybe_key<Key, Less> sep;
        size_t off = NodeHalf + (idx > NodeHalf ? 1 : 0);
        if (is_leaf()) {
            nn->_flags |= NODE_LEAF;
            link(*nn);
            move_to(*nn, off);
            if (idx <= NodeHalf) {
                do_insert(idx, std::move(k), nd, less);
            } else {
                nn->do_insert(idx - off, std::move(k), nd, less);
            }
            sep.emplace(std::move(copy_key(nn->_keys[0].v)));
        } else {
            if (idx == NodeHalf + 1) {
                move_to(*nn, off);
                sep.emplace(std::move(k));
                nn->_kids[0] = nd;
                nn->_kids[0].n->_parent = nn;
            } else {
                move_to(*nn, off + 1);
                sep.emplace(std::move(_keys[off]));
                nn->_kids[0] = _kids[off + 1];
                nn->_kids[0].n->_parent = nn;
                _num_keys--;
                if (idx <= NodeHalf) {
                    do_insert(idx, std::move(k), nd, less);
                } else {
                    nd.n->_parent = nn;
                    nn->do_insert(idx - off - 1, std::move(k), nd, less);
                }
            }
        }
        assert(equally_split(*nn));
        if (is_root()) {
            insert_into_root(*nn, std::move(sep.v), nodes);
        } else {
            insert_into_parent(*nn, std::move(sep.v), less, nodes);
        }
        sep.reset();
    }
    void do_insert(kid_index i, Key k, node_or_data nd, Less less) noexcept {
        assert(_num_keys < NodeSize);
        assert(i != 0 || left_kid_sorted(k, less));
        shift_right(i);
        _keys[i].emplace(std::move(k));
        _kids[i + 1] = nd;
        if (is_leaf()) {
            nd.d->attach(*this);
        }
    }
    void insert_into_parent(node& nn, Key sep, Less less, prealloc& nodes) noexcept {
        nn._parent = _parent;
        _parent->insert_key(std::move(sep), node_or_data{.n = &nn}, less, nodes);
    }
    void insert_into_root(node& nn, Key sep, prealloc& nodes) noexcept {
        tree* t = _root_tree;
        node* nr = nodes.pop();
        nr->_num_keys = 1;
        nr->_keys[0].emplace(std::move(sep));
        nr->_kids[0].n = this;
        nr->_kids[1].n = &nn;
        _flags &= ~node::NODE_ROOT;
        _parent = nr;
        nn._parent = nr;
        nr->_flags |= node::NODE_ROOT;
        t->do_set_root(nr);
    }
    void insert_key(Key k, node_or_data nd, Less less, prealloc& nodes) noexcept {
        kid_index i = index_for(k, less);
        insert(i, std::move(k), nd, less, nodes);
    }
    void insert(kid_index i, Key k, node_or_data nd, Less less, prealloc& nodes) noexcept {
        if (_num_keys == NodeSize) {
            insert_into_full(i, std::move(k), nd, less, nodes);
        } else {
            do_insert(i, std::move(k), nd, less);
        }
    }
    void insert(kid_index i, Key k, data* d, Less less) {
        prealloc nodes;
        node* cur = this;
        while (cur->_num_keys == NodeSize) {
            nodes.push();
            if (cur->is_root()) {
                nodes.push();
                break;
            }
            cur = cur->_parent;
        }
        insert(i, std::move(k), node_or_data{.d = d}, less, nodes);
        assert(nodes.empty());
    }
    void remove_from(key_index i, Less less) noexcept {
        _keys[i].reset();
        shift_left(i);
        if (!is_root()) {
            if (need_refill()) {
                refill(less);
            }
        } else if (_num_keys == 0 && !is_leaf()) {
            node* nr;
            nr = _kids[0].n;
            nr->_flags |= node::NODE_ROOT;
            _root_tree->do_set_root(nr);
            _flags &= ~node::NODE_ROOT;
            _parent = nullptr;
            drop();
        }
    }
    void merge_kids(node& t, node& n, key_index sep_idx, Less less) noexcept {
        n.merge_into(t, std::move(_keys[sep_idx].v));
        n.drop();
        remove_from(sep_idx, less);
    }
    void refill(Less less) noexcept {
        node& p = *_parent, *left, *right;
        assert(_num_keys > 0);
        kid_index i = p.index_for(_keys[0].v, less);
        assert(p._kids[i].n == this);
        left = i > 0 ? p._kids[i - 1].n : nullptr;
        right = i < p._num_keys ? p._kids[i + 1].n : nullptr;
        if (left != nullptr && left->can_grab_from()) {
            grab_from_left(*left, p._keys[i - 1]);
            return;
        }
        if (right != nullptr && right->can_grab_from()) {
            grab_from_right(*right, p._keys[i]);
            return;
        }
        if (left != nullptr && can_merge_with(*left)) {
            p.merge_kids(*left, *this, i - 1, less);
            return;
        }
        if (right != nullptr && can_merge_with(*right)) {
            p.merge_kids(*this, *right, i, less);
            return;
        }
        assert(_num_keys > 1);
    }
    void remove(kid_index ki, Less less) noexcept {
        key_index i = ki - 1;
        if (strict_separation_key && i == 0 && !is_leftmost()) {
            const Key& k = _keys[i].v;
            node* p = this;
            while (!p->is_root()) {
                p = p->_parent;
                kid_index j = p->index_for(k, less);
                if (j > 0) {
                    p->_keys[j - 1].replace(copy_key(_keys[1].v));
                    break;
                }
            }
        }
        remove_from(i, less);
    }
public:
    explicit node() noexcept : _num_keys(0) , _flags(0) , _parent(nullptr) { }
    ~node() {
        assert(_num_keys == 0);
        assert(is_root() || !is_leaf() || (get_prev() == this && get_next() == this));
    }
    node(node&& other) noexcept : _flags(other._flags) {
        if (is_leaf()) {
            if (!is_rightmost()) {
                set_next(other.get_next());
                get_next()->set_prev(this);
            } else {
                other._rightmost_tree->do_set_right(this);
            }
            if (!is_leftmost()) {
                set_prev(other.get_prev());
                get_prev()->set_next(this);
            } else {
                other._kids[0]._leftmost_tree->do_set_left(this);
            }
            other._flags &= ~(NODE_LEFTMOST | NODE_RIGHTMOST);
            other.set_next(&other);
            other.set_prev(&other);
        } else {
            _kids[0].n = other._kids[0].n;
            _kids[0].n->_parent = this;
        }
        other.move_to(*this, 0);
        if (!is_root()) {
            _parent = other._parent;
            kid_index i = _parent->index_for(&other);
            assert(_parent->_kids[i].n == &other);
            _parent->_kids[i].n = this;
        } else {
            other._root_tree->do_set_root(this);
        }
    }
    kid_index index_for(const data *d) const noexcept {
        kid_index i;
        for (i = 1; i <= _num_keys; i++) {
            if (_kids[i].d == d) {
                break;
            }
        }
        assert(i <= _num_keys);
        return i;
    }
private:
    class prealloc {
        std::vector<node*> _nodes;
    public:
        bool empty() noexcept { return _nodes.empty(); }
        void push() {
            _nodes.push_back(node::create());
        }
        node* pop() noexcept {
            assert(!_nodes.empty());
            node* ret = _nodes.back();
            _nodes.pop_back();
            return ret;
        }
        void drain() noexcept {
            while (!empty()) {
                node::destroy(*pop());
            }
        }
        ~prealloc() {
            drain();
        }
    };
};
template <typename K, typename T, typename Less, size_t NS, key_search S, with_debug D>
class data final {
    friend class validator<K, T, Less, NS>;
    template <typename c1, typename c2, typename c3, size_t s1, key_search p1, with_debug p2>
            friend class tree<c1, c2, c3, s1, p1, p2>::iterator;
    template <typename c1, typename c2, typename c3, size_t s1, key_search p1, with_debug p2>
            friend class tree<c1, c2, c3, s1, p1, p2>::iterator_base_const;
    template <typename c1, typename c2, typename c3, size_t s1, key_search p1, with_debug p2>
            friend class tree<c1, c2, c3, s1, p1, p2>::iterator_base_nonconst;
    using node = class node<K, T, Less, NS, S, D>;
    node* _leaf;
    T value;
public:
     ;
    template <typename Func>
    requires Disposer<Func, T>
    static void destroy(data& d, Func&& disp) noexcept {
        disp(&d.value);
        d._leaf = nullptr;
        current_allocator().destroy(&d);
    }
    template <typename... Args>
    data(Args&& ... args) : _leaf(nullptr), value(std::forward<Args>(args)...) {}
    data(data&& other) noexcept : _leaf(other._leaf), value(std::move(other.value)) {
        if (attached()) {
            auto i = _leaf->index_for(&other);
            _leaf->_kids[i].d = this;
            other._leaf = nullptr;
        }
    }
    ~data() { assert(!attached()); }
    bool attached() const noexcept { return _leaf != nullptr; }
    void attach(node& to) noexcept {
        assert(!attached());
        _leaf = &to;
    }
    void reattach(node* to) noexcept {
        assert(attached());
        _leaf = to;
    }
private:
    // Data node may describe a T without fixed size, e.g. an array that grows on
    // demand. So this helper returns the size of the memory chunk that's required
    // to carry the node with T of the payload size on board.
    //
    // The tree::iterator::reconstruct does this growing (or shrinking).
    size_t storage_size(size_t payload) const noexcept {
        return sizeof(data) - sizeof(T) + payload;
    }
public:
    size_t storage_size() const noexcept {
        return storage_size(size_for_allocation_strategy(value));
    }
};
} // namespace bplus
template <typename T>
concept BoundsKeeper = requires (T val, bool bit) {
    { val.is_head() } noexcept -> std::same_as<bool>;
    { val.set_head(bit) } noexcept -> std::same_as<void>;
    { val.is_tail() } noexcept -> std::same_as<bool>;
    { val.set_tail(bit) } noexcept -> std::same_as<void>;
    { val.with_train() } noexcept -> std::same_as<bool>;
    { val.set_train(bit) } noexcept -> std::same_as<void>;
};
template <typename T>
requires BoundsKeeper<T> && std::is_nothrow_move_constructible_v<T>
class intrusive_array {
    // Sanity constant to avoid infinite loops searching for tail
    static constexpr int max_len = std::numeric_limits<short int>::max();
    union maybe_constructed {
        maybe_constructed() { }
        ~maybe_constructed() { }
        T object;
        unsigned short train_len;
        static_assert(sizeof(T) >= sizeof(unsigned short));
    };
    maybe_constructed   _data[1];
    size_t number_of_elements() const noexcept {
        for (int i = 0; i < max_len; i++) {
            if (_data[i].object.is_tail()) {
                return i + 1;
            }
        }
        std::abort();
    }
public:
    size_t storage_size() const noexcept {
        size_t nr = number_of_elements();
        if (_data[0].object.with_train()) {
            nr += _data[nr].train_len;
        }
        return nr * sizeof(T);
    }
    using iterator = T*;
    using const_iterator = const T*;
    // Initial
    template <typename... Args>
    intrusive_array(Args&&... args) {
        new (&_data[0].object) T(std::forward<Args>(args)...);
        _data[0].object.set_head(true);
        _data[0].object.set_tail(true);
    }
    // Growing
    struct grow_tag {
        int add_pos;
    };
    template <typename... Args>
    intrusive_array(intrusive_array& from, grow_tag grow, Args&&... args) {
        // The add_pos is strongly _expected_ to be within bounds
        int i, off = 0;
        bool tail = false;
        for (i = 0; !tail; i++) {
            if (i == grow.add_pos) {
                off = 1;
                continue;
            }
            tail = from._data[i - off].object.is_tail();
            new (&_data[i].object) T(std::move(from._data[i - off].object));
        }
        assert(grow.add_pos <= i && i < max_len);
        new (&_data[grow.add_pos].object) T(std::forward<Args>(args)...);
        _data[0].object.set_head(true);
        _data[0].object.set_train(false);
        if (grow.add_pos == 0) {
            _data[1].object.set_head(false);
        }
        _data[i - off].object.set_tail(true);
        if (off == 0) {
            _data[i - 1].object.set_tail(false);
        }
    }
    // Shrinking
    struct shrink_tag {
        int del_pos;
    };
    intrusive_array(intrusive_array& from, shrink_tag shrink) {
        int i, off = 0;
        bool tail = false;
        for (i = 0; !tail; i++) {
            tail = from._data[i].object.is_tail();
            if (i == shrink.del_pos) {
                off = 1;
            } else {
                new (&_data[i - off].object) T(std::move(from._data[i].object));
            }
        }
        _data[0].object.set_head(true);
        _data[0].object.set_train(false);
        _data[i - off - 1].object.set_tail(true);
    }
    intrusive_array(const intrusive_array& other) = delete;
    intrusive_array(intrusive_array&& other) noexcept {
        bool tail = false;
        int i;
        for (i = 0; !tail; i++) {
            tail = other._data[i].object.is_tail();
            new (&_data[i].object) T(std::move(other._data[i].object));
        }
        if (_data[0].object.with_train()) {
            _data[i].train_len = other._data[i].train_len;
        }
    }
    ~intrusive_array() {
        bool tail = false;
        for (int i = 0; !tail; i++) {
            tail = _data[i].object.is_tail();
            _data[i].object.~T();
        }
    }
    void erase(int pos) noexcept {
        assert(!is_single_element());
        assert(pos < max_len);
        bool with_train = _data[0].object.with_train();
        bool tail = _data[pos].object.is_tail();
        _data[pos].object.~T();
        if (tail) {
            assert(pos > 0);
            _data[pos - 1].object.set_tail(true);
        } else {
            while (!tail) {
                new (&_data[pos].object) T(std::move(_data[pos + 1].object));
                _data[pos + 1].object.~T();
                tail = _data[pos++].object.is_tail();
            }
            _data[0].object.set_head(true);
        }
        _data[0].object.set_train(true);
        unsigned short train_len = with_train ? _data[pos + 1].train_len : 0;
        assert(train_len < max_len);
        _data[pos].train_len = train_len + 1;
    }
    T& operator[](int pos) noexcept { return _data[pos].object; }
    const T& operator[](int pos) const noexcept { return _data[pos].object; }
    iterator begin() noexcept { return &_data[0].object; }
    const_iterator begin() const noexcept { return &_data[0].object; }
    const_iterator cbegin() const noexcept { return &_data[0].object; }
    iterator end() noexcept { return &_data[number_of_elements()].object; }
    const_iterator end() const noexcept { return &_data[number_of_elements()].object; }
    const_iterator cend() const noexcept { return &_data[number_of_elements()].object; }
    size_t index_of(iterator i) const noexcept { return i - &_data[0].object; }
    size_t index_of(const_iterator i) const noexcept { return i - &_data[0].object; }
    bool is_single_element() const noexcept { return _data[0].object.is_tail(); }
    // A helper for keeping the array sorted
    template <typename K, typename Compare>
    requires Comparable<K, T, Compare>
    const_iterator lower_bound(const K& val, Compare cmp, bool& match) const {
        int i = 0;
        do {
            auto x = cmp(_data[i].object, val);
            if (x >= 0) {
                match = (x == 0);
                break;
            }
        } while (!_data[i++].object.is_tail());
        return &_data[i].object;
    }
    template <typename K, typename Compare>
    requires Comparable<K, T, Compare>
    iterator lower_bound(const K& val, Compare cmp, bool& match) {
        return const_cast<iterator>(const_cast<const intrusive_array*>(this)->lower_bound(val, std::move(cmp), match));
    }
    template <typename K, typename Compare>
    requires Comparable<K, T, Compare>
    const_iterator lower_bound(const K& val, Compare cmp) const {
        bool match = false;
        return lower_bound(val, cmp, match);
    }
    template <typename K, typename Compare>
    requires Comparable<K, T, Compare>
    iterator lower_bound(const K& val, Compare cmp) {
        return const_cast<iterator>(const_cast<const intrusive_array*>(this)->lower_bound(val, std::move(cmp)));
    }
    // And its peer ... just to be used
    template <typename K, typename Compare>
    requires Comparable<K, T, Compare>
    const_iterator upper_bound(const K& val, Compare cmp) const {
        int i = 0;
        do {
            if (cmp(_data[i].object, val) > 0) {
                break;
            }
        } while (!_data[i++].object.is_tail());
        return &_data[i].object;
    }
    template <typename K, typename Compare>
    requires Comparable<K, T, Compare>
    iterator upper_bound(const K& val, Compare cmp) {
        return const_cast<iterator>(const_cast<const intrusive_array*>(this)->upper_bound(val, std::move(cmp)));
    }
    template <typename Func>
    requires Disposer<Func, T>
    void for_each(Func&& fn) noexcept {
        bool tail = false;
        for (int i = 0; !tail; i++) {
            tail = _data[i].object.is_tail();
            fn(&_data[i].object);
        }
    }
    size_t size() const noexcept { return number_of_elements(); }
    static intrusive_array& from_element(T* ptr, int& idx) noexcept {
        idx = 0;
        while (!ptr->is_head()) {
            assert(idx < max_len); // may the force be with us...
            idx++;
            ptr--;
        }
        static_assert(offsetof(intrusive_array, _data[0].object) == 0);
        return *reinterpret_cast<intrusive_array*>(ptr);
    }
};
template <typename Key, typename T, typename Less, typename Compare, int NodeSize,
            bplus::key_search Search = bplus::key_search::binary, bplus::with_debug Debug = bplus::with_debug::no>
requires Comparable<T, T, Compare> && std::is_nothrow_move_constructible_v<T>
class double_decker {
public:
    using inner_array = intrusive_array<T>;
    using outer_tree = bplus::tree<Key, inner_array, Less, NodeSize, Search, Debug>;
    using outer_iterator = typename outer_tree::iterator;
    using outer_const_iterator = typename outer_tree::const_iterator;
private:
    outer_tree  _tree;
public:
    template <bool Const>
    class iterator_base {
        friend class double_decker;
        using outer_iterator = std::conditional_t<Const, typename double_decker::outer_const_iterator, typename double_decker::outer_iterator>;
    protected:
        outer_iterator _bucket;
        int _idx;
    public:
        iterator_base() = default;
        iterator_base(outer_iterator bkt, int idx) noexcept : _bucket(bkt), _idx(idx) {}
        using iterator_category = std::bidirectional_iterator_tag;
        using difference_type = ssize_t;
        using value_type = std::conditional_t<Const, const T, T>;
        using pointer = value_type*;
        using reference = value_type&;
        reference operator*() const noexcept { return (*_bucket)[_idx]; }
        pointer operator->() const noexcept { return &((*_bucket)[_idx]); }
        iterator_base& operator++() noexcept {
            if ((*_bucket)[_idx++].is_tail()) {
                _bucket++;
                _idx = 0;
            }
            return *this;
        }
        iterator_base operator++(int) noexcept {
            iterator_base cur = *this;
            operator++();
            return cur;
        }
        iterator_base& operator--() noexcept {
            if (_idx-- == 0) {
                _bucket--;
                _idx = _bucket->index_of(_bucket->end()) - 1;
            }
            return *this;
        }
        iterator_base operator--(int) noexcept {
            iterator_base cur = *this;
            operator--();
            return cur;
        }
        bool operator==(const iterator_base& o) const noexcept = default;
    };
    using const_iterator = iterator_base<true>;
    class iterator final : public iterator_base<false> {
        friend class double_decker;
        using super = iterator_base<false>;
        iterator(const const_iterator&& other) noexcept : super(std::move(other._bucket), other._idx) {}
    public:
        iterator() noexcept : super() {}
        iterator(outer_iterator bkt, int idx) noexcept : super(bkt, idx) {}
        iterator(T* ptr) noexcept {
            inner_array& arr = inner_array::from_element(ptr, super::_idx);
            super::_bucket = outer_iterator(&arr);
        }
        template <typename Func>
        requires Disposer<Func, T>
        iterator erase_and_dispose(Less less, Func&& disp) noexcept {
            disp(&**this); // * to deref this, * to call operator*, & to get addr from ref
            if (super::_bucket->is_single_element()) {
                outer_iterator bkt = super::_bucket.erase(less);
                return iterator(bkt, 0);
            }
            bool tail = (*super::_bucket)[super::_idx].is_tail();
            super::_bucket->erase(super::_idx);
            if (tail) {
                super::_bucket++;
                super::_idx = 0;
            }
            return *this;
        }
        iterator erase(Less less) noexcept { return erase_and_dispose(less, bplus::default_dispose<T>); }
    };
    struct bound_hint {
        bool match;
        bool key_match;
        bool key_tail;
        bool emplace_keeps_iterators() const noexcept { return !key_match; }
    };
    iterator begin() noexcept { return iterator(_tree.begin(), 0); }
    const_iterator begin() const noexcept { return const_iterator(_tree.begin(), 0); }
    const_iterator cbegin() const noexcept { return const_iterator(_tree.begin(), 0); }
    iterator end() noexcept { return iterator(_tree.end(), 0); }
    const_iterator end() const noexcept { return const_iterator(_tree.end(), 0); }
    const_iterator cend() const noexcept { return const_iterator(_tree.end(), 0); }
    explicit double_decker(Less less) noexcept : _tree(less) { }
    double_decker(const double_decker& other) = delete;
    double_decker(double_decker&& other) noexcept : _tree(std::move(other._tree)) {}
    iterator insert(Key k, T value, Compare cmp) {
        std::pair<outer_iterator, bool> oip = _tree.emplace(std::move(k), std::move(value));
        outer_iterator& bkt = oip.first;
        int idx = 0;
        if (!oip.second) {
            idx = bkt->index_of(bkt->lower_bound(value, cmp));
            size_t new_size = (bkt->size() + 1) * sizeof(T);
            bkt.reconstruct(new_size, *bkt,
                    typename inner_array::grow_tag{idx}, std::move(value));
        }
        return iterator(bkt, idx);
    }
    template <typename... Args>
    iterator emplace_before(iterator i, Key k, const bound_hint& hint, Args&&... args) {
        assert(!hint.match);
        outer_iterator& bucket = i._bucket;
        if (!hint.key_match) {
            outer_iterator nb = bucket.emplace_before(std::move(k), _tree.less(), std::forward<Args>(args)...);
            return iterator(nb, 0);
        }
        int idx = i._idx;
        if (hint.key_tail) {
            bucket--;
            idx = bucket->index_of(bucket->end());
        }
        size_t new_size = (bucket->size() + 1) * sizeof(T);
        bucket.reconstruct(new_size, *bucket,
                typename inner_array::grow_tag{idx}, std::forward<Args>(args)...);
        return iterator(bucket, idx);
    }
    template <typename K = Key>
    requires Comparable<K, T, Compare>
    const_iterator find(const K& key, Compare cmp) const {
        outer_const_iterator bkt = _tree.find(key);
        int idx = 0;
        if (bkt != _tree.end()) {
            bool match = false;
            idx = bkt->index_of(bkt->lower_bound(key, cmp, match));
            if (!match) {
                bkt = _tree.end();
                idx = 0;
            }
        }
        return const_iterator(bkt, idx);
    }
    template <typename K = Key>
    requires Comparable<K, T, Compare>
    iterator find(const K& k, Compare cmp) {
        return iterator(const_cast<const double_decker*>(this)->find(k, std::move(cmp)));
    }
    template <typename K = Key>
    requires Comparable<K, T, Compare>
    const_iterator lower_bound(const K& key, Compare cmp, bound_hint& hint) const {
        outer_const_iterator bkt = _tree.lower_bound(key, hint.key_match);
        hint.key_tail = false;
        hint.match = false;
        if (bkt == _tree.end() || !hint.key_match) {
            return const_iterator(bkt, 0);
        }
        int i = bkt->index_of(bkt->lower_bound(key, cmp, hint.match));
        if (i != 0 && (*bkt)[i - 1].is_tail()) {
            bkt++;
            i = 0;
            hint.key_tail = true;
        }
        return const_iterator(bkt, i);
    }
    template <typename K = Key>
    requires Comparable<K, T, Compare>
    iterator lower_bound(const K& key, Compare cmp, bound_hint& hint) {
        return iterator(const_cast<const double_decker*>(this)->lower_bound(key, std::move(cmp), hint));
    }
    template <typename K = Key>
    requires Comparable<K, T, Compare>
    const_iterator lower_bound(const K& key, Compare cmp) const {
        bound_hint hint;
        return lower_bound(key, cmp, hint);
    }
    template <typename K = Key>
    requires Comparable<K, T, Compare>
    iterator lower_bound(const K& key, Compare cmp) {
        return iterator(const_cast<const double_decker*>(this)->lower_bound(key, std::move(cmp)));
    }
    template <typename K = Key>
    requires Comparable<K, T, Compare>
    const_iterator upper_bound(const K& key, Compare cmp) const {
        bool key_match;
        outer_const_iterator bkt = _tree.lower_bound(key, key_match);
        if (bkt == _tree.end() || !key_match) {
            return const_iterator(bkt, 0);
        }
        int i = bkt->index_of(bkt->upper_bound(key, cmp));
        if (i != 0 && (*bkt)[i - 1].is_tail()) {
            // Beyond the end() boundary
            bkt++;
            i = 0;
        }
        return const_iterator(bkt, i);
    }
    template <typename K = Key>
    requires Comparable<K, T, Compare>
    iterator upper_bound(const K& key, Compare cmp) {
        return iterator(const_cast<const double_decker*>(this)->upper_bound(key, std::move(cmp)));
    }
    template <typename Func>
    requires Disposer<Func, T>
    void clear_and_dispose(Func&& disp) noexcept {
        _tree.clear_and_dispose([&disp] (inner_array* arr) noexcept {
            arr->for_each(disp);
        });
    }
    void clear() noexcept { clear_and_dispose(bplus::default_dispose<T>); }
    template <typename Func>
    requires Disposer<Func, T>
    iterator erase_and_dispose(iterator begin, iterator end, Func&& disp) noexcept {
        bool same_bucket = begin._bucket == end._bucket;
        // Drop the tail of the starting bucket if it's not fully erased
        while (begin._idx != 0) {
            if (same_bucket) {
                if (begin == end) {
                    return begin;
                }
                end._idx--;
            }
            begin = begin.erase_and_dispose(_tree.less(), disp);
        }
        // Drop all the buckets in between
        outer_iterator nb = _tree.erase_and_dispose(begin._bucket, end._bucket, [&disp] (inner_array* arr) noexcept {
            arr->for_each(disp);
        });
        assert(nb == end._bucket);
        iterator next(nb, 0);
        while (end._idx-- != 0) {
            next = next.erase_and_dispose(_tree.less(), disp);
        }
        return next;
    }
    iterator erase(iterator begin, iterator end) noexcept {
        return erase_and_dispose(begin, end, bplus::default_dispose<T>);
    }
    bool empty() const noexcept { return _tree.empty(); }
    static size_t estimated_object_memory_size_in_allocator(allocation_strategy& allocator, const T* obj) noexcept {
        return sizeof(typename outer_tree::data);
    }
};
class cache_entry;
namespace cache {
class autoupdating_underlying_reader;
class cache_streamed_mutation;
class cache_flat_mutation_reader;
class read_context;
class lsa_manager;
}
// Tracks accesses and performs eviction of cache entries.
class cache_tracker final {
public:
    friend class row_cache;
    friend class cache::read_context;
    friend class cache::autoupdating_underlying_reader;
    friend class cache::cache_flat_mutation_reader;
    struct stats {
        uint64_t partition_hits;
        uint64_t partition_misses;
        uint64_t row_hits;
        uint64_t dummy_row_hits;
        uint64_t row_misses;
        uint64_t partition_insertions;
        uint64_t row_insertions;
        uint64_t static_row_insertions;
        uint64_t concurrent_misses_same_key;
        uint64_t partition_merges;
        uint64_t rows_processed_from_memtable;
        uint64_t rows_dropped_from_memtable;
        uint64_t rows_merged_from_memtable;
        uint64_t dummy_processed_from_memtable;
        uint64_t rows_covered_by_range_tombstones_from_memtable;
        uint64_t partition_evictions;
        uint64_t partition_removals;
        uint64_t row_evictions;
        uint64_t row_removals;
        uint64_t partitions;
        uint64_t rows;
        uint64_t mispopulations;
        uint64_t underlying_recreations;
        uint64_t underlying_partition_skips;
        uint64_t underlying_row_skips;
        uint64_t reads;
        uint64_t reads_with_misses;
        uint64_t reads_done;
        uint64_t pinned_dirty_memory_overload;
        uint64_t range_tombstone_reads;
        uint64_t row_tombstone_reads;
    };
private:
    stats _stats{};
    seastar::metrics::metric_groups _metrics;
    logalloc::region _region;
    lru _lru;
    mutation_cleaner _garbage;
    mutation_cleaner _memtable_cleaner;
    mutation_application_stats& _app_stats;
private:
public:
    using register_metrics = bool_class<class register_metrics_tag>;
    // Inserts e such that it will be evicted right before more_recent in the absence of later touches.
    
};
class flat_mutation_reader_v2;
class reader_permit;

using namespace seastar;
class mutation_source;
class position_in_partition;
class flat_mutation_reader_v2;
namespace streamed_mutation {
    class forwarding_tag;
    using forwarding = bool_class<forwarding_tag>;
}
namespace mutation_reader {
    class partition_range_forwarding_tag;
    using forwarding = bool_class<partition_range_forwarding_tag>;
}
class mutation_fragment_v1_stream final {
    flat_mutation_reader_v2 _reader;
    schema_ptr _schema;
    reader_permit _permit;
    range_tombstone_assembler _rt_assembler;
    std::optional<clustering_row> _row;
    friend class mutation_fragment_v2; // so it sees our consumer methods
    mutation_fragment_opt consume(static_row mf) ;
    mutation_fragment_opt consume(clustering_row mf) ;
    mutation_fragment_opt consume(range_tombstone_change mf) ;
    mutation_fragment_opt consume(partition_start mf) ;
    mutation_fragment_opt consume(partition_end mf) ;
    future<mutation_fragment_opt> read_from_underlying() {
        auto mfp = co_await _reader();
        if (!mfp) [[unlikely]] {
            _rt_assembler.on_end_of_stream();
            co_return std::nullopt;
        }
        auto ret = std::move(*mfp).consume(*this);
        if (!ret) [[unlikely]] {
            // swallowed a range tombstone change, have to read more
            co_return co_await read_from_underlying();
        }
        co_return std::move(ret);
    }
    template<typename Arg>
    mutation_fragment wrap(Arg arg) const ;
public:
    const schema_ptr& schema() const noexcept ;
    future<mutation_fragment_opt> operator()() {
        if (_row) [[unlikely]] {
            co_return wrap(std::move(*std::exchange(_row, std::nullopt)));
        }
        if (_reader.is_end_of_stream()) [[unlikely]] {
            co_return std::nullopt;
        }
        co_return co_await read_from_underlying();
    }
    future<bool> has_more_fragments() {
        if (_row) [[unlikely]] {
            co_return true;
        }
        if (_reader.is_end_of_stream()) [[unlikely]] {
            co_return false;
        }
        co_return bool(co_await _reader.peek());
    }
    future<> fast_forward_to(const dht::partition_range& pr) ;
private:
    template<typename Consumer>
    struct consumer_adapter {
        mutation_fragment_v1_stream& _reader;
        std::optional<dht::decorated_key> _decorated_key;
        Consumer _consumer;
    private:
    };
public:
    template<typename Consumer>
    requires FlattenedConsumer<Consumer>
    // Stops when consumer returns stop_iteration::yes from consume_end_of_partition or end of stream is reached.
    // Next call will receive fragments from the next partition.
    // When consumer returns stop_iteration::yes from methods other than consume_end_of_partition then the read
    // of the current partition is ended, consume_end_of_partition is called and if it returns stop_iteration::no
    // then the read moves to the next partition.
    // Reference to the decorated key that is passed to consume_new_partition() remains valid until after
    // the call to consume_end_of_partition().
    //
    // This method is useful because most of current consumers use this semantic.
    //
    //
    // This method returns whatever is returned from Consumer::consume_end_of_stream().
    auto consume(Consumer consumer) {
        return do_with(consumer_adapter<Consumer>(*this, std::move(consumer)), [this] (consumer_adapter<Consumer>& adapter) {
            return consume_pausable(std::ref(adapter)).then([&adapter] {
                return adapter._consumer.consume_end_of_stream();
            });
        });
    }
    template<typename Consumer>
    requires FlatMutationReaderConsumer<Consumer>
    // Stops when consumer returns stop_iteration::yes or end of stream is reached.
    // Next call will start from the next mutation_fragment in the stream.
    future<> consume_pausable(Consumer consumer) {
        while (true) {
            auto mfp = co_await (*this)();
            if (!mfp) {
                co_return;
            }
            if constexpr (std::is_same_v<future<stop_iteration>, decltype(consumer(wrap(false)))>) {
                if (co_await consumer(std::move(*mfp)) == stop_iteration::yes) {
                    co_return;
                }
            } else if (consumer(std::move(*mfp)) == stop_iteration::yes) {
                co_return;
            }
        }
    }
};
// Reads a single partition from the stream. Returns empty optional if there are no more partitions to be read.
/// A partition_presence_checker quickly returns whether a key is known not to exist
/// in a data source (it may return false positives, but not false negatives).
enum class partition_presence_checker_result {
    definitely_doesnt_exist,
    maybe_exists
};
using partition_presence_checker = std::function<partition_presence_checker_result (const dht::decorated_key& key)>;
// mutation_source represents source of data in mutation form. The data source
// can be queried multiple times and in parallel. For each query it returns
// independent mutation_reader.
//
// The reader returns mutations having all the same schema, the one passed
// when invoking the source.
//
// When reading in reverse, a reverse schema has to be passed (compared to the
// table's schema), and a half-reverse (legacy) slice.
// See docs/dev/reverse-reads.md for more details.
// Partition-range forwarding is not yet supported in reverse mode.
class mutation_source {
    using partition_range = const dht::partition_range&;
    using io_priority = const io_priority_class&;
    using flat_reader_v2_factory_type = std::function<flat_mutation_reader_v2(schema_ptr,
                                                                        reader_permit,
                                                                        partition_range,
                                                                        const query::partition_slice&,
                                                                        io_priority,
                                                                        tracing::trace_state_ptr,
                                                                        streamed_mutation::forwarding,
                                                                        mutation_reader::forwarding)>;
    // We could have our own version of std::function<> that is nothrow
    // move constructible and save some indirection and allocation.
    // Probably not worth the effort though.
    lw_shared_ptr<flat_reader_v2_factory_type> _fn;
    lw_shared_ptr<std::function<partition_presence_checker()>> _presence_checker_factory;
private:
    friend class optimized_optional<mutation_source>;
public:
    // Creates a new reader.
    //
    // All parameters captured by reference must remain live as long as returned
    // mutation_reader or streamed_mutation obtained through it are alive.
};
// Returns a mutation_source which is the sum of given mutation_sources.
//
// Adding two mutation sources gives a mutation source which contains
// the sum of writes contained in the addends.
mutation_source make_combined_mutation_source(std::vector<mutation_source>);
// Represent mutation_source which can be snapshotted.
class snapshot_source {
private:
    std::function<mutation_source()> _func;
public:
    // Creates a new snapshot.
    // The returned mutation_source represents all earlier writes and only those.
    // Note though that the mutations in the snapshot may get compacted over time.
};
using mutation_source_opt = optimized_optional<mutation_source>;
namespace bi = boost::intrusive;
class row_cache;
class cache_tracker;
class flat_mutation_reader_v2;
namespace replica {
class memtable_entry;
}
namespace cache {
class autoupdating_underlying_reader;
class cache_flat_mutation_reader;
class read_context;
class lsa_manager;
}
// Intrusive set entry which holds partition data.
//
// TODO: Make memtables use this format too.
class cache_entry {
    schema_ptr _schema;
    dht::decorated_key _key;
    partition_entry _pe;
    // True when we know that there is nothing between this entry and the previous one in cache
    struct {
        bool _continuous : 1;
        bool _dummy_entry : 1;
        bool _head : 1;
        bool _tail : 1;
        bool _train : 1;
    } _flags{};
    friend class size_calculator;
public:
    friend class row_cache;
    friend class cache_tracker;
    bool is_head() const noexcept ;
    void set_head(bool v) noexcept ;
    bool is_tail() const noexcept ;
    void set_tail(bool v) noexcept ;
    bool with_train() const noexcept ;
    void set_train(bool v) noexcept ;
    struct dummy_entry_tag{};
    struct evictable_tag{};
    cache_entry(dummy_entry_tag)
        : _key{dht::token(), partition_key::make_empty()}
    {
        _flags._dummy_entry = true;
    }
    cache_entry(schema_ptr s, const dht::decorated_key& key, const mutation_partition& p)
        : _schema(std::move(s))
        , _key(key)
        , _pe(partition_entry::make_evictable(*_schema, mutation_partition(*_schema, p)))
    { }
    // It is assumed that pe is fully continuous
    // pe must be evictable.
    // Called when all contents have been evicted.
    // This object should unlink and destroy itself from the container.
    // Evicts contents of this entry.
    // The caller is still responsible for unlinking and destroying this entry.
};
//
// A data source which wraps another data source such that data obtained from the underlying data source
// is cached in-memory in order to serve queries faster.
//
// Cache populates itself automatically during misses.
//
// All updates to the underlying mutation source must be performed through one of the synchronizing methods.
// Those are the methods which accept external_updater, e.g. update(), invalidate().
// All synchronizers have strong exception guarantees. If they fail, the set of writes represented by
// cache didn't change.
// Synchronizers can be invoked concurrently with each other and other operations on cache.
//
class row_cache final {
public:
    using phase_type = utils::phased_barrier::phase_type;
    using partitions_type = double_decker<int64_t, cache_entry,
                            dht::raw_token_less_comparator, dht::ring_position_comparator,
                            16, bplus::key_search::linear>;
    static_assert(bplus::SimpleLessCompare<int64_t, dht::raw_token_less_comparator>);
    friend class cache::autoupdating_underlying_reader;
    friend class single_partition_populating_reader;
    friend class cache_entry;
    friend class cache::cache_flat_mutation_reader;
    friend class cache::lsa_manager;
    friend class cache::read_context;
    friend class partition_range_cursor;
    friend class cache_tester;
    // A function which adds new writes to the underlying mutation source.
    // All invocations of external_updater on given cache instance are serialized internally.
    // Must have strong exception guarantees. If throws, the underlying mutation source
    // must be left in the state in which it was before the call.
    class external_updater_impl {
    public:
        // FIXME: make execute() noexcept, that will require every updater to make execution exception safe,
        // also change function signature.
    };
    class external_updater {
        class non_prepared : public external_updater_impl {
            using Func = seastar::noncopyable_function<void()>;
            Func _func;
        public:
        };
        std::unique_ptr<external_updater_impl> _impl;
    public:
    };
public:
    struct stats {
        utils::timed_rate_moving_average hits;
        utils::timed_rate_moving_average misses;
        utils::timed_rate_moving_average reads_with_misses;
        utils::timed_rate_moving_average reads_with_no_misses;
    };
private:
    cache_tracker& _tracker;
    stats _stats{};
    schema_ptr _schema;
    partitions_type _partitions; // Cached partitions are complete.
    // The snapshots used by cache are versioned. The version number of a snapshot is
    // called the "population phase", or simply "phase". Between updates, cache
    // represents the same snapshot.
    //
    // Update doesn't happen atomically. Before it completes, some entries reflect
    // the old snapshot, while others reflect the new snapshot. After update
    // completes, all entries must reflect the new snapshot. There is a race between the
    // update process and populating reads. Since after the update all entries must
    // reflect the new snapshot, reads using the old snapshot cannot be allowed to
    // insert data which will no longer be reached by the update process. The whole
    // range can be therefore divided into two sub-ranges, one which was already
    // processed by the update and one which hasn't. Each key can be assigned a
    // population phase which determines to which range it belongs, as well as which
    // snapshot it reflects. The methods snapshot_of() and phase_of() can
    // be used to determine this.
    //
    // In general, reads are allowed to populate given range only if the phase
    // of the snapshot they use matches the phase of all keys in that range
    // when the population is committed. This guarantees that the range will
    // be reached by the update process or already has been in its entirety.
    // In case of phase conflict, current solution is to give up on
    // population. Since the update process is a scan, it's sufficient to
    // check when committing the population if the start and end of the range
    // have the same phases and that it's the same phase as that of the start
    // of the range at the time when reading began.
    mutation_source _underlying;
    phase_type _underlying_phase = partition_snapshot::min_phase;
    mutation_source_opt _prev_snapshot;
    // Positions >= than this are using _prev_snapshot, the rest is using _underlying.
    std::optional<dht::ring_position_ext> _prev_snapshot_pos;
    snapshot_source _snapshot_source;
    // There can be at most one update in progress.
    seastar::semaphore _update_sem = {1};
    logalloc::allocating_section _update_section;
    logalloc::allocating_section _populate_section;
    logalloc::allocating_section _read_section;
    struct previous_entry_pointer {
        std::optional<dht::decorated_key> _key;
         // Represents dht::ring_position_view::min()
        ;
        // TODO: store iterator here to avoid key comparison
    };
    template<typename CreateEntry, typename VisitEntry>
    requires requires(CreateEntry create, VisitEntry visit, partitions_type::iterator it, partitions_type::bound_hint hint) {
        { create(it, hint) } -> std::same_as<partitions_type::iterator>;
        { visit(it) } -> std::same_as<void>;
    }
    // Must be run under reclaim lock
    cache_entry& do_find_or_create_entry(const dht::decorated_key& key, const previous_entry_pointer* previous,
                                 CreateEntry&& create_entry, VisitEntry&& visit_entry);
    // Ensures that partition entry for given key exists in cache and returns a reference to it.
    // Prepares the entry for reading. "phase" must match the current phase of the entry.
    //
    // Since currently every entry has to have a complete tombstone, it has to be provided here.
    // The entry which is returned will have the tombstone applied to it.
    //
    // Must be run under reclaim lock
    // Creates (or touches) a cache entry for missing partition so that sstables are not
    // poked again for it.
    // Only active phases are accepted.
    // Reference valid only until next deferring point.
    // Returns population phase for given position in the ring.
    // snapshot_for_phase() can be called to obtain mutation_source for given phase, but
    // only until the next deferring point.
    // Should be only called outside update().
    struct snapshot_and_phase {
        mutation_source& snapshot;
        phase_type phase;
    };
    // Optimized version of:
    //
    //  { snapshot_for_phase(phase_of(pos)), phase_of(pos) };
    //
    // Merges the memtable into cache with configurable logic for handling memtable entries.
    // The Updater gets invoked for every entry in the memtable with a lower bound iterator
    // into _partitions (cache_i), and the memtable entry.
    // It is invoked inside allocating section and in the context of cache's allocator.
    // All memtable entries will be removed.
    ;
    // Clears given memtable invalidating any affected cache elements.
    // A function which updates cache to the current snapshot.
    // It's responsible for advancing _prev_snapshot_pos between deferring points.
    //
    // Must have strong failure guarantees. Upon failure, it should still leave the cache
    // in a state consistent with the update it is performing.
    using internal_updater = std::function<future<>()>;
    // Atomically updates the underlying mutation source and synchronizes the cache.
    //
    // Strong failure guarantees. If returns a failed future, the underlying mutation
    // source was and cache are not modified.
    //
    // internal_updater is only kept alive until its invocation returns.
public:
public:
    // Implements mutation_source for this cache, see mutation_reader.hh
    // User needs to ensure that the row_cache object stays alive
    // as long as the reader is used.
    // The range must not wrap around.
    // Same as make_reader, but returns an empty optional instead of a no-op reader when there is nothing to
    // read. This is an optimization.
public:
    // Populate cache from given mutation, which must be fully continuous.
    // Intended to be used only in tests.
    // Can only be called prior to any reads.
    // Finds the entry in cache for a given key.
    // Intended to be used only in tests.
    // Synchronizes cache with the underlying data source from a memtable which
    // has just been flushed to the underlying data source.
    // The memtable can be queried during the process, but must not be written.
    // After the update is complete, memtable is empty.
    // Like update(), synchronizes cache with an incremental change to the underlying
    // mutation source, but instead of inserting and merging data, invalidates affected ranges.
    // Can be thought of as a more fine-grained version of invalidate(), which invalidates
    // as few elements as possible.
    // Refreshes snapshot. Must only be used if logical state in the underlying data
    // source hasn't changed.
    // Moves given partition to the front of LRU if present in cache.
    // Detaches current contents of given partition from LRU, so
    // that they are not evicted by memory reclaimer.
    // Synchronizes cache with the underlying mutation source
    // by invalidating ranges which were modified. This will force
    // them to be re-read from the underlying mutation source
    // during next read overlapping with the invalidated ranges.
    //
    // The ranges passed to invalidate() must include all
    // data which changed since last synchronization. Failure
    // to do so may result in reads seeing partial writes,
    // which would violate write atomicity.
    //
    // Guarantees that readers created after invalidate()
    // completes will see all writes from the underlying
    // mutation source made prior to the call to invalidate().
    // Evicts entries from cache.
    //
    // Note that this does not synchronize with the underlying source,
    // it is assumed that the underlying source didn't change.
    // If it did, use invalidate() instead.
    friend class just_cache_scanning_reader;
    friend class scanning_and_populating_reader;
    friend class range_populating_reader;
    friend class cache_tracker;
    friend class mark_end_as_continuous;
};
namespace cache {
class lsa_manager {
    row_cache &_cache;
public:
     ;
     ;
     ;
};
}
class partition_snapshot_row_cursor;
// A non-owning reference to a row inside partition_snapshot which
// maintains it's position and thus can be kept across reference invalidation points.
class partition_snapshot_row_weakref final {
    mutation_partition::rows_type::iterator _it;
    partition_snapshot::change_mark _change_mark;
    position_in_partition _pos = position_in_partition::min();
    bool _in_latest = false;
public:
    // Makes this object point to a row pointed to by given partition_snapshot_row_cursor.
    // Returns true iff the pointer is pointing at a row.
public:
    // Sets the iterator in latest version for the current position.
public:
    // Returns the position of the row.
    // Call only when pointing at a row.
    // Returns true iff the object is valid.
    // Call only when valid.
    // Brings the object back to validity and returns true iff the snapshot contains the row.
    // When not pointing at a row, returns false.
};
// Allows iterating over rows of mutation_partition represented by given partition_snapshot.
//
// The cursor initially has a position before all rows and is not pointing at any row.
// To position the cursor, use advance_to().
//
// All methods should be called with the region of the snapshot locked. The cursor is invalidated
// when that lock section is left, or if the snapshot is modified.
//
// When the cursor is invalidated, it still maintains its previous position. It can be brought
// back to validity by calling maybe_refresh(), or advance_to().
//
// Insertion of row entries after cursor's position invalidates the cursor.
// Exceptions thrown from mutators invalidate the cursor.
//
// Range tombstone information is accessible via range_tombstone() and range_tombstone_for_row()
// functions. range_tombstone() returns the tombstone for the interval which strictly precedes
// the current row, and range_tombstone_for_row() returns the information for the row itself.
// If the interval which precedes the row is not continuous, then range_tombstone() is empty.
// If range_tombstone() is not empty then the interval is continuous.
class partition_snapshot_row_cursor final {
    friend class partition_snapshot_row_weakref;
    struct position_in_version {
        mutation_partition::rows_type::iterator it;
        utils::immutable_collection<mutation_partition::rows_type> rows;
        int version_no;
        bool unique_owner = false;
        is_continuous continuous = is_continuous::no; // Range continuity in the direction of lower keys (in cursor schema domain).
        // Range tombstone in the direction of lower keys (in cursor schema domain).
        // Excludes the row. In the reverse mode, the row may have a different range tombstone.
        tombstone rt;
    };
    const schema& _schema; // query domain
    partition_snapshot& _snp;
    // _heap contains iterators which are ahead of the cursor.
    // _current_row contains iterators which are directly below the cursor.
    utils::small_vector<position_in_version, 2> _heap; // query domain order
    utils::small_vector<position_in_version, 2> _current_row;
    // For !_reversed cursors points to the entry which
    // is the lower_bound() of the current position in table schema order.
    // For _reversed cursors it can be either lower_bound() in table order
    // or lower_bound() in cursor's order, so should not be relied upon.
    // if current entry is in the latest version then _latest_it points to it,
    // also in _reversed mode.
    std::optional<mutation_partition::rows_type::iterator> _latest_it;
    // Continuity and range tombstone corresponding to ranges which are not represented in _heap because the cursor
    // went pass all the entries in those versions.
    bool _background_continuity = false;
    tombstone _background_rt;
    bool _continuous{};
    bool _dummy{};
    const bool _unique_owner;
    const bool _reversed;
    const bool _digest_requested;
    tombstone _range_tombstone;
    tombstone _range_tombstone_for_row;
    position_in_partition _position; // table domain
    partition_snapshot::change_mark _change_mark;
    position_in_partition_view to_table_domain(position_in_partition_view pos) const {
        if (_reversed) [[unlikely]] {
            return pos.reversed();
        }
        return pos;
    }
    position_in_partition_view to_query_domain(position_in_partition_view pos) const {
        if (_reversed) [[unlikely]] {
            return pos.reversed();
        }
        return pos;
    }
    struct version_heap_less_compare {
        rows_entry::tri_compare _cmp;
        partition_snapshot_row_cursor& _cur;
    public:
        explicit version_heap_less_compare(partition_snapshot_row_cursor& cur) 
        ;
    };
    // Removes the next row from _heap and puts it into _current_row
    // lower_bound is in the query schema domain
    // Advances the cursor to the next row.
    // The @keep denotes whether the entries should be kept in partition version.
    // If there is no next row, returns false and the cursor is no longer pointing at a row.
    // Can be only called on a valid cursor pointing at a row.
    // When throws, the cursor is invalidated and its position is not changed.
public:
    // When reversed is true then the cursor will operate in reversed direction.
    // When reversed, s must be a reversed schema relative to snp->schema()
    // Positions and fragments accepted and returned by the cursor are from the domain of s.
    // Iterators are from the table's schema domain.
    // If is_in_latest_version() then this returns an iterator to the entry under cursor in the latest version.
    // Returns true iff the iterators obtained since the cursor was last made valid
    // are still valid. Note that this doesn't mean that the cursor itself is valid.
    // Marks the iterators as valid without refreshing them.
    // Call only when the iterators are known to be valid.
    // Advances cursor to the first entry with position >= pos, if such entry exists.
    // If no such entry exists, the cursor is positioned at an extreme position in the direction of
    // the cursor (min for reversed cursor, max for forward cursor) and not pointing at a row
    // but still valid.
    //
    // continuous() is always valid after the call, even if not pointing at a row.
    // Returns true iff the cursor is pointing at a row after the call.
    // Brings back the cursor to validity.
    // Can be only called when cursor is pointing at a row.
    //
    // Semantically equivalent to:
    //
    //   advance_to(position());
    //
    // but avoids work if not necessary.
    //
    // Changes to attributes of the current row (e.g. continuity) don't have to be reflected.
    // Brings back the cursor to validity, pointing at the first row with position not smaller
    // than the current position. Returns false iff no such row exists.
    // Assumes that rows are not inserted into the snapshot (static). They can be removed.
    // Moves the cursor to the first entry with position >= pos.
    // If no such entry exists, the cursor is still moved, although
    // it won't be pointing at a row. Still, continuous() will be valid.
    //
    // Returns true iff there can't be any clustering row entries
    // between lower_bound (inclusive) and the position to which the cursor
    // was advanced.
    //
    // May be called when cursor is not valid.
    // The cursor is valid after the call.
    // Must be called under reclaim lock.
    // When throws, the cursor is invalidated and its position is not changed.
    // Call only when valid.
    // Returns true iff the cursor is pointing at a row.
    // Advances to the next row, if any.
    // If there is no next row, advances to the extreme position in the direction of the cursor
    // (position_in_partition::before_all_clustering_rows() or position_in_partition::after_all_clustering_rows)
    // and does not point at a row.
    // Information about the range, continuous() and range_tombstone(), is still valid in this case.
    // Call only when valid, not necessarily pointing at a row.
    // Can be called when cursor is pointing at a row.
    // Returns true iff the key range adjacent to the cursor's position from the side of smaller keys
    // is marked as continuous.
    // Can be called when cursor is valid, not necessarily pointing at a row.
    // Returns the range tombstone for the key range adjacent to the cursor's position from the side of smaller keys.
    // Excludes the range for the row itself. That information is returned by range_tombstone_for_row().
    // It's possible that range_tombstone() is empty and range_tombstone_for_row() is not empty.
    // Can be called when cursor is pointing at a row.
    // Returns the range tombstone covering the row under the cursor.
    // Can be called when cursor is pointing at a row.
    // Can be called only when cursor is valid and pointing at a row, and !dummy().
    // Can be called only when cursor is valid and pointing at a row.
    // Can be called only when cursor is valid and pointing at a row.
    // Can be called only when cursor is valid and pointing at a row.
    // Monotonic exception guarantees.
    template <typename Consumer>
    requires std::is_invocable_v<Consumer, deletable_row>
    void consume_row(Consumer&& consumer) {
        for (position_in_version& v : _current_row) {
            if (v.unique_owner) {
                consumer(std::move(v.it->row()));
            } else {
                consumer(deletable_row(_schema, v.it->row()));
            }
        }
    }
    // Can be called only when cursor is valid and pointing at a row.
    template <typename Consumer>
    requires std::is_invocable_v<Consumer, const deletable_row&>
    void consume_row(Consumer&& consumer) const {
        for (const position_in_version& v : _current_row) {
            consumer(v.it->row());
        }
    }
    // Returns memory footprint of row entries under the cursor.
    // Can be called only when cursor is valid and pointing at a row.
    size_t memory_usage() const {
        size_t result = 0;
        for (const position_in_version& v : _current_row) {
            result += v.it->memory_usage(_schema);
        }
        return result;
    }
    struct ensure_result {
        rows_entry& row;
        mutation_partition_v2::rows_type::iterator it;
        bool inserted = false;
    };
    // Makes sure that a rows_entry for the row under the cursor exists in the latest version.
    // Doesn't change logical value or continuity of the snapshot.
    // Can be called only when cursor is valid and pointing at a row.
    // The cursor remains valid after the call and points at the same row as before.
    // Use only with evictable snapshots.
    // Returns a pointer to rows_entry with given position in latest version or
    // creates a neutral one, provided that it belongs to a continuous range.
    // Otherwise returns nullptr.
    // Doesn't change logical value of mutation_partition or continuity of the snapshot.
    // The cursor doesn't have to be valid.
    // The cursor is invalid after the call.
    // When returns an engaged optional, the attributes of the cursor: continuous() and range_tombstone()
    // are valid, as if the cursor was advanced to the requested position.
    // Assumes the snapshot is evictable and not populated by means other than ensure_entry_if_complete().
    // Subsequent calls to ensure_entry_if_complete() or advance_to() must be given weakly monotonically increasing
    // positions unless iterators are invalidated across the calls.
    // The cursor must not be a reversed-order cursor.
    // Use only with evictable snapshots.
    // Brings the entry pointed to by the cursor to the front of the LRU
    // Cursor must be valid and pointing at a row.
    // Use only with evictable snapshots.
    // Position of the cursor in the cursor schema domain.
    // Can be called when cursor is pointing at a row, even when invalid, or when valid.
    // Position of the cursor in the table schema domain.
    // Can be called when cursor is pointing at a row, even when invalid, or when valid.
    ;
};
namespace query {
// Merges non-overlapping results into one
// Implements @Reducer concept from distributed.hh
class result_merger {
    std::vector<foreign_ptr<lw_shared_ptr<query::result>>> _partial;
    const uint64_t _max_rows;
    const uint32_t _max_partitions;
public:
    
    
    // FIXME: Eventually we should return a composite_query_result here
    // which holds the vector of query results and which can be quickly turned
    // into packet fragments by the transport layer without copying the data.
    foreign_ptr<lw_shared_ptr<query::result>> get();
};
}
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Warray-bounds"
#pragma GCC diagnostic pop
class xx_hasher {
    static constexpr size_t digest_size = 16;
    XXH64_state_t _state;
public:
    explicit xx_hasher(uint64_t seed = 0) noexcept ;
    void update(const char* ptr, size_t length) noexcept ;
    bytes finalize() ;
    std::array<uint8_t, digest_size> finalize_array() ;
    uint64_t finalize_uint64() ;
private:
     ;
};
// Used to specialize templates in order to fix a bug
// in handling null values: #4567
class legacy_xx_hasher_without_null_digest : public xx_hasher {
public:
    
};
template<typename H>
concept HasherReturningBytes = HasherReturning<H, bytes>;
class md5_hasher;
template <typename T, size_t size> class cryptopp_hasher : public hasher {
    struct impl;
    std::unique_ptr<impl> _impl;
public:
    
    ~cryptopp_hasher();
    
    cryptopp_hasher(const cryptopp_hasher&);
    cryptopp_hasher& operator=(cryptopp_hasher&&) noexcept;
    cryptopp_hasher& operator=(const cryptopp_hasher&);
    bytes finalize();
    std::array<uint8_t, size> finalize_array();
    void update(const char* ptr, size_t length) noexcept override;
    // Use update and finalize to compute the hash over the full view.
    
};
class md5_hasher final : public cryptopp_hasher<md5_hasher, 16> {};
class sha256_hasher final : public cryptopp_hasher<sha256_hasher, 32> {};
namespace query {
struct noop_hasher {
    
    std::array<uint8_t, 16> finalize_array() ;;
};
class digester final {
    std::variant<noop_hasher, md5_hasher, xx_hasher, legacy_xx_hasher_without_null_digest> _impl;
public:
    explicit digester(digest_algorithm algo) ;
    template<typename T, typename... Args>
    void feed_hash(const T& value, Args&&... args) ;;
    
};
using default_hasher = xx_hasher;
template<typename Hasher>
using using_hash_of_hash = std::negation<std::disjunction<std::is_same<Hasher, md5_hasher>, std::is_same<Hasher, noop_hasher>>>;
template<typename Hasher>
inline constexpr bool using_hash_of_hash_v = using_hash_of_hash<Hasher>::value;
}
namespace query {
class result::partition_writer {
    result_request _request;
    ser::after_qr_partition__key<bytes_ostream> _w;
    const partition_slice& _slice;
    // We are tasked with keeping track of the range
    // as well, since we are the primary "context"
    // when iterating "inside" a partition
    const clustering_row_ranges& _ranges;
    ser::query_result__partitions<bytes_ostream>& _pw;
    ser::vector_position _pos;
    digester& _digest;
    digester _digest_pos;
    uint64_t& _row_count;
    uint32_t& _partition_count;
    api::timestamp_type& _last_modified;
public:
    
    bool requested_digest() const ;
    bool requested_result() const ;
    ser::after_qr_partition__key<bytes_ostream> start() ;
    // Cancels the whole partition element.
    // Can be called at any stage of writing before this element is finalized.
    // Do not use this writer after that.
    void retract() ;
    const clustering_row_ranges& ranges() const ;
    const partition_slice& slice() const ;
    digester& digest() ;
    uint64_t& row_count() ;
    uint32_t& partition_count() ;
    api::timestamp_type& last_modified() ;
};
class result::builder {
    bytes_ostream _out;
    const partition_slice& _slice;
    ser::query_result__partitions<bytes_ostream> _w;
    result_request _request;
    uint64_t _row_count = 0;
    uint32_t _partition_count = 0;
    api::timestamp_type _last_modified = api::missing_timestamp;
    short_read _short_read;
    digester _digest;
    result_memory_accounter _memory_accounter;
    const uint64_t _tombstone_limit = query::max_tombstones;
    uint64_t _tombstones = 0;
public:
    builder(const partition_slice& slice, result_options options, result_memory_accounter memory_accounter, uint64_t tombstone_limit)
        : _slice(slice)
        , _w(ser::writer_of_query_result<bytes_ostream>(_out).start_partitions())
        , _request(options.request)
        , _digest(digester(options.digest_algo))
        , _memory_accounter(std::move(memory_accounter))
        , _tombstone_limit(tombstone_limit)
    { }
    builder(builder&&) = delete; // _out is captured by reference
    void mark_as_short_read() ;
    short_read is_short_read() const ;
    result_memory_accounter& memory_accounter() ;
    stop_iteration bump_and_check_tombstone_limit() ;
    
    
    uint32_t partition_count() const ;
    // Starts new partition and returns a builder for its contents.
    // Invalidates all previously obtained builders
    partition_writer add_partition(const schema& s, const partition_key& key) ;
    result build(std::optional<full_position> last_pos = {}) ;
};
}
class row;
class static_row;
class clustering_row;
class range_tombstone_change;
// Adds mutation to query::result.
class mutation_querier {
    const schema& _schema;
    query::result_memory_accounter& _memory_accounter;
    query::result::partition_writer _pw;
    ser::qr_partition__static_row__cells<bytes_ostream> _static_cells_wr;
    bool _live_data_in_static_row{};
    uint64_t _live_clustering_rows = 0;
    std::optional<ser::qr_partition__rows<bytes_ostream>> _rows_wr;
private:
    void query_static_row(const row& r, tombstone current_tombstone);
    void prepare_writers();
public:
    mutation_querier(const schema& s, query::result::partition_writer pw,
                     query::result_memory_accounter& memory_accounter);
    void consume(tombstone) ;
    // Requires that sr.has_any_live_data()
    stop_iteration consume(static_row&& sr, tombstone current_tombstone);
    // Requires that cr.has_any_live_data()
    stop_iteration consume(clustering_row&& cr, row_tombstone current_tombstone);
    stop_iteration consume(range_tombstone_change&&) ;
    uint64_t consume_end_of_stream();
};
class query_result_builder {
    const schema& _schema;
    query::result::builder& _rb;
    std::optional<mutation_querier> _mutation_consumer;
    // We need to remember that we requested stop, to mark the read as short in the end.
    stop_iteration _stop;
public:
    query_result_builder(const schema& s, query::result::builder& rb) noexcept;
    void consume_new_partition(const dht::decorated_key& dk);
    void consume(tombstone t);
    stop_iteration consume(static_row&& sr, tombstone t, bool);
    stop_iteration consume(clustering_row&& cr, row_tombstone t, bool);
    stop_iteration consume(range_tombstone_change&& rtc);
    stop_iteration consume_end_of_partition();
    void consume_end_of_stream();
};
// A data structure used to implement per-partition rate limiting. It accounts
// operations and enforces limits when it is detected that the operation rate
// is too high.
namespace db {
class rate_limiter_base {
public:
    static constexpr size_t op_count_bits = 20;
    static constexpr size_t time_window_bits = 12;
private:
    struct metrics {
        uint64_t allocations_on_empty = 0;
        uint64_t successful_lookups = 0;
        uint64_t failed_allocations = 0;
        uint64_t probe_count = 0;
    };
    // Represents a piece of the hashmap storage.
    struct entry {
    public:
        // The partition key token of the operation which allocated this entry.
        uint64_t token = 0;
        // The label of the operation which allocated this entry.
        // Labels are used to differentiate operations which should be counted
        // separately, e.g. reads and writes to the same table or writes
        // to two different tables.
        uint32_t label = 0;
        // The number of operations counted for given token/label.
        // It is virtually decremented on each window change, so the real
        // operation count is actually `op_count - _current_bucket`.
        // If the number drops to zero or below, the entry is considered
        // "expired" and may be overwritten by another operation.
        uint32_t op_count : op_count_bits = 0;
        // ID of the time window in which the entry was allocated.
        uint32_t time_window : time_window_bits = 0;
    };
    struct time_window_entry {
        // How many entries are there active within this time window?
        uint32_t entries_active = 0;
        // By how much should the counter should be decreased within
        // this time window?
        uint32_t lossy_counting_decrease = 0;
    };
public:
    struct can_proceed_tag{};
    using can_proceed = seastar::bool_class<can_proceed_tag>;
    // Identifies a type of operation which is counted separately from other
    // operations. For example, reads and writes for given table should have
    // separate labels.
    struct label {
    private:
        // The current ID used to identify the label in the rate limiter.
        // It is assigned on first use.
        uint32_t _label = 0;
        friend class rate_limiter_base;
    };
private:
    uint32_t _current_bucket = 0;
    uint32_t _current_ops_in_bucket = 0;
    uint32_t _current_entries_in_time_window = 0;
    uint32_t _next_label = 1;
    uint32_t _current_time_window = 0;
    const uint32_t _salt;
    utils::chunked_vector<entry> _entries;
    std::vector<time_window_entry> _time_window_history;
    metrics _metrics;
    seastar::metrics::metric_groups _metric_group;
private:
    void register_metrics();
protected:
    
public:
    
    // (For testing purposes only)
    // Increments the counter for given (label, token) and returns
    // the new value of the counter.
    // Increments the counter for given (label, token).
    // If the counter indicates that the partition is over the limit,
    // returns can_proceed::no with some probability.
    //
    // The `random_variable` parameter should be a value from range [0, 1).
    // It is used as the source of randomness - the function chooses a threshold
    // and accepts if and only if `random_variable` is below it.
    //
    // The probability is calculated in such a way that statistically
    // only `limit` operations per second are admitted.
};
template<typename ClockType>
class generic_rate_limiter : public rate_limiter_base {
private:
    seastar::timer<ClockType> _timer;
public:
};
extern template class generic_rate_limiter<seastar::lowres_clock>;
using rate_limiter = generic_rate_limiter<seastar::lowres_clock>;
}
namespace utils {
class rate_limiter {
private:
    timer<lowres_clock> _timer;
    size_t _units_per_s;
    semaphore _sem {0};
public:
};
}
namespace detail {
template<typename T, typename Comparator>
requires std::is_nothrow_copy_constructible_v<T> && std::is_nothrow_move_constructible_v<T>
class extremum_tracker {
    T _default_value;
    std::optional<T> _value;
public:
    explicit extremum_tracker(const T& default_value) noexcept
        : _default_value(default_value)
    {}
    void update(const T& value) noexcept {
        if (!_value || Comparator{}(value, *_value)) {
            _value = value;
        }
    }
    void update(const extremum_tracker& other) noexcept {
        if (other._value) {
            update(*other._value);
        }
    }
    const T& get() const noexcept {
        return _value ? *_value : _default_value;
    }
};
} // namespace detail
template <typename T>
using min_tracker = detail::extremum_tracker<T, std::less<T>>;
template <typename T>
using max_tracker = detail::extremum_tracker<T, std::greater<T>>;
template <typename T>
requires std::is_nothrow_copy_constructible_v<T> && std::is_nothrow_move_constructible_v<T>
class min_max_tracker {
    min_tracker<T> _min_tracker;
    max_tracker<T> _max_tracker;
public:
    min_max_tracker() noexcept
        : _min_tracker(std::numeric_limits<T>::min())
        , _max_tracker(std::numeric_limits<T>::max())
    {}
    min_max_tracker(const T& default_min, const T& default_max) noexcept
        : _min_tracker(default_min)
        , _max_tracker(default_max)
    {}
    void update(const T& value) noexcept {
        _min_tracker.update(value);
        _max_tracker.update(value);
    }
    void update(const min_max_tracker<T>& other) noexcept {
        _min_tracker.update(other._min_tracker);
        _max_tracker.update(other._max_tracker);
    }
    const T& min() const noexcept {
        return _min_tracker.get();
    }
    const T& max() const noexcept {
        return _max_tracker.get();
    }
};
// Stores statistics on all the updates done to a memtable
// The collected statistics are used for flushing memtable to the disk
struct encoding_stats {
    // The fixed epoch corresponds to the one used by Origin - 22/09/2015, 00:00:00, GMT-0:
    //        Calendar c = Calendar.getInstance(TimeZone.getTimeZone("GMT-0"), Locale.US);
    //        c.set(Calendar.YEAR, 2015);
    //        c.set(Calendar.MONTH, Calendar.SEPTEMBER);
    //        c.set(Calendar.DAY_OF_MONTH, 22);
    //        c.set(Calendar.HOUR_OF_DAY, 0);
    //        c.set(Calendar.MINUTE, 0);
    //        c.set(Calendar.SECOND, 0);
    //        c.set(Calendar.MILLISECOND, 0);
    //
    //        long TIMESTAMP_EPOCH = c.getTimeInMillis() * 1000; // timestamps should be in microseconds by convention
    //        int DELETION_TIME_EPOCH = (int)(c.getTimeInMillis() / 1000); // local deletion times are in seconds
    // Encoding stats are used for delta-encoding, so we want some default values
    // that are just good enough so we take some recent date in the past
    static constexpr int32_t deletion_time_epoch = 1442880000;
    static constexpr api::timestamp_type timestamp_epoch = api::timestamp_type(deletion_time_epoch) * 1000 * 1000;
    static constexpr int32_t ttl_epoch = 0;
    api::timestamp_type min_timestamp = timestamp_epoch;
    gc_clock::time_point min_local_deletion_time = gc_clock::time_point(gc_clock::duration(deletion_time_epoch));
    gc_clock::duration min_ttl = gc_clock::duration(ttl_epoch);
};
class encoding_stats_collector {
private:
    min_tracker<api::timestamp_type> min_timestamp;
    min_tracker<gc_clock::time_point> min_local_deletion_time;
    min_tracker<gc_clock::duration> min_ttl;
public:
};
class test_region_group;
namespace replica {
// Code previously under logalloc namespace
namespace dirty_memory_manager_logalloc {
class size_tracked_region;
struct region_evictable_occupancy_ascending_less_comparator {
};
using region_heap = boost::heap::binomial_heap<size_tracked_region*,
        boost::heap::compare<region_evictable_occupancy_ascending_less_comparator>,
        boost::heap::allocator<std::allocator<size_tracked_region*>>,
        //constant_time_size<true> causes corruption with boost < 1.60
        boost::heap::constant_time_size<false>>;
class size_tracked_region : public logalloc::region {
public:
    std::optional<region_heap::handle_type> _heap_handle;
};
// The region_group class keeps track of two memory use counts:
//
//  - real memory: this is LSA memory used by memtables, whether active or being
//            flushed. 
//  - spooled memory: this is LSA memory used by memtables undergoing a flush
//            that has been copied to an sstable. It a subset of
//            real memory. Once a flushing memtable is sealed, its spooled memory
//            drops to zero and real memory drops by the size of the memtable.
//
// Since the control loop is interested in (real memory) - (spooled memory), we keep
// track of the difference as unspooled memory.
//
// real memory and unspooled memory react to events in this way:
//   - data added to memtable: both real memory and unspooled memory increase by the same amount
//            (spooled memory does not change). Note the increase can actually be a decrease if
//            data was deleted or overwritten.
//   - part of the memtable was flushed to disk: unspooled memory decreases (spooled memory
//            increases)
//
// Users of a region_group configure reclaim with an unspooled memory soft limit (where
// sstable flush starts, but allocation can still continue), an unspooled memory hard limit (where
// allocation cannot proceed until sstable flush makes progress),
// and callbacks that are called when reclaiming is required and no longer necessary.
// There is also a real memory hard limit.
//
// These callbacks will be called when the dirty memory manager
// see relevant changes in the memory pressure conditions for this region_group. By specializing
// those methods - which are a nop by default - the callers initiate memtable flusing to
// free real and unspooled memory.
// The following restrictions apply to implementations of start_reclaiming() and stop_reclaiming():
//
//  - must not use any region or region_group objects, because they're invoked synchronously
//    with operations on those.
//
//  - must be noexcept, because they're called on the free path.
//
//  - the implementation may be called synchronously with any operation
//    which allocates memory, because these are called by memory reclaimer.
//    In particular, the implementation should not depend on memory allocation
//    because that may fail when in reclaiming context.
//
using reclaim_start_callback = noncopyable_function<void () noexcept>;
using reclaim_stop_callback = noncopyable_function<void () noexcept>;
struct reclaim_config {
    size_t unspooled_hard_limit = std::numeric_limits<size_t>::max();
    size_t unspooled_soft_limit = unspooled_hard_limit;
    size_t real_hard_limit = std::numeric_limits<size_t>::max();
    reclaim_start_callback start_reclaiming = [] () noexcept {};
    reclaim_stop_callback stop_reclaiming = [] () noexcept {};
};
// A container for memtables. Called "region_group" for historical
// reasons. Receives updates about memtable size change via the
// LSA region_listener interface.
class region_group : public logalloc::region_listener {
    using region_heap = dirty_memory_manager_logalloc::region_heap;
public:
    struct allocating_function {
    };
private:
    template <typename Func>
    struct concrete_allocating_function : public allocating_function {
        using futurator = futurize<std::result_of_t<Func()>>;
        typename futurator::promise_type pr;
        Func func;
    public:
    };
    class on_request_expiry {
        class blocked_requests_timed_out_error : public timed_out_error {
            const sstring _msg;
        public:
        };
        sstring _name;
    public:
    };
private:
    reclaim_config _cfg;
    bool _under_unspooled_pressure = false;
    bool _under_unspooled_soft_pressure = false;
    region_group* _subgroup = nullptr;
    size_t _real_total_memory = 0;
    bool _under_real_pressure = false;
    // It is a more common idiom to just hold the promises in the circular buffer and make them
    // ready. However, in the time between the promise being made ready and the function execution,
    // it could be that our memory usage went up again. To protect against that, we have to recheck
    // if memory is still available after the future resolves.
    //
    // But we can greatly simplify it if we store the function itself in the circular_buffer, and
    // execute it synchronously in release_requests() when we are sure memory is available.
    //
    // This allows us to easily provide strong execution guarantees while keeping all re-check
    // complication in release_requests and keep the main request execution path simpler.
    expiring_fifo<std::unique_ptr<allocating_function>, on_request_expiry, db::timeout_clock> _blocked_requests;
    uint64_t _blocked_requests_counter = 0;
    size_t _unspooled_total_memory = 0;
    region_heap _regions;
    condition_variable _relief;
    bool _shutdown_requested = false;
    future<> _releaser;
private:
public:
private:
public:
    bool under_unspooled_pressure() const noexcept ;
private:
public:
private:
    future<> release_queued_allocations();
    
    
private: // from region_listener
    virtual void moved(logalloc::region* old_address, logalloc::region* new_address) override;
public:
    // When creating a region_group, one can specify an optional throttle_threshold parameter. This
    // parameter won't affect normal allocations, but an API is provided, through the region_group's
    // method run_when_memory_available(), to make sure that a given function is only executed when
    // the total memory for the region group (and all of its parents) is lower or equal to the
    // region_group's throttle_treshold (and respectively for its parents).
    //
    // The deferred_work_sg parameter specifies a scheduling group in which to run allocations
    // (given to run_when_memory_available()) when they must be deferred due to lack of memory
    // at the time the call to run_when_memory_available() was made.
    // It would be easier to call update, but it is unfortunately broken in boost versions up to at
    // least 1.59.
    //
    // One possibility would be to just test for delta sigdness, but we adopt an explicit call for
    // two reasons:
    //
    // 1) it save us a branch
    // 2) some callers would like to pass delta = 0. For instance, when we are making a region
    //    evictable / non-evictable. Because the evictable occupancy changes, we would like to call
    //    the full update cycle even then.
    virtual void increase_usage(logalloc::region* r, ssize_t delta) override ;
    virtual void decrease_evictable_usage(logalloc::region* r) override ;
    virtual void decrease_usage(logalloc::region* r, ssize_t delta) override ;
    //
    // Make sure that the function specified by the parameter func only runs when this region_group,
    // as well as each of its ancestors have a memory_used() amount of memory that is lesser or
    // equal the throttle_threshold, as specified in the region_group's constructor.
    //
    // region_groups that did not specify a throttle_threshold will always allow for execution.
    //
    // In case current memory_used() is over the threshold, a non-ready future is returned and it
    // will be made ready at some point in the future, at which memory usage in the offending
    // region_group (either this or an ancestor) falls below the threshold.
    //
    // Requests that are not allowed for execution are queued and released in FIFO order within the
    // same region_group, but no guarantees are made regarding release ordering across different
    // region_groups.
    //
    // When timeout is reached first, the returned future is resolved with timed_out_error exception.
    template <typename Func>
    // We disallow future-returning functions here, because otherwise memory may be available
    // when we start executing it, but no longer available in the middle of the execution.
    requires (!is_future<std::invoke_result_t<Func>>::value)
    futurize_t<std::result_of_t<Func()>> run_when_memory_available(Func&& func, db::timeout_clock::time_point timeout);
    // returns a pointer to the largest region (in terms of memory usage) that sits below this
    // region group. This includes the regions owned by this region group as well as all of its
    // children.
    
    // Shutdown is mandatory for every user who has set a threshold
    // Can be called at most once.
    
    size_t blocked_requests() const noexcept;
    uint64_t blocked_requests_counter() const noexcept;
private:
    // Returns true if and only if constraints of this group are not violated.
    // That's taking into account any constraints imposed by enclosing (parent) groups.
    
    
    virtual void add(logalloc::region* child) override; // from region_listener
    virtual void del(logalloc::region* child) override; // from region_listener
    friend class ::test_region_group;
};
}
class dirty_memory_manager;
class sstable_write_permit final {
    friend class dirty_memory_manager;
    std::optional<semaphore_units<>> _permit;
    
    explicit sstable_write_permit(semaphore_units<>&& units) noexcept
            : _permit(std::move(units)) {
    }
public:
    sstable_write_permit(sstable_write_permit&&) noexcept = default;
};
class flush_permit {
    friend class dirty_memory_manager;
    dirty_memory_manager* _manager;
    std::optional<sstable_write_permit> _sstable_write_permit;
    semaphore_units<> _background_permit;
public:
};
class dirty_memory_manager {
    // We need a separate boolean, because from the LSA point of view, pressure may still be
    // mounting, in which case the pressure flag could be set back on if we force it off.
    bool _db_shutdown_requested = false;
    replica::database* _db;
    // The _region_group accounts for unspooled memory usage. It is defined as the real dirty
    // memory usage minus bytes that were already written to disk.
    dirty_memory_manager_logalloc::region_group _region_group;
    // We would like to serialize the flushing of memtables. While flushing many memtables
    // simultaneously can sustain high levels of throughput, the memory is not freed until the
    // memtable is totally gone. That means that if we have throttled requests, they will stay
    // throttled for a long time. Even when we have unspooled dirty, that only provides a rough
    // estimate, and we can't release requests that early.
    semaphore _flush_serializer;
    // We will accept a new flush before another one ends, once it is done with the data write.
    // That is so we can keep the disk always busy. But there is still some background work that is
    // left to be done. Mostly, update the caches and seal the auxiliary components of the SSTable.
    // This semaphore will cap the amount of background work that we have. Note that we're not
    // overly concerned about memtable memory, because dirty memory will put a limit to that. This
    // is mostly about dangling continuations. So that doesn't have to be a small number.
    static constexpr unsigned _max_background_work = 20;
    semaphore _background_work_flush_serializer = { _max_background_work };
    condition_variable _should_flush;
    int64_t _dirty_bytes_released_pre_accounted = 0;
    future<> _waiting_flush;
    unsigned _extraneous_flushes = 0;
    seastar::metrics::metric_groups _metrics;
public:
    // Limits and pressure conditions:
    // ===============================
    //
    // Unspooled Dirty
    // -------------
    // We can't free memory until the whole memtable is flushed because we need to keep it in memory
    // until the end, but we can fake freeing memory. When we are done with an element of the
    // memtable, we will update the region group pretending memory just went down by that amount.
    //
    // Because the amount of memory that we pretend to free should be close enough to the actual
    // memory used by the memtables, that effectively creates two sub-regions inside the dirty
    // region group, of equal size. In the worst case, we will have <memtable_total_space> dirty
    // bytes used, and half of that already spooled.
    //
    // Hard Limit
    // ----------
    // The total space that can be used by memtables in each group is defined by the threshold, but
    // we will only allow the region_group to grow to half of that. This is because of unspooled_dirty
    // as explained above. Because unspooled dirty is implemented by reducing the usage in the
    // region_group directly on partition written, we want to throttle every time half of the memory
    // as seen by the region_group. To achieve that we need to set the hard limit (first parameter
    // of the region_group_reclaimer) to 1/2 of the user-supplied threshold
    //
    // Soft Limit
    // ----------
    // When the soft limit is hit, no throttle happens. The soft limit exists because we don't want
    // to start flushing only when the limit is hit, but a bit earlier instead. If we were to start
    // flushing only when the hard limit is hit, workloads in which the disk is fast enough to cope
    // would see latency added to some requests unnecessarily.
    //
    // We then set the soft limit to 80 % of the unspooled dirty hard limit, which is equal to 40 % of
    // the user-supplied threshold.
private:
    friend class flush_permit;
};
namespace dirty_memory_manager_logalloc {
template <typename Func>
// We disallow future-returning functions here, because otherwise memory may be available
// when we start executing it, but no longer available in the middle of the execution.
requires (!is_future<std::invoke_result_t<Func>>::value)
futurize_t<std::result_of_t<Func()>>
region_group::run_when_memory_available(Func&& func, db::timeout_clock::time_point timeout) {
    bool blocked = 
        !_blocked_requests.empty()
        || under_unspooled_pressure()
        || _under_real_pressure;
    if (!blocked) {
        return futurize_invoke(func);
    }
    auto fn = std::make_unique<concrete_allocating_function<Func>>(std::forward<Func>(func));
    auto fut = fn->get_future();
    _blocked_requests.push_back(std::move(fn), timeout);
    ++_blocked_requests_counter;
    return fut;
}
inline
size_t
region_group::blocked_requests() const noexcept {
    return _blocked_requests.size();
}
inline
uint64_t
region_group::blocked_requests_counter() const noexcept {
    return _blocked_requests_counter;
}
}
extern thread_local dirty_memory_manager default_dirty_memory_manager;
}
namespace db {
class rp_set {
public:
    typedef std::unordered_map<segment_id_type, uint64_t> usage_map;
private:
    usage_map _usage;
};
}
namespace sstables {
// Some in-disk structures have an associated integer (of varying sizes) that
// represents how large they are. They can be a byte-length, in the case of a
// string, number of elements, in the case of an array, etc.
//
// For those elements, we encapsulate the underlying type in an outter
// structure that embeds how large is the in-disk size. It is a lot more
// convenient to embed it in the size than explicitly writing it in the parser.
// This way, we don't need to encode this information in multiple places at
// once - it is already part of the type.
template <typename Size>
struct disk_string {
    bytes value;
    explicit operator bytes_view() const {
        return value;
    }
    bool operator==(const disk_string& rhs) const {
        return value == rhs.value;
    }
};
struct disk_string_vint_size {
    bytes value;
    explicit operator bytes_view() const {
        return value;
    }
    bool operator==(const disk_string_vint_size& rhs) const {
        return value == rhs.value;
    }
};
template <typename Size>
struct disk_string_view {
    bytes_view value;
};
template<typename SizeType>
struct disk_data_value_view {
    atomic_cell_value_view value;
};
template <typename Size, typename Members>
requires std::is_integral_v<Size>
struct disk_array {
    utils::chunked_vector<Members> elements;
};
// A wrapper struct for integers to be written using variable-length encoding
template <typename T>
requires std::is_integral_v<T>
struct vint {
    T value;
};
// Same as disk_array but with its size serialized as variable-length integer
template <typename Members>
struct disk_array_vint_size {
    utils::chunked_vector<Members> elements;
};
template <typename Size, typename Members>
requires std::is_integral_v<Size>
struct disk_array_ref {
    const utils::chunked_vector<Members>& elements;
    disk_array_ref(const utils::chunked_vector<Members>& elements) : elements(elements) {}
};
template <typename Size, typename Key, typename Value>
struct disk_hash {
    std::unordered_map<Key, Value, std::hash<Key>> map;
};
template <typename TagType, TagType Tag, typename T>
struct disk_tagged_union_member {
    // stored as: tag, value-size-on-disk, value
    using tag_type = TagType;
    
    using type = T;
    T value;
};
template <typename TagType, typename... Members>
struct disk_tagged_union {
    using variant_type = boost::variant<Members...>;
    variant_type data;
};
// Each element of Members... is a disk_tagged_union_member<>
template <typename TagType, typename... Members>
struct disk_set_of_tagged_union {
    using tag_type = TagType;
    using key_type = std::conditional_t<std::is_enum<TagType>::value, std::underlying_type_t<TagType>, TagType>;
    using hash_type = std::conditional_t<std::is_enum<TagType>::value, enum_hash<TagType>, TagType>;
    using value_type = boost::variant<Members...>;
    std::unordered_map<tag_type, value_type, hash_type> data;
     ;
     ;
     ;
    struct serdes;
    static struct serdes s_serdes;
};
}
namespace std {
template <typename Size>
struct hash<sstables::disk_string<Size>> {
    size_t operator()(const sstables::disk_string<Size>& s) const {
        return std::hash<bytes>()(s.value);
    }
};
}
namespace utils {
struct streaming_histogram {
    // TreeMap to hold bins of histogram.
    std::map<double, uint64_t> bin;
    // maximum bin size for this histogram
    uint32_t max_bin_size;
    // FIXME: convert Java code below.
#if 0
    public Map<Double, Long> getAsMap()
    {
        return Collections.unmodifiableMap(bin);
    }
    public static class StreamingHistogramSerializer implements ISerializer<StreamingHistogram>
    {
        public void serialize(StreamingHistogram histogram, DataOutputPlus out) throws IOException
        {
            out.writeInt(histogram.maxBinSize);
            Map<Double, Long> entries = histogram.getAsMap();
            out.writeInt(entries.size());
            for (Map.Entry<Double, Long> entry : entries.entrySet())
            {
                out.writeDouble(entry.getKey());
                out.writeLong(entry.getValue());
            }
        }
        public StreamingHistogram deserialize(DataInput in) throws IOException
        {
            int maxBinSize = in.readInt();
            int size = in.readInt();
            Map<Double, Long> tmp = new HashMap<>(size);
            for (int i = 0; i < size; i++)
            {
                tmp.put(in.readDouble(), in.readLong());
            }
            return new StreamingHistogram(maxBinSize, tmp);
        }
        public long serializedSize(StreamingHistogram histogram, TypeSizes typeSizes)
        {
            long size = typeSizes.sizeof(histogram.maxBinSize);
            Map<Double, Long> entries = histogram.getAsMap();
            size += typeSizes.sizeof(entries.size());
            // size of entries = size * (8(double) + 8(long))
            size += entries.size() * (8 + 8);
            return size;
        }
    }
    @Override
    public boolean equals(Object o)
    {
        if (this == o)
            return true;
        if (!(o instanceof StreamingHistogram))
            return false;
        StreamingHistogram that = (StreamingHistogram) o;
        return maxBinSize == that.maxBinSize && bin.equals(that.bin);
    }
    @Override
    public int hashCode()
    {
        return Objects.hashCode(bin.hashCode(), maxBinSize);
    }
#endif
};
}
namespace sstables {
class key_view {
    managed_bytes_view _bytes;
public:
    
    template <std::invocable<bytes_view> Func>
    std::invoke_result_t<Func, bytes_view> with_linearized(Func&& func) const {
        return ::with_linearized(_bytes, func);
    }
};
// Our internal representation differs slightly (in the way it serializes) from Origin.
// In order to be able to achieve read and write compatibility for sstables - so they can
// be imported and exported - we need to always convert a key to this representation.
class key {
public:
    enum class kind {
        before_all_keys,
        regular,
        after_all_keys,
    };
private:
    kind _kind;
    bytes _bytes;
public:
     ;
    // Unfortunately, the _bytes field for the partition_key are not public. We can't move.
    explicit operator bytes_view() const {
        return _bytes;
    }
    const bytes& get_bytes() const {
        return _bytes;
    }
    friend key minimum_key();
};
 ;
 ;
class decorated_key_view {
    dht::token _token;
    key_view _partition_key;
public:
};
}
namespace sstables {
class sstable;
};
// Customize deleter so that lw_shared_ptr can work with an incomplete sstable class
namespace seastar {
template <>
struct lw_shared_ptr_deleter<sstables::sstable> {
    static void dispose(sstables::sstable* sst);
};
}
namespace sstables {
using shared_sstable = seastar::lw_shared_ptr<sstable>;
using sstable_list = std::unordered_set<shared_sstable>;
std::string to_string(const shared_sstable& sst, bool include_origin = true);
} // namespace sstables
template <>
struct fmt::formatter<sstables::shared_sstable> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const sstables::shared_sstable& sst, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", sstables::to_string(sst));
    }
};
namespace std {
 std::ostream& operator<<(std::ostream& os, const sstables::shared_sstable& sst) ;
} // namespace std
namespace sstables {
using run_id = utils::tagged_uuid<struct run_id_tag>;
} // namespace sstables
// While the sstable code works with char, bytes_view works with int8_t
// (signed char). Rather than change all the code, let's do a cast.
static bytes_view to_bytes_view(const temporary_buffer<char>& b) ;
namespace sstables {
template<typename T>
concept Writer =
    requires(T& wr, const char* data, size_t size) {
        { wr.write(data, size) } -> std::same_as<void>;
    };
struct sample_describer_for_self_describing_concept {
    // A describer can return any type, but we can't check any type in a concept.
    // Pick "long" arbitrarily and check that describe_type returns long too in that case.
    ;
};
template <typename T>
concept self_describing = requires (T& obj, sstable_version_types v, sample_describer_for_self_describing_concept d) {
    { obj.describe_type(v, d) } -> std::same_as<long>;
};
struct commitlog_interval {
    db::replay_position start;
    db::replay_position end;
};
struct deletion_time {
    int32_t local_deletion_time;
    int64_t marked_for_delete_at;
     ;
};
struct option {
    disk_string<uint16_t> key;
    disk_string<uint16_t> value;
     ;
};
struct filter {
    uint32_t hashes;
    disk_array<uint32_t, uint64_t> buckets;
     ;
    // Create an always positive filter if nothing else is specified.
};
// Do this so we don't have to copy on write time. We can just keep a reference.
struct filter_ref {
    uint32_t hashes;
    disk_array_ref<uint32_t, uint64_t> buckets;
     ;
};
enum class indexable_element {
    partition,
    cell
};
class summary_entry {
public:
    int64_t raw_token;
    bytes_view key;
    uint64_t position;
};
// Note: Sampling level is present in versions ka and higher. We ATM only support ka,
// so it's always there. But we need to make this conditional if we ever want to support
// other formats.
struct summary_ka {
    struct header {
        // The minimum possible amount of indexes per group (sampling level)
        uint32_t min_index_interval;
        // The number of entries in the Summary File
        uint32_t size;
        // The memory to be consumed to map the whole Summary into memory.
        uint64_t memory_size;
        // The actual sampling level.
        uint32_t sampling_level;
        // The number of entries the Summary *would* have if the sampling
        // level would be equal to min_index_interval.
        uint32_t size_at_full_sampling;
    } header;
    // The position in the Summary file for each of the indexes.
    // NOTE1 that its actual size is determined by the "size" parameter, not
    // by its preceding size_at_full_sampling
    // NOTE2: They are laid out in *MEMORY* order, not BE.
    // NOTE3: The sizes in this array represent positions in the memory stream,
    // not the file. The memory stream effectively begins after the header,
    // so every position here has to be added of sizeof(header).
    utils::chunked_vector<uint32_t> positions;   // can be large, so use a deque instead of a vector
    utils::chunked_vector<summary_entry> entries;
    disk_string<uint32_t> first_key;
    disk_string<uint32_t> last_key;
    // NOTE4: There is a structure written by Cassandra into the end of the Summary
    // file, after the field last_key, that we haven't understand yet, but we know
    // that its content isn't related to the summary itself.
    // The structure is basically as follow:
    // struct { disk_string<uint16_t>; uint32_t; uint64_t; disk_string<uint16_t>; }
    // Another interesting fact about this structure is that it is apparently always
    // filled with the same data. It's too early to judge that the data is useless.
    // However, it was tested that Cassandra loads successfully a Summary file with
    // this structure removed from it. Anyway, let's pay attention to it.
private:
    class summary_data_memory {
        unsigned _size;
        std::unique_ptr<bytes::value_type[]> _data;
    public:
    };
    unsigned _buffer_size = 1 << 10;
    std::vector<summary_data_memory> _summary_data = {};
    unsigned _summary_index_pos = 0;
};
using summary = summary_ka;
class file_writer;
struct metadata {
};
;
;
// serialized_size() implementation for metadata class
template <typename Component>
class metadata_base : public metadata {
public:
};
struct validation_metadata : public metadata_base<validation_metadata> {
    disk_string<uint16_t> partitioner;
    double filter_chance;
     ;
};
struct compaction_metadata : public metadata_base<compaction_metadata> {
    disk_array<uint32_t, uint32_t> ancestors; // DEPRECATED, not available in sstable format mc.
    disk_array<uint32_t, uint8_t> cardinality;
     ;
};
struct stats_metadata : public metadata_base<stats_metadata> {
    utils::estimated_histogram estimated_partition_size;
    utils::estimated_histogram estimated_cells_count;
    db::replay_position position;
    int64_t min_timestamp;
    int64_t max_timestamp;
    int32_t min_local_deletion_time; // 3_x only
    int32_t max_local_deletion_time;
    int32_t min_ttl; // 3_x only
    int32_t max_ttl; // 3_x only
    double compression_ratio;
    utils::streaming_histogram estimated_tombstone_drop_time;
    uint32_t sstable_level;
    uint64_t repaired_at;
    disk_array<uint32_t, disk_string<uint16_t>> min_column_names;
    disk_array<uint32_t, disk_string<uint16_t>> max_column_names;
    bool has_legacy_counter_shards;
    int64_t columns_count; // 3_x only
    int64_t rows_count; // 3_x only
    db::replay_position commitlog_lower_bound; // 3_x only
    disk_array<uint32_t, commitlog_interval> commitlog_intervals; // 3_x only
    std::optional<locator::host_id> originating_host_id; // 3_11_11 and later (me format)
     ;
};
using bytes_array_vint_size = disk_string_vint_size;
struct serialization_header : public metadata_base<serialization_header> {
    vint<uint64_t> min_timestamp_base;
    vint<uint64_t> min_local_deletion_time_base;
    vint<uint64_t> min_ttl_base;
    bytes_array_vint_size pk_type_name;
    disk_array_vint_size<bytes_array_vint_size> clustering_key_types_names;
    struct column_desc {
        bytes_array_vint_size name;
        bytes_array_vint_size type_name;
         ;
    };
    disk_array_vint_size<column_desc> static_columns;
    disk_array_vint_size<column_desc> regular_columns;
     ;
    // mc serialization header minimum values are delta-encoded based on the default timestamp epoch times
    // Note: following conversions rely on min_*_base.value being unsigned to prevent signed integer overflow
};
struct disk_token_bound {
    uint8_t exclusive; // really a boolean
    disk_string<uint16_t> token;
     ;
};
struct disk_token_range {
    disk_token_bound left;
    disk_token_bound right;
     ;
};
// Scylla-specific sharding information.  This is a set of token
// ranges that are spanned by this sstable.  When loading the
// sstable, we can see which shards own data in the sstable by
// checking each such range.
struct sharding_metadata {
    disk_array<uint32_t, disk_token_range> token_ranges;
     ;
};
// Scylla-specific list of features an sstable supports.
enum sstable_feature : uint8_t {
    NonCompoundPIEntries = 0,       // See #2993
    NonCompoundRangeTombstones = 1, // See #2986
    ShadowableTombstones = 2, // See #3885
    CorrectStaticCompact = 3, // See #4139
    CorrectEmptyCounters = 4, // See #4363
    CorrectUDTsInCollections = 5, // See #6130
    End = 6,
};
// Scylla-specific features enabled for a particular sstable.
struct sstable_enabled_features {
    uint64_t enabled_features;
     ;
};
// Numbers are found on disk, so they do matter. Also, setting their sizes of
// that of an uint32_t is a bit wasteful, but it simplifies the code a lot
// since we can now still use a strongly typed enum without introducing a
// notion of "disk-size" vs "memory-size".
enum class metadata_type : uint32_t {
    Validation = 0,
    Compaction = 1,
    Stats = 2,
    Serialization = 3,
};
enum class scylla_metadata_type : uint32_t {
    Sharding = 1,
    Features = 2,
    ExtensionAttributes = 3,
    RunIdentifier = 4,
    LargeDataStats = 5,
    SSTableOrigin = 6,
    ScyllaBuildId = 7,
    ScyllaVersion = 8,
};
// UUID is used for uniqueness across nodes, such that an imported sstable
// will not have its run identifier conflicted with the one of a local sstable.
struct run_identifier {
    // UUID is used for uniqueness across nodes, such that an imported sstable
    // will not have its run identifier conflicted with the one of a local sstable.
    run_id id;
     ;
};
// Types of large data statistics.
//
// Note: For extensibility, never reuse an identifier,
// only add new ones, since these are stored on stable storage.
enum class large_data_type : uint32_t {
    partition_size = 1,     // partition size, in bytes
    row_size = 2,           // row size, in bytes
    cell_size = 3,          // cell size, in bytes
    rows_in_partition = 4,  // number of rows in a partition
    elements_in_collection = 5,// number of elements in a collection
};
struct large_data_stats_entry {
    uint64_t max_value;
    uint64_t threshold;
    uint32_t above_threshold;
     ;
};
struct scylla_metadata {
    using extension_attributes = disk_hash<uint32_t, disk_string<uint32_t>, disk_string<uint32_t>>;
    using large_data_stats = disk_hash<uint32_t, large_data_type, large_data_stats_entry>;
    using sstable_origin = disk_string<uint32_t>;
    using scylla_build_id = disk_string<uint32_t>;
    using scylla_version = disk_string<uint32_t>;
    disk_set_of_tagged_union<scylla_metadata_type,
            disk_tagged_union_member<scylla_metadata_type, scylla_metadata_type::Sharding, sharding_metadata>,
            disk_tagged_union_member<scylla_metadata_type, scylla_metadata_type::Features, sstable_enabled_features>,
            disk_tagged_union_member<scylla_metadata_type, scylla_metadata_type::ExtensionAttributes, extension_attributes>,
            disk_tagged_union_member<scylla_metadata_type, scylla_metadata_type::RunIdentifier, run_identifier>,
            disk_tagged_union_member<scylla_metadata_type, scylla_metadata_type::LargeDataStats, large_data_stats>,
            disk_tagged_union_member<scylla_metadata_type, scylla_metadata_type::SSTableOrigin, sstable_origin>,
            disk_tagged_union_member<scylla_metadata_type, scylla_metadata_type::ScyllaBuildId, scylla_build_id>,
            disk_tagged_union_member<scylla_metadata_type, scylla_metadata_type::ScyllaVersion, scylla_version>
            > data;
     ;
};
static constexpr int DEFAULT_CHUNK_SIZE = 65536;
// checksums are generated using adler32 algorithm.
struct checksum {
    uint32_t chunk_size;
    utils::chunked_vector<uint32_t> checksums;
     ;
};
}
namespace std {
template <>
struct hash<sstables::metadata_type> : enum_hash<sstables::metadata_type> {};
}
namespace sstables {
// Special value to represent expired (i.e., 'dead') liveness info
constexpr static int64_t expired_liveness_ttl = std::numeric_limits<int32_t>::max();
// Corresponding to Cassandra's NO_DELETION_TIME
constexpr static int64_t no_deletion_time = std::numeric_limits<int32_t>::max();
// Corresponding to Cassandra's MAX_DELETION_TIME
constexpr static int64_t max_deletion_time = std::numeric_limits<int32_t>::max() - 1;
struct statistics {
    disk_array<uint32_t, std::pair<metadata_type, uint32_t>> offsets; // ordered by metadata_type
    std::unordered_map<metadata_type, std::unique_ptr<metadata>> contents;
};
enum class column_mask : uint8_t {
    none = 0x0,
    deletion = 0x01,
    expiration = 0x02,
    counter = 0x04,
    counter_update = 0x08,
    range_tombstone = 0x10,
    shadowable = 0x40
};
class unfiltered_flags_m final {
    static constexpr uint8_t END_OF_PARTITION = 0x01u;
    static constexpr uint8_t IS_MARKER = 0x02u;
    static constexpr uint8_t HAS_TIMESTAMP = 0x04u;
    static constexpr uint8_t HAS_TTL = 0x08u;
    static constexpr uint8_t HAS_DELETION = 0x10u;
    static constexpr uint8_t HAS_ALL_COLUMNS = 0x20u;
    static constexpr uint8_t HAS_COMPLEX_DELETION = 0x40u;
    static constexpr uint8_t HAS_EXTENDED_FLAGS = 0x80u;
    uint8_t _flags;
public:
};
class unfiltered_extended_flags_m final {
    static const uint8_t IS_STATIC = 0x01u;
    // This flag is used by Cassandra but not supported by Scylla because
    // Scylla's representation of shadowable tombstones is different.
    // We only check it on reading and error out if set but never set ourselves.
    static const uint8_t HAS_CASSANDRA_SHADOWABLE_DELETION = 0x02u;
    // This flag is Scylla-specific and used for writing shadowable tombstones.
    static const uint8_t HAS_SCYLLA_SHADOWABLE_DELETION = 0x80u;
    uint8_t _flags;
public:
};
class column_flags_m final {
    static const uint8_t IS_DELETED = 0x01u;
    static const uint8_t IS_EXPIRING = 0x02u;
    static const uint8_t HAS_EMPTY_VALUE = 0x04u;
    static const uint8_t USE_ROW_TIMESTAMP = 0x08u;
    static const uint8_t USE_ROW_TTL = 0x10u;
    uint8_t _flags;
public:
};
}
class frozen_mutation;
class row_cache;
namespace bi = boost::intrusive;
namespace replica {
class memtable_entry {
    schema_ptr _schema;
    dht::decorated_key _key;
    partition_entry _pe;
    struct {
        bool _head : 1;
        bool _tail : 1;
        bool _train : 1;
    } _flags{};
public:
    bool is_head() const noexcept ;
    void set_head(bool v) noexcept ;
    bool is_tail() const noexcept ;
    void set_tail(bool v) noexcept ;
    bool with_train() const noexcept ;
    void set_train(bool v) noexcept ;
    friend class memtable;
    memtable_entry(schema_ptr s, dht::decorated_key key, mutation_partition p)
        : _schema(std::move(s))
        , _key(std::move(key))
        , _pe(*_schema, std::move(p))
    { }
    memtable_entry(memtable_entry&& o) noexcept;
    // Frees elements of the entry in batches.
    // Returns stop_iteration::yes iff there are no more elements to free.
    // Makes the entry conform to given schema.
    // Must be called under allocating section of the region which owns the entry.
};
}
namespace replica {
class dirty_memory_manager;
struct table_stats;
// Managed by lw_shared_ptr<>.
class memtable final : public enable_lw_shared_from_this<memtable>, private dirty_memory_manager_logalloc::size_tracked_region {
public:
    using partitions_type = double_decker<int64_t, memtable_entry,
                            dht::raw_token_less_comparator, dht::ring_position_comparator,
                            16, bplus::key_search::linear>;
private:
    dirty_memory_manager& _dirty_mgr;
    mutation_cleaner _cleaner;
    memtable_list *_memtable_list;
    schema_ptr _schema;
    logalloc::allocating_section _read_section;
    logalloc::allocating_section _allocating_section;
    partitions_type partitions;
    size_t nr_partitions = 0;
    db::replay_position _replay_position;
    db::rp_set _rp_set;
    // mutation source to which reads fall-back after mark_flushed()
    // so that memtable contents can be moved away while there are
    // still active readers. This is needed for this mutation_source
    // to be monotonic (not loose writes). Monotonicity of each
    // mutation_source is necessary for the combined mutation source to be
    // monotonic. That combined source in this case is cache + memtable.
    mutation_source_opt _underlying;
    uint64_t _flushed_memory = 0;
    bool _merged_into_cache = false;
    replica::table_stats& _table_stats;
    class memtable_encoding_stats_collector : public encoding_stats_collector {
    private:
        min_max_tracker<api::timestamp_type> min_max_timestamp;
    public:
    } _stats_collector;
    friend class ::row_cache;
    friend class memtable_entry;
    friend class flush_reader;
    friend class flush_memory_accounter;
    friend class partition_snapshot_read_accounter;
private:
public:
    // Used for testing that want to control the flush process.
    // Clears this memtable gradually without consuming the whole CPU.
    // Never resolves with a failed future.
    // Applies mutation to this memtable.
    // The mutation is upgraded to current schema.
    // The mutation is upgraded to current schema.
public:
    // Creates a reader of data in this memtable for given partition range.
    //
    // Live readers share ownership of the memtable instance, so caller
    // doesn't need to ensure that memtable remains live.
    //
    // The 'range' parameter must be live as long as the reader is being used
    //
    // Mutations returned by the reader will all have given schema.
    // Same as make_flat_reader, but returns an empty optional instead of a no-op reader when there is nothing to
    // read. This is an optimization.
    friend class iterator_reader;
};
}
// makes sure that cache update handles real dirty memory correctly.
class real_dirty_memory_accounter {
    replica::dirty_memory_manager& _mgr;
    cache_tracker& _tracker;
    uint64_t _bytes;
    uint64_t _uncommitted = 0;
public:
    // Needs commit() to take effect, or when this object is destroyed.
};
namespace cql3 {
namespace statements {
struct index_target {
    static const sstring target_option_name;
    static const sstring custom_index_option_name;
    static const boost::regex target_regex;
    enum class target_type {
        regular_values, collection_values, keys, keys_and_values, full
    };
    using single_column = ::shared_ptr<column_identifier>;
    using multiple_columns = std::vector<::shared_ptr<column_identifier>>;
    using value_type = std::variant<single_column, multiple_columns>;
    const value_type value;
    target_type type;
    // Parses index_target::target_type from it's textual form.
    // e.g. from_sstring("keys") == index_target::target_type::keys
    // Parses index_target::target_type from index target string form.
    // e.g. from_target_string("keys(some_column)") == index_target::target_type::keys
    // Parses column name from index target string form
    // e.g. column_name_from_target_string("keys(some_column)") == "some_column"
    // A CQL column's name may contain any characters. If we use this string
    // as-is inside a target string, it may confuse us when we later try to
    // parse the resulting string (e.g., see issue #10707). We should
    // therefore use the function escape_target_column() to "escape" the
    // target column name, and the reverse function unescape_target_column().
    class raw {
    public:
        using single_column = ::shared_ptr<column_identifier::raw>;
        using multiple_columns = std::vector<::shared_ptr<column_identifier::raw>>;
        using value_type = std::variant<single_column, multiple_columns>;
        const value_type value;
        const target_type type;
    };
};
}
}
namespace cql3::expr {
enum class oper_t;
}
namespace secondary_index {
class index {
    index_metadata _im;
    cql3::statements::index_target::target_type _target_type;
    sstring _target_column;
public:
    struct supports_expression_v {
        enum class value_type {
            UsualYes,
            CollectionYes,
            No,
        };
        value_type value;
    };
};
class secondary_index_manager {
    data_dictionary::table _cf;
    /// The key of the map is the name of the index as stored in system tables.
    std::unordered_map<sstring, index> _indices;
public:
private:
};
}
namespace db {
using commitlog_force_sync = bool_class<class force_sync_tag>;
}
using namespace seastar;
namespace db {
class snapshot_ctl : public peering_sharded_service<snapshot_ctl> {
public:
    using skip_flush = bool_class<class skip_flush_tag>;
    using snap_views = bool_class<class snap_views_tag>;
    struct snapshot_details {
        int64_t live;
        int64_t total;
        sstring cf;
        sstring ks;
    };
    explicit snapshot_ctl(sharded<replica::database>& db) : _db(db) {}
    future<> stop() {
        return _ops.close();
    }
    future<> take_snapshot(sstring tag, skip_flush sf = skip_flush::no) {
        return take_snapshot(tag, {}, sf);
    }
    future<> take_snapshot(sstring tag, std::vector<sstring> keyspace_names, skip_flush sf = skip_flush::no);
    future<std::unordered_map<sstring, std::vector<snapshot_details>>> get_snapshot_details();
private:
    sharded<replica::database>& _db;
    seastar::rwlock _lock;
    seastar::gate _ops;
    ;
    ;
};
}
namespace sstables {
struct writer_offset_tracker {
    uint64_t offset = 0;
};
class write_monitor {
public:
};
struct reader_position_tracker {
    uint64_t position = 0;
    uint64_t total_read_size = 0;
};
class read_monitor {
public:
    // parameters are the current position in the data file
};
struct noop_read_monitor final : public read_monitor {
};
struct read_monitor_generator {
};
struct no_read_monitoring final : public read_monitor_generator {
    ;
};
}
namespace utils {
class estimated_histogram;
}
namespace sstables {
class sstable_set_impl;
class incremental_selector_impl;
struct sstable_first_key_less_comparator {
};
// Structure holds all sstables (a.k.a. fragments) that belong to same run identifier, which is an UUID.
// SStables in that same run will not overlap with one another.
class sstable_run {
public:
    using sstable_set = std::set<shared_sstable, sstable_first_key_less_comparator>;
private:
    sstable_set _all;
private:
public:
    // Returns false if sstable being inserted cannot satisfy the disjoint invariant. Then caller should pick another run for it.
    // Data size of the whole run, meaning it's a sum of the data size of all its fragments.
};
class sstable_set : public enable_lw_shared_from_this<sstable_set> {
    std::unique_ptr<sstable_set_impl> _impl;
    schema_ptr _schema;
public:
    // Return all runs which contain any of the input sstables.
    // Return all sstables. It's not guaranteed that sstable_set will keep a reference to the returned list, so user should keep it.
    // Prefer for_each_sstable() over all() for iteration purposes, as the latter may have to copy all sstables into a temporary
    // Calls func for each sstable or until it returns stop_iteration::yes
    // Returns the last stop_iteration value.
    // Used to incrementally select sstables from sstable set using ring-position.
    // sstable set must be alive during the lifetime of the selector.
    class incremental_selector {
        std::unique_ptr<incremental_selector_impl> _impl;
        dht::ring_position_comparator _cmp;
        mutable std::optional<dht::partition_range> _current_range;
        mutable std::optional<nonwrapping_range<dht::ring_position_view>> _current_range_view;
        mutable std::vector<shared_sstable> _current_sstables;
        mutable dht::ring_position_ext _current_next_position = dht::ring_position_view::min();
    public:
        struct selection {
            const std::vector<shared_sstable>& sstables;
            dht::ring_position_view next_position;
        };
        // Return the sstables that intersect with `pos` and the next
        // position where the intersecting sstables change.
        // To walk through the token range incrementally call `select()`
        // with `dht::ring_position_view::min()` and then pass back the
        // returned `next_position` on each next call until
        // `next_position` becomes `dht::ring_position::max()`.
        //
        // Successive calls to `select()' have to pass weakly monotonic
        // positions (incrementability).
        //
        // NOTE: both `selection.sstables` and `selection.next_position`
        // are only guaranteed to be valid until the next call to
        // `select()`.
    };
    /// Read a range from the sstable set.
    ///
    /// The reader is unrestricted, but will account its resource usage on the
    /// semaphore belonging to the passed-in permit.
    // Filters out mutations that don't belong to the current shard.
    friend class compound_sstable_set;
};
using offstrategy = bool_class<class offstrategy_tag>;
/// Return the amount of overlapping in a set of sstables. 0 is returned if set is disjoint.
///
/// The 'sstables' parameter must be a set of sstables sorted by first key.
}
class compaction_manager;
class compaction_weight_registration {
    compaction_manager* _cm;
    int _weight;
public:
    // Release immediately the weight hold by this object
};
namespace compaction {
class table_state;
class strategy_control;
struct compaction_state;
using owned_ranges_ptr = lw_shared_ptr<const dht::token_range_vector>;
} // namespace compaction
namespace sstables {
enum class compaction_type {
    Compaction = 0,
    Cleanup = 1,
    Validation = 2, // Origin uses this for a compaction that is used exclusively for repair
    Scrub = 3,
    Index_build = 4,
    Reshard = 5,
    Upgrade = 6,
    Reshape = 7,
};
struct compaction_completion_desc {
    // Old, existing SSTables that should be deleted and removed from the SSTable set.
    std::vector<shared_sstable> old_sstables;
    // New, fresh SSTables that should be added to SSTable set, replacing the old ones.
    std::vector<shared_sstable> new_sstables;
    // Set of compacted partition ranges that should be invalidated in the cache.
    dht::partition_range_vector ranges_for_cache_invalidation;
};
// creates a new SSTable for a given shard
using compaction_sstable_creator_fn = std::function<shared_sstable(shard_id shard)>;
// Replaces old sstable(s) by new one(s) which contain all non-expired data.
using compaction_sstable_replacer_fn = std::function<void(compaction_completion_desc)>;
class compaction_type_options {
public:
    struct regular {
    };
    struct cleanup {
    };
    struct upgrade {
    };
    struct scrub {
        enum class mode {
            abort, // abort scrub on the first sign of corruption
            skip, // skip corrupt data, including range of rows and/or partitions that are out-of-order
            segregate, // segregate out-of-order data into streams that all contain data with correct order
            validate, // validate data, printing all errors found (sstables are only read, not rewritten)
        };
        mode operation_mode = mode::abort;
        enum class quarantine_mode {
            include, // scrub all sstables, including quarantined
            exclude, // scrub only non-quarantined sstables
            only, // scrub only quarantined sstables
        };
        quarantine_mode quarantine_operation_mode = quarantine_mode::include;
    };
    struct reshard {
    };
    struct reshape {
    };
private:
    using options_variant = std::variant<regular, cleanup, upgrade, scrub, reshard, reshape>;
private:
    options_variant _options;
private:
public:
    static compaction_type_options make_regular() ;
     ;
};
class dummy_tag {};
using has_only_fully_expired = seastar::bool_class<dummy_tag>;
struct compaction_descriptor {
    // List of sstables to be compacted.
    std::vector<sstables::shared_sstable> sstables;
    // This is a snapshot of the table's sstable set, used only for the purpose of expiring tombstones.
    // If this sstable set cannot be provided, expiration will be disabled to prevent data from being resurrected.
    std::optional<sstables::sstable_set> all_sstables_snapshot;
    // Level of sstable(s) created by compaction procedure.
    int level;
    // Threshold size for sstable(s) to be created.
    uint64_t max_sstable_bytes;
    // Can split large partitions at clustering boundary.
    bool can_split_large_partition = false;
    // Run identifier of output sstables.
    sstables::run_id run_identifier;
    // The options passed down to the compaction code.
    // This also selects the kind of compaction to do.
    compaction_type_options options = compaction_type_options::make_regular();
    // If engaged, compaction will cleanup the input sstables by skipping non-owned ranges.
    compaction::owned_ranges_ptr owned_ranges;
    compaction_sstable_creator_fn creator;
    compaction_sstable_replacer_fn replacer;
    ::io_priority_class io_priority = default_priority_class();
    // Denotes if this compaction task is comprised solely of completely expired SSTables
    sstables::has_only_fully_expired has_only_fully_expired = has_only_fully_expired::no;
    static constexpr int default_level = 0;
    static constexpr uint64_t default_max_sstable_bytes = std::numeric_limits<uint64_t>::max();
    // Return fan-in of this job, which is equal to its number of runs.
    // Enables garbage collection for this descriptor, meaning that compaction will be able to purge expired data
    // Returns total size of all sstables contained in this descriptor
};
}
class reader_permit;
class compaction_backlog_tracker;
namespace sstables {
class sstable_set;
class compaction_strategy;
class sstables_manager;
struct sstable_writer_config;
}
namespace compaction {
class compaction_strategy_state;
}
namespace compaction {
class table_state {
public:
    
    // min threshold as defined by table.
};
} // namespace compaction
namespace fmt {
template <>
struct formatter<compaction::table_state> : formatter<std::string_view> {
     ;
};
} // namespace fmt
namespace compaction {
// Used by manager to set goals and constraints on compaction strategies
class strategy_control {
public:
};
}
struct mutation_source_metadata;
class compaction_backlog_tracker;
using namespace compaction;
namespace sstables {
class compaction_strategy_impl;
class sstable;
class sstable_set;
struct compaction_descriptor;
struct resharding_descriptor;
class compaction_strategy {
    ::shared_ptr<compaction_strategy_impl> _compaction_strategy_impl;
public:
    // Return a list of sstables to be compacted after applying the strategy.
    
    std::vector<compaction_descriptor> get_cleanup_compaction_jobs(table_state& table_s, std::vector<shared_sstable> candidates) const;
    // Some strategies may look at the compacted and resulting sstables to
    // get some useful information for subsequent compactions.
    // Return if parallel compaction is allowed by strategy.
    // Return if optimization to rule out sstables based on clustering key filter should be applied.
    // An estimation of number of compaction for strategy to be satisfied.
    static sstring name(compaction_strategy_type type) ;
    // Returns whether or not interposer consumer is used by a given strategy.
    // Informs the caller (usually the compaction manager) about what would it take for this set of
    // SSTables closer to becoming in-strategy. If this returns an empty compaction descriptor, this
    // means that the sstable set is already in-strategy.
    //
    // The caller can specify one of two modes: strict or relaxed. In relaxed mode the tolerance for
    // what is considered offstrategy is higher. It can be used, for instance, for when the system
    // is restarting and previous compactions were likely in-flight. In strict mode, we are less
    // tolerant to invariant breakages.
    //
    // The caller should also pass a maximum number of SSTables which is the maximum amount of
    // SSTables that can be added into a single job.
};
// Creates a compaction_strategy object from one of the strategies available.
}
namespace locator { class topology; }
namespace service {
namespace storage_proxy_stats {
// split statistics counters
struct split_stats {
    static seastar::metrics::label datacenter_label;
private:
    struct stats_counter {
        uint64_t val = 0;
    };
    // counter of operations performed on a local Node
    stats_counter _local;
    // counters of operations performed on external Nodes aggregated per Nodes' DCs
    std::unordered_map<sstring, stats_counter> _dc_stats;
    // collectd registrations container
    seastar::metrics::metric_groups _metrics;
    // a prefix string that will be used for a collectd counters' description
    sstring _short_description_prefix;
    sstring _long_description_prefix;
    // a statistics category, e.g. "client" or "replica"
    sstring _category;
    // type of operation (data/digest/mutation_data)
    sstring _op_type;
    // whether to register per-endpoint metrics automatically
    bool _auto_register_metrics;
public:
};
struct write_stats {
    // total write attempts
    split_stats writes_attempts;
    split_stats writes_errors;
    split_stats background_replica_writes_failed;
    // write attempts due to Read Repair logic
    split_stats read_repair_write_attempts;
    utils::timed_rate_moving_average write_unavailables;
    utils::timed_rate_moving_average write_timeouts;
    utils::timed_rate_moving_average write_rate_limited_by_replicas;
    utils::timed_rate_moving_average write_rate_limited_by_coordinator;
    utils::timed_rate_moving_average_summary_and_histogram write;
    utils::timed_rate_moving_average cas_write_unavailables;
    utils::timed_rate_moving_average cas_write_timeouts;
    utils::timed_rate_moving_average_summary_and_histogram cas_write;
    utils::estimated_histogram cas_write_contention;
    uint64_t writes = 0;
    // A CQL write query arrived to a non-replica node and was
    // forwarded by a coordinator to a replica
    uint64_t writes_coordinator_outside_replica_set = 0;
    // A CQL read query arrived to a non-replica node and was
    // forwarded by a coordinator to a replica
    uint64_t reads_coordinator_outside_replica_set = 0;
    uint64_t background_writes = 0; // client no longer waits for the write
    uint64_t throttled_writes = 0; // total number of writes ever delayed due to throttling
    uint64_t throttled_base_writes = 0; // current number of base writes delayed due to view update backlog
    uint64_t background_writes_failed = 0;
    uint64_t writes_failed_due_to_too_many_in_flight_hints = 0;
    uint64_t cas_write_unfinished_commit = 0;
    uint64_t cas_write_condition_not_met = 0;
    uint64_t cas_write_timeout_due_to_uncertainty = 0;
    uint64_t cas_failed_read_round_optimization = 0;
    uint16_t cas_now_pruning = 0;
    uint64_t cas_prune = 0;
    uint64_t cas_coordinator_dropped_prune = 0;
    uint64_t cas_replica_dropped_prune = 0;
    std::chrono::microseconds last_mv_flow_control_delay; // delay added for MV flow control in the last request
public:
protected:
    seastar::metrics::metric_groups _metrics;
};
struct stats : public write_stats {
    seastar::metrics::metric_groups _metrics;
    utils::timed_rate_moving_average read_timeouts;
    utils::timed_rate_moving_average read_unavailables;
    utils::timed_rate_moving_average read_rate_limited_by_replicas;
    utils::timed_rate_moving_average read_rate_limited_by_coordinator;
    utils::timed_rate_moving_average range_slice_timeouts;
    utils::timed_rate_moving_average range_slice_unavailables;
    utils::timed_rate_moving_average cas_read_timeouts;
    utils::timed_rate_moving_average cas_read_unavailables;
    utils::estimated_histogram cas_read_contention;
    uint64_t read_repair_attempts = 0;
    uint64_t read_repair_repaired_blocking = 0;
    uint64_t read_repair_repaired_background = 0;
    uint64_t global_read_repairs_canceled_due_to_concurrent_write = 0;
    // number of mutations received as a coordinator
    uint64_t received_mutations = 0;
    // number of counter updates received as a leader
    uint64_t received_counter_updates = 0;
    // number of forwarded mutations
    uint64_t forwarded_mutations = 0;
    uint64_t forwarding_errors = 0;
    // number of read requests received as a replica
    uint64_t replica_data_reads = 0;
    uint64_t replica_digest_reads = 0;
    uint64_t replica_mutation_data_reads = 0;
    uint64_t replica_cross_shard_ops = 0;
    utils::timed_rate_moving_average_summary_and_histogram read;
    utils::timed_rate_moving_average_summary_and_histogram range;
    utils::timed_rate_moving_average_summary_and_histogram cas_read;
    uint64_t reads = 0;
    uint64_t foreground_reads = 0; // client still waits for the read
    uint64_t read_retries = 0; // read is retried with new limit
    uint64_t speculative_digest_reads = 0;
    uint64_t speculative_data_reads = 0;
    uint64_t cas_read_unfinished_commit = 0;
    uint64_t cas_foreground = 0;
    uint64_t cas_total_running = 0;
    uint64_t cas_total_operations = 0;
    // Data read attempts
    split_stats data_read_attempts;
    split_stats data_read_completed;
    split_stats data_read_errors;
    // Digest read attempts
    split_stats digest_read_attempts;
    split_stats digest_read_completed;
    split_stats digest_read_errors;
    // Mutation data read attempts
    split_stats mutation_data_read_attempts;
    split_stats mutation_data_read_completed;
    split_stats mutation_data_read_errors;
public:
};
struct global_write_stats {
    seastar::metrics::metric_groups _metrics;
    uint64_t background_write_bytes = 0;
    uint64_t queued_write_bytes = 0;
};
struct global_stats : public global_write_stats {
};
}
}
namespace db {
namespace view {
struct stats : public service::storage_proxy_stats::write_stats {
    int64_t view_updates_pushed_local = 0;
    int64_t view_updates_pushed_remote = 0;
    int64_t view_updates_failed_local = 0;
    int64_t view_updates_failed_remote = 0;
    using label_instance = seastar::metrics::label_instance;
private:
    label_instance _ks_label;
    label_instance _cf_label;
};
} // namespace view
} // namespace db
namespace db::view {
struct update_backlog {
    size_t current;
    size_t max;
};
}
// row_locker provides a mechanism needed by the Materialized Views code to
// lock clustering rows or entire partitions. The locks are shared/exclusive
// (a.k.a. read/write) locks, and locking a row always first locks the
// partition containing it with a shared lock.
//
// Each row_locker is local to a shard (obviously), and to one specific
// column_family. row_locker needs to know the column_family's schema, and
// if that schema is updated the upgrade() method should be called so that
// row_locker could release its shared-pointer to the old schema, and take
// the new.
class row_locker {
public:
    struct single_lock_stats {
        uint64_t lock_acquisitions = 0;
        uint64_t operations_currently_waiting_for_lock = 0;
        utils::time_estimated_histogram estimated_waiting_for_lock;
    };
    struct stats {
        single_lock_stats exclusive_row;
        single_lock_stats shared_row;
        single_lock_stats exclusive_partition;
        single_lock_stats shared_partition;
    };
    struct latency_stats_tracker {
        single_lock_stats& lock_stats;
        utils::latency_counter waiting_latency;
    };
    // row_locker's locking functions lock_pk(), lock_ck() return a
    // "lock_holder" object. When the caller destroys the object it received,
    // the lock is released. The same type "lock_holder" is used regardless
    // of whether a row or partition was locked, for read or write.
    class lock_holder {
        row_locker* _locker;
        // The lock holder pointers to the partition and clustering keys,
        // which are stored inside the _two_level_locks hash table (we may
        // only drop them from the hash table when all the lock holders for
        // this partition or row are released).
        const dht::decorated_key* _partition;
        bool _partition_exclusive;
        const clustering_key_prefix* _row;
        bool _row_exclusive;
    public:
        // Allow move (noexcept) but disallow copy
    };
private:
    schema_ptr _schema;
    using lock_type = basic_rwlock<db::timeout_clock>;
    struct two_level_lock {
        lock_type _partition_lock;
        struct clustering_key_prefix_less {
            // Since the schema object may change, we need to use the
            // row_locker's current schema every time.
            const row_locker* locker;
        };
        std::map<clustering_key_prefix, lock_type, clustering_key_prefix_less> _row_locks;
    };
    struct decorated_key_hash {
    };
    struct decorated_key_equals_comparator {
        const row_locker* locker;
    };
    std::unordered_map<dht::decorated_key, two_level_lock, decorated_key_hash, decorated_key_equals_comparator> _two_level_locks;
public:
    // row_locker needs to know the column_family's schema because key
    // comparisons needs the schema.
    // If new_schema is different from the current schema, convert this
    // row_locker to use the new schema, and hold the shared pointer to the
    // new schema instead of the old schema. This is a trivial operation
    // requiring just comparison/assignment - the hash tables do not need
    // to be rebuilt on upgrade().
    // Lock an entire partition with a shared or exclusive lock.
    // The key is assumed to belong to the schema saved by row_locker. If you
    // got a schema with the key, and not sure it's not a new version of the
    // schema, call upgrade() before taking the lock.
    // Lock a clustering row with a shared or exclusive lock.
    // Also, first, takes a shared lock on the partition.
    // The key is assumed to belong to the schema saved by row_locker. If you
    // got a schema with the key, and not sure it's not a new version of the
    // schema, call upgrade() before taking the lock.
};
// Simple proportional controller to adjust shares for processes for which a backlog can be clearly
// defined.
//
// Goal is to consume the backlog as fast as we can, but not so fast that we steal all the CPU from
// incoming requests, and at the same time minimize user-visible fluctuations in the quota.
//
// What that translates to is we'll try to keep the backlog's firt derivative at 0 (IOW, we keep
// backlog constant). As the backlog grows we increase CPU usage, decreasing CPU usage as the
// backlog diminishes.
//
// The exact point at which the controller stops determines the desired CPU usage. As the backlog
// grows and approach a maximum desired, we need to be more aggressive. We will therefore define two
// thresholds, and increase the constant as we cross them.
//
// Doing that divides the range in three (before the first, between first and second, and after
// second threshold), and we'll be slow to grow in the first region, grow normally in the second
// region, and aggressively in the third region.
//
// The constants q1 and q2 are used to determine the proportional factor at each stage.
class backlog_controller {
public:
    struct scheduling_group {
        seastar::scheduling_group cpu = default_scheduling_group();
        seastar::io_priority_class io = default_priority_class();
    };
protected:
    struct control_point {
        float input;
        float output;
    };
    scheduling_group _scheduling_group;
    timer<> _update_timer;
    std::vector<control_point> _control_points;
    std::function<float()> _current_backlog;
    // updating shares for an I/O class may contact another shard and returns a future.
    future<> _inflight_update;
    // Used when the controllers are disabled and a static share is used
    // When that option is deprecated we should remove this.
    float _static_shares;
public:
};
// memtable flush CPU controller.
//
// - First threshold is the soft limit line,
// - Maximum is the point in which we'd stop consuming request,
// - Second threshold is halfway between them.
//
// Below the soft limit, we are in no particular hurry to flush, since it means we're set to
// complete flushing before we a new memtable is ready. The quota is dirty * q1, and q1 is set to a
// low number.
//
// The first half of the virtual dirty region is where we expect to be usually, so we have a low
// slope corresponding to a sluggish response between q1 * soft_limit and q2.
//
// In the second half, we're getting close to the hard dirty limit so we increase the slope and
// become more responsive, up to a maximum quota of qmax.
class flush_controller : public backlog_controller {
    static constexpr float hard_dirty_limit = 1.0f;
public:
};
class compaction_controller : public backlog_controller {
public:
    static constexpr unsigned normalization_factor = 30;
    static constexpr float disable_backlog = std::numeric_limits<double>::infinity();
};
namespace bi = boost::intrusive;
using namespace seastar;
class flat_mutation_reader_v2;
using flat_mutation_reader_v2_opt = optimized_optional<flat_mutation_reader_v2>;
/// Specific semaphore for controlling reader concurrency
///
/// Use `make_permit()` to create a permit to track the resource consumption
/// of a specific read. The permit should be created before the read is even
/// started so it is available to track resource consumption from the start.
/// Reader concurrency is dual limited by count and memory.
/// The semaphore can be configured with the desired limits on
/// construction. New readers will only be admitted when there is both
/// enough count and memory units available. Readers are admitted in
/// FIFO order.
/// Semaphore's `name` must be provided in ctor and its only purpose is
/// to increase readability of exceptions: both timeout exceptions and
/// queue overflow exceptions (read below) include this `name` in messages.
/// It's also possible to specify the maximum allowed number of waiting
/// readers by the `max_queue_length` constructor parameter. When the
/// number of waiting readers becomes equal or greater than
/// `max_queue_length` (upon calling `obtain_permit()`) an exception of
/// type `std::runtime_error` is thrown. Optionally, some additional
/// code can be executed just before throwing (`prethrow_action` 
/// constructor parameter).
///
/// The semaphore has 3 layers of defense against consuming more memory
/// than desired:
/// 1) After memory consumption is larger than the configured memory limit,
///    no more reads are admitted
/// 2) After memory consumption is larger than `_serialize_limit_multiplier`
///    times the configured memory limit, reads are serialized: only one of them
///    is allowed to make progress, the rest is made to wait before they can
///    consume more memory. Enforced via `request_memory()`.
/// 4) After memory consumption is larger than `_kill_limit_multiplier`
///    times the configured memory limit, reads are killed, by `consume()`
///    throwing `std::bad_alloc`.
///
/// This makes `_kill_limit_multiplier` times the memory limit the effective
/// upper bound of the memory consumed by reads.
///
/// The semaphore also acts as an execution stage for reads. This
/// functionality is exposed via \ref with_permit() and \ref
/// with_ready_permit().
class reader_concurrency_semaphore {
public:
    using resources = reader_resources;
    friend class reader_permit;
    enum class evict_reason {
        permit, // evicted due to permit shortage
        time, // evicted due to expiring ttl
        manual, // evicted manually via `try_evict_one_inactive_read()`
    };
    using eviction_notify_handler = noncopyable_function<void(evict_reason)>;
    struct stats {
        // The number of inactive reads evicted to free up permits.
        uint64_t permit_based_evictions = 0;
        // The number of inactive reads evicted due to expiring.
        uint64_t time_based_evictions = 0;
        // The number of inactive reads currently registered.
        uint64_t inactive_reads = 0;
        // Total number of successful reads executed through this semaphore.
        uint64_t total_successful_reads = 0;
        // Total number of failed reads executed through this semaphore.
        uint64_t total_failed_reads = 0;
        // Total number of reads rejected because the admission queue reached its max capacity
        uint64_t total_reads_shed_due_to_overload = 0;
        // Total number of reads killed due to the memory consumption reaching the kill limit.
        uint64_t total_reads_killed_due_to_kill_limit = 0;
        // Total number of reads admitted, via all admission paths.
        uint64_t reads_admitted = 0;
        // Total number of reads enqueued to wait for admission.
        uint64_t reads_enqueued_for_admission = 0;
        // Total number of reads enqueued to wait for memory.
        uint64_t reads_enqueued_for_memory = 0;
        // Total number of reads admitted immediately, without queueing
        uint64_t reads_admitted_immediately = 0;
        // Total number of reads enqueued because ready_list wasn't empty
        uint64_t reads_queued_because_ready_list = 0;
        // Total number of reads enqueued because there are permits who need CPU to make progress
        uint64_t reads_queued_because_need_cpu_permits = 0;
        // Total number of reads enqueued because there weren't enough memory resources
        uint64_t reads_queued_because_memory_resources = 0;
        // Total number of reads enqueued because there weren't enough count resources
        uint64_t reads_queued_because_count_resources = 0;
        // Total number of reads enqueued to be maybe admitted after evicting some inactive reads
        uint64_t reads_queued_with_eviction = 0;
        // Total number of permits created so far.
        uint64_t total_permits = 0;
        // Current number of permits.
        uint64_t current_permits = 0;
        // Current number permits needing CPU to make progress.
        uint64_t need_cpu_permits = 0;
        // Current number of permits awaiting I/O or an operation running on a remote shard.
        uint64_t awaits_permits = 0;
        // Current number of reads reading from the disk.
        uint64_t disk_reads = 0;
        // The number of sstables read currently.
        uint64_t sstables_read = 0;
        // Permits waiting on something: admission, memory or execution
        uint64_t waiters = 0;
    };
    using permit_list_type = bi::list<
            reader_permit::impl,
            bi::base_hook<bi::list_base_hook<bi::link_mode<bi::auto_unlink>>>,
            bi::constant_time_size<false>>;
    using read_func = noncopyable_function<future<>(reader_permit)>;
private:
    struct inactive_read;
public:
    class inactive_read_handle {
        reader_permit_opt _permit;
        friend class reader_concurrency_semaphore;
    private:
    public:
    };
private:
    resources _initial_resources;
    resources _resources;
    struct wait_queue {
        // Stores entries for permits waiting to be admitted.
        permit_list_type _admission_queue;
        // Stores entries for serialized permits waiting to obtain memory.
        permit_list_type _memory_queue;
    public:
    };
    wait_queue _wait_list;
    permit_list_type _ready_list;
    condition_variable _ready_list_cv;
    permit_list_type _inactive_reads;
    // Stores permits that are not in any of the above list.
    permit_list_type _permit_list;
    sstring _name;
    size_t _max_queue_length = std::numeric_limits<size_t>::max();
    utils::updateable_value<uint32_t> _serialize_limit_multiplier;
    utils::updateable_value<uint32_t> _kill_limit_multiplier;
    stats _stats;
    bool _stopped = false;
    bool _evicting = false;
    gate _close_readers_gate;
    gate _permit_gate;
    std::optional<future<>> _execution_loop_future;
    reader_permit::impl* _blessed_permit = nullptr;
private:
    [[nodiscard]] flat_mutation_reader_v2 detach_inactive_reader(reader_permit::impl&, evict_reason reason) noexcept;
    [[nodiscard]] std::exception_ptr check_queue_size(std::string_view queue_name);
    // Add the permit to the wait queue and return the future which resolves when
    // the permit is admitted (popped from the queue).
    enum class wait_on { admission, memory };
    // Check whether permit can be admitted or not.
    // The wait list is not taken into consideration, this is the caller's
    // responsibility.
    // A return value of can_admit::maybe means admission might be possible if
    // some of the inactive readers are evicted.
    enum class can_admit { no, maybe, yes };
    enum class reason { all_ok = 0, ready_list, need_cpu_permits, memory_resources, count_resources };
    struct admit_result { can_admit decision; reason why; };
    // Request more memory for the permit.
    // Request is instantly granted while memory consumption of all reads is
    // below _kill_limit_multiplier.
    // After memory consumption goes above the above limit, only one reader
    // (permit) is allowed to make progress, this method will block for all other
    // one, until:
    // * The blessed read finishes and a new blessed permit is choosen.
    // * Memory consumption falls below the limit.
    // closes reader in the background.
    
    
    // Throws std::bad_alloc if memory consumed is oom_kill_limit_multiply_threshold more than the memory limit.
public:
    struct no_limits { };
    /// Create a semaphore with the specified limits
    ///
    /// The semaphore's name has to be unique!
    /// Create a semaphore with practically unlimited count and memory.
    ///
    /// And conversely, no queue limit either.
    /// The semaphore's name has to be unique!
    /// A helper constructor *only for tests* that supplies default arguments.
    /// The other constructors have default values removed so 'production-code'
    /// is forced to specify all of them manually to avoid bugs.
    struct for_tests{};
    /// Returns the name of the semaphore
    ///
    /// If the semaphore has no name, "unnamed reader concurrency semaphore" is returned.
    /// Register an inactive read.
    ///
    /// The semaphore will evict this read when there is a shortage of
    /// permits. This might be immediate, during this register call.
    /// Clients can use the returned handle to unregister the read, when it
    /// stops being inactive and hence evictable, or to set the optional
    /// notify_handler and ttl.
    ///
    /// The semaphore takes ownership of the passed in reader for the duration
    /// of its inactivity and it may evict it to free up resources if necessary.
    /// Set the inactive read eviction notification handler and optionally eviction ttl.
    ///
    /// The semaphore may evict this read when there is a shortage of
    /// permits or after the given ttl expired.
    ///
    /// The notification handler will be called when the inactive read is evicted
    /// passing with the reason it was evicted to the handler.
    ///
    /// Note that the inactive read might have already been evicted if
    /// the caller may yield after the register_inactive_read returned the handle
    /// and before calling set_notify_handler. In this case, the caller must revalidate
    /// the inactive_read_handle before calling this function.
    /// Unregister the previously registered inactive read.
    ///
    /// If the read was not evicted, the inactive read object, passed in to the
    /// register call, will be returned. Otherwise a nullptr is returned.
    /// Try to evict an inactive read.
    ///
    /// Return true if an inactive read was evicted and false otherwise
    /// (if there was no reader to evict).
    /// Clear all inactive reads.
    /// Evict all inactive reads the belong to the table designated by the id.
private:
    // The following two functions are extension points for
    // future inheriting classes that needs to run some stop
    // logic just before or just after the current stop logic.
public:
    /// Stop the reader_concurrency_semaphore and clear all inactive reads.
    ///
    /// Wait on all async background work to complete.
    /// Make an admitted permit
    ///
    /// The permit is already in an admitted state after being created, this
    /// method includes waiting for admission.
    /// The permit is associated with a schema, which is the schema of the table
    /// the read is executed against, and the operation name, which should be a
    /// name such that we can identify the operation which created this permit.
    /// Ideally this should be a unique enough name that we not only can identify
    /// the kind of read, but the exact code-path that was taken.
    ///
    /// Some permits cannot be associated with any table, so passing nullptr as
    /// the schema parameter is allowed.
    /// Make a tracking only permit
    ///
    /// The permit is not admitted. It is intended for reads that bypass the
    /// normal concurrency control, but whose resource usage we still want to
    /// keep track of, as part of that concurrency control.
    /// The permit is associated with a schema, which is the schema of the table
    /// the read is executed against, and the operation name, which should be a
    /// name such that we can identify the operation which created this permit.
    /// Ideally this should be a unique enough name that we not only can identify
    /// the kind of read, but the exact code-path that was taken.
    ///
    /// Some permits cannot be associated with any table, so passing nullptr as
    /// the schema parameter is allowed.
    /// Run the function through the semaphore's execution stage with an admitted permit
    ///
    /// First a permit is obtained via the normal admission route, as if
    /// it was created  with \ref obtain_permit(), then func is enqueued to be
    /// run by the semaphore's execution loop. This emulates an execution stage,
    /// as it allows batching multiple funcs to be run together. Unlike an
    /// execution stage, with_permit() accepts a type-erased function, which
    /// allows for more flexibility in what functions are batched together.
    /// Use only functions that share most of their code to benefit from the
    /// instruction-cache warm-up!
    ///
    /// The permit is associated with a schema, which is the schema of the table
    /// the read is executed against, and the operation name, which should be a
    /// name such that we can identify the operation which created this permit.
    /// Ideally this should be a unique enough name that we not only can identify
    /// the kind of read, but the exact code-path that was taken.
    ///
    /// Some permits cannot be associated with any table, so passing nullptr as
    /// the schema parameter is allowed.
    /// Run the function through the semaphore's execution stage with a pre-admitted permit
    ///
    /// Same as \ref with_permit(), but it uses an already admitted
    /// permit. Should only be used when a permit is already readily
    /// available, e.g. when resuming a saved read. Using
    /// \ref obtain_permit(), then \ref with_ready_permit() is less
    /// optimal then just using \ref with_permit().
    /// Set the total resources of the semaphore to \p r.
    ///
    /// After this call, \ref initial_resources() will reflect the new value.
    /// Available resources will be adjusted by the delta.
    /// Dump diagnostics printout
    ///
    /// Use max-lines to cap the number of (permit) lines in the report.
    /// Use 0 for unlimited.
};
namespace query {
extern logging::logger qrlogger;
/// Consume a page worth of data from the reader.
///
/// Uses `compaction_state` for compacting the fragments and `consumer` for
/// building the results.
/// Returns a future containing a tuple with the last consumed clustering key,
/// or std::nullopt if the last row wasn't a clustering row, and whatever the
/// consumer's `consume_end_of_stream()` method returns.
 ;
class querier_base {
    friend class querier_utils;
public:
    struct querier_config {
        uint32_t tombstone_warn_threshold {0}; // 0 disabled
    };
protected:
    schema_ptr _schema;
    reader_permit _permit;
    lw_shared_ptr<const dht::partition_range> _range;
    std::unique_ptr<const query::partition_slice> _slice;
    std::variant<flat_mutation_reader_v2, reader_concurrency_semaphore::inactive_read_handle> _reader;
    dht::partition_ranges_view _query_ranges;
    querier_config _qr_config;
public:
};
/// One-stop object for serving queries.
///
/// Encapsulates all state and logic for serving all pages for a given range
/// of a query on a given shard. Can be used with any CompactedMutationsConsumer
/// certified result-builder.
/// Intended to be created on the first page of a query then saved and reused on
/// subsequent pages.
/// (1) Create with the parameters of your query.
/// (2) Call consume_page() with your consumer to consume the contents of the
///     next page.
/// (3) At the end of the page save the querier if you expect more pages.
///     The are_limits_reached() method can be used to determine whether the
///     page was filled or not. Also check your result builder for short reads.
///     Most result builders have memory-accounters that will stop the read
///     once some memory limit was reached. This is called a short read as the
///     read stops before the row and/or partition limits are reached.
/// (4) At the beginning of the next page validate whether it can be used with
///     the page's schema and start position. In case a schema or position
///     mismatch is detected the querier shouldn't be used to produce the next
///     page. It should be dropped instead and a new one should be created
///     instead.
class querier : public querier_base {
    lw_shared_ptr<compact_for_query_state_v2> _compaction_state;
public:
     ;
};
/// Local state of a multishard query.
///
/// This querier is not intended to be used directly to read pages. Instead it
/// is merely a shard local state of a suspended multishard query and is
/// intended to be used for storing the state of the query on each shard where
/// it executes. It stores the local reader and the referenced parameters it was
/// created with (similar to other queriers).
/// For position validation purposes (at lookup) the reader's position is
/// considered to be the same as that of the query.
class shard_mutation_querier : public querier_base {
    std::unique_ptr<const dht::partition_range_vector> _query_ranges;
    full_position _nominal_pos;
private:
public:
};
/// Special-purpose cache for saving queriers between pages.
///
/// Queriers are saved at the end of the page and looked up at the beginning of
/// the next page. The lookup() always removes the querier from the cache, it
/// has to be inserted again at the end of the page.
/// Lookup provides the following extra logic, special to queriers:
/// * It accepts a factory function which is used to create a new querier if
///     the lookup fails (see below). This allows for simple call sites.
/// * It does range matching. A query sometimes will result in multiple querier
///     objects executing on the same node and shard paralelly. To identify the
///     appropriate querier lookup() will consider - in addition to the lookup
///     key - the read range.
/// * It does schema version and position checking. In some case a subsequent
///     page will have a different schema version or will start from a position
///     that is before the end position of the previous page. lookup() will
///     recognize these cases and drop the previous querier and create a new one.
///
/// Inserted queriers will have a TTL. When this expires the querier is
/// evicted. This is to avoid excess and unnecessary resource usage due to
/// abandoned queriers.
/// Registers cached readers with the reader concurrency semaphore, as inactive
/// readers, so the latter can evict them if needed.
/// Keeps the total memory consumption of cached queriers
/// below max_queriers_memory_usage by evicting older entries upon inserting
/// new ones if the the memory consupmtion would go above the limit.
class querier_cache {
public:
    static const std::chrono::seconds default_entry_ttl;
    struct stats {
        // The number of inserts into the cache.
        uint64_t inserts = 0;
        // The number of cache lookups.
        uint64_t lookups = 0;
        // The subset of lookups that missed.
        uint64_t misses = 0;
        // The subset of lookups that hit but the looked up querier had to be
        // dropped due to position mismatch.
        uint64_t drops = 0;
        // The number of queriers evicted due to their TTL expiring.
        uint64_t time_based_evictions = 0;
        // The number of queriers evicted to free up resources to be able to
        // create new readers.
        uint64_t resource_based_evictions = 0;
        // The number of queriers currently in the cache.
        uint64_t population = 0;
    };
    using index = std::unordered_multimap<query_id, std::unique_ptr<querier_base>>;
private:
    index _data_querier_index;
    index _mutation_querier_index;
    index _shard_mutation_querier_index;
    std::chrono::seconds _entry_ttl;
    stats _stats;
    gate _closing_gate;
private:
    ;
    ;
public:
    // this is captured
    /// Lookup a data querier in the cache.
    ///
    /// Queriers are found based on `key` and `range`. There may be multiple
    /// queriers for the same `key` differentiated by their read range. Since
    /// each subsequent page may have a narrower read range then the one before
    /// it ranges cannot be simply matched based on equality. For matching we
    /// use the fact that the coordinator splits the query range into
    /// non-overlapping ranges. Thus both bounds of any range, or in case of
    /// singular ranges only the start bound are guaranteed to be unique.
    ///
    /// The found querier is checked for a matching position and schema version.
    /// The start position of the querier is checked against the start position
    /// of the page using the `range' and `slice'.
    /// Lookup a mutation querier in the cache.
    ///
    /// See \ref lookup_data_querier().
    /// Lookup a shard mutation querier in the cache.
    ///
    /// See \ref lookup_data_querier().
    /// Change the ttl of cache entries
    ///
    /// Applies only to entries inserted after the change.
    /// Evict a querier.
    ///
    /// Return true if a querier was evicted and false otherwise (if the cache
    /// is empty).
    /// Close all queriers and wait on background work.
    ///
    /// Should be used before destroying the querier_cache.
};
} // namespace query
namespace ser {
template <typename T>
class serializer;
};
class cache_temperature {
    float hit_rate;
public:
    explicit cache_temperature(float hr)  ;
    friend struct ser::serializer<cache_temperature>;
};
namespace data_dictionary {
class user_types_metadata {
    std::unordered_map<bytes, user_type> _user_types;
public:
    bool has_type(const bytes& name) const ;
    friend std::ostream& operator<<(std::ostream& os, const user_types_metadata& m);
};
class user_types_storage {
public:
    virtual const user_types_metadata& get(const sstring& ks) const = 0;
    virtual ~user_types_storage() = default;
};
class dummy_user_types_storage : public user_types_storage {
    user_types_metadata _empty;
public:
    virtual const user_types_metadata& get(const sstring& ks) const override ;
};
}
namespace data_dictionary {
struct storage_options {
    struct local {
        static constexpr std::string_view name = "LOCAL";
    };
    struct s3 {
        sstring bucket;
        sstring endpoint;
        static constexpr std::string_view name = "S3";
        
    };
    using value_type = std::variant<local, s3>;
    value_type value = local{};
    
};
} // namespace data_dictionary
namespace data_dictionary {
class keyspace_metadata final : public keyspace_element {
    sstring _name;
    sstring _strategy_name;
    locator::replication_strategy_config_options _strategy_options;
    std::unordered_map<sstring, schema_ptr> _cf_meta_data;
    bool _durable_writes;
    user_types_metadata _user_types;
    lw_shared_ptr<const storage_options> _storage_options;
public:
};
}
using namespace seastar;
struct sstring_hash {
    using is_transparent = void;
    size_t operator()(std::string_view v) const noexcept;
};
struct sstring_eq {
    using is_transparent = void;
    bool operator()(std::string_view a, std::string_view b) const noexcept ;
};
template <typename K, typename V, typename... Ts>
struct flat_hash_map : public absl::flat_hash_map<K, V, Ts...> {
};
template <typename V>
struct flat_hash_map<sstring, V>
    : public absl::flat_hash_map<sstring, V, sstring_hash, sstring_eq> {};
using namespace seastar;
namespace utils {
class barrier_aborted_exception : public std::exception {
public:
    
};
// Shards-coordination mechanism that allows shards to wait each other at
// certain points. The barrier should be copied to each shard, then when
// each shard calls .arrive_and_wait()-s it will be blocked and woken up
// after all other shards do the same. The call to .arrive_and_wait() is
// not one-shot but is re-entrable. Every time a shard calls it it gets
// blocked until the corresponding step from others.
//
// Calling the arrive_and_wait() by one shard in one "phase" must be done
// exactly one time. If not called other shards will be blocked for ever,
// the second call will trigger the respective assertion.
//
// A recommended usage is inside sharded<> service. For example
//
//   class foo {
//       cross_shard_barrier barrier;
//       foo(cross_shard_barrier b) : barrier(std::move(b)) {}
//   };
//
//   sharded<foo> f;
//
//   // Start a sharded service and spread the barrier between instances
//   co_await f.start(cross_shard_barrier());
//
//   // On each shard start synchronizing instances with each-other
//   f.invoke_on_all([] (auto& f) {
//      co_await f.do_something();
//      co_await f.barrier.arrive_and_wait();
//      co_await f.do_something_else();
//      co_await f.barrier.arrive_and_wait();
//      co_await f.cleanup();
//   });
//
// In the above example each shard will only call the do_something_else()
// after _all_ other shards complete their do_something()s. Respectively,
// the cleanup() on each shard will only start after do_something_else()
// completes on _all_ of them.
class cross_shard_barrier {
    struct barrier {
        std::atomic<int> counter;
        std::atomic<bool> alive;
        std::vector<std::optional<promise<>>> wakeup;
    };
    std::shared_ptr<barrier> _b;
public:
    // The 'solo' mode turns all the synchronization off, calls to
    // arrive_and_wait() never block. Eliminates the need to mess
    // with conditional usage in callers.
    struct solo {};
private:
};
} // namespace utils
namespace sstables {
extern logging::logger sstlog;
class generation_type {
public:
    using int_t = int64_t;
private:
    utils::UUID _value;
public:
    
    // use zero as the timestamp to differentiate from the regular timeuuid,
    // and use the least_sig_bits to encode the value of generation identifier.
    explicit constexpr generation_type(int_t value) noexcept
        : _value(utils::UUID_gen::create_time(std::chrono::milliseconds::zero()), value) {}
    constexpr int_t as_int() const noexcept ;
    
    // convert to data_value
    //
    // this function is used when performing queries to SSTABLES_REGISTRY in
    // the "system_keyspace", since its "generation" column cannot be a variant
    // of bigint and timeuuid, we need to use a single value to represent these
    // two types, and single value should allow us to tell the type of the
    // original value of generation identifier, so we can convert the value back
    // to the generation when necessary without losing its type information.
    // since the timeuuid always encodes the timestamp in its MSB, and the timestamp
    // should always be greater than zero, we use this fact to tell a regular
    // timeuuid from a timeuuid converted from a bigint -- we just use zero
    // for its timestamp of the latter.
    
};
 ;
template <typename Target = std::vector<sstables::generation_type>>
Target generations_from_values(std::initializer_list<generation_type::int_t> values) {
    return boost::copy_range<Target>(values | boost::adaptors::transformed([] (auto value) {
        return generation_type(value);
    }));
}
class sstable_generation_generator {
    // We still want to do our best to keep the generation numbers shard-friendly.
    // Each destination shard will manage its own generation counter.
    //
    // operator() is called by multiple shards in parallel when performing reshard,
    // so we have to use atomic<> here.
    using int_t = sstables::generation_type::int_t;
    int_t _last_generation;
public:
};
} //namespace sstables
namespace std {
template <>
struct hash<sstables::generation_type> {
    size_t operator()(const sstables::generation_type& generation) const noexcept {
        return hash<sstables::generation_type::int_t>{}(generation.as_int());
    }
};
// for min_max_tracker
template <>
struct numeric_limits<sstables::generation_type> : public numeric_limits<sstables::generation_type::int_t> {
    static constexpr sstables::generation_type min() noexcept {
        return sstables::generation_type{numeric_limits<sstables::generation_type::int_t>::min()};
    }
    static constexpr sstables::generation_type max() noexcept {
        return sstables::generation_type{numeric_limits<sstables::generation_type::int_t>::max()};
    }
};
} //namespace std
template <>
struct fmt::formatter<sstables::generation_type> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const sstables::generation_type& generation, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", generation.as_int());
    }
};
namespace bs2 = boost::signals2;
using disk_error_signal_type = bs2::signal_type<void (), bs2::keywords::mutex_type<bs2::dummy_mutex>>::type;
extern thread_local disk_error_signal_type commit_error;
extern thread_local disk_error_signal_type sstable_read_error;
extern thread_local disk_error_signal_type sstable_write_error;
extern thread_local disk_error_signal_type general_disk_error;
using io_error_handler = std::function<void (std::exception_ptr)>;
// stores a function that generates a io handler for a given signal.
using io_error_handler_gen = std::function<io_error_handler (disk_error_signal_type&)>;
io_error_handler default_io_error_handler(disk_error_signal_type& signal);
// generates handler that handles exception for a given signal
io_error_handler_gen default_io_error_handler_gen();
extern thread_local io_error_handler commit_error_handler;
extern thread_local io_error_handler sstable_write_error_handler;
extern thread_local io_error_handler general_disk_error_handler;
template<typename Func, typename... Args>
requires std::invocable<Func, Args&&...>
        && (!is_future<std::invoke_result_t<Func, Args&&...>>::value)
std::invoke_result_t<Func, Args&&...>
do_io_check(const io_error_handler& error_handler, Func&& func, Args&&... args) {
    try {
        // calling function
        return func(std::forward<Args>(args)...);
    } catch (...) {
        error_handler(std::current_exception());
        throw;
    }
}
template<typename Func, typename... Args>
requires std::invocable<Func, Args&&...>
        && is_future<std::invoke_result_t<Func, Args&&...>>::value
auto do_io_check(const io_error_handler& error_handler, Func&& func, Args&&... args) noexcept {
    return futurize_invoke(func, std::forward<Args>(args)...).handle_exception([&error_handler] (auto ep) {
        error_handler(ep);
        return futurize<std::result_of_t<Func(Args&&...)>>::make_exception_future(ep);
    });
}
template<typename Func, typename... Args>
auto commit_io_check(Func&& func, Args&&... args) noexcept(is_future<std::result_of_t<Func(Args&&...)>>::value) {
    return do_io_check(commit_error_handler, std::forward<Func>(func), std::forward<Args>(args)...);
}
template<typename Func, typename... Args>
auto sstable_io_check(const io_error_handler& error_handler, Func&& func, Args&&... args) noexcept(is_future<std::result_of_t<Func(Args&&...)>>::value) {
    return do_io_check(error_handler, std::forward<Func>(func), std::forward<Args>(args)...);
}
template<typename Func, typename... Args>
auto io_check(const io_error_handler& error_handler, Func&& func, Args&&... args) noexcept(is_future<std::result_of_t<Func(Args&&...)>>::value) {
    return do_io_check(error_handler, general_disk_error, std::forward<Func>(func), std::forward<Args>(args)...);
}
template<typename Func, typename... Args>
auto io_check(Func&& func, Args&&... args) noexcept(is_future<std::result_of_t<Func(Args&&...)>>::value) {
    return do_io_check(general_disk_error_handler, std::forward<Func>(func), std::forward<Args>(args)...);
}
#if defined(_WIN32)
#else
#endif
namespace rust {
inline namespace cxxbridge1 {
struct unsafe_bitcopy_t;
namespace {
template <typename T>
class impl;
}
#ifndef CXXBRIDGE1_RUST_STRING
#define CXXBRIDGE1_RUST_STRING
// https://cxx.rs/binding/string.html
class String final {
public:
  String() noexcept;
  String(const String &) noexcept;
  String(String &&) noexcept;
  ~String() noexcept;
  String(const std::string &);
  String(const char *);
  String(const char *, std::size_t);
  String(const char16_t *);
  String(const char16_t *, std::size_t);
  // Replace invalid Unicode data with the replacement character (U+FFFD).
  static String lossy(const std::string &) noexcept;
  static String lossy(const char *) noexcept;
  static String lossy(const char *, std::size_t) noexcept;
  static String lossy(const char16_t *) noexcept;
  static String lossy(const char16_t *, std::size_t) noexcept;
  String &operator=(const String &) &noexcept;
  String &operator=(String &&) &noexcept;
  explicit operator std::string() const;
  // Note: no null terminator.
  const char *data() const noexcept;
  std::size_t size() const noexcept;
  std::size_t length() const noexcept;
  bool empty() const noexcept;
  const char *c_str() noexcept;
  std::size_t capacity() const noexcept;
  void reserve(size_t new_cap) noexcept;
  using iterator = char *;
  iterator begin() noexcept;
  iterator end() noexcept;
  using const_iterator = const char *;
  const_iterator begin() const noexcept;
  const_iterator end() const noexcept;
  const_iterator cbegin() const noexcept;
  const_iterator cend() const noexcept;
  bool operator==(const String &) const noexcept;
  bool operator!=(const String &) const noexcept;
  bool operator<(const String &) const noexcept;
  bool operator<=(const String &) const noexcept;
  bool operator>(const String &) const noexcept;
  bool operator>=(const String &) const noexcept;
  void swap(String &) noexcept;
  // Internal API only intended for the cxxbridge code generator.
  String(unsafe_bitcopy_t, const String &) noexcept;
private:
  struct lossy_t;
  String(lossy_t, const char *, std::size_t) noexcept;
  String(lossy_t, const char16_t *, std::size_t) noexcept;
  friend void swap(String &lhs, String &rhs) noexcept { lhs.swap(rhs); }
  // Size and alignment statically verified by rust_string.rs.
  std::array<std::uintptr_t, 3> repr;
};
#endif // CXXBRIDGE1_RUST_STRING
#ifndef CXXBRIDGE1_RUST_STR
#define CXXBRIDGE1_RUST_STR
// https://cxx.rs/binding/str.html
class Str final {
public:
  Str() noexcept;
  Str(const String &) noexcept;
  Str(const std::string &);
  Str(const char *);
  Str(const char *, std::size_t);
  Str &operator=(const Str &) &noexcept = default;
  explicit operator std::string() const;
  // Note: no null terminator.
  const char *data() const noexcept;
  std::size_t size() const noexcept;
  std::size_t length() const noexcept;
  bool empty() const noexcept;
  // Important in order for System V ABI to pass in registers.
  Str(const Str &) noexcept = default;
  ~Str() noexcept = default;
  using iterator = const char *;
  using const_iterator = const char *;
  const_iterator begin() const noexcept;
  const_iterator end() const noexcept;
  const_iterator cbegin() const noexcept;
  const_iterator cend() const noexcept;
  bool operator==(const Str &) const noexcept;
  bool operator!=(const Str &) const noexcept;
  bool operator<(const Str &) const noexcept;
  bool operator<=(const Str &) const noexcept;
  bool operator>(const Str &) const noexcept;
  bool operator>=(const Str &) const noexcept;
  void swap(Str &) noexcept;
private:
  class uninit;
  Str(uninit) noexcept;
  friend impl<Str>;
  std::array<std::uintptr_t, 2> repr;
};
#endif // CXXBRIDGE1_RUST_STR
#ifndef CXXBRIDGE1_RUST_SLICE
namespace detail {
template <bool>
struct copy_assignable_if {};
template <>
struct copy_assignable_if<false> {
  copy_assignable_if() noexcept = default;
  copy_assignable_if(const copy_assignable_if &) noexcept = default;
  copy_assignable_if &operator=(const copy_assignable_if &) &noexcept = delete;
  copy_assignable_if &operator=(copy_assignable_if &&) &noexcept = default;
};
} // namespace detail
// https://cxx.rs/binding/slice.html
template <typename T>
class Slice final
    : private detail::copy_assignable_if<std::is_const<T>::value> {
public:
  using value_type = T;
  Slice() noexcept;
  Slice(T *, std::size_t count) noexcept;
  Slice &operator=(const Slice<T> &) &noexcept = default;
  Slice &operator=(Slice<T> &&) &noexcept = default;
  T *data() const noexcept;
  std::size_t size() const noexcept;
  std::size_t length() const noexcept;
  bool empty() const noexcept;
  T &operator[](std::size_t n) const noexcept;
  T &at(std::size_t n) const;
  T &front() const noexcept;
  T &back() const noexcept;
  // Important in order for System V ABI to pass in registers.
  Slice(const Slice<T> &) noexcept = default;
  ~Slice() noexcept = default;
  class iterator;
  iterator begin() const noexcept;
  iterator end() const noexcept;
  void swap(Slice &) noexcept;
private:
  class uninit;
  Slice(uninit) noexcept;
  friend impl<Slice>;
  friend void sliceInit(void *, const void *, std::size_t) noexcept;
  friend void *slicePtr(const void *) noexcept;
  friend std::size_t sliceLen(const void *) noexcept;
  std::array<std::uintptr_t, 2> repr;
};
template <typename T>
class Slice<T>::iterator final {
public:
  using iterator_category = std::random_access_iterator_tag;
  using value_type = T;
  using difference_type = std::ptrdiff_t;
  using pointer = typename std::add_pointer<T>::type;
  using reference = typename std::add_lvalue_reference<T>::type;
  reference operator*() const noexcept;
  pointer operator->() const noexcept;
  reference operator[](difference_type) const noexcept;
  iterator &operator++() noexcept;
  iterator operator++(int) noexcept;
  iterator &operator--() noexcept;
  iterator operator--(int) noexcept;
  iterator &operator+=(difference_type) noexcept;
  iterator &operator-=(difference_type) noexcept;
  iterator operator+(difference_type) const noexcept;
  iterator operator-(difference_type) const noexcept;
  difference_type operator-(const iterator &) const noexcept;
  bool operator==(const iterator &) const noexcept;
  bool operator!=(const iterator &) const noexcept;
  bool operator<(const iterator &) const noexcept;
  bool operator<=(const iterator &) const noexcept;
  bool operator>(const iterator &) const noexcept;
  bool operator>=(const iterator &) const noexcept;
private:
  friend class Slice;
  void *pos;
  std::size_t stride;
};
#endif // CXXBRIDGE1_RUST_SLICE
#ifndef CXXBRIDGE1_RUST_BOX
// https://cxx.rs/binding/box.html
template <typename T>
class Box final {
public:
  using element_type = T;
  using const_pointer =
      typename std::add_pointer<typename std::add_const<T>::type>::type;
  using pointer = typename std::add_pointer<T>::type;
  Box() = delete;
  Box(Box &&) noexcept;
  ~Box() noexcept;
  explicit Box(const T &);
  explicit Box(T &&);
  Box &operator=(Box &&) &noexcept;
  const T *operator->() const noexcept;
  const T &operator*() const noexcept;
  T *operator->() noexcept;
  T &operator*() noexcept;
  template <typename... Fields>
  static Box in_place(Fields &&...);
  void swap(Box &) noexcept;
  // Important: requires that `raw` came from an into_raw call. Do not pass a
  // pointer from `new` or any other source.
  static Box from_raw(T *) noexcept;
  T *into_raw() noexcept;
   using value_type = element_type;
private:
  class uninit;
  class allocation;
  Box(uninit) noexcept;
  void drop() noexcept;
  friend void swap(Box &lhs, Box &rhs) noexcept { lhs.swap(rhs); }
  T *ptr;
};
#endif // CXXBRIDGE1_RUST_BOX
#ifndef CXXBRIDGE1_RUST_VEC
// https://cxx.rs/binding/vec.html
template <typename T>
class Vec final {
public:
  using value_type = T;
  Vec() noexcept;
  Vec(std::initializer_list<T>);
  Vec(const Vec &);
  Vec(Vec &&) noexcept;
  ~Vec() noexcept;
  Vec &operator=(Vec &&) &noexcept;
  Vec &operator=(const Vec &) &;
  std::size_t size() const noexcept;
  bool empty() const noexcept;
  const T *data() const noexcept;
  T *data() noexcept;
  std::size_t capacity() const noexcept;
  const T &operator[](std::size_t n) const noexcept;
  const T &at(std::size_t n) const;
  const T &front() const noexcept;
  const T &back() const noexcept;
  T &operator[](std::size_t n) noexcept;
  T &at(std::size_t n);
  T &front() noexcept;
  T &back() noexcept;
  void reserve(std::size_t new_cap);
  void push_back(const T &value);
  void push_back(T &&value);
  template <typename... Args>
  void emplace_back(Args &&...args);
  void truncate(std::size_t len);
  void clear();
  using iterator = typename Slice<T>::iterator;
  iterator begin() noexcept;
  iterator end() noexcept;
  using const_iterator = typename Slice<const T>::iterator;
  const_iterator begin() const noexcept;
  const_iterator end() const noexcept;
  const_iterator cbegin() const noexcept;
  const_iterator cend() const noexcept;
  void swap(Vec &) noexcept;
  // Internal API only intended for the cxxbridge code generator.
  Vec(unsafe_bitcopy_t, const Vec &) noexcept;
private:
  void reserve_total(std::size_t new_cap) noexcept;
  void set_len(std::size_t len) noexcept;
  void drop() noexcept;
  friend void swap(Vec &lhs, Vec &rhs) noexcept { lhs.swap(rhs); }
  // Size and alignment statically verified by rust_vec.rs.
  std::array<std::uintptr_t, 3> repr;
};
#endif // CXXBRIDGE1_RUST_VEC
#ifndef CXXBRIDGE1_RUST_FN
// https://cxx.rs/binding/fn.html
template <typename Signature>
class Fn;
template <typename Ret, typename... Args>
class Fn<Ret(Args...)> final {
public:
  Ret operator()(Args... args) const noexcept;
  Fn operator*() const noexcept;
private:
  Ret (*trampoline)(Args..., void *fn) noexcept;
  void *fn;
};
#endif // CXXBRIDGE1_RUST_FN
#ifndef CXXBRIDGE1_RUST_ERROR
#define CXXBRIDGE1_RUST_ERROR
// https://cxx.rs/binding/result.html
class Error final : public std::exception {
public:
  Error(const Error &);
  Error(Error &&) noexcept;
  ~Error() noexcept override;
  Error &operator=(const Error &) &;
  Error &operator=(Error &&) &noexcept;
  const char *what() const noexcept override;
private:
  Error() noexcept = default;
  friend impl<Error>;
  const char *msg;
  std::size_t len;
};
#endif // CXXBRIDGE1_RUST_ERROR
#ifndef CXXBRIDGE1_RUST_ISIZE
#define CXXBRIDGE1_RUST_ISIZE
#if defined(_WIN32)
using isize = SSIZE_T;
#else
using isize = ssize_t;
#endif
#endif // CXXBRIDGE1_RUST_ISIZE
std::ostream &operator<<(std::ostream &, const String &);
std::ostream &operator<<(std::ostream &, const Str &);
#ifndef CXXBRIDGE1_RUST_OPAQUE
#define CXXBRIDGE1_RUST_OPAQUE
// Base class of generated opaque Rust types.
class Opaque {
public:
  Opaque() = delete;
  Opaque(const Opaque &) = delete;
  ~Opaque() = delete;
};
#endif // CXXBRIDGE1_RUST_OPAQUE
template <typename T>
std::size_t size_of();
template <typename T>
std::size_t align_of();
// IsRelocatable<T> is used in assertions that a C++ type passed by value
// between Rust and C++ is soundly relocatable by Rust.
//
// There may be legitimate reasons to opt out of the check for support of types
// that the programmer knows are soundly Rust-movable despite not being
// recognized as such by the C++ type system due to a move constructor or
// destructor. To opt out of the relocatability check, do either of the
// following things in any header used by `include!` in the bridge.
//
//      --- if you define the type:
//      struct MyType {
//        ...
//    +   using IsRelocatable = std::true_type;
//      };
//
//      --- otherwise:
//    + template <>
//    + struct rust::IsRelocatable<MyType> : std::true_type {};
template <typename T>
struct IsRelocatable;
using u8 = std::uint8_t;
using u16 = std::uint16_t;
using u32 = std::uint32_t;
using u64 = std::uint64_t;
using usize = std::size_t; // see static asserts in cxx.cc
using i8 = std::int8_t;
using i16 = std::int16_t;
using i32 = std::int32_t;
using i64 = std::int64_t;
using f32 = float;
using f64 = double;
// Snake case aliases for use in code that uses this style for type names.
using string = String;
using str = Str;
template <typename T>
using slice = Slice<T>;
template <typename T>
using box = Box<T>;
template <typename T>
using vec = Vec<T>;
using error = Error;
template <typename Signature>
using fn = Fn<Signature>;
template <typename T>
using is_relocatable = IsRelocatable<T>;
////////////////////////////////////////////////////////////////////////////////
/// end public API, begin implementation details
#ifndef CXXBRIDGE1_PANIC
#define CXXBRIDGE1_PANIC
template <typename Exception>
void panic [[noreturn]] (const char *msg);
#endif // CXXBRIDGE1_PANIC
#ifndef CXXBRIDGE1_RUST_FN
#define CXXBRIDGE1_RUST_FN
template <typename Ret, typename... Args>
Ret Fn<Ret(Args...)>::operator()(Args... args) const noexcept {
  return (*this->trampoline)(std::forward<Args>(args)..., this->fn);
}
template <typename Ret, typename... Args>
Fn<Ret(Args...)> Fn<Ret(Args...)>::operator*() const noexcept {
  return *this;
}
#endif // CXXBRIDGE1_RUST_FN
#ifndef CXXBRIDGE1_RUST_BITCOPY_T
#define CXXBRIDGE1_RUST_BITCOPY_T
struct unsafe_bitcopy_t final {
  explicit unsafe_bitcopy_t() = default;
};
#endif // CXXBRIDGE1_RUST_BITCOPY_T
#ifndef CXXBRIDGE1_RUST_BITCOPY
#define CXXBRIDGE1_RUST_BITCOPY
constexpr unsafe_bitcopy_t unsafe_bitcopy{};
#endif // CXXBRIDGE1_RUST_BITCOPY
#ifndef CXXBRIDGE1_RUST_SLICE
#define CXXBRIDGE1_RUST_SLICE
template <typename T>
Slice<T>::Slice() noexcept {
  sliceInit(this, reinterpret_cast<void *>(align_of<T>()), 0);
}
template <typename T>
Slice<T>::Slice(T *s, std::size_t count) noexcept {
  assert(s != nullptr || count == 0);
  sliceInit(this,
            s == nullptr && count == 0
                ? reinterpret_cast<void *>(align_of<T>())
                : const_cast<typename std::remove_const<T>::type *>(s),
            count);
}
template <typename T>
T *Slice<T>::data() const noexcept {
  return reinterpret_cast<T *>(slicePtr(this));
}
template <typename T>
std::size_t Slice<T>::size() const noexcept {
  return sliceLen(this);
}
template <typename T>
std::size_t Slice<T>::length() const noexcept {
  return this->size();
}
template <typename T>
bool Slice<T>::empty() const noexcept {
  return this->size() == 0;
}
template <typename T>
T &Slice<T>::operator[](std::size_t n) const noexcept {
  assert(n < this->size());
  auto ptr = static_cast<char *>(slicePtr(this)) + size_of<T>() * n;
  return *reinterpret_cast<T *>(ptr);
}
template <typename T>
T &Slice<T>::at(std::size_t n) const {
  if (n >= this->size()) {
    panic<std::out_of_range>("rust::Slice index out of range");
  }
  return (*this)[n];
}
template <typename T>
T &Slice<T>::front() const noexcept {
  assert(!this->empty());
  return (*this)[0];
}
template <typename T>
T &Slice<T>::back() const noexcept {
  assert(!this->empty());
  return (*this)[this->size() - 1];
}
template <typename T>
typename Slice<T>::iterator::reference
Slice<T>::iterator::operator*() const noexcept {
  return *static_cast<T *>(this->pos);
}
template <typename T>
typename Slice<T>::iterator::pointer
Slice<T>::iterator::operator->() const noexcept {
  return static_cast<T *>(this->pos);
}
template <typename T>
typename Slice<T>::iterator::reference Slice<T>::iterator::operator[](
    typename Slice<T>::iterator::difference_type n) const noexcept {
  auto ptr = static_cast<char *>(this->pos) + this->stride * n;
  return *reinterpret_cast<T *>(ptr);
}
template <typename T>
typename Slice<T>::iterator &Slice<T>::iterator::operator++() noexcept {
  this->pos = static_cast<char *>(this->pos) + this->stride;
  return *this;
}
template <typename T>
typename Slice<T>::iterator Slice<T>::iterator::operator++(int) noexcept {
  auto ret = iterator(*this);
  this->pos = static_cast<char *>(this->pos) + this->stride;
  return ret;
}
template <typename T>
typename Slice<T>::iterator &Slice<T>::iterator::operator--() noexcept {
  this->pos = static_cast<char *>(this->pos) - this->stride;
  return *this;
}
template <typename T>
typename Slice<T>::iterator Slice<T>::iterator::operator--(int) noexcept {
  auto ret = iterator(*this);
  this->pos = static_cast<char *>(this->pos) - this->stride;
  return ret;
}
template <typename T>
typename Slice<T>::iterator &Slice<T>::iterator::operator+=(
    typename Slice<T>::iterator::difference_type n) noexcept {
  this->pos = static_cast<char *>(this->pos) + this->stride * n;
  return *this;
}
template <typename T>
typename Slice<T>::iterator &Slice<T>::iterator::operator-=(
    typename Slice<T>::iterator::difference_type n) noexcept {
  this->pos = static_cast<char *>(this->pos) - this->stride * n;
  return *this;
}
template <typename T>
typename Slice<T>::iterator Slice<T>::iterator::operator+(
    typename Slice<T>::iterator::difference_type n) const noexcept {
  auto ret = iterator(*this);
  ret.pos = static_cast<char *>(this->pos) + this->stride * n;
  return ret;
}
template <typename T>
typename Slice<T>::iterator Slice<T>::iterator::operator-(
    typename Slice<T>::iterator::difference_type n) const noexcept {
  auto ret = iterator(*this);
  ret.pos = static_cast<char *>(this->pos) - this->stride * n;
  return ret;
}
template <typename T>
typename Slice<T>::iterator::difference_type
Slice<T>::iterator::operator-(const iterator &other) const noexcept {
  auto diff = std::distance(static_cast<char *>(other.pos),
                            static_cast<char *>(this->pos));
  return diff / this->stride;
}
template <typename T>
bool Slice<T>::iterator::operator==(const iterator &other) const noexcept {
  return this->pos == other.pos;
}
template <typename T>
bool Slice<T>::iterator::operator!=(const iterator &other) const noexcept {
  return this->pos != other.pos;
}
template <typename T>
bool Slice<T>::iterator::operator<(const iterator &other) const noexcept {
  return this->pos < other.pos;
}
template <typename T>
bool Slice<T>::iterator::operator<=(const iterator &other) const noexcept {
  return this->pos <= other.pos;
}
template <typename T>
bool Slice<T>::iterator::operator>(const iterator &other) const noexcept {
  return this->pos > other.pos;
}
template <typename T>
bool Slice<T>::iterator::operator>=(const iterator &other) const noexcept {
  return this->pos >= other.pos;
}
template <typename T>
typename Slice<T>::iterator Slice<T>::begin() const noexcept {
  iterator it;
  it.pos = slicePtr(this);
  it.stride = size_of<T>();
  return it;
}
template <typename T>
typename Slice<T>::iterator Slice<T>::end() const noexcept {
  iterator it = this->begin();
  it.pos = static_cast<char *>(it.pos) + it.stride * this->size();
  return it;
}
template <typename T>
void Slice<T>::swap(Slice &rhs) noexcept {
  std::swap(*this, rhs);
}
#endif // CXXBRIDGE1_RUST_SLICE
#ifndef CXXBRIDGE1_RUST_BOX
#define CXXBRIDGE1_RUST_BOX
template <typename T>
class Box<T>::uninit {};
template <typename T>
class Box<T>::allocation {
  static T *alloc() noexcept;
  static void dealloc(T *) noexcept;
public:
  allocation() noexcept : ptr(alloc()) {}
  ~allocation() noexcept {
    if (this->ptr) {
      dealloc(this->ptr);
    }
  }
  T *ptr;
};
template <typename T>
Box<T>::Box(Box &&other) noexcept : ptr(other.ptr) {
  other.ptr = nullptr;
}
template <typename T>
Box<T>::Box(const T &val) {
  allocation alloc;
  ::new (alloc.ptr) T(val);
  this->ptr = alloc.ptr;
  alloc.ptr = nullptr;
}
template <typename T>
Box<T>::Box(T &&val) {
  allocation alloc;
  ::new (alloc.ptr) T(std::move(val));
  this->ptr = alloc.ptr;
  alloc.ptr = nullptr;
}
template <typename T>
Box<T>::~Box() noexcept {
  if (this->ptr) {
    this->drop();
  }
}
template <typename T>
Box<T> &Box<T>::operator=(Box &&other) &noexcept {
  if (this->ptr) {
    this->drop();
  }
  this->ptr = other.ptr;
  other.ptr = nullptr;
  return *this;
}
template <typename T>
const T *Box<T>::operator->() const noexcept {
  return this->ptr;
}
template <typename T>
const T &Box<T>::operator*() const noexcept {
  return *this->ptr;
}
template <typename T>
T *Box<T>::operator->() noexcept {
  return this->ptr;
}
template <typename T>
T &Box<T>::operator*() noexcept {
  return *this->ptr;
}
template <typename T>
template <typename... Fields>
Box<T> Box<T>::in_place(Fields &&...fields) {
  allocation alloc;
  auto ptr = alloc.ptr;
  ::new (ptr) T{std::forward<Fields>(fields)...};
  alloc.ptr = nullptr;
  return from_raw(ptr);
}
template <typename T>
void Box<T>::swap(Box &rhs) noexcept {
  using std::swap;
  swap(this->ptr, rhs.ptr);
}
template <typename T>
Box<T> Box<T>::from_raw(T *raw) noexcept {
  Box box = uninit{};
  box.ptr = raw;
  return box;
}
template <typename T>
T *Box<T>::into_raw() noexcept {
  T *raw = this->ptr;
  this->ptr = nullptr;
  return raw;
}
template <typename T>
Box<T>::Box(uninit) noexcept {}
#endif // CXXBRIDGE1_RUST_BOX
#ifndef CXXBRIDGE1_RUST_VEC
#define CXXBRIDGE1_RUST_VEC
template <typename T>
Vec<T>::Vec(std::initializer_list<T> init) : Vec{} {
  this->reserve_total(init.size());
  std::move(init.begin(), init.end(), std::back_inserter(*this));
}
template <typename T>
Vec<T>::Vec(const Vec &other) : Vec() {
  this->reserve_total(other.size());
  std::copy(other.begin(), other.end(), std::back_inserter(*this));
}
template <typename T>
Vec<T>::Vec(Vec &&other) noexcept : repr(other.repr) {
  new (&other) Vec();
}
template <typename T>
Vec<T>::~Vec() noexcept {
  this->drop();
}
template <typename T>
Vec<T> &Vec<T>::operator=(Vec &&other) &noexcept {
  this->drop();
  this->repr = other.repr;
  new (&other) Vec();
  return *this;
}
template <typename T>
Vec<T> &Vec<T>::operator=(const Vec &other) & {
  if (this != &other) {
    this->drop();
    new (this) Vec(other);
  }
  return *this;
}
template <typename T>
bool Vec<T>::empty() const noexcept {
  return this->size() == 0;
}
template <typename T>
T *Vec<T>::data() noexcept {
  return const_cast<T *>(const_cast<const Vec<T> *>(this)->data());
}
template <typename T>
const T &Vec<T>::operator[](std::size_t n) const noexcept {
  assert(n < this->size());
  auto data = reinterpret_cast<const char *>(this->data());
  return *reinterpret_cast<const T *>(data + n * size_of<T>());
}
template <typename T>
const T &Vec<T>::at(std::size_t n) const {
  if (n >= this->size()) {
    panic<std::out_of_range>("rust::Vec index out of range");
  }
  return (*this)[n];
}
template <typename T>
const T &Vec<T>::front() const noexcept {
  assert(!this->empty());
  return (*this)[0];
}
template <typename T>
const T &Vec<T>::back() const noexcept {
  assert(!this->empty());
  return (*this)[this->size() - 1];
}
template <typename T>
T &Vec<T>::operator[](std::size_t n) noexcept {
  assert(n < this->size());
  auto data = reinterpret_cast<char *>(this->data());
  return *reinterpret_cast<T *>(data + n * size_of<T>());
}
template <typename T>
T &Vec<T>::at(std::size_t n) {
  if (n >= this->size()) {
    panic<std::out_of_range>("rust::Vec index out of range");
  }
  return (*this)[n];
}
template <typename T>
T &Vec<T>::front() noexcept {
  assert(!this->empty());
  return (*this)[0];
}
template <typename T>
T &Vec<T>::back() noexcept {
  assert(!this->empty());
  return (*this)[this->size() - 1];
}
template <typename T>
void Vec<T>::reserve(std::size_t new_cap) {
  this->reserve_total(new_cap);
}
template <typename T>
void Vec<T>::push_back(const T &value) {
  this->emplace_back(value);
}
template <typename T>
void Vec<T>::push_back(T &&value) {
  this->emplace_back(std::move(value));
}
template <typename T>
template <typename... Args>
void Vec<T>::emplace_back(Args &&...args) {
  auto size = this->size();
  this->reserve_total(size + 1);
  ::new (reinterpret_cast<T *>(reinterpret_cast<char *>(this->data()) +
                               size * size_of<T>()))
      T(std::forward<Args>(args)...);
  this->set_len(size + 1);
}
template <typename T>
void Vec<T>::clear() {
  this->truncate(0);
}
template <typename T>
typename Vec<T>::iterator Vec<T>::begin() noexcept {
  return Slice<T>(this->data(), this->size()).begin();
}
template <typename T>
typename Vec<T>::iterator Vec<T>::end() noexcept {
  return Slice<T>(this->data(), this->size()).end();
}
template <typename T>
typename Vec<T>::const_iterator Vec<T>::begin() const noexcept {
  return this->cbegin();
}
template <typename T>
typename Vec<T>::const_iterator Vec<T>::end() const noexcept {
  return this->cend();
}
template <typename T>
typename Vec<T>::const_iterator Vec<T>::cbegin() const noexcept {
  return Slice<const T>(this->data(), this->size()).begin();
}
template <typename T>
typename Vec<T>::const_iterator Vec<T>::cend() const noexcept {
  return Slice<const T>(this->data(), this->size()).end();
}
template <typename T>
void Vec<T>::swap(Vec &rhs) noexcept {
  using std::swap;
  swap(this->repr, rhs.repr);
}
// Internal API only intended for the cxxbridge code generator.
template <typename T>
Vec<T>::Vec(unsafe_bitcopy_t, const Vec &bits) noexcept : repr(bits.repr) {}
#endif // CXXBRIDGE1_RUST_VEC
#ifndef CXXBRIDGE1_IS_COMPLETE
#define CXXBRIDGE1_IS_COMPLETE
namespace detail {
namespace {
template <typename T, typename = std::size_t>
struct is_complete : std::false_type {};
template <typename T>
struct is_complete<T, decltype(sizeof(T))> : std::true_type {};
} // namespace
} // namespace detail
#endif // CXXBRIDGE1_IS_COMPLETE
#ifndef CXXBRIDGE1_LAYOUT
#define CXXBRIDGE1_LAYOUT
class layout {
  template <typename T>
  friend std::size_t size_of();
  template <typename T>
  friend std::size_t align_of();
  template <typename T>
  static typename std::enable_if<std::is_base_of<Opaque, T>::value,
                                 std::size_t>::type
  do_size_of() {
    return T::layout::size();
  }
  template <typename T>
  static typename std::enable_if<!std::is_base_of<Opaque, T>::value,
                                 std::size_t>::type
  do_size_of() {
    return sizeof(T);
  }
  template <typename T>
  static
      typename std::enable_if<detail::is_complete<T>::value, std::size_t>::type
      size_of() {
    return do_size_of<T>();
  }
  template <typename T>
  static typename std::enable_if<std::is_base_of<Opaque, T>::value,
                                 std::size_t>::type
  do_align_of() {
    return T::layout::align();
  }
  template <typename T>
  static typename std::enable_if<!std::is_base_of<Opaque, T>::value,
                                 std::size_t>::type
  do_align_of() {
    return alignof(T);
  }
  template <typename T>
  static
      typename std::enable_if<detail::is_complete<T>::value, std::size_t>::type
      align_of() {
    return do_align_of<T>();
  }
};
template <typename T>
std::size_t size_of() {
  return layout::size_of<T>();
}
template <typename T>
std::size_t align_of() {
  return layout::align_of<T>();
}
#endif // CXXBRIDGE1_LAYOUT
#ifndef CXXBRIDGE1_RELOCATABLE
#define CXXBRIDGE1_RELOCATABLE
namespace detail {
template <typename... Ts>
struct make_void {
  using type = void;
};
template <typename... Ts>
using void_t = typename make_void<Ts...>::type;
template <typename Void, template <typename...> class, typename...>
struct detect : std::false_type {};
template <template <typename...> class T, typename... A>
struct detect<void_t<T<A...>>, T, A...> : std::true_type {};
template <template <typename...> class T, typename... A>
using is_detected = detect<void, T, A...>;
template <typename T>
using detect_IsRelocatable = typename T::IsRelocatable;
template <typename T>
struct get_IsRelocatable
    : std::is_same<typename T::IsRelocatable, std::true_type> {};
} // namespace detail
template <typename T>
struct IsRelocatable
    : std::conditional<
          detail::is_detected<detail::detect_IsRelocatable, T>::value,
          detail::get_IsRelocatable<T>,
          std::integral_constant<
              bool, std::is_trivially_move_constructible<T>::value &&
                        std::is_trivially_destructible<T>::value>>::type {};
#endif // CXXBRIDGE1_RELOCATABLE
} // namespace cxxbridge1
} // namespace rust
namespace wasmtime {
  enum class ValKind : ::std::uint8_t;
  struct Instance;
  struct Module;
  struct Store;
  struct Memory;
  struct Engine;
  struct Func;
  struct Val;
  struct ValVec;
  struct Fut;
}
namespace wasmtime {
#ifndef CXXBRIDGE1_ENUM_wasmtime$ValKind
#define CXXBRIDGE1_ENUM_wasmtime$ValKind
enum class ValKind : ::std::uint8_t {
  I32 = 0,
  I64 = 1,
  F32 = 2,
  F64 = 3,
  V128 = 4,
  FuncRef = 5,
  ExternRef = 6,
};
#endif // CXXBRIDGE1_ENUM_wasmtime$ValKind
#ifndef CXXBRIDGE1_STRUCT_wasmtime$Instance
#define CXXBRIDGE1_STRUCT_wasmtime$Instance
struct Instance final : public ::rust::Opaque {
  ~Instance() = delete;
private:
  friend ::rust::layout;
  struct layout {
    static ::std::size_t size() noexcept;
    static ::std::size_t align() noexcept;
  };
};
#endif // CXXBRIDGE1_STRUCT_wasmtime$Instance
#ifndef CXXBRIDGE1_STRUCT_wasmtime$Module
#define CXXBRIDGE1_STRUCT_wasmtime$Module
struct Module final : public ::rust::Opaque {
  ::std::size_t raw_size() const noexcept;
  bool is_compiled() const noexcept;
  void compile(::wasmtime::Engine &engine);
  void release() noexcept;
  void add_user() noexcept;
  void remove_user() noexcept;
  ::std::size_t user_count() const noexcept;
  ~Module() = delete;
private:
  friend ::rust::layout;
  struct layout {
    static ::std::size_t size() noexcept;
    static ::std::size_t align() noexcept;
  };
};
#endif // CXXBRIDGE1_STRUCT_wasmtime$Module
#ifndef CXXBRIDGE1_STRUCT_wasmtime$Store
#define CXXBRIDGE1_STRUCT_wasmtime$Store
struct Store final : public ::rust::Opaque {
  ~Store() = delete;
private:
  friend ::rust::layout;
  struct layout {
    static ::std::size_t size() noexcept;
    static ::std::size_t align() noexcept;
  };
};
#endif // CXXBRIDGE1_STRUCT_wasmtime$Store
#ifndef CXXBRIDGE1_STRUCT_wasmtime$Memory
#define CXXBRIDGE1_STRUCT_wasmtime$Memory
struct Memory final : public ::rust::Opaque {
  ::std::uint8_t *data(::wasmtime::Store const &store) const noexcept;
  ::std::uint64_t size(::wasmtime::Store const &store) const noexcept;
  ::std::uint64_t grow(::wasmtime::Store &store, ::std::uint64_t delta);
  ~Memory() = delete;
private:
  friend ::rust::layout;
  struct layout {
    static ::std::size_t size() noexcept;
    static ::std::size_t align() noexcept;
  };
};
#endif // CXXBRIDGE1_STRUCT_wasmtime$Memory
#ifndef CXXBRIDGE1_STRUCT_wasmtime$Engine
#define CXXBRIDGE1_STRUCT_wasmtime$Engine
struct Engine final : public ::rust::Opaque {
  ~Engine() = delete;
private:
  friend ::rust::layout;
  struct layout {
    static ::std::size_t size() noexcept;
    static ::std::size_t align() noexcept;
  };
};
#endif // CXXBRIDGE1_STRUCT_wasmtime$Engine
#ifndef CXXBRIDGE1_STRUCT_wasmtime$Func
#define CXXBRIDGE1_STRUCT_wasmtime$Func
struct Func final : public ::rust::Opaque {
  ~Func() = delete;
private:
  friend ::rust::layout;
  struct layout {
    static ::std::size_t size() noexcept;
    static ::std::size_t align() noexcept;
  };
};
#endif // CXXBRIDGE1_STRUCT_wasmtime$Func
#ifndef CXXBRIDGE1_STRUCT_wasmtime$Val
#define CXXBRIDGE1_STRUCT_wasmtime$Val
struct Val final : public ::rust::Opaque {
  ::wasmtime::ValKind kind() const;
  ::std::int32_t i32() const;
  ::std::int64_t i64() const;
  float f32() const;
  double f64() const;
  ~Val() = delete;
private:
  friend ::rust::layout;
  struct layout {
    static ::std::size_t size() noexcept;
    static ::std::size_t align() noexcept;
  };
};
#endif // CXXBRIDGE1_STRUCT_wasmtime$Val
#ifndef CXXBRIDGE1_STRUCT_wasmtime$ValVec
#define CXXBRIDGE1_STRUCT_wasmtime$ValVec
struct ValVec final : public ::rust::Opaque {
  void push_i32(::std::int32_t val);
  void push_i64(::std::int64_t val);
  void push_f32(float val);
  void push_f64(double val);
  ::rust::Box<::wasmtime::Val> pop_val();
  ~ValVec() = delete;
private:
  friend ::rust::layout;
  struct layout {
    static ::std::size_t size() noexcept;
    static ::std::size_t align() noexcept;
  };
};
#endif // CXXBRIDGE1_STRUCT_wasmtime$ValVec
#ifndef CXXBRIDGE1_STRUCT_wasmtime$Fut
#define CXXBRIDGE1_STRUCT_wasmtime$Fut
struct Fut final : public ::rust::Opaque {
  bool resume();
  ~Fut() = delete;
private:
  friend ::rust::layout;
  struct layout {
    static ::std::size_t size() noexcept;
    static ::std::size_t align() noexcept;
  };
};
#endif // CXXBRIDGE1_STRUCT_wasmtime$Fut
::rust::Box<::wasmtime::Instance> create_instance(::wasmtime::Engine const &engine, ::wasmtime::Module const &module, ::wasmtime::Store &store);
::rust::Box<::wasmtime::Module> create_module(::wasmtime::Engine &engine, ::rust::Str script);
::rust::Box<::wasmtime::Store> create_store(::wasmtime::Engine &engine, ::std::uint64_t total_fuel, ::std::uint64_t yield_fuel);
::rust::Box<::wasmtime::Memory> get_memory(::wasmtime::Instance const &instance, ::wasmtime::Store &store);
::std::uint32_t get_abi(::wasmtime::Instance const &instance, ::wasmtime::Store &store, ::wasmtime::Memory const &memory);
::rust::Box<::wasmtime::Engine> create_engine(::std::uint32_t max_size);
::rust::Box<::wasmtime::Engine> create_test_engine(::std::uint32_t max_size, ::std::size_t fail_after);
::rust::Box<::wasmtime::Func> create_func(::wasmtime::Instance const &instance, ::wasmtime::Store &store, ::rust::Str function_name);
::rust::Box<::wasmtime::ValVec> get_val_vec();
::rust::Box<::wasmtime::Fut> get_func_future(::wasmtime::Store &store, ::wasmtime::Func const &func, ::wasmtime::ValVec const &args, ::wasmtime::ValVec &rets);
} // namespace wasmtime
class cell_locker;
class cell_locker_stats;
class locked_cell;
class mutation;
class frozen_mutation;
class reconcilable_result;
namespace s3 { struct endpoint_config; }
namespace service {
class storage_proxy;
class storage_service;
class migration_notifier;
class raft_group_registry;
}
namespace gms {
class feature_service;
}
namespace sstables {
class sstable;
class compaction_descriptor;
class compaction_completion_desc;
class storage_manager;
class sstables_manager;
class compaction_data;
class sstable_set;
class directory_semaphore;
}
namespace ser {
template<typename T>
class serializer;
}
namespace gms {
class gossiper;
}
namespace db {
class commitlog;
class config;
class extensions;
class rp_handle;
class data_listeners;
class large_data_handler;
class system_keyspace;
class table_selector;

namespace view {
class view_update_generator;
}
}
class mutation_reordered_with_truncate_exception : public std::exception {};
class column_family_test;
class table_for_tests;
class database_test;
extern logging::logger dblog;
namespace replica {
using shared_memtable = lw_shared_ptr<memtable>;
// We could just add all memtables, regardless of types, to a single list, and
// then filter them out when we read them. Here's why I have chosen not to do
// it:
//
// First, some of the methods in which a memtable is involved (like seal) are
// assume a commitlog, and go through great care of updating the replay
// position, flushing the log, etc.  We want to bypass those, and that has to
// be done either by sprikling the seal code with conditionals, or having a
// separate method for each seal.
//
// Also, if we ever want to put some of the memtables in as separate allocator
// region group to provide for extra QoS, having the classes properly wrapped
// will make that trivial: just pass a version of new_memtable() that puts it
// in a different region, while the list approach would require a lot of
// conditionals as well.
//
// If we are going to have different methods, better have different instances
// of a common class.
class memtable_list {
public:
    using seal_immediate_fn_type = std::function<future<> (flush_permit&&)>;
private:
    std::vector<shared_memtable> _memtables;
    seal_immediate_fn_type _seal_immediate_fn;
    std::function<schema_ptr()> _current_schema;
    replica::dirty_memory_manager* _dirty_memory_manager;
    std::optional<shared_future<>> _flush_coalescing;
    seastar::scheduling_group _compaction_scheduling_group;
    replica::table_stats& _table_stats;
public:
    using iterator = decltype(_memtables)::iterator;
    using const_iterator = decltype(_memtables)::const_iterator;
public:
    
    // # 8904 - this method is akin to std::set::erase(key_type), not
    // erase(iterator). Should be tolerant against non-existing.
    // Synchronously swaps the active memtable with a new, empty one,
    // returning the old memtables list.
    // Exception safe.
    // This is used for explicit flushes. Will queue the memtable for flushing and proceed when the
    // dirty_memory_manager allows us to. We will not seal at this time since the flush itself
    // wouldn't happen anyway. Keeping the memtable in memory will potentially increase the time it
    // spends in memory allowing for more coalescing opportunities.
    // The returned future<> resolves when any pending flushes are complete and the memtable is sealed.
private:
};
}
using sstable_list = sstables::sstable_list;
namespace replica {
class distributed_loader;
class table_populator;
// The CF has a "stats" structure. But we don't want all fields here,
// since some of them are fairly complex for exporting to collectd. Also,
// that structure matches what we export via the API, so better leave it
// untouched. And we need more fields. We will summarize it in here what
// we need.
struct cf_stats {
    int64_t pending_memtables_flushes_count = 0;
    int64_t pending_memtables_flushes_bytes = 0;
    int64_t failed_memtables_flushes_count = 0;
    // number of time the clustering filter was executed
    int64_t clustering_filter_count = 0;
    // sstables considered by the filter (so dividing this by the previous one we get average sstables per read)
    int64_t sstables_checked_by_clustering_filter = 0;
    // number of times the filter passed the fast-path checks
    int64_t clustering_filter_fast_path_count = 0;
    // how many sstables survived the clustering key checks
    int64_t surviving_sstables_after_clustering_filter = 0;
    // How many view updates were dropped due to overload.
    int64_t dropped_view_updates = 0;
    // How many times view building was paused (e.g. due to node unavailability)
    int64_t view_building_paused = 0;
    // How many view updates were processed for all tables
    uint64_t total_view_updates_pushed_local = 0;
    uint64_t total_view_updates_pushed_remote = 0;
    uint64_t total_view_updates_failed_local = 0;
    uint64_t total_view_updates_failed_remote = 0;
};
class table;
using column_family = table;
struct table_stats;
using column_family_stats = table_stats;
class database_sstable_write_monitor;
class compaction_group;
using enable_backlog_tracker = bool_class<class enable_backlog_tracker_tag>;
extern const ssize_t new_reader_base_cost;
struct table_stats {
    int64_t memtable_switch_count = 0;
    int64_t pending_flushes = 0;
    int64_t live_disk_space_used = 0;
    int64_t total_disk_space_used = 0;
    int64_t live_sstable_count = 0;
    int64_t pending_compactions = 0;
    int64_t memtable_partition_insertions = 0;
    int64_t memtable_partition_hits = 0;
    int64_t memtable_range_tombstone_reads = 0;
    int64_t memtable_row_tombstone_reads = 0;
    mutation_application_stats memtable_app_stats;
    utils::timed_rate_moving_average_summary_and_histogram reads{256};
    utils::timed_rate_moving_average_summary_and_histogram writes{256};
    utils::timed_rate_moving_average_summary_and_histogram cas_prepare{256};
    utils::timed_rate_moving_average_summary_and_histogram cas_accept{256};
    utils::timed_rate_moving_average_summary_and_histogram cas_learn{256};
    utils::estimated_histogram estimated_sstable_per_read{35};
    utils::timed_rate_moving_average_and_histogram tombstone_scanned;
    utils::timed_rate_moving_average_and_histogram live_scanned;
    utils::estimated_histogram estimated_coordinator_read;
};
using storage_options = data_dictionary::storage_options;
class table : public enable_lw_shared_from_this<table> {
public:
    struct config {
        std::vector<sstring> all_datadirs;
        sstring datadir;
        bool enable_disk_writes = true;
        bool enable_disk_reads = true;
        bool enable_cache = true;
        bool enable_commitlog = true;
        bool enable_incremental_backups = false;
        utils::updateable_value<bool> compaction_enforce_min_threshold{false};
        bool enable_dangerous_direct_import_of_cassandra_counters = false;
        replica::dirty_memory_manager* dirty_memory_manager = &default_dirty_memory_manager;
        reader_concurrency_semaphore* streaming_read_concurrency_semaphore;
        reader_concurrency_semaphore* compaction_concurrency_semaphore;
        replica::cf_stats* cf_stats = nullptr;
        seastar::scheduling_group memtable_scheduling_group;
        seastar::scheduling_group memtable_to_cache_scheduling_group;
        seastar::scheduling_group compaction_scheduling_group;
        seastar::scheduling_group memory_compaction_scheduling_group;
        seastar::scheduling_group statement_scheduling_group;
        seastar::scheduling_group streaming_scheduling_group;
        bool enable_metrics_reporting = false;
        db::timeout_semaphore* view_update_concurrency_semaphore;
        size_t view_update_concurrency_semaphore_limit;
        db::data_listeners* data_listeners = nullptr;
        // Not really table-specific (it's a global configuration parameter), but stored here
        // for easy access from `table` member functions:
        utils::updateable_value<bool> reversed_reads_auto_bypass_cache{false};
        utils::updateable_value<bool> enable_optimized_reversed_reads{true};
        uint32_t tombstone_warn_threshold{0};
        unsigned x_log2_compaction_groups{0};
    };
    struct no_commitlog {};
    struct snapshot_details {
        int64_t total;
        int64_t live;
    };
    struct cache_hit_rate {
        cache_temperature rate;
        lowres_clock::time_point last_updated;
    };
private:
    schema_ptr _schema;
    config _config;
    locator::effective_replication_map_ptr _erm;
    lw_shared_ptr<const storage_options> _storage_opts;
    mutable table_stats _stats;
    mutable db::view::stats _view_stats;
    mutable row_locker::stats _row_locker_stats;
    uint64_t _failed_counter_applies_to_memtable = 0;
    ;
    // The value of the parameter controls the number of compaction groups in this table.
    // 0 (default) means 1 compaction group. 3 means 8 compaction groups.
    const unsigned _x_log2_compaction_groups = 0;
    compaction_manager& _compaction_manager;
    sstables::compaction_strategy _compaction_strategy;
    std::vector<std::unique_ptr<compaction_group>> _compaction_groups;
    // Compound SSTable set for all the compaction groups, which is useful for operations spanning all of them.
    lw_shared_ptr<sstables::sstable_set> _sstables;
    // Control background fibers waiting for sstables to be deleted
    seastar::gate _sstable_deletion_gate;
    // This semaphore ensures that an operation like snapshot won't have its selected
    // sstables deleted by compaction in parallel, a race condition which could
    // easily result in failure.
    seastar::named_semaphore _sstable_deletion_sem = {1, named_semaphore_exception_factory{"sstable deletion"}};
    // Ensures that concurrent updates to sstable set will work correctly
    seastar::named_semaphore _sstable_set_mutation_sem = {1, named_semaphore_exception_factory{"sstable set mutation"}};
    mutable row_cache _cache; // Cache covers only sstables.
    // Initialized when the table is populated via update_sstables_known_generation.
    std::optional<sstables::sstable_generation_generator> _sstable_generation_generator;
    db::replay_position _highest_rp;
    db::replay_position _flush_rp;
    db::replay_position _lowest_allowed_rp;
    // Provided by the database that owns this commitlog
    db::commitlog* _commitlog;
    bool _durable_writes;
    sstables::sstables_manager& _sstables_manager;
    secondary_index::secondary_index_manager _index_manager;
    bool _compaction_disabled_by_user = false;
    bool _tombstone_gc_enabled = true;
    utils::phased_barrier _flush_barrier;
    std::vector<view_ptr> _views;
    std::unique_ptr<cell_locker> _counter_cell_locks; // Memory-intensive; allocate only when needed.
    // Labels used to identify writes and reads for this table in the rate_limiter structure.
    db::rate_limiter::label _rate_limiter_label_for_writes;
    db::rate_limiter::label _rate_limiter_label_for_reads;
    seastar::metrics::metric_groups _metrics;
    // holds average cache hit rate of all shards
    // recalculated periodically
    cache_temperature _global_cache_hit_rate = cache_temperature(0.0f);
    // holds cache hit rates per each node in a cluster
    // may not have information for some node, since it fills
    // in dynamically
    std::unordered_map<gms::inet_address, cache_hit_rate> _cluster_cache_hit_rates;
    // Operations like truncate, flush, query, etc, may depend on a column family being alive to
    // complete.  Some of them have their own gate already (like flush), used in specialized wait
    // logic. That is particularly useful if there is a particular
    // order in which we need to close those gates. For all the others operations that don't have
    // such needs, we have this generic _async_gate, which all potentially asynchronous operations
    // have to get.  It will be closed by stop().
    seastar::gate _async_gate;
    double _cached_percentile = -1;
    lowres_clock::time_point _percentile_cache_timestamp;
    std::chrono::milliseconds _percentile_cache_value;
    // Phaser used to synchronize with in-progress writes. This is useful for code that,
    // after some modification, needs to ensure that news writes will see it before
    // it can proceed, such as the view building code.
    utils::phased_barrier _pending_writes_phaser;
    // Corresponding phaser for in-progress reads.
    utils::phased_barrier _pending_reads_phaser;
    // Corresponding phaser for in-progress streams
    utils::phased_barrier _pending_streams_phaser;
    // Corresponding phaser for in-progress flushes
    utils::phased_barrier _pending_flushes_phaser;
    // This field cashes the last truncation time for the table.
    // The master resides in system.truncated table
    db_clock::time_point _truncated_at = db_clock::time_point::min();
    bool _is_bootstrap_or_replace = false;
public:
    // Ensures that concurrent preemptible mutations to sstable lists will produce correct results.
    // User will hold this permit until done with all updates. As soon as it's released, another concurrent
    // attempt to update the lists will be able to proceed.
    struct sstable_list_builder {
        using permit_t = semaphore_units<seastar::named_semaphore_exception_factory>;
        permit_t permit;
        // Builds new sstable set from existing one, with new sstables added to it and old sstables removed from it.
    };
private:
    using compaction_group_ptr = std::unique_ptr<compaction_group>;
    std::vector<std::unique_ptr<compaction_group>> make_compaction_groups();
    // Return compaction group if table owns a single one. Otherwise, null is returned.
    // Select a compaction group from a given token.
    // Select a compaction group from a given key.
    // Select a compaction group from a given sstable based on its token range.
    // Returns a list of all compaction groups.
    const std::vector<std::unique_ptr<compaction_group>>& compaction_groups() const noexcept;
    // Safely iterate through compaction groups, while performing async operations on them.
    // Helpers which add sstable on behalf of a compaction group and refreshes compound set.
    // Caller must keep m alive.
    struct merge_comparator;
    // update the sstable generation, making sure (in calculate_generation_for_new_table)
    // that new new sstables don't overwrite this one.
private:
private:
    mutation_source_opt _virtual_reader;
    std::optional<noncopyable_function<future<>(const frozen_mutation&)>> _virtual_writer;
    // Creates a mutation reader which covers given sstables.
    // Caller needs to ensure that column_family remains live (FIXME: relax this).
    // The 'range' parameter must be live as long as the reader is used.
    // Mutations returned by the reader will all have given schema.
    // Compound sstable set must be refreshed whenever any of its managed sets are changed
    std::chrono::steady_clock::time_point _sstable_writes_disabled_at;
    // reserve_fn will be called before any element is added to readers
public:
    // This function should be called when this column family is ready for writes, IOW,
    // to produce SSTables. Extensive details about why this is important can be found
    // in Scylla's Github Issue #1014
    //
    // Nothing should be writing to SSTables before we have the chance to populate the
    // existing SSTables and calculate what should the next generation number be.
    //
    // However, if that happens, we want to protect against it in a way that does not
    // involve overwriting existing tables. This is one of the ways to do it: every
    // column family starts in an unwriteable state, and when it can finally be written
    // to, we mark it as writeable.
    //
    // Note that this *cannot* be a part of add_column_family. That adds a column family
    // to a db in memory only, and if anybody is about to write to a CF, that was most
    // likely already called. We need to call this explicitly when we are sure we're ready
    // to issue disk operations safely.
    // Creates a mutation reader which covers all data sources for this column family.
    // Caller needs to ensure that column_family remains live (FIXME: relax this).
    // Note: for data queries use query() instead.
    // The 'range' parameter must be live as long as the reader is used.
    // Mutations returned by the reader will all have given schema.
    // If I/O needs to be issued to read anything in the specified range, the operations
    // will be scheduled under the priority class given by pc.
    // The streaming mutation reader differs from the regular mutation reader in that:
    //  - Reflects all writes accepted by replica prior to creation of the
    //    reader and a _bounded_ amount of writes which arrive later.
    //  - Does not populate the cache
    // Requires ranges to be sorted and disjoint.
    // Single range overload.
    // Stream reader from the given sstables
    // Queries can be satisfied from multiple data sources, so they are returned
    // as temporaries.
    //
    // FIXME: in case a query is satisfied from a single memtable, avoid a copy
    using const_mutation_partition_ptr = std::unique_ptr<const mutation_partition>;
    using const_row_ptr = std::unique_ptr<const row>;
    // Return all active memtables, where there will be one per compaction group
    // TODO: expose stats, whatever, instead of exposing active memtables themselves.
    future<std::vector<locked_cell>> lock_counter_cells(const mutation& m, db::timeout_clock::time_point timeout);
private:
public:
     // 'this' is being captured during construction
    
    const schema_ptr& schema() const ;
    void set_schema(schema_ptr);
    // Applies given mutation to this column family
    // The mutation is always upgraded to current schema.
    // Returns at most "cmd.limit" rows
    // The saved_querier parameter is an input-output parameter which contains
    // the saved querier from the previous page (if there was one) and after
    // completion it contains the to-be saved querier for the next page (if
    // there is one). Pass nullptr when queriers are not saved.
    // Performs a query on given data source returning data in reconcilable form.
    //
    // Reads at most row_limit rows. If less rows are returned, the data source
    // didn't have more live data satisfying the query.
    //
    // Any cells which have expired according to query_time are returned as
    // deleted cells and do not count towards live data. The mutations are
    // compact, meaning that any cell which is covered by higher-level tombstone
    // is absent in the results.
    //
    // 'source' doesn't have to survive deferring.
    //
    // The saved_querier parameter is an input-output parameter which contains
    // the saved querier from the previous page (if there was one) and after
    // completion it contains the to-be saved querier for the next page (if
    // there is one). Pass nullptr when queriers are not saved.
     // discards memtable(s) without flushing them to disk.
    // Start a compaction of all sstables in a process known as major compaction
    // Active memtable is flushed first to guarantee that data like tombstone,
    // sitting in the memtable, will be compacted with shadowed data.
private:
    using snapshot_file_set = foreign_ptr<std::unique_ptr<std::unordered_set<sstring>>>;
    // Writes the table schema and the manifest of all files in the snapshot directory.
public:
    future<std::unordered_map<sstring, snapshot_details>> get_snapshot_details();
    future<std::unordered_set<sstring>> get_sstables_by_partition_key(const sstring& key) const;
    // Triggers offstrategy compaction, if needed, in the background.
    // Performs offstrategy compaction, if needed, returning
    // a future<bool> that is resolved when offstrategy_compaction completes.
    // The future value is true iff offstrategy compaction was required.
    // Reader's schema must be the same as the base schema of each of the views.
private:
    mutable row_locker _row_locker;
    // One does not need to wait on this future if all we are interested in, is
    // initiating the write.  The writes initiated here will eventually
    // complete, and the seastar::gate below will make sure they are all
    // completed before we stop() this column_family.
    //
    // But it is possible to synchronously wait for the seal to complete by
    // waiting on this future. This is useful in situations where we want to
    // synchronously flush data to disk.
    //
    // The function never fails.
    // It either succeeds eventually after retrying or aborts.
public:
    // Iterate over all partitions.  Protocol is the same as std::all_of(),
    // so that iteration can be stopped by returning false.
    // Testing purposes.
    // to let test classes access calculate_generation_for_new_table
    friend class ::column_family_test;
    friend class ::table_for_tests;
    friend class distributed_loader;
    friend class table_populator;
private:
    timer<> _off_strategy_trigger;
public:
    // FIXME: get rid of it once no users.
    // Safely iterate through table states, while performing async operations on them.
    // Add sst to or remove it from the sstables_requiring_cleanup set.
    // Uncoditionally erase sst from `sstables_requiring_cleanup`
    // Returns true iff sst was found and erased.
    // Returns true if the sstable requries cleanup.
    // Returns true if any of the sstables requries cleanup.
    friend class compaction_group;
};
using user_types_metadata = data_dictionary::user_types_metadata;
using keyspace_metadata = data_dictionary::keyspace_metadata;
class keyspace {
public:
    struct config {
        std::vector<sstring> all_datadirs;
        sstring datadir;
        bool enable_commitlog = true;
        bool enable_disk_reads = true;
        bool enable_disk_writes = true;
        bool enable_cache = true;
        bool enable_incremental_backups = false;
        utils::updateable_value<bool> compaction_enforce_min_threshold{false};
        bool enable_dangerous_direct_import_of_cassandra_counters = false;
        replica::dirty_memory_manager* dirty_memory_manager = &default_dirty_memory_manager;
        reader_concurrency_semaphore* streaming_read_concurrency_semaphore;
        reader_concurrency_semaphore* compaction_concurrency_semaphore;
        replica::cf_stats* cf_stats = nullptr;
        seastar::scheduling_group memtable_scheduling_group;
        seastar::scheduling_group memtable_to_cache_scheduling_group;
        seastar::scheduling_group compaction_scheduling_group;
        seastar::scheduling_group memory_compaction_scheduling_group;
        seastar::scheduling_group statement_scheduling_group;
        seastar::scheduling_group streaming_scheduling_group;
        bool enable_metrics_reporting = false;
        db::timeout_semaphore* view_update_concurrency_semaphore = nullptr;
        size_t view_update_concurrency_semaphore_limit;
    };
private:
    locator::replication_strategy_ptr _replication_strategy;
    locator::vnode_effective_replication_map_ptr _effective_replication_map;
    lw_shared_ptr<keyspace_metadata> _metadata;
    config _config;
    locator::effective_replication_map_factory& _erm_factory;
public:
    explicit keyspace(lw_shared_ptr<keyspace_metadata> metadata, config cfg, locator::effective_replication_map_factory& erm_factory);
    sstring column_family_directory(const sstring& base_path, const sstring& name, table_id uuid) const;
};
using no_such_keyspace = data_dictionary::no_such_keyspace;
using no_such_column_family = data_dictionary::no_such_column_family;
struct database_config {
    seastar::scheduling_group memtable_scheduling_group;
    seastar::scheduling_group memtable_to_cache_scheduling_group; // FIXME: merge with memtable_scheduling_group
    seastar::scheduling_group compaction_scheduling_group;
    seastar::scheduling_group memory_compaction_scheduling_group;
    seastar::scheduling_group statement_scheduling_group;
    seastar::scheduling_group streaming_scheduling_group;
    seastar::scheduling_group gossip_scheduling_group;
    size_t available_memory;
    std::optional<sstables::sstable_version_types> sstables_format;
};
struct string_pair_eq {
    using is_transparent = void;
    using spair = std::pair<std::string_view, std::string_view>;
    bool operator()(spair lhs, spair rhs) const;
};
class db_user_types_storage;
// Policy for distributed<database>:
//   broadcast metadata writes
//   local metadata reads
//   use shard_of() for data
class database : public peering_sharded_service<database> {
    friend class ::database_test;
public:
    enum class table_kind {
        system,
        user,
    };
    struct drain_progress {
        int32_t total_cfs;
        int32_t remaining_cfs;
        drain_progress& operator+=(const drain_progress& other) ;
    };
private:
    replica::cf_stats _cf_stats;
    static constexpr size_t max_count_concurrent_reads{100};
    // Assume a queued read takes up 1kB of memory, and allow 2% of memory to be filled up with such reads.
    // They're rather heavyweight, so limit more
    static constexpr size_t max_count_streaming_concurrent_reads{10};
    static constexpr size_t max_count_system_concurrent_reads{10};
    ;
    size_t max_memory_pending_view_updates() const ;
    struct db_stats {
        uint64_t total_writes = 0;
        uint64_t total_writes_failed = 0;
        uint64_t total_writes_timedout = 0;
        uint64_t total_writes_rate_limited = 0;
        uint64_t total_reads = 0;
        uint64_t total_reads_failed = 0;
        uint64_t total_reads_rate_limited = 0;
        uint64_t short_data_queries = 0;
        uint64_t short_mutation_queries = 0;
        uint64_t multishard_query_unpopped_fragments = 0;
        uint64_t multishard_query_unpopped_bytes = 0;
        uint64_t multishard_query_failed_reader_stops = 0;
        uint64_t multishard_query_failed_reader_saves = 0;
    };
    lw_shared_ptr<db_stats> _stats;
    std::shared_ptr<db_user_types_storage> _user_types;
    std::unique_ptr<cell_locker_stats> _cl_stats;
    const db::config& _cfg;
    dirty_memory_manager _system_dirty_memory_manager;
    dirty_memory_manager _dirty_memory_manager;
    database_config _dbcfg;
    backlog_controller::scheduling_group _flush_sg;
    flush_controller _memtable_controller;
    drain_progress _drain_progress {};
    reader_concurrency_semaphore _read_concurrency_sem;
    reader_concurrency_semaphore _streaming_concurrency_sem;
    reader_concurrency_semaphore _compaction_concurrency_sem;
    reader_concurrency_semaphore _system_read_concurrency_sem;
    db::timeout_semaphore _view_update_concurrency_sem{max_memory_pending_view_updates()};
    cache_tracker _row_cache_tracker;
    seastar::shared_ptr<db::view::view_update_generator> _view_update_generator;
    inheriting_concrete_execution_stage<
            future<>,
            database*,
            schema_ptr,
            const frozen_mutation&,
            tracing::trace_state_ptr,
            db::timeout_clock::time_point,
            db::commitlog_force_sync,
            db::per_partition_rate_limit::info> _apply_stage;
    flat_hash_map<sstring, keyspace> _keyspaces;
    std::unordered_map<table_id, lw_shared_ptr<column_family>> _column_families;
    using ks_cf_to_uuid_t =
        flat_hash_map<std::pair<sstring, sstring>, table_id, utils::tuple_hash, string_pair_eq>;
    ks_cf_to_uuid_t _ks_cf_to_uuid;
    std::unique_ptr<db::commitlog> _commitlog;
    std::unique_ptr<db::commitlog> _schema_commitlog;
    utils::updateable_value_source<table_schema_version> _version;
    uint32_t _schema_change_count = 0;
    // compaction_manager object is referenced by all column families of a database.
    compaction_manager& _compaction_manager;
    seastar::metrics::metric_groups _metrics;
    bool _enable_incremental_backups = false;
    bool _shutdown = false;
    bool _enable_autocompaction_toggle = false;
    bool _uses_schema_commitlog = false;
    query::querier_cache _querier_cache;
    std::unique_ptr<db::large_data_handler> _large_data_handler;
    std::unique_ptr<db::large_data_handler> _nop_large_data_handler;
    std::unique_ptr<sstables::sstables_manager> _user_sstables_manager;
    std::unique_ptr<sstables::sstables_manager> _system_sstables_manager;
    query::result_memory_limiter _result_memory_limiter;
    friend db::data_listeners;
    std::unique_ptr<db::data_listeners> _data_listeners;
    service::migration_notifier& _mnotifier;
    gms::feature_service& _feat;
    std::vector<std::any> _listeners;
    const locator::shared_token_metadata& _shared_token_metadata;
    sharded<sstables::directory_semaphore>& _sst_dir_semaphore;
    utils::cross_shard_barrier _stop_barrier;
    db::rate_limiter _rate_limiter;
    serialized_action _update_memtable_flush_static_shares_action;
    utils::observer<float> _memtable_flush_static_shares_observer;
public:
    
    
    const data_dictionary::user_types_storage& user_types() const noexcept;
private:
    using system_keyspace = bool_class<struct system_keyspace_tag>;
    ;
public:
    static table_schema_version empty_version;
    class autocompaction_toggle_guard {
        database& _db;
    public:
    };
    column_family& find_column_family(std::string_view ks, std::string_view name);
    
    
    
    /// Revert the system read concurrency to the normal value.
    ///
    /// When started the database uses a higher initial concurrency for system
    /// reads, to speed up startup. After startup this should be reverted to
    /// the normal concurrency.
    /// Checks whether per-partition rate limit can be applied to the operation or not.
    /// Tries to account given operation to the rate limit when the coordinator is a replica.
    /// This function can be called ONLY when rate limiting can be applied to the operation (see `can_apply_per_partition_rate_limit`)
    /// AND the current node/shard is a replica for the given operation.
    ///
    /// nullopt -> the decision should be delegated to replicas
    /// can_proceed::no -> operation should be rejected
    /// can_proceed::yes -> operation should be accepted
    future<std::tuple<lw_shared_ptr<query::result>, cache_temperature>> query(schema_ptr, const query::read_command& cmd, query::result_options opts,
                                                                  const dht::partition_range_vector& ranges, tracing::trace_state_ptr trace_state,
                                                                  db::timeout_clock::time_point timeout, db::per_partition_rate_limit::info rate_limit_info = std::monostate{});
    future<std::tuple<reconcilable_result, cache_temperature>> query_mutations(schema_ptr, const query::read_command& cmd, const dht::partition_range& range,
                                                tracing::trace_state_ptr trace_state, db::timeout_clock::time_point timeout);
    // Apply the mutation atomically.
    // Throws timed_out_error when timeout is reached.
    // Apply mutations atomically.
    // On restart, either all mutations will be replayed or none of them.
    // All mutations must belong to the same commitlog domain.
    // All mutations must be owned by the current shard (in terms of dht::shard_of).
    // Mutations may be partially visible to reads during the call.
    // Mutations may be partially visible to reads until restart on exception (FIXME).
    struct snapshot_details_result {
        sstring snapshot_name;
        db::snapshot_ctl::snapshot_details details;
    };
    future<std::vector<snapshot_details_result>> get_snapshot_details();
    // Returns the list of ranges held by this endpoint
    // The returned list is sorted, and its elements are non overlapping and non wrap-around.
    // flush a table identified by the given id on all shards.
    // flush a single table in a keyspace on all shards.
    // flush a list of tables in a keyspace on all shards.
    // flush all tables in a keyspace on all shards.
public:
private:
    static future<std::vector<foreign_ptr<lw_shared_ptr<table>>>> get_table_on_all_shards(sharded<database>& db, table_id uuid);
    struct table_truncate_state;
public:
    // If truncated_at_opt is not given, it is set to db_clock::now right after flush/clear.
    // drops the table on all shards and removes the table directory if there are no snapshots
    // Get the maximum result size for an unlimited query, appropriate for the
    // query class, which is deduced from the current scheduling group.
    // Get the reader concurrency semaphore, appropriate for the query class,
    // which is deduced from the current scheduling group.
    // Convenience method to obtain an admitted permit. See reader_concurrency_semaphore::obtain_permit().
};
} // namespace replica
// Creates a streaming reader that reads from all shards.
//
// Shard readers are created via `table::make_streaming_reader()`.
// Range generator must generate disjoint, monotonically increasing ranges.
//
// ~~ Definitions ~~
//
// Mergeable type is a type which has an associated "apply" binary operation (T x T -> T)
// which forms a commutative semigroup with instances of that type.
//
// ReversiblyMergeable type is a Mergeable type which has two binary operations associated,
// "apply_reversibly" and "revert", both working on objects of that type (T x T -> T x T)
// with the following properties:
//
//   apply_reversibly(x, y) = (x', y')
//   revert(x', y') = (x'', y'')
//
//   x'  = apply(x, y)
//   x'' = x
//   apply(x'', y'') = apply(x, y)
//
// Note that it is not guaranteed that y'' = y and the state of y' is unspecified.
//
// ~~ API ~~
//
// "apply_reversibly" and "revert" are usually implemented as instance methods or functions
// mutating both arguments to store the result of the operation in them.
//
// "revert" is not allowed to throw. If "apply_reversibly" throws the objects on which it operates
// are left in valid states, with guarantees the same as if a successful apply_reversibly() was
// followed by revert().
//
template<typename T>
struct default_reversible_applier {
};
template<typename T>
struct default_reverter {
};
namespace seastar {
    template<typename> class output_stream;
}
namespace rjson {
class error : public std::exception {
    std::string _msg;
public:
    
    error(const std::string& msg) : _msg(msg) {}
};
}
namespace rjson {
// The internal namespace is a workaround for the fact that fmt::format
// also has a to_string_view function and erroneously looks up our rjson::to_string_view
// if this allocator is in the rjson namespace.
namespace internal {
// Implements an interface conforming to the one in rapidjson/allocators.h,
// but throws rjson::error on allocation failures
class throwing_allocator : public rapidjson::CrtAllocator {
    using base = rapidjson::CrtAllocator;
public:
    static const bool kNeedFree = base::kNeedFree;
};
}
using allocator = internal::throwing_allocator;
using encoding = rapidjson::UTF8<>;
using document = rapidjson::GenericDocument<encoding, allocator, allocator>;
using value = rapidjson::GenericValue<encoding, allocator>;
using string_ref_type = value::StringRefType;
using string_buffer = rapidjson::GenericStringBuffer<encoding, allocator>;
using writer = rapidjson::Writer<string_buffer, encoding, encoding, allocator>;
using type = rapidjson::Type;
// The default value is derived from the days when rjson resided in alternator:
// - the original DynamoDB nested level limit is 32
// - it's raised by 7 for alternator to make it safer and more cool
// - the value is doubled because the nesting level is bumped twice
//   for every alternator object - because each alternator object
//   consists of a 2-level JSON object.
inline constexpr size_t default_max_nested_level = 78;
class malformed_value : public error {
public:
    malformed_value(std::string_view name, std::string_view value);
};
class missing_value : public error {
public:
    
};
// Returns an object representing JSON's null
 
// Returns an empty JSON object - {}
 rjson::value empty_object() ;
// Returns an empty JSON array - []
 rjson::value empty_array() ;
// Returns an empty JSON string - ""
 rjson::value empty_string() ;
// Convert the JSON value to a string with JSON syntax, the opposite of parse().
// The representation is dense - without any redundant indentation.
std::string print(const rjson::value& value, size_t max_nested_level = default_max_nested_level);
// Writes the JSON value to the output_stream, similar to print() -> string, but
// directly. Note this has potentially more data copy overhead and should be 
// reserved for larger values where not creating huge linear strings is useful.
// (Because data will first be written to N buffers before actually committed to stream, 
//  where N = <n bytes text>/k (k dependent on buffer/write block behaviour of stream),
//  and also copy at least 2n bytes (assuming underlying stream is buffered).
//  This may or may not be more data copying/overhead than creating a string directly
//  and printing it, but again, it will guarantee fragmented buffer behaviour).
// Note: input value must remain valid for the call, but is _not_ required to be valid
// until future resolves - i.e. the full json -> text conversion is done _before_ 
// pushing fully to stream. I.e. it is valid to do `return print(rjson::value("... something..."), os);`
seastar::future<> print(const rjson::value& value, seastar::output_stream<char>&, size_t max_nested_level = default_max_nested_level);
// Returns a string_view to the string held in a JSON value (which is
// assumed to hold a string, i.e., v.IsString() == true). This is a view
// to the existing data - no copying is done.
 std::string_view to_string_view(const rjson::value& v) ;
// Copies given JSON value - involves allocation
rjson::value copy(const rjson::value& value);
// Parses a JSON value from given string or raw character array.
// The string/char array liveness does not need to be persisted,
// as parse() will allocate member names and values.
// Throws rjson::error if parsing failed.
rjson::value parse(std::string_view str, size_t max_nested_level = default_max_nested_level);
// Parses a JSON value returns a disengaged optional on failure.
// NOTICE: any error context will be lost, so this function should
// be used only if one does not care why parsing failed.
// Needs to be run in thread context
// chunked_content holds a non-contiguous buffer of bytes - such as bytes
// read by util::read_entire_stream(). We assume that chunked_content does
// not contain any empty buffers (the vector can be empty, meaning empty
// content - but individual buffers cannot).
using chunked_content = std::vector<temporary_buffer<char>>;
// Additional variants of parse() and parse_yieldable() that work on non-
// contiguous chunked_content. The chunked_content is moved into the parsing
// function so that we can start freeing chunks as soon as we parse them.
// Creates a JSON value (of JSON string type) out of internal string representations.
// The string value is copied, so str's liveness does not need to be persisted.

// Returns a pointer to JSON member if it exists, nullptr otherwise
// Returns a reference to JSON member if it exists, throws otherwise
 ;
 ;
// The various add*() functions below *add* a new member to a JSON object.
// They all assume that a member with the same key (name) doesn't already
// exist in that object, so they are meant to be used just to build a new
// object from scratch. If a member with the same name *may* exist, and
// might need to be replaced, use the replace*() functions instead.
// The benefit of the add*() functions is that they are faster (O(1),
// compared to O(n) for the replace* function that need to inspect the
// existing members).
// Adds a member to a given JSON object by moving the member - allocates the name.
// Throws if base is not a JSON object.
// Assumes a member with the same name does not yet exist in base.

// Adds a string member to a given JSON object by assigning its reference - allocates the name.
// NOTICE: member string liveness must be ensured to be at least as long as base's.
// Throws if base is not a JSON object.
// Assumes a member with the same name does not yet exist in base.
// Adds a member to a given JSON object by moving the member.
// NOTICE: name liveness must be ensured to be at least as long as base's.
// Throws if base is not a JSON object.
// Assumes a member with the same name does not yet exist in base.
// Adds a string member to a given JSON object by assigning its reference.
// NOTICE: name liveness must be ensured to be at least as long as base's.
// NOTICE: member liveness must be ensured to be at least as long as base's.
// Throws if base is not a JSON object.
// Assumes a member with the same name does not yet exist in base.
template<typename T>
requires (!std::is_constructible_v<string_ref_type, T>)
void add(rjson::value& base, rjson::string_ref_type name, T&& member) {
    extern allocator the_allocator;
    rjson::value v;
    v.Set(std::forward<T>(member), the_allocator);
    add(base, std::move(name), std::move(v));
}
// Set a member in a given JSON object by moving the member - allocates the name.
// If a member with the same name already exist in base, it is replaced.
// Throws if base is not a JSON object.
void replace_with_string_name(rjson::value& base, std::string_view name, rjson::value&& member);
// Adds a value to a JSON list by moving the item to its end.
// Throws if base_array is not a JSON array.
// Remove a member from a JSON object. Throws if value isn't an object.

struct single_value_comp {
    bool operator()(const rjson::value& r1, const rjson::value& r2) const;
};
// Helper function for parsing a JSON straight into a map
// of strings representing their values - useful for various
// database helper functions.
// This function exists for historical reasons - existing infrastructure
// relies on being able to transform a JSON string into a map of sstrings.
template<typename Map>
requires (std::is_same_v<Map, std::map<sstring, sstring>> || std::is_same_v<Map, std::unordered_map<sstring, sstring>>)
Map parse_to_map(std::string_view raw) {
    Map map;
    rjson::value root = rjson::parse(raw);
    if (root.IsNull()) {
        return map;
    }
    if (!root.IsObject()) {
        throw rjson::error("Only json objects can be transformed to maps. Encountered: " + std::string(raw));
    }
    for (auto it = root.MemberBegin(); it != root.MemberEnd(); ++it) {
        if (it->value.IsString()) {
            map.emplace(sstring(rjson::to_string_view(it->name)), sstring(rjson::to_string_view(it->value)));
        } else {
            map.emplace(sstring(rjson::to_string_view(it->name)), sstring(rjson::print(it->value)));
        }
    }
    return map;
}
// This function exists for historical reasons as well.
rjson::value from_string_map(const std::map<sstring, sstring>& map);
// The function operates on sstrings for historical reasons.
sstring quote_json_string(const sstring& value);
 bytes base64_decode(const value& v) ;
} // end namespace rjson
namespace std {
std::ostream& operator<<(std::ostream& os, const rjson::value& v);
}
class schema;
using schema_ptr = seastar::lw_shared_ptr<const schema>;
namespace locator {
class token_metadata;
} // namespace locator
namespace service {
class migration_notifier;
class storage_proxy;
class query_state;
} // namespace service
class mutation;
class partition_key;
namespace replica {
class database;
}
namespace cdc {
struct operation_result_tracker;
class db_context;
class metadata;

/// \brief CDC service, responsible for schema listeners
///
/// CDC service will listen for schema changes and iff CDC is enabled/changed
/// create/modify/delete corresponding log tables etc as part of the schema change. 
///
class cdc_service final : public async_sharded_service<cdc::cdc_service> {
    class impl;
    std::unique_ptr<impl> _impl;
public:
    
    cdc_service(service::storage_proxy&, cdc::metadata&, service::migration_notifier&);
    // If any of the mutations are cdc enabled, optionally selects preimage, and adds the
    // appropriate augments to set the log entries.
    // Iff post-image is enabled for any of these, a non-empty callback is also
    // returned to be invoked post the mutation query.
    future<std::tuple<std::vector<mutation>, lw_shared_ptr<operation_result_tracker>>> augment_mutation_call(
        lowres_clock::time_point timeout,
        std::vector<mutation>&& mutations,
        tracing::trace_state_ptr tr_state,
        db::consistency_level write_cl
        );
};
struct db_context final {
    service::storage_proxy& _proxy;
    service::migration_notifier& _migration_notifier;
    cdc::metadata& _cdc_metadata;
};
// cdc log table operation
enum class operation : int8_t {
    // note: these values will eventually be read by a third party, probably not privvy to this
    // enum decl, so don't change the constant values (or the datatype).
    pre_image = 0, update = 1, insert = 2, row_delete = 3, partition_delete = 4,
    range_delete_start_inclusive = 5, range_delete_start_exclusive = 6, range_delete_end_inclusive = 7, range_delete_end_exclusive = 8,
    post_image = 9,
};
} // namespace cdc
namespace db {
class per_partition_rate_limit_options;
}
struct schema_builder {
public:
    enum class compact_storage { no, yes };
    using static_configurator = noncopyable_function<void(const sstring& ks_name, const sstring& cf_name, schema_static_props&)>;
private:
    schema::raw_schema _raw;
    std::optional<compact_storage> _compact_storage;
    std::optional<table_schema_version> _version;
    std::optional<raw_view_info> _view_info;
    schema_builder(const schema::raw_schema&);
    static std::vector<static_configurator>& static_configurators();
public:
    schema_builder(std::string_view ks_name, std::string_view cf_name,
            std::optional<table_id> = { },
            data_type regular_column_name_type = utf8_type);
    schema_builder(
            std::optional<table_id> id,
            std::string_view ks_name,
            std::string_view cf_name,
            std::vector<schema::column> partition_key,
            std::vector<schema::column> clustering_key,
            std::vector<schema::column> regular_columns,
            std::vector<schema::column> static_columns,
            data_type regular_column_name_type,
            sstring comment = "");
    schema_builder(const schema_ptr);
    
    
    const sstring& cf_name() const ;
    schema_builder& set_comment(const sstring& s) ;
    
    
    gc_clock::duration default_time_to_live() const ;
    schema_builder& set_compaction_strategy_options(std::map<sstring, sstring>&& options);
    
    
    schema_builder& set_is_compound(bool is_compound) ;
    schema_builder& set_is_counter(bool is_counter) {
        _raw._is_counter = is_counter;
        return *this;
    }
    schema_builder& with_partitioner(sstring name);
    schema_builder& with_sharder(unsigned shard_count, unsigned sharding_ignore_msb_bits);
    class default_names {
    public:
        
        default_names(const schema::raw_schema&);
        
        sstring clustering_name();
        sstring compact_value_name();
    private:
        sstring unique_name(const sstring&, size_t&, size_t) const;
        const schema::raw_schema& _raw;
        size_t _partition_index, _clustering_index, _compact_index;
    };
    column_definition& find_column(const cql3::column_identifier&);
    bool has_column(const cql3::column_identifier&);
    schema_builder& with_column_ordered(const column_definition& c);
    schema_builder& with_column(bytes name, data_type type, column_kind kind = column_kind::regular_column, column_view_virtual view_virtual = column_view_virtual::no);
    schema_builder& with_computed_column(bytes name, data_type type, column_kind kind, column_computation_ptr computation);
    schema_builder& remove_column(bytes name);
    schema_builder& without_column(sstring name, api::timestamp_type timestamp);
    schema_builder& without_column(sstring name, data_type, api::timestamp_type timestamp);
    schema_builder& rename_column(bytes from, bytes to);
    schema_builder& alter_column_type(bytes name, data_type new_type);
    schema_builder& mark_column_computed(bytes name, column_computation_ptr computation);
    // Adds information about collection that existed in the past but the column
    // has since been removed. For adding colllections that are still alive
    // use with_column().
    schema_builder& with_collection(bytes name, data_type type);
    schema_builder& with(compact_storage);
    schema_builder& with_version(table_schema_version);
    schema_builder& with_view_info(table_id base_id, sstring base_name, bool include_all_columns, sstring where_clause);
    
    
    ;
    
    // Equivalent to with(cp).build()
    
    schema_ptr build();
private:
    friend class default_names;
    void prepare_dense_schema(schema::raw_schema& raw);
    schema_builder& with_column(bytes name, data_type type, column_kind kind, column_id component_index, column_view_virtual view_virtual = column_view_virtual::no, column_computation_ptr computation = nullptr);
};
namespace db {
class schema_ctxt;
}
class schema_registry;
using async_schema_loader = std::function<future<frozen_schema>(table_schema_version)>;
using schema_loader = std::function<frozen_schema(table_schema_version)>;
class schema_version_not_found : public std::runtime_error {
public:
    
};
class schema_version_loading_failed : public std::runtime_error {
public:
};
//
// Presence in schema_registry is controlled by different processes depending on
// life cycle stage:
//   1) Initially it's controlled by the loader. When loading fails, entry is removed by the loader.
//   2) When loading succeeds, the entry is controlled by live schema_ptr. It remains present as long as
//      there's any live schema_ptr.
//   3) When last schema_ptr dies, entry is deactivated. Currently it is removed immediately, later we may
//      want to keep it around for some time to reduce cache misses.
//
// In addition to the above the entry is controlled by lw_shared_ptr<> to cope with races between loaders.
//
class schema_registry_entry : public enable_lw_shared_from_this<schema_registry_entry> {
    using erase_clock = seastar::lowres_clock;
    enum class state {
        INITIAL, LOADING, LOADED
    };
    state _state;
    table_schema_version _version; // always valid
    schema_registry& _registry; // always valid
    async_schema_loader _loader; // valid when state == LOADING
    shared_promise<schema_ptr> _schema_promise; // valid when state == LOADING
    std::optional<frozen_schema> _frozen_schema; // engaged when state == LOADED
    // valid when state == LOADED
    // This is != nullptr when there is an alive schema_ptr associated with this entry.
    const ::schema* _schema = nullptr;
    enum class sync_state { NOT_SYNCED, SYNCING, SYNCED };
    sync_state _sync_state;
    shared_promise<> _synced_promise; // valid when _sync_state == SYNCING
    timer<erase_clock> _erase_timer;
    friend class schema_registry;
public:
    ;
    
    
     // call only when state >= LOADED
    // Can be called from other shards
    bool is_synced() const;
    // Initiates asynchronous schema sync or returns ready future when is already synced.
    future<> maybe_sync(std::function<future<>()> sync);
    // Marks this schema version as synced. Syncing cannot be in progress.
    void mark_synced();
    // Can be called from other shards
    frozen_schema frozen() const;
    // Can be called from other shards
    table_schema_version version() const ;
public:
    // Called by class schema
    void detach_schema() noexcept;
};
//
// Keeps track of different versions of table schemas. A per-shard object.
//
// For every schema_ptr obtained through getters, as long as the schema pointed to is
// alive the registry will keep its entry. To ensure remote nodes can query current node
// for schema version, make sure that schema_ptr for the request is alive around the call.
//
class schema_registry {
    std::unordered_map<table_schema_version, lw_shared_ptr<schema_registry_entry>> _entries;
    std::unique_ptr<db::schema_ctxt> _ctxt;
    friend class schema_registry_entry;
    schema_registry_entry& get_entry(table_schema_version) const;
    // Duration for which unused entries are kept alive to avoid
    // too frequent re-requests and syncs. Default is 1 second.
    schema_registry_entry::erase_clock::duration grace_period() const;
public:
    ~schema_registry();
    // workaround to this object being magically appearing from nowhere.
    void init(const db::schema_ctxt&);
    // Looks up schema by version or loads it using supplied loader.
    schema_ptr get_or_load(table_schema_version, const schema_loader&);
    // Looks up schema by version or returns an empty pointer if not available.
    schema_ptr get_or_null(table_schema_version) const;
    // Like get_or_load() which takes schema_loader but the loader may be
    // deferring. The loader is copied must be alive only until this method
    // returns. If the loader fails, the future resolves with
    // schema_version_loading_failed.
    future<schema_ptr> get_or_load(table_schema_version, const async_schema_loader&);
    // Looks up schema version. Throws schema_version_not_found when not found
    // or loading is in progress.
    schema_ptr get(table_schema_version) const;
    // Looks up schema version. Throws schema_version_not_found when not found
    // or loading is in progress.
    
    // Attempts to add given schema to the registry. If the registry already
    // knows about the schema, returns existing entry, otherwise returns back
    // the schema which was passed as argument. Users should prefer to use the
    // schema_ptr returned by this method instead of the one passed to it,
    // because doing so ensures that the entry will be kept in the registry as
    // long as the schema is actively used.
    
};
schema_registry& local_schema_registry();
// Schema pointer which can be safely accessed/passed across shards via
// const&. Useful for ensuring that schema version obtained on one shard is
// automatically propagated to other shards, no matter how long the processing
// chain will last.
class global_schema_ptr {
    schema_ptr _ptr;
    schema_ptr _base_schema;
    unsigned _cpu_of_origin;
public:
    // Note: the schema_ptr must come from the current shard and can't be nullptr.
    global_schema_ptr(const schema_ptr&);
    // The other may come from a different shard.
    global_schema_ptr(const global_schema_ptr& other);
    // The other must come from current shard.
    global_schema_ptr(global_schema_ptr&& other) noexcept;
    // May be invoked across shards. Always returns an engaged pointer.
    schema_ptr get() const;
    operator schema_ptr() const { return get(); }
};
namespace service {
class priority_manager {
    ::io_priority_class _commitlog_priority;
    ::io_priority_class _mt_flush_priority;
    ::io_priority_class _streaming_priority;
    ::io_priority_class _sstable_query_read;
    ::io_priority_class _compaction_priority;
public:
};
}
namespace dht {
// Utilities for sharding ring partition_range:s
// A ring_position range's data is divided into sub-ranges, where each sub-range's data
// is owned by a single shard. Note that multiple non-overlapping sub-ranges may map to a
// single shard, and some shards may not receive any sub-range.
//
// This module provides utilities for determining the sub-ranges to shard mapping. The utilities
// generate optimal mappings: each range that you get is the largest possible, so you
// get the minimum number of ranges possible. You can get many ranges, so operate on them
// one (or a few) at a time, rather than accumulating them.
// A mapping between a partition_range and a shard. All positions within `ring_range` are
// owned by `shard`.
//
// The classes that return ring_position_range_and_shard make `ring_range` as large as
// possible (maximizing the number of tokens), so the total number of such ranges is minimized.
// Successive ranges therefore always have a different `shard` than the previous return.
// (classes that return ring_position_range_and_shard_and_element can have the same `shard`
// in successive returns, if `element` is different).
struct ring_position_range_and_shard {
    dht::partition_range ring_range;
    unsigned shard;
};
// Incrementally divides a `partition_range` into sub-ranges wholly owned by a single shard.
class ring_position_range_sharder {
    const sharder& _sharder;
    dht::partition_range _range;
    bool _done = false;
public:
    // Initializes the ring_position_range_sharder with a given range to subdivide.
    // Fetches the next range-shard mapping. When the input range is exhausted, std::nullopt is
    // returned. The returned ranges are contiguous and non-overlapping, and together span the
    // entire input range.
};
// A mapping between a partition_range and a shard (like ring_position_range_and_shard) extended
// by having a reference to input range index. See ring_position_range_vector_sharder for use.
//
// The classes that return ring_position_range_and_shard_and_element make `ring_range` as large as
// possible (maximizing the number of tokens), so the total number of such ranges is minimized.
// Successive ranges therefore always have a different `shard` than the previous return.
// (classes that return ring_position_range_and_shard_and_element can have the same `shard`
// in successive returns, if `element` is different).
struct ring_position_range_and_shard_and_element : ring_position_range_and_shard {
    unsigned element;
};
// Incrementally divides several non-overlapping `partition_range`:s into sub-ranges wholly owned by
// a single shard.
//
// Similar to ring_position_range_sharder, but instead of stopping when the input range is exhauseted,
// moves on to the next input range (input ranges are supplied in a vector).
//
// This has two use cases:
// 1. vnodes. A vnode cannot be described by a single range, since
//    one vnode wraps around from the largest token back to the smallest token. Hence it must be
//    described as a vector of two ranges, (largest_token, +inf) and (-inf, smallest_token].
// 2. sstable shard mappings. An sstable has metadata describing which ranges it owns, and this is
//    used to see what shards these ranges map to (and therefore to see if the sstable is shared or
//    not, and which shards share it).
class ring_position_range_vector_sharder {
    using vec_type = dht::partition_range_vector;
    vec_type _ranges;
    const sharder& _sharder;
    vec_type::iterator _current_range;
    std::optional<ring_position_range_sharder> _current_sharder;
private:
public:
    // Initializes the `ring_position_range_vector_sharder` with the ranges to be processesd.
    // Input ranges should be non-overlapping (although nothing bad will happen if they do
    // overlap).
    // Fetches the next range-shard mapping. When the input range is exhausted, std::nullopt is
    // returned. Within an input range, results are contiguous and non-overlapping (but since input
    // ranges usually are discontiguous, overall the results are not contiguous). Together, the results
    // span the input ranges.
    //
    // The result is augmented with an `element` field which indicates the index from the input vector
    // that the result belongs to.
    //
    // Results are returned sorted by index within the vector first, then within each vector item
};
// Incrementally divides a `partition_range` into sub-ranges wholly owned by a single shard.
// Unlike ring_position_range_sharder, it only returns result for a shard number provided by the caller.
class selective_token_range_sharder {
    const sharder& _sharder;
    dht::token_range _range;
    shard_id _shard;
    bool _done = false;
    shard_id _next_shard;
    dht::token _start_token;
    std::optional<range_bound<dht::token>> _start_boundary;
public:
    // Initializes the selective_token_range_sharder with a token range and shard_id of interest.
    // Returns the next token_range that is both wholly contained within the input range and also
    // wholly owned by the input shard_id. When the input range is exhausted, std::nullopt is returned.
    // Note if the range does not intersect the shard at all, std::nullopt will be returned immediately.
};
} // dht
namespace streaming {
class stream_state {
public:
    streaming::plan_id plan_id;
    sstring description;
    std::vector<session_info> sessions;
    bool has_failed_session() const ;
};
} // namespace streaming
extern logger startlog;
class supervisor {
public:
    static constexpr auto systemd_ready_msg = "READY=1";
    static constexpr auto systemd_status_msg_prefix = "STATUS";
public:
private:
};
namespace cql_transport {
namespace messages {
class result_message {
    std::vector<sstring> _warnings;
public:
    class visitor;
    class visitor_base;
    
    
    virtual bool is_exception() const ;
    virtual void throw_if_exception() const ;
    //
    // Message types:
    //
    class void_message;
    class set_keyspace;
    class prepared;
    class schema_change;
    class rows;
    class bounce_to_shard;
    class exception;
};

}
}
namespace cql3 {
class query_options;
}
namespace cql3 {
using prepared_cache_entry = std::unique_ptr<statements::prepared_statement>;
struct prepared_cache_entry_size {
};
typedef bytes cql_prepared_id_type;
typedef int32_t thrift_prepared_id_type;
/// \brief The key of the prepared statements cache
///
/// We are going to store the CQL and Thrift prepared statements in the same cache therefore we need generate the key
/// that is going to be unique in both cases. Thrift use int32_t as a prepared statement ID, CQL - MD5 digest.
///
/// We are going to use an std::pair<CQL_PREP_ID_TYPE, int64_t> as a key. For CQL statements we will use {CQL_PREP_ID, std::numeric_limits<int64_t>::max()} as a key
/// and for Thrift - {CQL_PREP_ID_TYPE(0), THRIFT_PREP_ID}. This way CQL and Thrift keys' values will never collide.
class prepared_cache_key_type {
public:
    using cache_key_type = std::pair<cql_prepared_id_type, int64_t>;
private:
    cache_key_type _key;
public:
    
    const cache_key_type& key() const ;
    static const cql_prepared_id_type& cql_id(const prepared_cache_key_type& key) ;
    
    
};
class prepared_statements_cache {
public:
    struct stats {
        uint64_t prepared_cache_evictions = 0;
        uint64_t privileged_entries_evictions_on_size = 0;
        uint64_t unprivileged_entries_evictions_on_size = 0;
    };
    static stats& shard_stats() ;
    struct prepared_cache_stats_updater {
        static void inc_hits() noexcept ;
        static void inc_misses() noexcept ;
        static void inc_blocks() noexcept ;
        static void inc_evictions() noexcept ;
        static void inc_privileged_on_cache_size_eviction() noexcept ;
    };
private:
    using cache_key_type = typename prepared_cache_key_type::cache_key_type;
    // Keep the entry in the "unprivileged" cache section till 2 hits because
    // every prepared statement is accessed at least twice in the cache:
    //  1) During PREPARE
    //  2) During EXECUTE
    //
    // Therefore a typical "pollution" (when a cache entry is used only once) would involve
    // 2 cache hits.
    using cache_type = utils::loading_cache<cache_key_type, prepared_cache_entry, 2, utils::loading_cache_reload_enabled::no, prepared_cache_entry_size, utils::tuple_hash, std::equal_to<cache_key_type>, prepared_cache_stats_updater, prepared_cache_stats_updater>;
    using cache_value_ptr = typename cache_type::value_ptr;
    using checked_weak_ptr = typename statements::prepared_statement::checked_weak_ptr;
public:
    static const std::chrono::minutes entry_expiry;
    using key_type = prepared_cache_key_type;
    using value_type = checked_weak_ptr;
    using statement_is_too_big = typename cache_type::entry_is_too_big;
private:
    cache_type _cache;
public:
     ;
    // "Touch" the corresponding cache entry in order to bump up its reference count.
    template <typename Pred>
    requires std::is_invocable_r_v<bool, Pred, ::shared_ptr<cql_statement>>
    void remove_if(Pred&& pred) {
        _cache.remove_if([&pred] (const prepared_cache_entry& e) {
            return pred(e->statement);
        });
    }
    size_t size() const {
        return _cache.size();
    }
    size_t memory_footprint() const {
        return _cache.memory_footprint();
    }
    future<> stop() {
        return _cache.stop();
    }
};
}
namespace std { // for prepared_statements_cache log printouts
 std::ostream& operator<<(std::ostream& os, const typename cql3::prepared_cache_key_type::cache_key_type& p) ;
 std::ostream& operator<<(std::ostream& os, const cql3::prepared_cache_key_type& p) ;
template<>
struct hash<cql3::prepared_cache_key_type> final {
    size_t operator()(const cql3::prepared_cache_key_type& k) const {
        return utils::tuple_hash()(k.key());
    }
};
}
namespace cql3 {
struct authorized_prepared_statements_cache_size {
    size_t operator()(const statements::prepared_statement::checked_weak_ptr& val) ;
};
class authorized_prepared_statements_cache_key {
public:
    using cache_key_type = std::pair<auth::authenticated_user, typename cql3::prepared_cache_key_type::cache_key_type>;
    struct view {
        const auth::authenticated_user& user_ref;
        const cql3::prepared_cache_key_type& prep_cache_key_ref;
    };
    struct view_hasher {
        size_t operator()(const view& kv) ;
    };
    struct view_equal {
        
        
    };
private:
    cache_key_type _key;
public:
    authorized_prepared_statements_cache_key(auth::authenticated_user user, cql3::prepared_cache_key_type prepared_cache_key)
        : _key(std::move(user), std::move(prepared_cache_key.key())) {}
    cache_key_type& key() ;
    const cache_key_type& key() const ;
    bool operator==(const authorized_prepared_statements_cache_key&) const = default;
    static size_t hash(const auth::authenticated_user& user, const cql3::prepared_cache_key_type::cache_key_type& prep_cache_key) ;
};
/// \class authorized_prepared_statements_cache
/// \brief A cache of previously authorized statements.
///
/// Entries are inserted every time a new statement is authorized.
/// Entries are evicted in any of the following cases:
///    - When the corresponding prepared statement is not valid anymore.
///    - Periodically, with the same period as the permission cache is refreshed.
///    - If the corresponding entry hasn't been used for \ref entry_expiry.
class authorized_prepared_statements_cache {
public:
    struct stats {
        uint64_t authorized_prepared_statements_cache_evictions = 0;
        uint64_t authorized_prepared_statements_privileged_entries_evictions_on_size = 0;
        uint64_t authorized_prepared_statements_unprivileged_entries_evictions_on_size = 0;
    };
    static stats& shard_stats() ;
    struct authorized_prepared_statements_cache_stats_updater {
        static void inc_hits() noexcept ;
        static void inc_misses() noexcept ;
        static void inc_blocks() noexcept ;
        static void inc_evictions() noexcept ;
        
    };
private:
    using cache_key_type = authorized_prepared_statements_cache_key;
    using checked_weak_ptr = typename statements::prepared_statement::checked_weak_ptr;
    using cache_type = utils::loading_cache<cache_key_type,
                                            checked_weak_ptr,
                                            1,
                                            utils::loading_cache_reload_enabled::yes,
                                            authorized_prepared_statements_cache_size,
                                            std::hash<cache_key_type>,
                                            std::equal_to<cache_key_type>,
                                            authorized_prepared_statements_cache_stats_updater,
                                            authorized_prepared_statements_cache_stats_updater>;
public:
    using key_type = cache_key_type;
    using key_view_type = typename key_type::view;
    using key_view_hasher = typename key_type::view_hasher;
    using key_view_equal = typename key_type::view_equal;
    using value_type = checked_weak_ptr;
    using entry_is_too_big = typename cache_type::entry_is_too_big;
    using value_ptr = typename cache_type::value_ptr;
private:
    cache_type _cache;
public:
    // Choose the memory budget such that would allow us ~4K entries when a shard gets 1GB of RAM
};
}
namespace std {
template <>
struct hash<cql3::authorized_prepared_statements_cache_key> final {
    size_t operator()(const cql3::authorized_prepared_statements_cache_key& k) const {
        return cql3::authorized_prepared_statements_cache_key::hash(k.key().first, k.key().second);
    }
};
}
namespace wasm {
struct wasm_compile_task {
    seastar::noncopyable_function<void()> func;
    seastar::promise<rust::Box<wasmtime::Module>>& done;
    unsigned shard;
};
struct task_queue {
    std::mutex _mut;
    std::condition_variable _cv;
    std::queue<std::optional<wasm_compile_task>> _pending;
public:
};
class alien_thread_runner {
    task_queue _pending_queue;
    std::thread _thread;
public:
};
} // namespace wasm
namespace wasm {
class instance_cache;
struct exception : public std::exception {
    std::string _msg;
public:
};
struct instance_corrupting_exception : public exception {
};
struct startup_context {
    std::shared_ptr<alien_thread_runner> alien_runner;
    std::shared_ptr<rust::Box<wasmtime::Engine>> engine;
    size_t cache_size;
    size_t instance_size;
    seastar::lowres_clock::duration timer_period;
};
struct context {
    wasmtime::Engine& engine_ptr;
    std::optional<rust::Box<wasmtime::Module>> module;
    std::string function_name;
    instance_cache& cache;
    uint64_t yield_fuel;
    uint64_t total_fuel;
};
}
namespace wasm {
class module_handle {
    wasmtime::Module& _module;
    instance_cache& _cache;
public:
};
struct wasm_instance {
    rust::Box<wasmtime::Store> store;
    rust::Box<wasmtime::Instance> instance;
    rust::Box<wasmtime::Func> func;
    rust::Box<wasmtime::Memory> memory;
    module_handle mh;
};
// For each UDF full name and a scheduling group, we store a wasmtime instance
// that is usable after acquiring a corresponding mutex. This way, the instance
// can't be used in multiple continuations from the same scheduling group at the
// same time.
// The instance may be evicted only when it is not used, i.e. when the corresponding
// mutex is not held. When the instance is used, its size is not tracked, but
// it's limited by the size of its memory - it can't exceed a set value (1MB).
// After the instance stops being used, a timestamp of last use is recorded,
// and its size is added to the total size of all instances. Other, older instances
// may be evicted if the total size of all instances exceeds a set value (100MB).
// If the instance is not used for at least _timer_period, it is evicted after
// at most another _timer_period.
// Entries in the cache are created on the first use of a UDF in a given scheduling
// and they are stored in memory until the UDF is dropped. The number of such
// entries is limited by the number of stored UDFs multiplied by the number of
// scheduling groups.
class instance_cache {
public:
    struct stats {
        uint64_t cache_hits = 0;
        uint64_t cache_misses = 0;
        uint64_t cache_blocks = 0;
    };
private:
    stats _stats;
    seastar::metrics::metric_groups _metrics;
public:
private:
    using cache_key_type = db::functions::function_name;
    struct lru_entry_type;
    struct cache_entry_type {
        seastar::scheduling_group scheduling_group;
        std::vector<data_type> arg_types;
        seastar::shared_mutex mutex;
        std::optional<wasm_instance> instance;
        // iterator points to _lru.end() when the entry is being used (at that point, it is not in lru)
        std::list<lru_entry_type>::iterator it;
        wasmtime::Module& module;
    };
public:
    using value_type = lw_shared_ptr<cache_entry_type>;
private:
    struct lru_entry_type {
        value_type cache_entry;
        seastar::lowres_clock::time_point timestamp;
        size_t instance_size;
    };
private:
    std::unordered_multimap<cache_key_type, value_type> _cache;
    std::list<lru_entry_type> _lru;
    seastar::timer<seastar::lowres_clock> _timer;
    // The instance in cache time out after up to 2*_timer_period.
    seastar::lowres_clock::duration _timer_period;
    size_t _total_size = 0;
    size_t _max_size;
    size_t _max_instance_size;
    size_t _compiled_size = 0;
    // The reserved size for compiled code (which is not allocated by the seastar allocator)
    // is 50MB. We always leave some of this space free for the compilation of new instances
    // - we only find out the real compiled size after the compilation finishes. (During
    // the verification of the compiled code, we also allocate a new stack using this memory)
    size_t _max_compiled_size = 40 * 1024 * 1024;
public:
private:
public:
private:
    friend class module_handle;
    // Wasmtime instances hold references to modules, so the module can only be dropped
    // when all instances are dropped. For a given module, we can have at most one
    // instance for each scheduling group.
    // This function is called each time a new instance is created for a given module.
    // If there were no instances for the module before, i.e. this module was not
    // compiled, the module is compiled and the size of the compiled code is added
    // to the total size of compiled code. If the total size of compiled code exceeds
    // the maximum size as a result of this, the function will evict modules until
    // there is enough space for the new module. If it is not possible, the function
    // will throw an exception. If this function succeeds, the counter of instances
    // for the module is increased by one.
    // This function is called each time an instance for a given module is dropped.
    // If the counter of instances for the module reaches zero, the module is dropped
    // and the size of the compiled code is subtracted from the total size of compiled code.
    // When a WASM UDF is executed, a separate stack is first allocated for it.
    // This stack is used by the WASM code and it is not tracked by the seastar allocator.
    // This function will evict cached modules until the stack can be allocated. If enough
    // memory can't be freed, the function will throw an exception.
    // This function should be called after a WASM UDF finishes execution. Its stack is then
    // destroyed and this function accounts for the freed memory.
    // Evicts instances using lru until a module is no longer referenced by any of them.
public:
};
}
namespace data_dictionary {
class keyspace_metadata;
}
using keyspace_metadata = data_dictionary::keyspace_metadata;
class view_ptr;
class user_type_impl;
using user_type = seastar::shared_ptr<const user_type_impl>;
class schema;
using schema_ptr = seastar::lw_shared_ptr<const schema>;
class abstract_type;
using data_type = seastar::shared_ptr<const abstract_type>;
namespace db::functions {
class function_name;
}
class mutation;
class schema;
namespace service {
class migration_listener {
public:
    // The callback runs inside seastar thread
    // The callback runs inside seastar thread
    // The callback runs inside seastar thread
    // The callback runs inside seastar thread
    // called before adding/updating/dropping column family. 
    // listener can add additional type altering mutations if he knows what he is doing. 
    class only_view_notifications;
    class empty_listener;
};
class migration_listener::only_view_notifications : public migration_listener {
public:
};
class migration_listener::empty_listener : public only_view_notifications {
public:
    ;
    ;
    ;
};
class migration_notifier {
private:
    atomic_vector<migration_listener*> _listeners;
public:
    /// Register a migration listener on current shard.
    /// Unregister a migration listener on current shard.
};
}
namespace cql3 {
namespace statements {
class statement_type final {
    enum class type : size_t {
        insert = 0,
        update,
        del,
        select,
        last  // Keep me as last entry
    };
    const type _type;
public:
    static const statement_type INSERT;
    static const statement_type UPDATE;
    static const statement_type DELETE;
    static const statement_type SELECT;
    static constexpr size_t MAX_VALUE = size_t(type::last) - 1;
    explicit operator size_t() const {
        return size_t(_type);
    }
    bool operator==(const statement_type&) const = default;
};
}
}
namespace cql3 {
enum class source_selector : size_t {
    INTERNAL = 0u,
    USER,
    SIZE  // Keep me as the last entry
};
enum class ks_selector : size_t {
    SYSTEM = 0u,
    NONSYSTEM,
    SIZE  // Keep me as the last entry
};
enum class cond_selector : size_t {
    NO_CONDITIONS = 0u,
    WITH_CONDITIONS,
    SIZE  // Keep me as the last entry
};
// Shard-local CQL statistics
// @sa cql3/query_processor.cc explains the meaning of each counter
struct cql_stats {
    uint64_t batches = 0;
    uint64_t cas_batches = 0;
    uint64_t statements_in_batches = 0;
    uint64_t statements_in_cas_batches = 0;
    uint64_t batches_pure_logged = 0;
    uint64_t batches_pure_unlogged = 0;
    uint64_t batches_unlogged_from_logged = 0;
    uint64_t rows_read = 0;
    uint64_t reverse_queries = 0;
    int64_t secondary_index_creates = 0;
    int64_t secondary_index_drops = 0;
    int64_t secondary_index_reads = 0;
    int64_t secondary_index_rows_read = 0;
    int64_t filtered_reads = 0;
    int64_t filtered_rows_matched_total = 0;
    int64_t filtered_rows_read_total = 0;
    int64_t select_bypass_caches = 0;
    int64_t select_allow_filtering = 0;
    int64_t select_partition_range_scan = 0;
    int64_t select_partition_range_scan_no_bypass_cache = 0;
    int64_t select_parallelized = 0;
private:
    uint64_t _unpaged_select_queries[(size_t)ks_selector::SIZE] = {0ul};
    uint64_t _query_cnt[(size_t)source_selector::SIZE]
            [(size_t)ks_selector::SIZE]
            [(size_t)cond_selector::SIZE]
            [statements::statement_type::MAX_VALUE + 1] = {};
};
}
namespace utils {
managed_bytes_view_opt
buffer_view_to_managed_bytes_view(std::optional<ser::buffer_view<bytes_ostream::fragment_iterator>> bvo);
}
namespace cql3 {
class untyped_result_set;
class result_generator {
    schema_ptr _schema;
    foreign_ptr<lw_shared_ptr<query::result>> _result;
    lw_shared_ptr<const query::read_command> _command;
    shared_ptr<const selection::selection> _selection;
    cql_stats* _stats;
private:
    friend class untyped_result_set;
    template<typename Visitor>
    class query_result_visitor {
        const schema& _schema;
        std::vector<bytes> _partition_key;
        std::vector<bytes> _clustering_key;
        uint64_t _partition_row_count = 0;
        uint64_t _total_row_count = 0;
        Visitor& _visitor;
        const selection::selection& _selection;
    private:
    public:
    };
public:
     ;
};
}
namespace cql3 {
class metadata {
public:
    enum class flag : uint8_t {
        GLOBAL_TABLES_SPEC = 0,
        HAS_MORE_PAGES = 1,
        NO_METADATA = 2,
    };
    using flag_enum = super_enum<flag,
        flag::GLOBAL_TABLES_SPEC,
        flag::HAS_MORE_PAGES,
        flag::NO_METADATA>;
    using flag_enum_set = enum_set<flag_enum>;
    struct column_info {
    // Please note that columnCount can actually be smaller than names, even if names is not null. This is
    // used to include columns in the resultSet that we need to do post-query re-orderings
    // (SelectStatement.orderResults) but that shouldn't be sent to the user as they haven't been requested
    // (CASSANDRA-4911). So the serialization code will exclude any columns in name whose index is >= columnCount.
        std::vector<lw_shared_ptr<column_specification>> _names;
        uint32_t _column_count;
    };
private:
    flag_enum_set _flags;
private:
    lw_shared_ptr<column_info> _column_info;
    lw_shared_ptr<const service::pager::paging_state> _paging_state;
public:
    // The maximum number of values that the ResultSet can hold. This can be bigger than columnCount due to CASSANDRA-4911
private:
public:
};

class prepared_metadata {
public:
    enum class flag : uint32_t {
        GLOBAL_TABLES_SPEC = 0,
        // Denotes whether the prepared statement at hand is an LWT statement.
        //
        // Use the last available bit in the flags since we don't want to clash
        // with C* in case they add some other flag in one the next versions of binary protocol.
        LWT = 31
    };
    using flag_enum = super_enum<flag,
        flag::GLOBAL_TABLES_SPEC,
        flag::LWT>;
    using flag_enum_set = enum_set<flag_enum>;
    static constexpr flag_enum_set::mask_type LWT_FLAG_MASK = flag_enum_set::mask_for<flag::LWT>();
private:
    flag_enum_set _flags;
    std::vector<lw_shared_ptr<column_specification>> _names;
    std::vector<uint16_t> _partition_key_bind_indices;
public:
    
};
template<typename Visitor>
concept ResultVisitor = requires(Visitor& visitor, managed_bytes_view_opt val) {
    visitor.start_row();
    visitor.accept_value(std::move(val));
    visitor.end_row();
};
class result_set {
    using col_type = managed_bytes_opt;
    using row_type = std::vector<col_type>;
    using rows_type = utils::chunked_vector<row_type>;
    ::shared_ptr<metadata> _metadata;
    rows_type _rows;
    friend class result;
public:
    template<typename RowComparator>
    requires requires (RowComparator cmp, const row_type& row) {
        { cmp(row, row) } -> std::same_as<bool>;
    }
    void sort(const RowComparator& cmp) {
        std::sort(_rows.begin(), _rows.end(), cmp);
    }
    metadata& get_metadata();
    // Returns a range of rows. A row is a range of bytes_opt.
     ;
    class builder;
};
class result_set::builder {
    result_set _result;
    row_type _current_row;
public:
};
class result {
    mutable std::unique_ptr<cql3::result_set> _result_set;
    result_generator _result_generator;
    shared_ptr<const cql3::metadata> _metadata;
public:
     ;
};
}
namespace service {
class storage_proxy;
class query_state;
class client_state;
}
namespace cql_transport {
namespace messages {
class result_message;
}
}
namespace cql3 {
class query_processor;
class metadata;

class query_options;
class cql_statement {
    timeout_config_selector _timeout_config_selector;
public:
    // CQL statement text
    seastar::sstring raw_cql_statement;
};
class cql_statement_no_metadata : public cql_statement {
public:
    using cql_statement::cql_statement;
};
// Conditional modification statements and batches
// return a result set and have metadata, while same
// statements without conditions do not.
class cql_statement_opt_metadata : public cql_statement {
protected:
    // Result set metadata, may be empty for simple updates and batches
    seastar::shared_ptr<metadata> _metadata;
public:
    using cql_statement::cql_statement;
};
}
namespace cql_transport {
class event {
public:
    enum class event_type { TOPOLOGY_CHANGE, STATUS_CHANGE, SCHEMA_CHANGE };
    event_type type;
private:
public:
    class topology_change;
    class status_change;
    class schema_change;
};
class event::topology_change : public event {
public:
    enum class change_type { NEW_NODE, REMOVED_NODE, MOVED_NODE };
    change_type change;
    socket_address node;
};
class event::status_change : public event {
public:
    enum class status_type { UP, DOWN };
    status_type status;
    socket_address node;
};
class event::schema_change : public event {
public:
    enum class change_type { CREATED, UPDATED, DROPPED };
    enum class target_type { KEYSPACE, TABLE, TYPE, FUNCTION, AGGREGATE };
    change_type change;
    target_type target;
    // Every target is followed by at least a keyspace.
    sstring keyspace;
    // Target types other than keyspace have a list of arguments.
    std::vector<sstring> arguments;
    
    
};
}
namespace utils {
class bad_exception_container_access : public std::exception {
public:
    
};
// A variant-like type capable of holding one of the allowed exception types.
// This allows inspecting the exception in the error handling code without
// having to resort to costly rethrowing of std::exception_ptr, as is
// in the case of the usual exception handling.
//
// It's not as ergonomic as using exceptions with seastar, but allows for
// fast inspection and manipulation.
//
// The exception is held behind a std::shared_ptr. In order to minimize use
// of atomic operations, the copy constructor is deleted and copying is only
// possible by using the `clone()` method.
//
// This means that the moved-out exception container becomes "empty" and
// does not contain a valid exception.
template<typename... Exs>
struct exception_container {
private:
    using exception_variant = std::variant<Exs...>;
    // TODO: Idea for a possible improvement: get rid of the variant
    // and just store a pointer to an error allocated on the heap.
    // Keep an integer which identifies the variant.
    // Bonus points: if each error type has a unique, globally-assigned
    // identified integer, then conversion of the exception_container
    // to a container supporting a superset of errors becomes very cheap.
    std::shared_ptr<exception_variant> _eptr;
    // Users should use `clone()` in order to copy the exception container.
    // The copy constructor is made private in order to make copying explicit.
    exception_container(const exception_container&) = default;
    void check_nonempty() const ;
public:
    // Constructs an exception_container which does not contain any exception.
     // Must be explicitly copied with `clone()`
    template<typename Ex>
    requires VariantElement<Ex, exception_variant>
    exception_container(Ex&& ex)
            : _eptr(std::make_shared<exception_variant>(std::forward<Ex>(ex)))
    { }
    inline bool empty() const {
        return __builtin_expect(!_eptr, false);
    }
    inline operator bool() const {
        return !empty();
    }
    // Accepts a visitor.
    // If the container is empty, the visitor is called with
    // a bad_exception_container_access.
    auto accept(auto f) const {
        if (empty()) {
            return f(bad_exception_container_access());
        }
        return std::visit(std::move(f), *_eptr);
    }
    // Explicitly clones the exception container.
    exception_container clone() const noexcept {
        return exception_container(*this);
    }
    // Throws currently held exception as a C++ exception.
    // If the container is empty, it throws bad_exception_container_access.
    [[noreturn]] void throw_me() const {
        check_nonempty();
        std::visit([] (const auto& ex) { throw ex; }, *_eptr);
        std::terminate(); // Should be unreachable
    }
    // Creates an exceptional future from this error.
    // The exception is copied into the new exceptional future.
    // If the container is empty, returns an exceptional future
    // with the bad_exception_container_access exception.
    template<typename T = void>
    seastar::future<T> as_exception_future() const & {
        if (!_eptr) {
            return seastar::make_exception_future<T>(bad_exception_container_access());
        }
        return std::visit([] (const auto& ex) {
            return seastar::make_exception_future<T>(ex);
        }, *_eptr);
    }
    // Transforms this exception future into an exceptional future.
    // The exception is moved out and the container becomes empty.
    // If the container was empty, returns an exceptional future
    // with the bad_exception_container_access exception.
    template<typename T = void>
    seastar::future<T> into_exception_future() && {
        if (!_eptr) {
            return seastar::make_exception_future<T>(bad_exception_container_access());
        }
        auto f = std::visit([] (auto&& ex) {
            return seastar::make_exception_future<T>(std::move(ex));
        }, *_eptr);
        _eptr.reset();
        return f;
    }
};
 ;
template<typename T>
struct is_exception_container : std::false_type {};
template<typename... Exs>
struct is_exception_container<exception_container<Exs...>> : std::true_type {};
template<typename T>
concept ExceptionContainer = is_exception_container<T>::value;
}
// Basic utilities which allow to start working with boost::outcome::result
// in conjunction with our exception_container.
namespace bo = BOOST_OUTCOME_V2_NAMESPACE;
namespace utils {
// A policy which throws the container_error associated with the result
// if there was an attempt to access value while it was not present.
struct exception_container_throw_policy : bo::policy::base {
     ;
     ;
};
template<typename T, typename... Exs>
using result_with_exception = bo::result<T, exception_container<Exs...>, exception_container_throw_policy>;
template<typename R>
concept ExceptionContainerResult = bo::is_basic_result<R>::value && ExceptionContainer<typename R::error_type>;
template<typename F>
concept ExceptionContainerResultFuture = seastar::is_future<F>::value && ExceptionContainerResult<typename F::value_type>;
template<typename L, typename R>
concept ResultRebindableTo =
    bo::is_basic_result<L>::value &&
    bo::is_basic_result<R>::value &&
    std::same_as<typename L::error_type, typename R::error_type> &&
    std::same_as<typename L::no_value_policy_type, typename R::no_value_policy_type>;
// Creates a result type which has the same error type as R, but has a different value type.
// The name was inspired by std::allocator::rebind.
template<typename T, ExceptionContainerResult R>
using rebind_result = bo::result<T, typename R::error_type, exception_container_throw_policy>;
}
namespace exceptions {
// Allows to pass a coordinator exception as a value. With coordinator_result,
// it is possible to handle exceptions and inspect their type/value without
// resorting to costly rethrows. On the other hand, using them is more
// cumbersome than just using exceptions and exception futures.
//
// Not all exceptions are passed in this way, therefore the container
// does not allow all types of coordinator exceptions. On the other hand,
// an exception being listed here does not mean it is _always_ passed
// in an exception_container - it can be thrown in a regular fashion
// as well.
//
// It is advised to use this mechanism mainly for exceptions which can
// happen frequently, e.g. signalling timeouts, overloads or rate limits.
using coordinator_exception_container = utils::exception_container<
    mutation_write_timeout_exception,
    read_timeout_exception,
    read_failure_exception,
    rate_limit_exception
>;
template<typename T = void>
using coordinator_result = bo::result<T,
    coordinator_exception_container,
    utils::exception_container_throw_policy
>;
}
namespace cql_transport {
namespace messages {
class result_message::prepared : public result_message {
private:
    cql3::statements::prepared_statement::checked_weak_ptr _prepared;
    cql3::prepared_metadata _metadata;
    ::shared_ptr<const cql3::metadata> _result_metadata;
protected:
public:
    class cql;
    class thrift;
private:
};
class result_message::visitor {
public:
};
class result_message::visitor_base : public visitor {
public:
    ;
    ;
    ;
    ;
    ;
    ;
    ;
};
class result_message::void_message : public result_message {
public:
};
// This result is handled internally and should never be returned
// to a client. Any visitor should abort while handling it since
// it is a sure sign of a error.
class result_message::bounce_to_shard : public result_message {
    unsigned _shard;
    cql3::computed_function_values _cached_fn_calls;
public:
};
// This result is handled internally. It can be used to indicate an exception
// which needs to be handled without involving the C++ exception machinery,
// e.g. when a rate limit is reached.
class result_message::exception : public result_message {
private:
    exceptions::coordinator_exception_container _ex;
public:
    [[noreturn]] void throw_me() const {
        _ex.throw_me();
    }
    const exceptions::coordinator_exception_container& get_exception() const & {
        return _ex;
    }
    exceptions::coordinator_exception_container&& get_exception() && {
        return std::move(_ex);
    }
    virtual bool is_exception() const override {
        return true;
    }
    virtual void throw_if_exception() const override {
        throw_me();
    }
};
class result_message::set_keyspace : public result_message {
private:
    sstring _keyspace;
public:
};
class result_message::prepared::cql : public result_message::prepared {
    bytes _id;
public:
};
class result_message::prepared::thrift : public result_message::prepared {
    int32_t _id;
public:
};
class result_message::schema_change : public result_message {
private:
    shared_ptr<event::schema_change> _change;
public:
};
class result_message::rows : public result_message {
private:
    cql3::result _result;
public:
};
template<typename ResultMessagePtr>
requires requires (ResultMessagePtr ptr) {
    { ptr.operator->() } -> std::convertible_to<result_message*>;
    { ptr.get() }        -> std::convertible_to<result_message*>;
}
inline future<ResultMessagePtr> propagate_exception_as_future(ResultMessagePtr&& ptr) {
    if (!ptr.get() || !ptr->is_exception()) {
        return make_ready_future<ResultMessagePtr>(std::move(ptr));
    }
    auto eptr = dynamic_cast<result_message::exception*>(ptr.get());
    return std::move(*eptr).get_exception().into_exception_future<ResultMessagePtr>();
}
}
}
namespace service {
class migration_manager;
class query_state;
class forward_service;
class raft_group0_client;
}
namespace cql3 {
namespace statements {
class batch_statement;
namespace raw {
class parsed_statement;
}
}
class untyped_result_set;
class untyped_result_set_row;
struct internal_query_state;
class prepared_statement_is_too_big : public std::exception {
    sstring _msg;
public:
    static constexpr int max_query_prefix = 100;
};
class cql_config;
class query_options;
class cql_statement;
class query_processor : public seastar::peering_sharded_service<query_processor> {
public:
    class migration_subscriber;
    struct memory_config {
        size_t prepared_statment_cache_size = 0;
        size_t authorized_prepared_cache_size = 0;
    };
private:
    std::unique_ptr<migration_subscriber> _migration_subscriber;
    service::storage_proxy& _proxy;
    service::forward_service& _forwarder;
    data_dictionary::database _db;
    service::migration_notifier& _mnotifier;
    service::migration_manager& _mm;
    memory_config _mcfg;
    const cql_config& _cql_config;
    service::raft_group0_client& _group0_client;
    struct stats {
        uint64_t prepare_invocations = 0;
        uint64_t queries_by_cl[size_t(db::consistency_level::MAX_VALUE) + 1] = {};
    } _stats;
    cql_stats _cql_stats;
    seastar::metrics::metric_groups _metrics;
    class internal_state;
    std::unique_ptr<internal_state> _internal_state;
    prepared_statements_cache _prepared_cache;
    authorized_prepared_statements_cache _authorized_prepared_cache;
    std::function<void(uint32_t)> _auth_prepared_cache_cfg_cb;
    serialized_action _authorized_prepared_cache_config_action;
    utils::observer<uint32_t> _authorized_prepared_cache_update_interval_in_ms_observer;
    utils::observer<uint32_t> _authorized_prepared_cache_validity_in_ms_observer;
    // A map for prepared statements used internally (which we don't want to mix with user statement, in particular we
    // don't bother with expiration on those.
    std::unordered_map<sstring, std::unique_ptr<statements::prepared_statement>> _internal_statements;
    std::shared_ptr<rust::Box<wasmtime::Engine>> _wasm_engine;
    std::optional<wasm::instance_cache> _wasm_instance_cache;
    std::shared_ptr<wasm::alien_thread_runner> _alien_runner;
public:
    static const sstring CQL_VERSION;
    static std::vector<std::unique_ptr<statements::raw::parsed_statement>> parse_statements(std::string_view queries);
    // Like execute_prepared, but is allowed to return exceptions as result_message::exception.
    // The result_message::exception must be explicitly handled.
    /// Execute a client statement that was not prepared.
    // Like execute_direct, but is allowed to return exceptions as result_message::exception.
    // The result_message::exception must be explicitly handled.
    class cache_internal_tag;
    using cache_internal = bool_class<cache_internal_tag>;
    // NOTICE: Internal queries should be used with care, as they are expected
    // to be used for local tables (e.g. from the `system` keyspace).
    // Data modifications will usually be performed with consistency level ONE
    // and schema changes will not be announced to other nodes.
    // Because of that, changing global schema state (e.g. modifying non-local tables,
    // creating namespaces, etc) is explicitly forbidden via this interface.
    //
    // note: optimized for convenience, not performance.
    // Like execute_batch, but is allowed to return exceptions as result_message::exception.
    // The result_message::exception must be explicitly handled.
    friend class migration_subscriber;
private:
    ///
    /// \tparam ResultMsgType type of the returned result message (CQL or Thrift)
    /// \tparam PreparedKeyGenerator a function that generates the prepared statement cache key for given query and
    ///         keyspace
    /// \tparam IdGetter a function that returns the corresponding prepared statement ID (CQL or Thrift) for a given
    ////        prepared statement cache key
    /// \param query_string
    /// \param client_state
    /// \param id_gen prepared ID generator, called before the first deferring
    /// \param id_getter prepared ID getter, passed to deferred context by reference. The caller must ensure its
    ////       liveness.
    /// \return
     ;;
};
class query_processor::migration_subscriber : public service::migration_listener {
    query_processor* _qp;
public:
private:
    bool should_invalidate(
            sstring ks_name,
            std::optional<sstring> cf_name,
            ::shared_ptr<cql_statement> statement);
};
}
void eventually(noncopyable_function<void ()> f, size_t max_attempts = 17) ;
bool eventually_true(noncopyable_function<bool ()> f) ;
#define REQUIRE_EVENTUALLY_EQUAL(a, b) BOOST_REQUIRE(eventually_true([&] { return a == b; }))
#define CHECK_EVENTUALLY_EQUAL(a, b) BOOST_CHECK(eventually_true([&] { return a == b; }))
namespace replica {
class database;
}
namespace db {
class batchlog_manager;
}
namespace db::view {
class view_builder;
class view_update_generator;
}
namespace auth {
class service;
}
namespace cql3 {
    class query_processor;
}
namespace service {
class client_state;
class migration_manager;
class raft_group0_client;
class raft_group_registry;
}
class not_prepared_exception : public std::runtime_error {
public:
    not_prepared_exception(const cql3::prepared_cache_key_type& id) : std::runtime_error(format("Not prepared: {}", id)) {}
};
namespace db {
    class config;
}
struct scheduling_groups {
    scheduling_group compaction_scheduling_group;
    scheduling_group memory_compaction_scheduling_group;
    scheduling_group streaming_scheduling_group;
    scheduling_group statement_scheduling_group;
    scheduling_group memtable_scheduling_group;
    scheduling_group memtable_to_cache_scheduling_group;
    scheduling_group gossip_scheduling_group;
};
// Creating and destroying scheduling groups on each env setup and teardown
// doesn't work because it messes up execution stages due to scheduling groups
// having the same name but not having the same id on each run. So they are
// created once and used across all envs. This method allows retrieving them to
// be used in tests.
// Not thread safe!
future<scheduling_groups> get_scheduling_groups();
class cql_test_config {
public:
    seastar::shared_ptr<db::config> db_config;
    // Scheduling groups are overwritten unconditionally, see get_scheduling_groups().
    std::optional<replica::database_config> dbcfg;
    std::set<sstring> disabled_features;
    std::optional<cql3::query_processor::memory_config> qp_mcfg;
};
struct cql_test_init_configurables {
    db::extensions& extensions;
};
class cql_test_env {
public:
    ;
    virtual future<::shared_ptr<cql_transport::messages::result_message>> execute_cql(sstring_view text) = 0;
    
    /// Processes queries (which must be modifying queries) as a batch.
    
    virtual future<cql3::prepared_cache_key_type> prepare(sstring query) = 0;
    virtual future<::shared_ptr<cql_transport::messages::result_message>> execute_prepared(
        cql3::prepared_cache_key_type id,
        cql3::raw_value_vector_with_unset values,
        db::consistency_level cl = db::consistency_level::ONE) = 0;
    
    virtual future<std::vector<mutation>> get_modification_mutations(const sstring& text) = 0;
    
    virtual future<> require_keyspace_exists(const sstring& ks_name) = 0;
    virtual future<> require_table_exists(const sstring& ks_name, const sstring& cf_name) = 0;
    
    
    
    
    virtual service::client_state& local_client_state() = 0;
    virtual replica::database& local_db() = 0;
    virtual cql3::query_processor& local_qp() = 0;
    virtual distributed<replica::database>& db() = 0;
    
    
    virtual db::view::view_builder& local_view_builder() = 0;
    
    
};
future<> do_with_cql_env(std::function<future<>(cql_test_env&)> func, cql_test_config = {}, std::optional<cql_test_init_configurables> = {});
future<> do_with_cql_env_thread(std::function<void(cql_test_env&)> func, cql_test_config = {}, thread_attributes thread_attr = {}, std::optional<cql_test_init_configurables> = {});

// CQL test config with raft experimental feature enabled

namespace tests::data_model {
static constexpr api::timestamp_type previously_removed_column_timestamp = 100;
static constexpr api::timestamp_type data_timestamp = 200;
static constexpr api::timestamp_type column_removal_timestamp = 300;
class mutation_description {
public:
    struct expiry_info {
        gc_clock::duration ttl;
        gc_clock::time_point expiry_point;
    };
    struct row_marker {
        api::timestamp_type timestamp;
        std::optional<expiry_info> expiring;
        row_marker(api::timestamp_type timestamp = data_timestamp);
        row_marker(api::timestamp_type timestamp, gc_clock::duration ttl, gc_clock::time_point expiry_point);
    };
    using key = std::vector<bytes>;
    struct atomic_value {
        bytes value;
        api::timestamp_type timestamp;
        std::optional<expiry_info> expiring;
        atomic_value(bytes value, api::timestamp_type timestamp = data_timestamp);
        atomic_value(bytes value, api::timestamp_type timestamp, gc_clock::duration ttl, gc_clock::time_point expiry_point);
    };
    struct collection_element {
        bytes key;
        atomic_value value;
    };
    struct collection {
        tombstone tomb;
        std::vector<collection_element> elements;
        collection() = default;
        collection(std::initializer_list<collection_element> elements);
        collection(std::vector<collection_element> elements);
    };
    using value = std::variant<atomic_value, collection>;
    struct cell {
        sstring column_name;
        value data_value;
    };
    using row = std::vector<cell>;
    struct clustered_row {
        row_marker marker;
        row_tombstone tomb;
        row cells;
    };
    struct range_tombstone {
        nonwrapping_range<key> range;
        tombstone tomb;
    };
private:
    key _partition_key;
    tombstone _partition_tombstone;
    row _static_row;
    std::map<key, clustered_row> _clustered_rows;
    std::vector<range_tombstone> _range_tombstones;
private:
    static void remove_column(row& r, const sstring& name);
public:
    explicit mutation_description(key partition_key);
    void set_partition_tombstone(tombstone partition_tombstone);
    void add_static_cell(const sstring& column, value v);
    void add_clustered_cell(const key& ck, const sstring& column, value v);
    void add_clustered_row_marker(const key& ck, row_marker marker = row_marker(data_timestamp));
    void add_clustered_row_tombstone(const key& ck, row_tombstone tomb);
    void remove_static_column(const sstring& name);
    void remove_regular_column(const sstring& name);
    // Both overloads accept out-of-order ranges and will make sure the
    // range-tombstone is created with start <= end.
    void add_range_tombstone(const key& start, const key& end,
            tombstone tomb = tombstone(previously_removed_column_timestamp, gc_clock::time_point()));
    void add_range_tombstone(nonwrapping_range<key> range,
            tombstone tomb = tombstone(previously_removed_column_timestamp, gc_clock::time_point()));
    mutation build(schema_ptr s) const;
};
class table_description {
public:
    using column = std::tuple<sstring, data_type>;
    struct removed_column {
        sstring name;
        data_type type;
        api::timestamp_type removal_timestamp;
    };
private:
    std::vector<column> _partition_key;
    std::vector<column> _clustering_key;
    std::vector<column> _static_columns;
    std::vector<column> _regular_columns;
    std::vector<removed_column> _removed_columns;
    std::vector<mutation_description> _mutations;
    std::vector<sstring> _change_log;
private:
    static std::vector<column>::iterator find_column(std::vector<column>& columns, const sstring& name);
    static void add_column(std::vector<column>& columns, const sstring& name, data_type type);
    void add_old_column(const sstring& name, data_type type);
    void remove_column(std::vector<column>& columns, const sstring& name);
    static void alter_column_type(std::vector<column>& columns, const sstring& name, data_type new_type);
    schema_ptr build_schema() const;
    std::vector<mutation> build_mutations(schema_ptr s) const;
public:
    explicit table_description(std::vector<column> partition_key, std::vector<column> clustering_key);
    void add_static_column(const sstring& name, data_type type);
    void add_regular_column(const sstring& name, data_type type);
    void add_old_static_column(const sstring& name, data_type type);
    void add_old_regular_column(const sstring& name, data_type type);
    void remove_static_column(const sstring& name);
    void remove_regular_column(const sstring& name);
    void alter_partition_column_type(const sstring& name, data_type new_type);
    void alter_clustering_column_type(const sstring& name, data_type new_type);
    void alter_static_column_type(const sstring& name, data_type new_type);
    void alter_regular_column_type(const sstring& name, data_type new_type);
    void rename_partition_column(const sstring& from, const sstring& to);
    void rename_clustering_column(const sstring& from, const sstring& to);
    std::vector<mutation_description>& unordered_mutations() ;
    const std::vector<mutation_description>& unordered_mutations() const ;
    struct table {
        sstring schema_changes_log;
        schema_ptr schema;
        std::vector<mutation> mutations;
    };
    table build() const;
};
}
namespace exception_predicate {
/// Makes an exception predicate that applies \p check function to verify the exception and \p err
/// function to create an error message if the check fails.
extern std::function<bool(const std::exception&)> make(
        std::function<bool(const std::exception&)> check,
        std::function<sstring(const std::exception&)> err);
/// Returns a predicate that will check if the exception message contains the given fragment.
extern std::function<bool(const std::exception&)> message_contains(
        const sstring& fragment,
        const std::source_location& loc = std::source_location::current());
/// Returns a predicate that will check if the exception message equals the given text.
extern std::function<bool(const std::exception&)> message_equals(
        const sstring& text,
        const std::source_location& loc = std::source_location::current());
/// Returns a predicate that will check if the exception message matches the given regular expression.
extern std::function<bool(const std::exception&)> message_matches(
        const std::string& regex,
        const std::source_location& loc = std::source_location::current());
} // namespace exception_predicate
// Represents a non-contiguous subset of clustering_key domain of a particular schema.
// Can be treated like an ordered and non-overlapping sequence of position_range:s.
class clustering_interval_set {
    // Needed to make position_in_partition comparable, required by boost::icl::interval_set.
    class position_in_partition_with_schema {
        schema_ptr _schema;
        position_in_partition _pos;
    public:
        position_in_partition_with_schema()
            : _pos(position_in_partition::for_static_row())
        { }
        
    };
private:
    // We want to represent intervals of clustering keys, not position_in_partitions,
    // but clustering_key domain is not enough to represent all kinds of clustering ranges.
    // All intervals in this set are of the form [x, y).
    using set_type = boost::icl::interval_set<position_in_partition_with_schema>;
    using interval = boost::icl::interval<position_in_partition_with_schema>;
    set_type _set;
public:
    // Constructs from legacy clustering_row_ranges
    
    class position_range_iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = const position_range;
        using difference_type = std::ptrdiff_t;
        using pointer = const position_range*;
        using reference = const position_range&;
    private:
        set_type::iterator _i;
    public:
        position_range_iterator(set_type::iterator i) : _i(i) {}
        position_range operator*() const ;
        bool operator==(const position_range_iterator& other) const = default;
        position_range_iterator& operator++() ;
    };
public:
    // Returns true iff this set is fully contained in the other set.
    
    // Adds given clustering range to this interval set.
    // The range may overlap with this set.
    
    void add(const schema& s, const clustering_interval_set& other) ;
    position_range_iterator begin() const ;
    position_range_iterator end() const ;
};
extern logging::logger testlog;
class mutation_partition_assertion {
    schema_ptr _schema;
    mutation_partition _m;
private:
public:
    // If ck_ranges is passed, verifies only that information relevant for ck_ranges matches.
    // If ck_ranges is passed, verifies only that information relevant for ck_ranges matches.
};
class mutation_assertion {
    mutation _m;
public:
    // If ck_ranges is passed, verifies only that information relevant for ck_ranges matches.
    // Verifies that mutation data remains unchanged when upgraded to the new schema
};
class mutation_opt_assertions {
    mutation_opt _mo;
public:
};
class validating_consumer {
    mutation_fragment_stream_validator _validator;
public:
};
// A test log to use in all unit tests, including boost unit
// tests. Built-in boost logging log levels do not allow to filter
// out unimportant messages, which then clutter xunit-format XML
// output, so are not used for anything profuse.
extern seastar::logger testlog;
// Intended to be called in a seastar thread
class flat_reader_assertions_v2 {
    flat_mutation_reader_v2 _reader;
    dht::partition_range _pr;
    bool _ignore_deletion_time = false;
    bool _exact = false; // Don't ignore irrelevant fragments
    tombstone _rt;
private:
public:
    struct expected_column {
        column_id id;
        const sstring& name;
        bytes value;
        
    };
    
    
    using assert_function = noncopyable_function<void(const column_definition&, const atomic_cell_or_collection*)>;
    
    flat_reader_assertions_v2& may_produce_tombstones(position_range range) ;
    
    
    flat_reader_assertions_v2& produces(const schema& s, const mutation_fragment_v2& mf) ;
    flat_reader_assertions_v2& produces_end_of_stream() ;
    
     ;
    
    
    
    flat_reader_assertions_v2& fast_forward_to(position_range pr) ;
    flat_reader_assertions_v2& fast_forward_to(const clustering_key& ck1, const clustering_key& ck2) ;
};
flat_reader_assertions_v2 assert_that(flat_mutation_reader_v2 r) ;
struct local_shard_only_tag { };
using local_shard_only = bool_class<local_shard_only_tag>;
namespace tests {
struct key_size {
    size_t min;
    size_t max;
};
// Generate n partition keys for the given schema.
//
// Returned keys are unique (their token too), ordered and never empty.
// Parameters:
// * n - number of keys
// * s - schema of the keys, used also to obtain the sharder
// * shard - only generate keys for this shard (if engaged)
// * size - the min and max size of the key in bytes, if disengaged default
//          limits (1-128) are used. If you want exactly sized keys, use
//          ascii or bytes types only as the key types.


// Overload for a single key


// Generate n clustering keys
//
// Returned keys are unique, ordered and never empty.
// Parameters are the same as that of generate_partition_keys().
// If allow_prefixes is true, prefix keys may be generated too.
// Overload for a single key
} // namespace tests
// Helper for working with the following table:
//
//   CREATE TABLE ks.cf (pk text, ck text, v text, s1 text static, PRIMARY KEY (pk, ck));
//
class simple_schema {
public:
    using with_static = bool_class<class static_tag>;
    using with_collection = bool_class<class collection_tag>;
private:
    friend class global_simple_schema;
    schema_ptr _s = nullptr;
    api::timestamp_type _timestamp = api::min_timestamp;
    const column_definition* _v_def = nullptr;
    table_schema_version _v_def_version;
    with_static _ws;
    with_collection _wc;
private:
public:
public:
    // Make a clustering_key which is n-th in some arbitrary sequence of keys
    // Make a partition key which is n-th in some arbitrary sequence of keys.
    // There is no particular order for the keys, they're not in ring order.
    // Creates a sequence of keys in ring order
    // Returns n clustering keys in their natural order
};
// Allows a simple_schema to be transferred to another shard.
// Must be used in `cql_test_env`.
class global_simple_schema {
    global_schema_ptr _gs;
    api::timestamp_type _timestamp;
public:
};
using populate_fn = std::function<mutation_source(schema_ptr s, const std::vector<mutation>&)>;
using populate_fn_ex = std::function<mutation_source(schema_ptr s, const std::vector<mutation>&, gc_clock::time_point)>;
// Must be run in a seastar thread
enum are_equal { no, yes };
// Calls the provided function on mutation pairs, equal and not equal. Is supposed
// to exercise all potential ways two mutations may differ.
void for_each_mutation_pair(std::function<void(const mutation&, const mutation&, are_equal)>);
// Calls the provided function on mutations. Is supposed to exercise as many differences as possible.
void for_each_mutation(std::function<void(const mutation&)>);
// Returns true if mutations in schema s1 can be upgraded to s2.
// Merge mutations that have the same key.
// The returned vector has mutations with unique keys.
// run_mutation_source_tests() might pass in multiple mutations for the same key.
// Some tests need these deduplicated, which is what this method does.
std::vector<mutation> squash_mutations(std::vector<mutation> mutations);
class random_mutation_generator {
    class impl;
    std::unique_ptr<impl> _impl;
public:
    struct generate_counters_tag { };
    using generate_counters = bool_class<generate_counters_tag>;
    using generate_uncompactable = bool_class<class generate_uncompactable_tag>;
    // With generate_uncompactable::yes, the mutation will be uncompactable, that
    // is no higher level tombstone will cover lower level tombstones and no
    // tombstone will cover data, i.e. compacting the mutation will not result
    // in any changes.
    // Generates n mutations sharing the same schema nad sorted by their decorated keys.
    // Sets the number of distinct clustering keys which will be used in generated mutations.
};
void for_each_schema_change(std::function<void(schema_ptr, const std::vector<mutation>&,
                                               schema_ptr, const std::vector<mutation>&)>);
void compare_readers(const schema&, flat_mutation_reader_v2 authority, flat_mutation_reader_v2 tested, const std::vector<position_range>& fwd_ranges);
// Forward `r` to each range in `fwd_ranges` and consume all fragments produced by `r` in these ranges.
// Build a mutation out of these fragments.
//
// Assumes that for each subsequent `r1`, `r2` in `fwd_ranges`, `r1.end() <= r2.start()`.
// Must be run in a seastar::thread.
mutation forwardable_reader_to_mutation(flat_mutation_reader_v2 r, const std::vector<position_range>& fwd_ranges);
///
/// Random schema and random data generation related utilities.
///
class cql_test_env;
namespace tests {
class random_schema_specification {
    sstring _keyspace_name;
public:
    explicit random_schema_specification(sstring keyspace_name) : _keyspace_name(std::move(keyspace_name)) { }
    virtual ~random_schema_specification() = default;
    // Should be the same for all invocations
    const sstring& keyspace_name() const { return _keyspace_name; }
    // Should be unique on the instance level.
    virtual sstring table_name(std::mt19937& engine) = 0;
    // Should be unique on the instance level.
    virtual sstring udt_name(std::mt19937& engine) = 0;
    virtual std::vector<data_type> partition_key_columns(std::mt19937& engine) = 0;
    virtual std::vector<data_type> clustering_key_columns(std::mt19937& engine) = 0;
    virtual std::vector<data_type> regular_columns(std::mt19937& engine) = 0;
    virtual std::vector<data_type> static_columns(std::mt19937& engine) = 0;
};
/// Helper class that can generate a subset of all valid combination of types.
///
/// Can be used to implement concrete random schema specifications.
/// TODO: counters
class type_generator {
public:
    using is_multi_cell = bool_class<class is_multi_cell_tag>;
private:
    using generator = std::function<data_type(std::mt19937&, is_multi_cell)>;
private:
    random_schema_specification& _spec;
    std::vector<generator> _generators;
public:
    explicit type_generator(random_schema_specification& spec);
    // This is captured.
    type_generator(type_generator&&) = delete;
    data_type operator()(std::mt19937& engine, is_multi_cell multi_cell);
};
/// The default random schema specification.
///
/// Warning: reusing the same keyspace_name across specs can lead to user
/// defined type clashes.
std::unique_ptr<random_schema_specification> make_random_schema_specification(
        sstring keyspace_name,
        std::uniform_int_distribution<size_t> partition_column_count_dist = std::uniform_int_distribution<size_t>(1, 4),
        std::uniform_int_distribution<size_t> clustering_column_count_dist = std::uniform_int_distribution<size_t>(0, 4),
        std::uniform_int_distribution<size_t> regular_column_count_dist = std::uniform_int_distribution<size_t>(1, 4),
        std::uniform_int_distribution<size_t> static_column_count_dist = std::uniform_int_distribution<size_t>(0, 4));
/// Generate values for any type.
///
/// Values sizes:
/// * string types (ascii, utf8, bytes):
///     - 95.0% [   0,    32) characters.
///     -  4.5% [  32,   100) characters.
///     -  0.4% [ 100,  1000) characters.
///     -  0.1% [1000, 10000) characters.
/// * collections: max 16 elements.
/// * frozen collections: max 4 elements.
/// For native types, the intent is to cover the entire value range.
/// TODO: counters
class value_generator {
public:
    using atomic_value_generator = std::function<data_value(std::mt19937&, size_t, size_t)>;
    using generator = std::function<data_model::mutation_description::value(std::mt19937&)>;
    static const size_t no_size_in_bytes_limit{std::numeric_limits<size_t>::max()};
private:
    std::unordered_map<const abstract_type*, atomic_value_generator> _regular_value_generators;
    std::unordered_map<const abstract_type*, size_t> _regular_value_min_sizes;
public:
    value_generator();
    value_generator(value_generator&&) = delete;
    /// Only for atomic types.
    size_t min_size(const abstract_type& type);
    atomic_value_generator get_atomic_value_generator(const abstract_type& type);
    // Generate a value for the given type, according to the provided size constraints.
    // Controlling the size of values only really works with string-like types and collections of these.
    data_value generate_atomic_value(std::mt19937& engine, const abstract_type& type, size_t max_size_in_bytes = no_size_in_bytes_limit);
    data_value generate_atomic_value(std::mt19937& engine, const abstract_type& type, size_t min_size_in_bytes, size_t max_size_in_bytes);
    generator get_generator(const abstract_type& type);
    data_model::mutation_description::value generate_value(std::mt19937& engine, const abstract_type& type);
};
enum class timestamp_destination {
    partition_tombstone,
    row_marker,
    cell_timestamp,
    collection_cell_timestamp,
    row_tombstone,
    collection_tombstone,
    range_tombstone,
};
/// Functor that generates timestamps for various destinations.
using timestamp_generator = std::function<api::timestamp_type(std::mt19937& engine, timestamp_destination destination,
        api::timestamp_type min_timestamp)>;
/// The default timestamp generator.
///
/// Generates fully random timestamps in the range:
///     [api::min_timestamp, api::max_timestamp]
/// Ignores timestamp destination.
timestamp_generator default_timestamp_generator();
struct expiry_info {
    gc_clock::duration ttl;
    gc_clock::time_point expiry_point;
};
/// Functor that generates expiry for various destinations.
/// A disengaged optional means the cell doesn't expire. When the destination is
/// a tombstone, gc_clock::now() + schema::gc_grace_seconds() will be used as
/// the expiry instead.
/// The `expiry_info::ttl` is always ignored for tombstone destinations (because
/// they have a fixed ttl as determined by `schema::gc_grace_seconds()`.
using expiry_generator = std::function<std::optional<expiry_info>(std::mt19937& engine, timestamp_destination destination)>;
/// Always returns disengaged optionals.
expiry_generator no_expiry_expiry_generator();
/// Utility class wrapping a randomly generated schema.
///
/// The schema is generated when the class is constructed.
/// The generation is deterministic, the same seed will generate the same schema.
class random_schema {
    schema_ptr _schema;
private:
    static data_model::mutation_description::key make_key(uint32_t n, value_generator& gen, schema::const_iterator_range_type columns,
            size_t max_size_in_bytes);
    data_model::mutation_description::key make_partition_key(uint32_t n, value_generator& gen) const;
    data_model::mutation_description::key make_clustering_key(uint32_t n, value_generator& gen) const;
public:
    /// Create a random schema.
    ///
    /// Passing the same seed and spec will yield the same schema. Part of this
    /// guarantee rests on the spec, which, if a custom one is used, should
    /// make sure to honor this guarantee.
    random_schema(uint32_t seed, random_schema_specification& spec);
    schema_ptr schema() const {
        return _schema;
    }
    sstring cql() const;
    /// Create the generated schema as a table via CQL.
    ///
    /// Along with all its dependencies, like UDTs.
    /// The underlying schema_ptr instance is replaced with the one from the
    /// local table instance.
    future<> create_with_cql(cql_test_env& env);
    /// Make a partition key which is n-th in some arbitrary sequence of keys.
    ///
    /// There is no particular order for the keys, they're not in ring order.
    /// This method is deterministic, the pair of the seed used to generate the
    /// schema and `n` will map to the same generated value.
    data_model::mutation_description::key make_pkey(uint32_t n);
    /// Make n partition keys.
    ///
    /// Keys are in ring order.
    /// This method is deterministic, the pair of the seed used to generate the
    /// schema and `n` will map to the same generated values.
    std::vector<data_model::mutation_description::key> make_pkeys(size_t n);
    /// Make a clustering key which is n-th in some arbitrary sequence of keys.
    ///
    /// There is no particular order for the keys, they're not in clustering order.
    /// This method is deterministic, the pair of the seed used to generate the
    /// schema and `n` will map to the same generated value.
    data_model::mutation_description::key make_ckey(uint32_t n);
    /// Make up to n clustering keys.
    ///
    /// Key are in clustering order.
    /// This method is deterministic, the pair of the seed used to generate the
    /// schema and `n` will map to the same generated values.
    /// Fewer than n keys may be returned if the schema limits the clustering keys space.
    std::vector<data_model::mutation_description::key> make_ckeys(size_t n);
    data_model::mutation_description new_mutation(data_model::mutation_description::key pkey);
    /// Make a new mutation with a key produced via `make_pkey(n)`.
    data_model::mutation_description new_mutation(uint32_t n);
    /// Set the partition tombstone
    void set_partition_tombstone(std::mt19937& engine, data_model::mutation_description& md,
            timestamp_generator ts_gen = default_timestamp_generator(),
            expiry_generator exp_gen = no_expiry_expiry_generator());
    void add_row(std::mt19937& engine, data_model::mutation_description& md, data_model::mutation_description::key ckey,
            timestamp_generator ts_gen = default_timestamp_generator(),
            expiry_generator exp_gen = no_expiry_expiry_generator());
    /// Add a new row with a key produced via `make_ckey(n)`.
    void add_row(std::mt19937& engine, data_model::mutation_description& md, uint32_t n, timestamp_generator ts_gen = default_timestamp_generator(),
            expiry_generator exp_gen = no_expiry_expiry_generator());
    void add_static_row(std::mt19937& engine, data_model::mutation_description& md, timestamp_generator ts_gen = default_timestamp_generator(),
            expiry_generator exp_gen = no_expiry_expiry_generator());
    void delete_range(
            std::mt19937& engine,
            data_model::mutation_description& md,
            nonwrapping_range<data_model::mutation_description::key> range,
            timestamp_generator ts_gen = default_timestamp_generator(),
            expiry_generator exp_gen = no_expiry_expiry_generator());
};
/// Generate random mutations using the random schema.
///
/// `clustering_row_count_dist` and `range_tombstone_count_dist` will be used to
/// generate the respective counts for *each* partition. These params are
/// ignored if the schema has no clustering columns.
/// Mutations are returned in ring order. Does not contain duplicate partitions.
/// Futurized to avoid stalls.
future<std::vector<mutation>> generate_random_mutations(
        uint32_t seed,
        tests::random_schema& random_schema,
        timestamp_generator ts_gen = default_timestamp_generator(),
        expiry_generator exp_gen = no_expiry_expiry_generator(),
        std::uniform_int_distribution<size_t> partition_count_dist = std::uniform_int_distribution<size_t>(8, 16),
        std::uniform_int_distribution<size_t> clustering_row_count_dist = std::uniform_int_distribution<size_t>(16, 128),
        std::uniform_int_distribution<size_t> range_tombstone_count_dist = std::uniform_int_distribution<size_t>(4, 16));
future<std::vector<mutation>> generate_random_mutations(
        tests::random_schema& random_schema,
        timestamp_generator ts_gen = default_timestamp_generator(),
        expiry_generator exp_gen = no_expiry_expiry_generator(),
        std::uniform_int_distribution<size_t> partition_count_dist = std::uniform_int_distribution<size_t>(8, 16),
        std::uniform_int_distribution<size_t> clustering_row_count_dist = std::uniform_int_distribution<size_t>(16, 128),
        std::uniform_int_distribution<size_t> range_tombstone_count_dist = std::uniform_int_distribution<size_t>(4, 16));
/// Generate exactly partition_count partitions. See the more general overload above.
future<std::vector<mutation>> generate_random_mutations(tests::random_schema& random_schema, size_t partition_count);
} // namespace tests
namespace tests::random {
 std::default_random_engine& gen() ;
/// Produces random integers from a set of steps.
///
/// Each step has a weight and a uniform distribution that determines the range
/// of values for that step. The probability of the generated number to be from
/// any given step is Ws/Wt, where Ws is the weight of the step and Wt is the
/// sum of the weight of all steps.
template <typename Integer>
class stepped_int_distribution {
public:
    struct step {
        double weight;
        std::pair<Integer, Integer> range;
    };
private:
    std::discrete_distribution<Integer> _step_index_dist;
    std::vector<std::uniform_int_distribution<Integer>> _step_ranges;
public:
    explicit stepped_int_distribution(std::initializer_list<step> steps) {
        std::vector<double> step_weights;
        for (auto& s : steps) {
            step_weights.push_back(s.weight);
            _step_ranges.emplace_back(s.range.first, s.range.second);
        }
        _step_index_dist = std::discrete_distribution<Integer>{step_weights.begin(), step_weights.end()};
    }
    template <typename RandomEngine>
    Integer operator()(RandomEngine& engine) {
        return _step_ranges[_step_index_dist(engine)](engine);
    }
};
template<typename T, typename RandomEngine>
T get_int(T min, T max, RandomEngine& engine) {
    std::uniform_int_distribution<T> dist(min, max);
    return dist(engine);
}
template<typename T, typename RandomEngine>
T get_int(T max, RandomEngine& engine) {
    return get_int(T{0}, max, engine);
}
template<typename T, typename RandomEngine>
T get_int(RandomEngine& engine) {
    return get_int(T{0}, std::numeric_limits<T>::max(), engine);
}
template<typename T>
T get_int() ;
template<typename T>
T get_int(T max) ;
template<typename T>
T get_int(T min, T max) ;
template <typename Real, typename RandomEngine>
Real get_real(Real min, Real max, RandomEngine& engine) {
    auto dist = std::uniform_real_distribution<Real>(min, max);
    return dist(engine);
}
 ;
/// Returns true with probability p.
/// p = 1.0 means 100%.

template <typename Real, typename RandomEngine>
Real get_real(RandomEngine& engine) {
    return get_real<Real>(Real{0}, std::numeric_limits<Real>::max(), engine);
}
template <typename Real>
Real get_real(Real min, Real max) ;
template <typename Real>
Real get_real(Real max) ;
template <typename Real>
Real get_real() ;
 ;
 
 bytes get_bytes(size_t n) ;
 bytes get_bytes() ;
 ;
 
 sstring get_sstring() ;
// Picks a random subset of size `m` from the given vector.
template <typename T>
std::vector<T> random_subset(std::vector<T> v, unsigned m, std::mt19937& engine) {
    assert(m <= v.size());
    std::shuffle(v.begin(), v.end(), engine);
    return {v.begin(), v.begin() + m};
}
// Picks a random subset of size `m` from the set {0, ..., `n` - 1}.
template<typename T>
std::vector<T> random_subset(unsigned n, unsigned m, std::mt19937& engine) {
    assert(m <= n);
    std::vector<T> the_set(n);
    std::iota(the_set.begin(), the_set.end(), T{});
    return random_subset(std::move(the_set), m, engine);
}
}
namespace tests {
// Must be used in a seastar thread.
class reader_concurrency_semaphore_wrapper {
    std::unique_ptr<::reader_concurrency_semaphore> _semaphore;
public:
    ;
};
} // namespace tests
//
// Contains assertions for query::result_set objects
//
// Example use:
//
//  assert_that(rs)
//     .has(a_row().with_column("column_name", "value"));
//
class row_assertion {
    std::map<bytes, data_value> _expected_values;
    bool _only_that = false;
public:
private:
    friend class result_set_assertions;
};
class result_set_assertions {
    const query::result_set& _rs;
public:
};
// Make rs live as long as the returned assertion object is used
result_set_assertions assert_that(const query::result_set& rs) ;
class scylla_tests_cmdline_options_processor {
private:
    int _new_argc = 0;
    char** _new_argv = nullptr;
public:
    scylla_tests_cmdline_options_processor() = default;
    // Returns new argv if compaction group option was processed.
    std::pair<int, char**> process_cmdline_options(int argc, char** argv) { return { argc, argv }; }
};
int main(int argc, char** argv) {
    scylla_tests_cmdline_options_processor processor;
    auto [new_argc, new_argv] = processor.process_cmdline_options(argc, argv);
    return seastar::testing::entry_point(new_argc, new_argv);
}
// Thread safe alternatives to BOOST_REQUIRE_*, BOOST_CHECK_* and BOOST_FAIL().
// Use these if instead of the BOOST provided macros if you want to use them on
// multiple shards, to avoid problems due to the BOOST versions not being thread
// safe.
namespace tests {
[[nodiscard]] bool do_check(bool condition, std::source_location sl, std::string_view msg);
[[nodiscard]] inline bool check(bool condition, std::source_location sl = std::source_location::current()) {
    return do_check(condition, sl, {});
}
template <typename LHS, typename RHS>
[[nodiscard]] bool check_equal(const LHS& lhs, const RHS& rhs, std::source_location sl = std::source_location::current()) {
    const auto condition = (lhs == rhs);
    return do_check(condition, sl, fmt::format("{} {}= {}", lhs, condition ? "=" : "!", rhs));
}
void do_require(bool condition, std::source_location sl, std::string_view msg);
 
 ;

 

}



template <FragmentedView View>
int read_collection_size(View& in) {
    return read_simple<int32_t>(in);
}
template <FragmentedView View>
View read_collection_key(View& in) {
    auto size = read_simple<int32_t>(in);
    if (size == -2) {
        throw exceptions::invalid_request_exception("unset value is not supported inside collections");
    }
    if (size < 0) {
        throw exceptions::invalid_request_exception("null is not supported inside collections");
    }
    return read_simple_bytes(in, size);
}
template <FragmentedView View>
std::optional<View> read_collection_value(View& in) {
    auto size = read_simple<int32_t>(in);
    if (size == -1) {
        return std::nullopt;
    }
    if (size < 0) {
        throw exceptions::invalid_request_exception("unset value is not supported inside collections");
    }
    return read_simple_bytes(in, size);
}
template <FragmentedView View>
View read_collection_value_nonnull(View& in) {
    auto size = read_simple<int32_t>(in);
    if (size == -2) {
        throw exceptions::invalid_request_exception("unset value is not supported inside collections");
    }
    if (size < 0) {
        throw exceptions::invalid_request_exception("null is not supported inside collections");
    }
    return read_simple_bytes(in, size);
}
// iterator that takes a set or list in serialized form, and emits
// each element, still in serialized form
class listlike_partial_deserializing_iterator {
public:
    using iterator_category = std::input_iterator_tag;
    using value_type = managed_bytes_view;
    using difference_type = std::ptrdiff_t;
    using pointer = managed_bytes_view_opt*;
    using reference = managed_bytes_view_opt&;
private:
    managed_bytes_view* _in;
    int _remain;
    managed_bytes_view_opt _cur;
private:
    struct end_tag {};
    listlike_partial_deserializing_iterator(managed_bytes_view& in)
            : _in(&in) {
        _remain = read_collection_size(*_in);
        parse();
    }
    listlike_partial_deserializing_iterator(end_tag)
            : _remain(0) {
    }
public:
    managed_bytes_view_opt operator*() const { return _cur; }
    listlike_partial_deserializing_iterator& operator++() {
        --_remain;
        parse();
        return *this;
    }
    void operator++(int) ;
    bool operator==(const listlike_partial_deserializing_iterator& x) const {
        return _remain == x._remain;
    }
    static listlike_partial_deserializing_iterator begin(managed_bytes_view& in) {
        return { in };
    }
    static listlike_partial_deserializing_iterator end(managed_bytes_view in) {
        return { end_tag() };
    }
private:
    void parse() {
        if (_remain) {
            _cur = read_collection_value(*_in);
        } else {
            _cur = {};
        }
    }
};
// This file defines structures/functions derived from the Itanium C++ ABI.
// Source: https://itanium-cxx-abi.github.io/cxx-abi/abi-eh.html
namespace utils {
namespace abi {
// __cxa_exception (exception object header), as defined in section 2.2.1
struct cxa_exception { 
    std::type_info* exceptionType;
    void (*exceptionDestructor)(void*); 
    void (*unexpectedHandler)();
    std::terminate_handler terminateHandler;
    cxa_exception* nextException;
    int handlerCount;
    int	handlerSwitchValue;
    const char* actionRecord;
    const char* languageSpecificData;
    void* catchTemp;
    void* adjustedPtr;
    _Unwind_Exception unwindHeader;
};
// Given a pointer to the exception data, returns the pointer
// to the __cxa_exception header.
 cxa_exception* get_cxa_exception(void* eptr) ;
} // abi
} // utils
/// Represents a container which can preallocate space for future insertions
/// which can be used to reduce the number of overall memory re-allocation and item movement.
///
/// The number of items for which space is currently reserved is returned by capacity().
/// This includes items currently present in the container.
///
/// The number of items currently present is returned by size().
///
/// Invariant:
///
///   size() <= capacity()
///
/// Space is reserved by calling reserve(desired_capacity).
/// The post-condition of calling reserve() is:
///
///   capacity() >= desired_capacity
///
/// It is guaranteed insertion of (capacity() - size()) items does not
/// throw if T::value_type constructor and move constructor do not throw.
template <typename T>
concept ContainerWithCapacity = requires (T x, size_t desired_capacity, typename T::value_type e) {
    { x.reserve(desired_capacity) } -> std::same_as<void>;
    { x.capacity() } -> std::same_as<size_t>;
    { x.size() } -> std::same_as<size_t>;
};
static_assert(ContainerWithCapacity<std::vector<int>>);
/// Reserves space for at least desired_capacity - v.size() elements.
///
/// Amortizes space expansion so that a series of N calls to amortized_reserve(v, v.size() + 1)
/// starting from an empty container takes O(N) time overall.
///
/// Post-condition: v.capacity() >= desired_capacity
template <ContainerWithCapacity T>
void amortized_reserve(T& v, size_t desired_capacity) {
    if (desired_capacity > v.capacity()) {
        v.reserve(std::max(desired_capacity, v.capacity() * 2));
    }
}
namespace utils {
namespace ascii {
bool validate(const uint8_t *data, size_t len);
 
} // namespace ascii
} // namespace utils
// The declared below get_signature() method makes the Signature string for AWS
// authenticated requests as described in [1]. It can be used in two ways.
//
// First, if a request is about to be sent, the method can be used to create the
// signature value that'll later be included into Authorization header, Signature
// part. It's up to the caller to provide request with relevant headers and the
// signed_headers_map list.
//
// Second, for a received request this method can be used to calculate the signature
// that can later be compared with the request's Authorization header, Signature
// part for correctness.
//
// [1] https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-auth-using-authorization-header.html
namespace utils {
using hmac_sha256_digest = std::array<char, 32>;
namespace aws {
std::string get_signature(std::string_view access_key_id, std::string_view secret_access_key,
        std::string_view host, std::string_view canonical_uri, std::string_view method,
        std::optional<std::string_view> orig_datestamp, std::string_view signed_headers_str, const std::map<std::string_view, std::string_view>& signed_headers_map,
        const std::vector<temporary_buffer<char>>* body_content, std::string_view region, std::string_view service, std::string_view query_string);
// Convenience alias not to pass obscure nullptr argument to get_signature()
static inline constexpr std::vector<temporary_buffer<char>>* unsigned_content = nullptr;
// Same for datestamp checking
static inline auto omit_datestamp_expiration_check = std::nullopt;

} // aws namespace
} // utils namespace
// A coarser and faster version of std::steady_clock, using
// CLOCK_MONOTONIC_COARSE instead of CLOCK_MONOTONIC.
//
// Intended for measuring time taken by synchronous code paths (where
// seastar::lowres_clock is not suitable).
namespace utils {
struct coarse_steady_clock {
    using duration   = std::chrono::nanoseconds;
    using rep        = duration::rep;
    using period     = duration::period;
    using time_point = std::chrono::time_point<coarse_steady_clock, duration>;
    static constexpr bool is_steady = true;
    static time_point now() noexcept {
        timespec tp;
        clock_gettime(CLOCK_MONOTONIC_COARSE, &tp);
        return time_point(std::chrono::seconds(tp.tv_sec) + std::chrono::nanoseconds(tp.tv_nsec));
    };
    static duration get_resolution() noexcept {
        timespec tp;
        clock_getres(CLOCK_MONOTONIC_COARSE, &tp);
        return std::chrono::seconds(tp.tv_sec) + std::chrono::nanoseconds(tp.tv_nsec);
    }
};
};
class data_input {
public:
    data_input(const bytes_view& v)
            : _view(v) {
    }
    data_input(bytes_view&& v)
            : _view(std::move(v)) {
    }
    
    
    
    
    
    
    
    
    void ensure(size_t s) const ;
    template<typename T> T peek() const;
    template<typename T> T read();
    bytes_view read_view(size_t len) ;
     ;
    
private:
    template<typename T> size_t ssize(const T &) const;
    template<std::integral T>
     T peek_primitive() const ;
    bytes_view _view;
};
template<> inline sstring data_input::peek<sstring>() const {
    auto len = peek<uint16_t>();
    ensure(sizeof(uint16_t) + len);
    return sstring(reinterpret_cast<const char*>(_view.data()) + sizeof(uint16_t), len);
}
template<> inline size_t data_input::ssize<sstring>(const sstring & s) const {
    return sizeof(uint16_t) + s.size();
}
template<> inline bytes data_input::peek<bytes>() const {
    auto len = peek<uint32_t>();
    ensure(sizeof(uint32_t) + len);
    return bytes(_view.data() + sizeof(uint32_t), len);
}
template<> inline size_t data_input::ssize<bytes>(const bytes & s) const {
    return sizeof(uint32_t) + s.size();
}
template<> inline bytes_view data_input::peek<bytes_view>() const {
    auto len = peek<uint32_t>();
    ensure(sizeof(uint32_t) + len);
    return bytes_view(_view.data() + sizeof(uint32_t), len);
}
template<> inline size_t data_input::ssize<bytes_view>(const bytes_view& v) const {
    return sizeof(uint32_t) + v.size();
}
template<> inline size_t data_input::ssize(const bool &) const {
    return sizeof(uint8_t);
}
template<> inline bool data_input::peek<bool>() const {
    return peek<uint8_t>() != 0;
}
template<typename T>
inline T data_input::peek() const {
    return peek_primitive<T>();
}
template<typename T> inline T data_input::read() {
    auto t = peek<T>();
    _view.remove_prefix(ssize(t));
    return std::move(t);
}
template<typename T> inline size_t data_input::ssize(const T &) const {
    return sizeof(T);
}
class tombstone_gc_extension : public schema_extension {
    tombstone_gc_options _tombstone_gc_options;
public:
    static constexpr auto NAME = "tombstone_gc";
    tombstone_gc_extension() = default;
    tombstone_gc_extension(const tombstone_gc_options& opts) : _tombstone_gc_options(opts) {}
    explicit tombstone_gc_extension(std::map<seastar::sstring, seastar::sstring> tags) : _tombstone_gc_options(std::move(tags)) {}
    explicit tombstone_gc_extension(const bytes& b) : _tombstone_gc_options(tombstone_gc_extension::deserialize(b)) {}
    explicit tombstone_gc_extension(const seastar::sstring& s) {
        throw std::logic_error("Cannot create tombstone_gc_extension info from string");
    }
    bytes serialize() const override {
        return ser::serialize_to_buffer<bytes>(_tombstone_gc_options.to_map());
    }
    static std::map<seastar::sstring, seastar::sstring> deserialize(const bytes_view& buffer) {
        return ser::deserialize_from_buffer(buffer, boost::type<std::map<seastar::sstring, seastar::sstring>>());
    }
    const tombstone_gc_options& get_options() const {
        return _tombstone_gc_options;
    }
};
#ifndef DATE_H
#define DATE_H
// The MIT License (MIT)
//
// Copyright (c) 2015, 2016 Howard Hinnant
// Copyright (c) 2016 Adrian Colomitchi
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
//
// Our apologies.  When the previous paragraph was written, lowercase had not yet
// been invented (that woud involve another several millennia of evolution).
// We did not mean to shout.
#if !(__cplusplus >= 201402)
#endif
namespace date
{
//---------------+
// Configuration |
//---------------+
// MSVC's constexpr support is still a WIP, even in VS2015.
// Fall back to a lesser mode to support it.
// TODO: Remove this or retest later once MSVC's constexpr improves.
#if defined(_MSC_VER) && _MSC_VER <= 1900 && ! defined(__clang__)
// MS cl compiler pre VS2017
#  define CONSTDATA const
#  define CONSTCD11
#  define CONSTCD14
#  define NOEXCEPT _NOEXCEPT
#elif __cplusplus >= 201402
// C++14
#  define CONSTDATA constexpr const
#  define CONSTCD11 constexpr
#  define CONSTCD14 constexpr
#  define NOEXCEPT noexcept
#else
// C++11
#  define CONSTDATA constexpr const
#  define CONSTCD11 constexpr
#  define CONSTCD14
#  define NOEXCEPT noexcept
#endif
//-----------+
// Interface |
//-----------+
// durations
using days = std::chrono::duration
    <int64_t, std::ratio_multiply<std::ratio<24>, std::chrono::hours::period>>;
using weeks = std::chrono::duration
    <int, std::ratio_multiply<std::ratio<7>, days::period>>;
using years = std::chrono::duration
    <int, std::ratio_multiply<std::ratio<146097, 400>, days::period>>;
using months = std::chrono::duration
    <int, std::ratio_divide<years::period, std::ratio<12>>>;
// time_point
template <class Duration>
    using sys_time = std::chrono::time_point<std::chrono::system_clock, Duration>;
using sys_days    = sys_time<days>;
using sys_seconds = sys_time<std::chrono::seconds>;
struct local_t {};
template <class Duration>
    using local_time = std::chrono::time_point<local_t, Duration>;
using local_seconds = local_time<std::chrono::seconds>;
using local_days    = local_time<days>;
// types
struct last_spec
{
    explicit last_spec() = default;
};
class day;
class month;
class year;
class weekday;
class weekday_indexed;
class weekday_last;
class month_day;
class month_day_last;
class month_weekday;
class month_weekday_last;
class year_month;
class year_month_day;
class year_month_day_last;
class year_month_weekday;
class year_month_weekday_last;
// date composition operators
CONSTCD11 year_month operator/(const year& y, const month& m) NOEXCEPT;
CONSTCD11 year_month operator/(const year& y, int          m) NOEXCEPT;
CONSTCD11 month_day operator/(const day& d, const month& m) NOEXCEPT;
CONSTCD11 month_weekday operator/(const weekday_indexed& wdi, const month& m) NOEXCEPT;
CONSTCD11 year_month_day operator/(const year_month& ym, const day& d) NOEXCEPT;
CONSTCD11 year_month_day operator/(const year_month& ym, int        d) NOEXCEPT;




CONSTCD11
    year_month_day_last operator/(const year_month& ym,   last_spec) NOEXCEPT;




CONSTCD11
year_month_weekday
operator/(const year_month& ym, const weekday_indexed& wdi) NOEXCEPT;
CONSTCD11
year_month_weekday
operator/(const year&        y, const month_weekday&   mwd) NOEXCEPT;
CONSTCD11
year_month_weekday
operator/(int                y, const month_weekday&   mwd) NOEXCEPT;
CONSTCD11
year_month_weekday
operator/(const month_weekday& mwd, const year&          y) NOEXCEPT;
CONSTCD11
year_month_weekday
operator/(const month_weekday& mwd, int                  y) NOEXCEPT;
CONSTCD11
year_month_weekday_last
operator/(const year_month& ym, const weekday_last& wdl) NOEXCEPT;
CONSTCD11
year_month_weekday_last
operator/(const year& y, const month_weekday_last& mwdl) NOEXCEPT;



// Detailed interface
// day
class day
{
    unsigned char d_;
public:
    
    explicit CONSTCD11 day(unsigned d) NOEXCEPT;
    CONSTCD14 day& operator++()    NOEXCEPT;
    CONSTCD14 day  operator++(int) NOEXCEPT;
    CONSTCD14 day& operator--()    NOEXCEPT;
    CONSTCD14 day  operator--(int) NOEXCEPT;
    CONSTCD14 day& operator+=(const days& d) NOEXCEPT;
    CONSTCD14 day& operator-=(const days& d) NOEXCEPT;
    CONSTCD11 explicit operator unsigned() const NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
};






CONSTCD11 day  operator+(const day&  x, const days& y) NOEXCEPT;
CONSTCD11 day  operator+(const days& x, const day&  y) NOEXCEPT;
CONSTCD11 day  operator-(const day&  x, const days& y) NOEXCEPT;
CONSTCD11 days operator-(const day&  x, const day&  y) NOEXCEPT;
;
// month
class month
{
    unsigned char m_;
public:
    
    explicit CONSTCD11 month(unsigned m) NOEXCEPT;
    CONSTCD14 month& operator++()    NOEXCEPT;
    CONSTCD14 month  operator++(int) NOEXCEPT;
    CONSTCD14 month& operator--()    NOEXCEPT;
    CONSTCD14 month  operator--(int) NOEXCEPT;
    CONSTCD14 month& operator+=(const months& m) NOEXCEPT;
    CONSTCD14 month& operator-=(const months& m) NOEXCEPT;
    CONSTCD11 explicit operator unsigned() const NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
};
CONSTCD11 bool operator==(const month& x, const month& y) NOEXCEPT;
CONSTCD11 bool operator!=(const month& x, const month& y) NOEXCEPT;




CONSTCD14 month  operator+(const month&  x, const months& y) NOEXCEPT;
CONSTCD14 month  operator+(const months& x,  const month& y) NOEXCEPT;
CONSTCD14 month  operator-(const month&  x, const months& y) NOEXCEPT;

;
// year
class year
{
    int64_t y_;
public:
    year() = default;
    explicit CONSTCD11 year(int64_t y) NOEXCEPT;
    CONSTCD14 year& operator++()    NOEXCEPT;
    CONSTCD14 year  operator++(int) NOEXCEPT;
    CONSTCD14 year& operator--()    NOEXCEPT;
    CONSTCD14 year  operator--(int) NOEXCEPT;
    CONSTCD14 year& operator+=(const years& y) NOEXCEPT;
    CONSTCD14 year& operator-=(const years& y) NOEXCEPT;
    CONSTCD11 year operator-() const NOEXCEPT;
    CONSTCD11 year operator+() const NOEXCEPT;
    CONSTCD11 bool is_leap() const NOEXCEPT;
    CONSTCD11 explicit operator int64_t() const NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
    static CONSTCD11 year min() NOEXCEPT;
    static CONSTCD11 year max() NOEXCEPT;
};






CONSTCD11 year  operator+(const year&  x, const years& y) NOEXCEPT;
CONSTCD11 year  operator+(const years& x, const year&  y) NOEXCEPT;
CONSTCD11 year  operator-(const year&  x, const years& y) NOEXCEPT;
CONSTCD11 years operator-(const year&  x, const year&  y) NOEXCEPT;
;
// weekday
class weekday
{
    unsigned char wd_;
public:
    
    explicit CONSTCD11 weekday(unsigned wd) NOEXCEPT;
    explicit weekday(int) = delete;
    CONSTCD11 weekday(const sys_days& dp) NOEXCEPT;
    CONSTCD11 explicit weekday(const local_days& dp) NOEXCEPT;
    CONSTCD14 weekday& operator++()    NOEXCEPT;
    CONSTCD14 weekday  operator++(int) NOEXCEPT;
    CONSTCD14 weekday& operator--()    NOEXCEPT;
    CONSTCD14 weekday  operator--(int) NOEXCEPT;
    CONSTCD14 weekday& operator+=(const days& d) NOEXCEPT;
    CONSTCD14 weekday& operator-=(const days& d) NOEXCEPT;
    CONSTCD11 explicit operator unsigned() const NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
    CONSTCD11 weekday_indexed operator[](unsigned index) const NOEXCEPT;
    CONSTCD11 weekday_last    operator[](last_spec)      const NOEXCEPT;
private:
    static CONSTCD11 unsigned char weekday_from_days(int z) NOEXCEPT;
};
CONSTCD11 bool operator==(const weekday& x, const weekday& y) NOEXCEPT;
CONSTCD11 bool operator!=(const weekday& x, const weekday& y) NOEXCEPT;
CONSTCD14 weekday operator+(const weekday& x, const days&    y) NOEXCEPT;
CONSTCD14 weekday operator+(const days&    x, const weekday& y) NOEXCEPT;
CONSTCD14 weekday operator-(const weekday& x, const days&    y) NOEXCEPT;

;
// weekday_indexed
class weekday_indexed
{
    unsigned char wd_    : 4;
    unsigned char index_ : 4;
public:
    CONSTCD11 weekday_indexed(const date::weekday& wd, unsigned index) NOEXCEPT;
    CONSTCD11 date::weekday weekday() const NOEXCEPT;
    CONSTCD11 unsigned index() const NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
};


template<class CharT, class Traits>
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const weekday_indexed& wdi);
// weekday_last
class weekday_last
{
    date::weekday wd_;
public:
    explicit CONSTCD11 weekday_last(const date::weekday& wd) NOEXCEPT;
    CONSTCD11 date::weekday weekday() const NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
};


;
// year_month
class year_month
{
    date::year  y_;
    date::month m_;
public:
    
    CONSTCD11 year_month(const date::year& y, const date::month& m) NOEXCEPT;
    CONSTCD11 date::year  year()  const NOEXCEPT;
    CONSTCD11 date::month month() const NOEXCEPT;
    CONSTCD14 year_month& operator+=(const months& dm) NOEXCEPT;
    CONSTCD14 year_month& operator-=(const months& dm) NOEXCEPT;
    CONSTCD14 year_month& operator+=(const years& dy) NOEXCEPT;
    CONSTCD14 year_month& operator-=(const years& dy) NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
};






CONSTCD14 year_month operator+(const year_month& ym, const months& dm) NOEXCEPT;
CONSTCD14 year_month operator+(const months& dm, const year_month& ym) NOEXCEPT;
CONSTCD14 year_month operator-(const year_month& ym, const months& dm) NOEXCEPT;




template<class CharT, class Traits>
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const year_month& ym);
// month_day
class month_day
{
    date::month m_;
    date::day   d_;
public:
    month_day() = default;
    CONSTCD11 month_day(const date::month& m, const date::day& d) NOEXCEPT;
    CONSTCD11 date::month month() const NOEXCEPT;
    CONSTCD11 date::day   day() const NOEXCEPT;
    CONSTCD14 bool ok() const NOEXCEPT;
};
CONSTCD11 bool operator==(const month_day& x, const month_day& y) NOEXCEPT;





;
// month_day_last
class month_day_last
{
    date::month m_;
public:
    CONSTCD11 explicit month_day_last(const date::month& m) NOEXCEPT;
    CONSTCD11 date::month month() const NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
};




CONSTCD11 bool operator<=(const month_day_last& x, const month_day_last& y) NOEXCEPT;

;
// month_weekday
class month_weekday
{
    date::month           m_;
    date::weekday_indexed wdi_;
public:
    CONSTCD11 month_weekday(const date::month& m,
                            const date::weekday_indexed& wdi) NOEXCEPT;
    CONSTCD11 date::month           month()           const NOEXCEPT;
    CONSTCD11 date::weekday_indexed weekday_indexed() const NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
};


template<class CharT, class Traits>
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const month_weekday& mwd);
// month_weekday_last
class month_weekday_last
{
    date::month        m_;
    date::weekday_last wdl_;
public:
    CONSTCD11 month_weekday_last(const date::month& m,
                                 const date::weekday_last& wd) NOEXCEPT;
    CONSTCD11 date::month        month()        const NOEXCEPT;
    CONSTCD11 date::weekday_last weekday_last() const NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
};
CONSTCD11
    bool operator==(const month_weekday_last& x, const month_weekday_last& y) NOEXCEPT;

;
// class year_month_day
class year_month_day
{
    date::year  y_;
    date::month m_;
    date::day   d_;
public:
    year_month_day() = default;
    CONSTCD11 year_month_day(const date::year& y, const date::month& m,
                             const date::day& d) NOEXCEPT;
    CONSTCD14 year_month_day(const year_month_day_last& ymdl) NOEXCEPT;
    CONSTCD14 year_month_day(sys_days dp) NOEXCEPT;
    CONSTCD14 explicit year_month_day(local_days dp) NOEXCEPT;
    CONSTCD14 year_month_day& operator+=(const months& m) NOEXCEPT;
    CONSTCD14 year_month_day& operator-=(const months& m) NOEXCEPT;
    CONSTCD14 year_month_day& operator+=(const years& y)  NOEXCEPT;
    CONSTCD14 year_month_day& operator-=(const years& y)  NOEXCEPT;
    CONSTCD11 date::year  year()  const NOEXCEPT;
    CONSTCD11 date::month month() const NOEXCEPT;
    CONSTCD11 date::day   day()   const NOEXCEPT;
    CONSTCD14 operator sys_days() const NOEXCEPT;
    CONSTCD14 explicit operator local_days() const NOEXCEPT;
    CONSTCD14 bool ok() const NOEXCEPT;
private:
    static CONSTCD14 year_month_day from_days(days dp) NOEXCEPT;
    CONSTCD14 days to_days() const NOEXCEPT;
};




CONSTCD11 bool operator<=(const year_month_day& x, const year_month_day& y) NOEXCEPT;
CONSTCD11 bool operator>=(const year_month_day& x, const year_month_day& y) NOEXCEPT;
CONSTCD14 year_month_day operator+(const year_month_day& ymd, const months& dm) NOEXCEPT;
CONSTCD14 year_month_day operator+(const months& dm, const year_month_day& ymd) NOEXCEPT;
CONSTCD14 year_month_day operator-(const year_month_day& ymd, const months& dm) NOEXCEPT;



;
// year_month_day_last
class year_month_day_last
{
    date::year           y_;
    date::month_day_last mdl_;
public:
    CONSTCD11 year_month_day_last(const date::year& y,
                                  const date::month_day_last& mdl) NOEXCEPT;
    CONSTCD14 year_month_day_last& operator+=(const months& m) NOEXCEPT;
    CONSTCD14 year_month_day_last& operator-=(const months& m) NOEXCEPT;
    CONSTCD14 year_month_day_last& operator+=(const years& y)  NOEXCEPT;
    CONSTCD14 year_month_day_last& operator-=(const years& y)  NOEXCEPT;
    CONSTCD11 date::year           year()           const NOEXCEPT;
    CONSTCD11 date::month          month()          const NOEXCEPT;
    CONSTCD11 date::month_day_last month_day_last() const NOEXCEPT;
    CONSTCD14 date::day            day()            const NOEXCEPT;
    CONSTCD14 operator sys_days() const NOEXCEPT;
    CONSTCD14 explicit operator local_days() const NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
};






CONSTCD14
year_month_day_last
operator+(const year_month_day_last& ymdl, const months& dm) NOEXCEPT;
CONSTCD14
year_month_day_last
operator+(const months& dm, const year_month_day_last& ymdl) NOEXCEPT;
CONSTCD11
year_month_day_last
operator+(const year_month_day_last& ymdl, const years& dy) NOEXCEPT;
CONSTCD11
year_month_day_last
operator+(const years& dy, const year_month_day_last& ymdl) NOEXCEPT;
CONSTCD14
year_month_day_last
operator-(const year_month_day_last& ymdl, const months& dm) NOEXCEPT;
CONSTCD11
year_month_day_last
operator-(const year_month_day_last& ymdl, const years& dy) NOEXCEPT;
template<class CharT, class Traits>
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const year_month_day_last& ymdl);
// year_month_weekday
class year_month_weekday
{
    date::year            y_;
    date::month           m_;
    date::weekday_indexed wdi_;
public:
    CONSTCD11 year_month_weekday(const date::year& y, const date::month& m,
                                   const date::weekday_indexed& wdi) NOEXCEPT;
    CONSTCD14 year_month_weekday(const sys_days& dp) NOEXCEPT;
    CONSTCD14 explicit year_month_weekday(const local_days& dp) NOEXCEPT;
    CONSTCD14 year_month_weekday& operator+=(const months& m) NOEXCEPT;
    CONSTCD14 year_month_weekday& operator-=(const months& m) NOEXCEPT;
    CONSTCD14 year_month_weekday& operator+=(const years& y)  NOEXCEPT;
    CONSTCD14 year_month_weekday& operator-=(const years& y)  NOEXCEPT;
    CONSTCD11 date::year year() const NOEXCEPT;
    CONSTCD11 date::month month() const NOEXCEPT;
    CONSTCD11 date::weekday weekday() const NOEXCEPT;
    CONSTCD11 unsigned index() const NOEXCEPT;
    CONSTCD11 date::weekday_indexed weekday_indexed() const NOEXCEPT;
    CONSTCD14 operator sys_days() const NOEXCEPT;
    CONSTCD14 explicit operator local_days() const NOEXCEPT;
    CONSTCD14 bool ok() const NOEXCEPT;
private:
    static CONSTCD14 year_month_weekday from_days(days dp) NOEXCEPT;
    CONSTCD14 days to_days() const NOEXCEPT;
};


CONSTCD14
year_month_weekday
operator+(const year_month_weekday& ymwd, const months& dm) NOEXCEPT;
CONSTCD14
year_month_weekday
operator+(const months& dm, const year_month_weekday& ymwd) NOEXCEPT;


CONSTCD14
year_month_weekday
operator-(const year_month_weekday& ymwd, const months& dm) NOEXCEPT;
CONSTCD11
year_month_weekday
operator-(const year_month_weekday& ymwd, const years& dy) NOEXCEPT;
template<class CharT, class Traits>
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const year_month_weekday& ymwdi);
// year_month_weekday_last
class year_month_weekday_last
{
    date::year y_;
    date::month m_;
    date::weekday_last wdl_;
public:
    CONSTCD11 year_month_weekday_last(const date::year& y, const date::month& m,
                                      const date::weekday_last& wdl) NOEXCEPT;
    CONSTCD14 year_month_weekday_last& operator+=(const months& m) NOEXCEPT;
    CONSTCD14 year_month_weekday_last& operator-=(const months& m) NOEXCEPT;
    CONSTCD14 year_month_weekday_last& operator+=(const years& y) NOEXCEPT;
    CONSTCD14 year_month_weekday_last& operator-=(const years& y) NOEXCEPT;
    CONSTCD11 date::year year() const NOEXCEPT;
    CONSTCD11 date::month month() const NOEXCEPT;
    CONSTCD11 date::weekday weekday() const NOEXCEPT;
    CONSTCD11 date::weekday_last weekday_last() const NOEXCEPT;
    CONSTCD14 operator sys_days() const NOEXCEPT;
    CONSTCD14 explicit operator local_days() const NOEXCEPT;
    CONSTCD11 bool ok() const NOEXCEPT;
private:
    CONSTCD14 days to_days() const NOEXCEPT;
};


CONSTCD14
year_month_weekday_last
operator+(const year_month_weekday_last& ymwdl, const months& dm) NOEXCEPT;
CONSTCD14
year_month_weekday_last
operator+(const months& dm, const year_month_weekday_last& ymwdl) NOEXCEPT;


CONSTCD14
year_month_weekday_last
operator-(const year_month_weekday_last& ymwdl, const months& dm) NOEXCEPT;
CONSTCD11
year_month_weekday_last
operator-(const year_month_weekday_last& ymwdl, const years& dy) NOEXCEPT;
;
#if !defined(_MSC_VER) || (_MSC_VER >= 1900)
inline namespace literals
{


// CONSTDATA date::month jan{1};
// CONSTDATA date::month feb{2};
// CONSTDATA date::month mar{3};
// CONSTDATA date::month apr{4};
// CONSTDATA date::month may{5};
// CONSTDATA date::month jun{6};
// CONSTDATA date::month jul{7};
// CONSTDATA date::month aug{8};
// CONSTDATA date::month sep{9};
// CONSTDATA date::month oct{10};
// CONSTDATA date::month nov{11};
// CONSTDATA date::month dec{12};
//
// CONSTDATA date::weekday sun{0u};
// CONSTDATA date::weekday mon{1u};
// CONSTDATA date::weekday tue{2u};
// CONSTDATA date::weekday wed{3u};
// CONSTDATA date::weekday thu{4u};
// CONSTDATA date::weekday fri{5u};
// CONSTDATA date::weekday sat{6u};
}  // inline namespace literals
#endif // !defined(_MSC_VER) || (_MSC_VER >= 1900)
//----------------+
// Implementation |
//----------------+
// utilities
namespace detail {
template<class CharT, class Traits = std::char_traits<CharT>>
class save_stream
{
    std::basic_ostream<CharT, Traits>& os_;
    CharT fill_;
    std::ios::fmtflags flags_;
    std::locale loc_;
public:
    
    
    
    explicit save_stream(std::basic_ostream<CharT, Traits>& os)
        : os_(os)
        , fill_(os.fill())
        , flags_(os.flags())
        , loc_(os.getloc())
        {}
};
#ifdef __GNUC__
// GCC complains about __int128 with -pedantic or -pedantic-errors
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wpedantic"
#endif
template <class T>
struct choose_trunc_type
{
    static const int digits = std::numeric_limits<T>::digits;
    using type = typename std::conditional
                 <
                     digits < 32,
                     std::int32_t,
                     typename std::conditional
                     <
                         digits < 64,
                         std::int64_t,
#ifdef __SIZEOF_INT128__
                         __int128
#else
                         std::int64_t
#endif
                     >::type
                 >::type;
};
#ifdef __GNUC__
#pragma GCC diagnostic pop
#endif
template <class T>
CONSTCD11
inline
typename std::enable_if
<
    !std::chrono::treat_as_floating_point<T>::value,
    T
>::type
trunc(T t) NOEXCEPT
{
    return t;
}
template <class T>
CONSTCD14
inline
typename std::enable_if
<
    std::chrono::treat_as_floating_point<T>::value,
    T
>::type
trunc(T t) NOEXCEPT
{
    using namespace std;
    using I = typename choose_trunc_type<T>::type;
    CONSTDATA auto digits = numeric_limits<T>::digits;
    static_assert(digits < numeric_limits<I>::digits, "");
    CONSTDATA auto max = I{1} << (digits-1);
    CONSTDATA auto min = -max;
    const auto negative = t < T{0};
    if (min <= t && t <= max && t != 0 && t == t)
    {
        t = static_cast<T>(static_cast<I>(t));
        if (t == 0 && negative)
            t = -t;
    }
    return t;
}
}  // detail
// trunc towards zero
template <class To, class Rep, class Period>
CONSTCD11
inline
To
trunc(const std::chrono::duration<Rep, Period>& d)
;
#ifndef HAS_CHRONO_ROUNDING
#  if defined(_MSC_FULL_VER) && _MSC_FULL_VER >= 190023918
#    define HAS_CHRONO_ROUNDING 1
#  elif defined(__cpp_lib_chrono) && __cplusplus > 201402 && __cpp_lib_chrono >= 201510
#    define HAS_CHRONO_ROUNDING 1
#  elif defined(_LIBCPP_VERSION) && __cplusplus > 201402 && _LIBCPP_VERSION >= 3800
#    define HAS_CHRONO_ROUNDING 1
#  else
#    define HAS_CHRONO_ROUNDING 0
#  endif
#endif  // HAS_CHRONO_ROUNDING
#if HAS_CHRONO_ROUNDING == 0
// round down
template <class To, class Rep, class Period>
;
// round to nearest, to even on tie
template <class To, class Rep, class Period>
;
// round up
template <class To, class Rep, class Period>
;
template <class Rep, class Period,
          class = typename std::enable_if
          <
              std::numeric_limits<Rep>::is_signed
          >::type>
CONSTCD11
std::chrono::duration<Rep, Period>
abs(std::chrono::duration<Rep, Period> d)
{
    return d >= d.zero() ? d : -d;
}
// round down
template <class To, class Clock, class FromDuration>
CONSTCD11
inline
std::chrono::time_point<Clock, To>
floor(const std::chrono::time_point<Clock, FromDuration>& tp)
{
    using std::chrono::time_point;
    return time_point<Clock, To>{floor<To>(tp.time_since_epoch())};
}
// round to nearest, to even on tie
template <class To, class Clock, class FromDuration>
CONSTCD11
inline
std::chrono::time_point<Clock, To>
round(const std::chrono::time_point<Clock, FromDuration>& tp)
{
    using std::chrono::time_point;
    return time_point<Clock, To>{round<To>(tp.time_since_epoch())};
}
// round up
template <class To, class Clock, class FromDuration>
CONSTCD11
inline
std::chrono::time_point<Clock, To>
ceil(const std::chrono::time_point<Clock, FromDuration>& tp)
{
    using std::chrono::time_point;
    return time_point<Clock, To>{ceil<To>(tp.time_since_epoch())};
}
#else  // HAS_CHRONO_ROUNDING == 1
using std::chrono::floor;
using std::chrono::ceil;
using std::chrono::round;
using std::chrono::abs;
#endif  // HAS_CHRONO_ROUNDING
// trunc towards zero
template <class To, class Clock, class FromDuration>
CONSTCD11
inline
std::chrono::time_point<Clock, To>
trunc(const std::chrono::time_point<Clock, FromDuration>& tp)
{
    using std::chrono::time_point;
    return time_point<Clock, To>{trunc<To>(tp.time_since_epoch())};
}
// day
CONSTCD11 inline day::day(unsigned d) NOEXCEPT : d_(static_cast<unsigned char>(d)) {}
CONSTCD14 inline day& day::operator++() NOEXCEPT {++d_; return *this;}
CONSTCD14 inline day day::operator++(int) NOEXCEPT {auto tmp(*this); ++(*this); return tmp;}
CONSTCD14 inline day& day::operator--() NOEXCEPT {--d_; return *this;}
CONSTCD14 inline day day::operator--(int) NOEXCEPT {auto tmp(*this); --(*this); return tmp;}
CONSTCD14 inline day& day::operator+=(const days& d) NOEXCEPT {*this = *this + d; return *this;}
CONSTCD14 inline day& day::operator-=(const days& d) NOEXCEPT {*this = *this - d; return *this;}
CONSTCD11 inline day::operator unsigned() const NOEXCEPT {return d_;}
CONSTCD11 inline bool day::ok() const NOEXCEPT {return 1 <= d_ && d_ <= 31;}
CONSTCD11
inline
bool
operator==(const day& x, const day& y) NOEXCEPT
{
    return static_cast<unsigned>(x) == static_cast<unsigned>(y);
}
CONSTCD11
inline
bool
operator!=(const day& x, const day& y) NOEXCEPT
{
    return !(x == y);
}
CONSTCD11
inline
bool
operator<(const day& x, const day& y) NOEXCEPT
{
    return static_cast<unsigned>(x) < static_cast<unsigned>(y);
}
CONSTCD11
inline
bool
operator>(const day& x, const day& y) NOEXCEPT
{
    return y < x;
}
CONSTCD11
inline
bool
operator<=(const day& x, const day& y) NOEXCEPT
{
    return !(y < x);
}
CONSTCD11
inline
bool
operator>=(const day& x, const day& y) NOEXCEPT
{
    return !(x < y);
}
CONSTCD11
inline
days
operator-(const day& x, const day& y) NOEXCEPT
{
    return days{static_cast<days::rep>(static_cast<unsigned>(x)
                                     - static_cast<unsigned>(y))};
}
CONSTCD11
inline
day
operator+(const day& x, const days& y) NOEXCEPT
{
    return day{static_cast<unsigned>(x) + static_cast<unsigned>(y.count())};
}
CONSTCD11
inline
day
operator+(const days& x, const day& y) NOEXCEPT
{
    return y + x;
}
CONSTCD11
inline
day
operator-(const day& x, const days& y) NOEXCEPT
{
    return x + -y;
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const day& d)
{
    detail::save_stream<CharT, Traits> _(os);
    os.fill('0');
    os.flags(std::ios::dec | std::ios::right);
    os.width(2);
    os << static_cast<unsigned>(d);
    return os;
}
// month
CONSTCD11 inline month::month(unsigned m) NOEXCEPT : m_(static_cast<decltype(m_)>(m)) {}
CONSTCD14 inline month& month::operator++() NOEXCEPT {if (++m_ == 13) m_ = 1; return *this;}
CONSTCD14 inline month month::operator++(int) NOEXCEPT {auto tmp(*this); ++(*this); return tmp;}
CONSTCD14 inline month& month::operator--() NOEXCEPT {if (--m_ == 0) m_ = 12; return *this;}
CONSTCD14 inline month month::operator--(int) NOEXCEPT {auto tmp(*this); --(*this); return tmp;}
CONSTCD14
inline
month&
month::operator+=(const months& m) NOEXCEPT
{
    *this = *this + m;
    return *this;
}
CONSTCD14
inline
month&
month::operator-=(const months& m) NOEXCEPT
{
    *this = *this - m;
    return *this;
}
CONSTCD11 inline month::operator unsigned() const NOEXCEPT {return m_;}
CONSTCD11 inline bool month::ok() const NOEXCEPT {return 1 <= m_ && m_ <= 12;}
CONSTCD11
inline
bool
operator==(const month& x, const month& y) NOEXCEPT
{
    return static_cast<unsigned>(x) == static_cast<unsigned>(y);
}
CONSTCD11
inline
bool
operator!=(const month& x, const month& y) NOEXCEPT
{
    return !(x == y);
}
CONSTCD11
inline
bool
operator<(const month& x, const month& y) NOEXCEPT
{
    return static_cast<unsigned>(x) < static_cast<unsigned>(y);
}
CONSTCD11
inline
bool
operator>(const month& x, const month& y) NOEXCEPT
{
    return y < x;
}
CONSTCD11
inline
bool
operator<=(const month& x, const month& y) NOEXCEPT
{
    return !(y < x);
}
CONSTCD11
inline
bool
operator>=(const month& x, const month& y) NOEXCEPT
{
    return !(x < y);
}
CONSTCD14
inline
months
operator-(const month& x, const month& y) NOEXCEPT
{
    auto const d = static_cast<unsigned>(x) - static_cast<unsigned>(y);
    return months(d <= 11 ? d : d + 12);
}
CONSTCD14
inline
month
operator+(const month& x, const months& y) NOEXCEPT
{
    auto const mu = static_cast<long long>(static_cast<unsigned>(x)) - 1 + y.count();
    auto const yr = (mu >= 0 ? mu : mu-11) / 12;
    return month{static_cast<unsigned>(mu - yr * 12 + 1)};
}
CONSTCD14
inline
month
operator+(const months& x, const month& y) NOEXCEPT
{
    return y + x;
}
CONSTCD14
inline
month
operator-(const month& x, const months& y) NOEXCEPT
{
    return x + -y;
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const month& m)
{
    switch (static_cast<unsigned>(m))
    {
    case 1:
        os << "Jan";
        break;
    case 2:
        os << "Feb";
        break;
    case 3:
        os << "Mar";
        break;
    case 4:
        os << "Apr";
        break;
    case 5:
        os << "May";
        break;
    case 6:
        os << "Jun";
        break;
    case 7:
        os << "Jul";
        break;
    case 8:
        os << "Aug";
        break;
    case 9:
        os << "Sep";
        break;
    case 10:
        os << "Oct";
        break;
    case 11:
        os << "Nov";
        break;
    case 12:
        os << "Dec";
        break;
    default:
        os << static_cast<unsigned>(m) << " is not a valid month";
        break;
    }
    return os;
}
// year
CONSTCD11 inline year::year(int64_t y) NOEXCEPT : y_(static_cast<decltype(y_)>(y)) {}
CONSTCD14 inline year& year::operator++() NOEXCEPT {++y_; return *this;}
CONSTCD14 inline year year::operator++(int) NOEXCEPT {auto tmp(*this); ++(*this); return tmp;}
CONSTCD14 inline year& year::operator--() NOEXCEPT {--y_; return *this;}
CONSTCD14 inline year year::operator--(int) NOEXCEPT {auto tmp(*this); --(*this); return tmp;}
CONSTCD14 inline year& year::operator+=(const years& y) NOEXCEPT {*this = *this + y; return *this;}
CONSTCD14 inline year& year::operator-=(const years& y) NOEXCEPT {*this = *this - y; return *this;}
CONSTCD11 inline year year::operator-() const NOEXCEPT {return year{-y_};}
CONSTCD11 inline year year::operator+() const NOEXCEPT {return *this;}
CONSTCD11
inline
bool
year::is_leap() const NOEXCEPT
{
    return y_ % 4 == 0 && (y_ % 100 != 0 || y_ % 400 == 0);
}
CONSTCD11 inline year::operator int64_t() const NOEXCEPT {return y_;}
CONSTCD11 inline bool year::ok() const NOEXCEPT {return true;}
CONSTCD11
inline
year
year::min() NOEXCEPT
{
    return year{std::numeric_limits<int64_t>::min()};
}
CONSTCD11
inline
year
year::max() NOEXCEPT
{
    return year{std::numeric_limits<int64_t>::max()};
}
CONSTCD11
inline
bool
operator==(const year& x, const year& y) NOEXCEPT
{
    return static_cast<int64_t>(x) == static_cast<int64_t>(y);
}
CONSTCD11
inline
bool
operator!=(const year& x, const year& y) NOEXCEPT
{
    return !(x == y);
}
CONSTCD11
inline
bool
operator<(const year& x, const year& y) NOEXCEPT
{
    return static_cast<int64_t>(x) < static_cast<int64_t>(y);
}
CONSTCD11
inline
bool
operator>(const year& x, const year& y) NOEXCEPT
{
    return y < x;
}
CONSTCD11
inline
bool
operator<=(const year& x, const year& y) NOEXCEPT
{
    return !(y < x);
}
CONSTCD11
inline
bool
operator>=(const year& x, const year& y) NOEXCEPT
{
    return !(x < y);
}
CONSTCD11
inline
years
operator-(const year& x, const year& y) NOEXCEPT
{
    return years{static_cast<int64_t>(x) - static_cast<int64_t>(y)};
}
CONSTCD11
inline
year
operator+(const year& x, const years& y) NOEXCEPT
{
    return year{static_cast<int64_t>(x) + y.count()};
}
CONSTCD11
inline
year
operator+(const years& x, const year& y) NOEXCEPT
{
    return y + x;
}
CONSTCD11
inline
year
operator-(const year& x, const years& y) NOEXCEPT
{
    return year{static_cast<int64_t>(x) - y.count()};
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const year& y)
{
    detail::save_stream<CharT, Traits> _(os);
    os.fill('0');
    os.flags(std::ios::dec | std::ios::internal);
    os.width(4 + (y < year{0}));
    os << static_cast<int64_t>(y);
    return os;
}
// weekday
CONSTCD11
inline
unsigned char
weekday::weekday_from_days(int z) NOEXCEPT
{
    return static_cast<unsigned char>(static_cast<unsigned>(
        z >= -4 ? (z+4) % 7 : (z+5) % 7 + 6));
}
CONSTCD11
inline
weekday::weekday(unsigned wd) NOEXCEPT
    : wd_(static_cast<decltype(wd_)>(wd))
    {}
CONSTCD11
inline
weekday::weekday(const sys_days& dp) NOEXCEPT
    : wd_(weekday_from_days(dp.time_since_epoch().count()))
    {}
CONSTCD11
inline
weekday::weekday(const local_days& dp) NOEXCEPT
    : wd_(weekday_from_days(dp.time_since_epoch().count()))
    {}
CONSTCD14 inline weekday& weekday::operator++() NOEXCEPT {if (++wd_ == 7) wd_ = 0; return *this;}
CONSTCD14 inline weekday weekday::operator++(int) NOEXCEPT {auto tmp(*this); ++(*this); return tmp;}
CONSTCD14 inline weekday& weekday::operator--() NOEXCEPT {if (wd_-- == 0) wd_ = 6; return *this;}
CONSTCD14 inline weekday weekday::operator--(int) NOEXCEPT {auto tmp(*this); --(*this); return tmp;}
CONSTCD14
inline
weekday&
weekday::operator+=(const days& d) NOEXCEPT
{
    *this = *this + d;
    return *this;
}
CONSTCD14
inline
weekday&
weekday::operator-=(const days& d) NOEXCEPT
{
    *this = *this - d;
    return *this;
}
CONSTCD11
inline
weekday::operator unsigned() const NOEXCEPT
{
    return static_cast<unsigned>(wd_);
}
CONSTCD11 inline bool weekday::ok() const NOEXCEPT {return wd_ <= 6;}
CONSTCD11
inline
bool
operator==(const weekday& x, const weekday& y) NOEXCEPT
{
    return static_cast<unsigned>(x) == static_cast<unsigned>(y);
}
CONSTCD11
inline
bool
operator!=(const weekday& x, const weekday& y) NOEXCEPT
{
    return !(x == y);
}
CONSTCD14
inline
days
operator-(const weekday& x, const weekday& y) NOEXCEPT
{
    auto const diff = static_cast<unsigned>(x) - static_cast<unsigned>(y);
    return days{diff <= 6 ? diff : diff + 7};
}
CONSTCD14
inline
weekday
operator+(const weekday& x, const days& y) NOEXCEPT
{
    auto const wdu = static_cast<long long>(static_cast<unsigned>(x)) + y.count();
    auto const wk = (wdu >= 0 ? wdu : wdu-6) / 7;
    return weekday{static_cast<unsigned>(wdu - wk * 7)};
}
CONSTCD14
inline
weekday
operator+(const days& x, const weekday& y) NOEXCEPT
{
    return y + x;
}
CONSTCD14
inline
weekday
operator-(const weekday& x, const days& y) NOEXCEPT
{
    return x + -y;
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const weekday& wd)
{
    switch (static_cast<unsigned>(wd))
    {
    case 0:
        os << "Sun";
        break;
    case 1:
        os << "Mon";
        break;
    case 2:
        os << "Tue";
        break;
    case 3:
        os << "Wed";
        break;
    case 4:
        os << "Thu";
        break;
    case 5:
        os << "Fri";
        break;
    case 6:
        os << "Sat";
        break;
    default:
        os << static_cast<unsigned>(wd) << " is not a valid weekday";
        break;
    }
    return os;
}
#if !defined(_MSC_VER) || (_MSC_VER >= 1900)
inline namespace literals
{
CONSTCD11
inline
date::day
operator "" _d(unsigned long long d) NOEXCEPT
{
    return date::day{static_cast<unsigned>(d)};
}
CONSTCD11
inline
date::year
operator "" _y(unsigned long long y) NOEXCEPT
{
    return date::year(static_cast<int64_t>(y));
}
#endif  // !defined(_MSC_VER) || (_MSC_VER >= 1900)
CONSTDATA date::last_spec last{};
CONSTDATA date::month jan{1};
CONSTDATA date::month feb{2};
CONSTDATA date::month mar{3};
CONSTDATA date::month apr{4};
CONSTDATA date::month may{5};
CONSTDATA date::month jun{6};
CONSTDATA date::month jul{7};
CONSTDATA date::month aug{8};
CONSTDATA date::month sep{9};
CONSTDATA date::month oct{10};
CONSTDATA date::month nov{11};
CONSTDATA date::month dec{12};
CONSTDATA date::weekday sun{0u};
CONSTDATA date::weekday mon{1u};
CONSTDATA date::weekday tue{2u};
CONSTDATA date::weekday wed{3u};
CONSTDATA date::weekday thu{4u};
CONSTDATA date::weekday fri{5u};
CONSTDATA date::weekday sat{6u};
#if !defined(_MSC_VER) || (_MSC_VER >= 1900)
}  // inline namespace literals
#endif
// weekday_indexed
CONSTCD11
inline
weekday
weekday_indexed::weekday() const NOEXCEPT
{
    return date::weekday{static_cast<unsigned>(wd_)};
}
CONSTCD11 inline unsigned weekday_indexed::index() const NOEXCEPT {return index_;}
CONSTCD11
inline
bool
weekday_indexed::ok() const NOEXCEPT
{
    return weekday().ok() && 1 <= index_ && index_ <= 5;
}
CONSTCD11
inline
weekday_indexed::weekday_indexed(const date::weekday& wd, unsigned index) NOEXCEPT
    : wd_(static_cast<decltype(wd_)>(static_cast<unsigned>(wd)))
    , index_(static_cast<decltype(index_)>(index))
    {}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const weekday_indexed& wdi)
{
    return os << wdi.weekday() << '[' << wdi.index() << ']';
}
CONSTCD11
inline
weekday_indexed
weekday::operator[](unsigned index) const NOEXCEPT
{
    return {*this, index};
}
CONSTCD11
inline
bool
operator==(const weekday_indexed& x, const weekday_indexed& y) NOEXCEPT
{
    return x.weekday() == y.weekday() && x.index() == y.index();
}
CONSTCD11
inline
bool
operator!=(const weekday_indexed& x, const weekday_indexed& y) NOEXCEPT
{
    return !(x == y);
}
// weekday_last
CONSTCD11 inline date::weekday weekday_last::weekday() const NOEXCEPT {return wd_;}
CONSTCD11 inline bool weekday_last::ok() const NOEXCEPT {return wd_.ok();}
CONSTCD11 inline weekday_last::weekday_last(const date::weekday& wd) NOEXCEPT : wd_(wd) {}
CONSTCD11
inline
bool
operator==(const weekday_last& x, const weekday_last& y) NOEXCEPT
{
    return x.weekday() == y.weekday();
}
CONSTCD11
inline
bool
operator!=(const weekday_last& x, const weekday_last& y) NOEXCEPT
{
    return !(x == y);
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const weekday_last& wdl)
{
    return os << wdl.weekday() << "[last]";
}
CONSTCD11
inline
weekday_last
weekday::operator[](last_spec) const NOEXCEPT
{
    return weekday_last{*this};
}
// year_month
CONSTCD11
inline
year_month::year_month(const date::year& y, const date::month& m) NOEXCEPT
    : y_(y)
    , m_(m)
    {}
CONSTCD11 inline year year_month::year() const NOEXCEPT {return y_;}
CONSTCD11 inline month year_month::month() const NOEXCEPT {return m_;}
CONSTCD11 inline bool year_month::ok() const NOEXCEPT {return y_.ok() && m_.ok();}
CONSTCD14
inline
year_month&
year_month::operator+=(const months& dm) NOEXCEPT
{
    *this = *this + dm;
    return *this;
}
CONSTCD14
inline
year_month&
year_month::operator-=(const months& dm) NOEXCEPT
{
    *this = *this - dm;
    return *this;
}
CONSTCD14
inline
year_month&
year_month::operator+=(const years& dy) NOEXCEPT
{
    *this = *this + dy;
    return *this;
}
CONSTCD14
inline
year_month&
year_month::operator-=(const years& dy) NOEXCEPT
{
    *this = *this - dy;
    return *this;
}
CONSTCD11
inline
bool
operator==(const year_month& x, const year_month& y) NOEXCEPT
{
    return x.year() == y.year() && x.month() == y.month();
}
CONSTCD11
inline
bool
operator!=(const year_month& x, const year_month& y) NOEXCEPT
{
    return !(x == y);
}
CONSTCD11
inline
bool
operator<(const year_month& x, const year_month& y) NOEXCEPT
{
    return x.year() < y.year() ? true
        : (x.year() > y.year() ? false
        : (x.month() < y.month()));
}
CONSTCD11
inline
bool
operator>(const year_month& x, const year_month& y) NOEXCEPT
{
    return y < x;
}
CONSTCD11
inline
bool
operator<=(const year_month& x, const year_month& y) NOEXCEPT
{
    return !(y < x);
}
CONSTCD11
inline
bool
operator>=(const year_month& x, const year_month& y) NOEXCEPT
{
    return !(x < y);
}
CONSTCD14
inline
year_month
operator+(const year_month& ym, const months& dm) NOEXCEPT
{
    auto dmi = static_cast<int>(static_cast<unsigned>(ym.month())) - 1 + dm.count();
    auto dy = (dmi >= 0 ? dmi : dmi-11) / 12;
    dmi = dmi - dy * 12 + 1;
    return (ym.year() + years(dy)) / month(static_cast<unsigned>(dmi));
}
CONSTCD14
inline
year_month
operator+(const months& dm, const year_month& ym) NOEXCEPT
{
    return ym + dm;
}
CONSTCD14
inline
year_month
operator-(const year_month& ym, const months& dm) NOEXCEPT
{
    return ym + -dm;
}
CONSTCD11
inline
months
operator-(const year_month& x, const year_month& y) NOEXCEPT
{
    return (x.year() - y.year()) +
            months(static_cast<unsigned>(x.month()) - static_cast<unsigned>(y.month()));
}
CONSTCD11
inline
year_month
operator+(const year_month& ym, const years& dy) NOEXCEPT
{
    return (ym.year() + dy) / ym.month();
}
CONSTCD11
inline
year_month
operator+(const years& dy, const year_month& ym) NOEXCEPT
{
    return ym + dy;
}
CONSTCD11
inline
year_month
operator-(const year_month& ym, const years& dy) NOEXCEPT
{
    return ym + -dy;
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const year_month& ym)
{
    return os << ym.year() << '/' << ym.month();
}
// month_day
CONSTCD11
inline
month_day::month_day(const date::month& m, const date::day& d) NOEXCEPT
    : m_(m)
    , d_(d)
    {}
CONSTCD11 inline date::month month_day::month() const NOEXCEPT {return m_;}
CONSTCD11 inline date::day month_day::day() const NOEXCEPT {return d_;}
CONSTCD14
inline
bool
month_day::ok() const NOEXCEPT
{
    CONSTDATA date::day d[] =
    {
        date::day(31), date::day(29), date::day(31),
        date::day(30), date::day(31), date::day(30),
        date::day(31), date::day(31), date::day(30),
        date::day(31), date::day(30), date::day(31)
    };
    return m_.ok() && date::day{1} <= d_ && d_ <= d[static_cast<unsigned>(m_)-1];
}
CONSTCD11
inline
bool
operator==(const month_day& x, const month_day& y) NOEXCEPT
{
    return x.month() == y.month() && x.day() == y.day();
}
CONSTCD11
inline
bool
operator!=(const month_day& x, const month_day& y) NOEXCEPT
{
    return !(x == y);
}
CONSTCD11
inline
bool
operator<(const month_day& x, const month_day& y) NOEXCEPT
{
    return x.month() < y.month() ? true
        : (x.month() > y.month() ? false
        : (x.day() < y.day()));
}
CONSTCD11
inline
bool
operator>(const month_day& x, const month_day& y) NOEXCEPT
{
    return y < x;
}
CONSTCD11
inline
bool
operator<=(const month_day& x, const month_day& y) NOEXCEPT
{
    return !(y < x);
}
CONSTCD11
inline
bool
operator>=(const month_day& x, const month_day& y) NOEXCEPT
{
    return !(x < y);
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const month_day& md)
{
    return os << md.month() << '/' << md.day();
}
// month_day_last
CONSTCD11 inline month month_day_last::month() const NOEXCEPT {return m_;}
CONSTCD11 inline bool month_day_last::ok() const NOEXCEPT {return m_.ok();}
CONSTCD11 inline month_day_last::month_day_last(const date::month& m) NOEXCEPT : m_(m) {}
CONSTCD11
inline
bool
operator==(const month_day_last& x, const month_day_last& y) NOEXCEPT
{
    return x.month() == y.month();
}
CONSTCD11
inline
bool
operator!=(const month_day_last& x, const month_day_last& y) NOEXCEPT
{
    return !(x == y);
}
CONSTCD11
inline
bool
operator<(const month_day_last& x, const month_day_last& y) NOEXCEPT
{
    return x.month() < y.month();
}
CONSTCD11
inline
bool
operator>(const month_day_last& x, const month_day_last& y) NOEXCEPT
{
    return y < x;
}
CONSTCD11
inline
bool
operator<=(const month_day_last& x, const month_day_last& y) NOEXCEPT
{
    return !(y < x);
}
CONSTCD11
inline
bool
operator>=(const month_day_last& x, const month_day_last& y) NOEXCEPT
{
    return !(x < y);
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const month_day_last& mdl)
{
    return os << mdl.month() << "/last";
}
// month_weekday
CONSTCD11
inline
month_weekday::month_weekday(const date::month& m,
                             const date::weekday_indexed& wdi) NOEXCEPT
    : m_(m)
    , wdi_(wdi)
    {}
CONSTCD11 inline month month_weekday::month() const NOEXCEPT {return m_;}
CONSTCD11
inline
weekday_indexed
month_weekday::weekday_indexed() const NOEXCEPT
{
    return wdi_;
}
CONSTCD11
inline
bool
month_weekday::ok() const NOEXCEPT
{
    return m_.ok() && wdi_.ok();
}
CONSTCD11
inline
bool
operator==(const month_weekday& x, const month_weekday& y) NOEXCEPT
{
    return x.month() == y.month() && x.weekday_indexed() == y.weekday_indexed();
}
CONSTCD11
inline
bool
operator!=(const month_weekday& x, const month_weekday& y) NOEXCEPT
{
    return !(x == y);
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const month_weekday& mwd)
{
    return os << mwd.month() << '/' << mwd.weekday_indexed();
}
// month_weekday_last
CONSTCD11
inline
month_weekday_last::month_weekday_last(const date::month& m,
                                       const date::weekday_last& wdl) NOEXCEPT
    : m_(m)
    , wdl_(wdl)
    {}
CONSTCD11 inline month month_weekday_last::month() const NOEXCEPT {return m_;}
CONSTCD11
inline
weekday_last
month_weekday_last::weekday_last() const NOEXCEPT
{
    return wdl_;
}
CONSTCD11
inline
bool
month_weekday_last::ok() const NOEXCEPT
{
    return m_.ok() && wdl_.ok();
}
CONSTCD11
inline
bool
operator==(const month_weekday_last& x, const month_weekday_last& y) NOEXCEPT
{
    return x.month() == y.month() && x.weekday_last() == y.weekday_last();
}
CONSTCD11
inline
bool
operator!=(const month_weekday_last& x, const month_weekday_last& y) NOEXCEPT
{
    return !(x == y);
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const month_weekday_last& mwdl)
{
    return os << mwdl.month() << '/' << mwdl.weekday_last();
}
// year_month_day_last
CONSTCD11
inline
year_month_day_last::year_month_day_last(const date::year& y,
                                         const date::month_day_last& mdl) NOEXCEPT
    : y_(y)
    , mdl_(mdl)
    {}
CONSTCD14
inline
year_month_day_last&
year_month_day_last::operator+=(const months& m) NOEXCEPT
{
    *this = *this + m;
    return *this;
}
CONSTCD14
inline
year_month_day_last&
year_month_day_last::operator-=(const months& m) NOEXCEPT
{
    *this = *this - m;
    return *this;
}
CONSTCD14
inline
year_month_day_last&
year_month_day_last::operator+=(const years& y) NOEXCEPT
{
    *this = *this + y;
    return *this;
}
CONSTCD14
inline
year_month_day_last&
year_month_day_last::operator-=(const years& y) NOEXCEPT
{
    *this = *this - y;
    return *this;
}
CONSTCD11 inline year year_month_day_last::year() const NOEXCEPT {return y_;}
CONSTCD11 inline month year_month_day_last::month() const NOEXCEPT {return mdl_.month();}
CONSTCD11
inline
month_day_last
year_month_day_last::month_day_last() const NOEXCEPT
{
    return mdl_;
}
CONSTCD14
inline
day
year_month_day_last::day() const NOEXCEPT
{
    CONSTDATA date::day d[] =
    {
        date::day(31), date::day(28), date::day(31),
        date::day(30), date::day(31), date::day(30),
        date::day(31), date::day(31), date::day(30),
        date::day(31), date::day(30), date::day(31)
    };
    return month() != feb || !y_.is_leap() ?
        d[static_cast<unsigned>(month()) - 1] : date::day{29};
}
CONSTCD14
inline
year_month_day_last::operator sys_days() const NOEXCEPT
{
    return sys_days(year()/month()/day());
}
CONSTCD14
inline
year_month_day_last::operator local_days() const NOEXCEPT
{
    return local_days(year()/month()/day());
}
CONSTCD11
inline
bool
year_month_day_last::ok() const NOEXCEPT
{
    return y_.ok() && mdl_.ok();
}
CONSTCD11
inline
bool
operator==(const year_month_day_last& x, const year_month_day_last& y) NOEXCEPT
{
    return x.year() == y.year() && x.month_day_last() == y.month_day_last();
}
CONSTCD11
inline
bool
operator!=(const year_month_day_last& x, const year_month_day_last& y) NOEXCEPT
{
    return !(x == y);
}
CONSTCD11
inline
bool
operator<(const year_month_day_last& x, const year_month_day_last& y) NOEXCEPT
{
    return x.year() < y.year() ? true
        : (x.year() > y.year() ? false
        : (x.month_day_last() < y.month_day_last()));
}
CONSTCD11
inline
bool
operator>(const year_month_day_last& x, const year_month_day_last& y) NOEXCEPT
{
    return y < x;
}
CONSTCD11
inline
bool
operator<=(const year_month_day_last& x, const year_month_day_last& y) NOEXCEPT
{
    return !(y < x);
}
CONSTCD11
inline
bool
operator>=(const year_month_day_last& x, const year_month_day_last& y) NOEXCEPT
{
    return !(x < y);
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const year_month_day_last& ymdl)
{
    return os << ymdl.year() << '/' << ymdl.month_day_last();
}
CONSTCD14
inline
year_month_day_last
operator+(const year_month_day_last& ymdl, const months& dm) NOEXCEPT
{
    return (ymdl.year() / ymdl.month() + dm) / last;
}
CONSTCD14
inline
year_month_day_last
operator+(const months& dm, const year_month_day_last& ymdl) NOEXCEPT
{
    return ymdl + dm;
}
CONSTCD14
inline
year_month_day_last
operator-(const year_month_day_last& ymdl, const months& dm) NOEXCEPT
{
    return ymdl + (-dm);
}
CONSTCD11
inline
year_month_day_last
operator+(const year_month_day_last& ymdl, const years& dy) NOEXCEPT
{
    return {ymdl.year()+dy, ymdl.month_day_last()};
}
CONSTCD11
inline
year_month_day_last
operator+(const years& dy, const year_month_day_last& ymdl) NOEXCEPT
{
    return ymdl + dy;
}
CONSTCD11
inline
year_month_day_last
operator-(const year_month_day_last& ymdl, const years& dy) NOEXCEPT
{
    return ymdl + (-dy);
}
// year_month_day
CONSTCD11
inline
year_month_day::year_month_day(const date::year& y, const date::month& m,
                               const date::day& d) NOEXCEPT
    : y_(y)
    , m_(m)
    , d_(d)
    {}
CONSTCD14
inline
year_month_day::year_month_day(const year_month_day_last& ymdl) NOEXCEPT
    : y_(ymdl.year())
    , m_(ymdl.month())
    , d_(ymdl.day())
    {}
CONSTCD14
inline
year_month_day::year_month_day(sys_days dp) NOEXCEPT
    : year_month_day(from_days(dp.time_since_epoch()))
    {}
CONSTCD14
inline
year_month_day::year_month_day(local_days dp) NOEXCEPT
    : year_month_day(from_days(dp.time_since_epoch()))
    {}
CONSTCD11 inline year year_month_day::year() const NOEXCEPT {return y_;}
CONSTCD11 inline month year_month_day::month() const NOEXCEPT {return m_;}
CONSTCD11 inline day year_month_day::day() const NOEXCEPT {return d_;}
CONSTCD14
inline
year_month_day&
year_month_day::operator+=(const months& m) NOEXCEPT
{
    *this = *this + m;
    return *this;
}
CONSTCD14
inline
year_month_day&
year_month_day::operator-=(const months& m) NOEXCEPT
{
    *this = *this - m;
    return *this;
}
CONSTCD14
inline
year_month_day&
year_month_day::operator+=(const years& y) NOEXCEPT
{
    *this = *this + y;
    return *this;
}
CONSTCD14
inline
year_month_day&
year_month_day::operator-=(const years& y) NOEXCEPT
{
    *this = *this - y;
    return *this;
}
CONSTCD14
inline
days
year_month_day::to_days() const NOEXCEPT
{
    static_assert(std::numeric_limits<unsigned>::digits >= 18,
             "This algorithm has not been ported to a 16 bit unsigned integer");
    static_assert(std::numeric_limits<int>::digits >= 20,
             "This algorithm has not been ported to a 16 bit signed integer");
    auto const y = static_cast<int64_t>(y_) - (m_ <= feb);
    auto const m = static_cast<unsigned>(m_);
    auto const d = static_cast<unsigned>(d_);
    auto const era = (y >= 0 ? y : y-399) / 400;
    auto const yoe = static_cast<unsigned>(y - era * 400);       // [0, 399]
    auto const doy = (153*(m > 2 ? m-3 : m+9) + 2)/5 + d-1;      // [0, 365]
    auto const doe = yoe * 365 + yoe/4 - yoe/100 + doy;          // [0, 146096]
    return days{era * 146097 + static_cast<int64_t>(doe) - 719468};
}
CONSTCD14
inline
year_month_day::operator sys_days() const NOEXCEPT
{
    return sys_days{to_days()};
}
CONSTCD14
inline
year_month_day::operator local_days() const NOEXCEPT
{
    return local_days{to_days()};
}
CONSTCD14
inline
bool
year_month_day::ok() const NOEXCEPT
{
    if (!(y_.ok() && m_.ok()))
        return false;
    return date::day{1} <= d_ && d_ <= (y_ / m_ / last).day();
}
CONSTCD11
inline
bool
operator==(const year_month_day& x, const year_month_day& y) NOEXCEPT
{
    return x.year() == y.year() && x.month() == y.month() && x.day() == y.day();
}
CONSTCD11
inline
bool
operator!=(const year_month_day& x, const year_month_day& y) NOEXCEPT
{
    return !(x == y);
}
CONSTCD11
inline
bool
operator<(const year_month_day& x, const year_month_day& y) NOEXCEPT
{
    return x.year() < y.year() ? true
        : (x.year() > y.year() ? false
        : (x.month() < y.month() ? true
        : (x.month() > y.month() ? false
        : (x.day() < y.day()))));
}
CONSTCD11
inline
bool
operator>(const year_month_day& x, const year_month_day& y) NOEXCEPT
{
    return y < x;
}
CONSTCD11
inline
bool
operator<=(const year_month_day& x, const year_month_day& y) NOEXCEPT
{
    return !(y < x);
}
CONSTCD11
inline
bool
operator>=(const year_month_day& x, const year_month_day& y) NOEXCEPT
{
    return !(x < y);
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const year_month_day& ymd)
{
    detail::save_stream<CharT, Traits> _(os);
    os.fill('0');
    os.flags(std::ios::dec | std::ios::right);
    os << ymd.year() << '-';
    os.width(2);
    os << static_cast<unsigned>(ymd.month()) << '-';
    os << ymd.day();
    return os;
}
CONSTCD14
inline
year_month_day
year_month_day::from_days(days dp) NOEXCEPT
{
    static_assert(std::numeric_limits<unsigned>::digits >= 18,
             "This algorithm has not been ported to a 16 bit unsigned integer");
    static_assert(std::numeric_limits<int>::digits >= 20,
             "This algorithm has not been ported to a 16 bit signed integer");
    auto const z = dp.count() + 719468;
    auto const era = (z >= 0 ? z : z - 146096) / 146097;
    auto const doe = static_cast<unsigned>(z - era * 146097);          // [0, 146096]
    auto const yoe = (doe - doe/1460 + doe/36524 - doe/146096) / 365;  // [0, 399]
    auto const y = static_cast<days::rep>(yoe) + era * 400;
    auto const doy = doe - (365*yoe + yoe/4 - yoe/100);                // [0, 365]
    auto const mp = (5*doy + 2)/153;                                   // [0, 11]
    auto const d = doy - (153*mp+2)/5 + 1;                             // [1, 31]
    auto const m = mp < 10 ? mp+3 : mp-9;                              // [1, 12]
    return year_month_day{date::year{y + (m <= 2)}, date::month(m), date::day(d)};
}
CONSTCD14
inline
year_month_day
operator+(const year_month_day& ymd, const months& dm) NOEXCEPT
{
    return (ymd.year() / ymd.month() + dm) / ymd.day();
}
CONSTCD14
inline
year_month_day
operator+(const months& dm, const year_month_day& ymd) NOEXCEPT
{
    return ymd + dm;
}
CONSTCD14
inline
year_month_day
operator-(const year_month_day& ymd, const months& dm) NOEXCEPT
{
    return ymd + (-dm);
}
CONSTCD11
inline
year_month_day
operator+(const year_month_day& ymd, const years& dy) NOEXCEPT
{
    return (ymd.year() + dy) / ymd.month() / ymd.day();
}
CONSTCD11
inline
year_month_day
operator+(const years& dy, const year_month_day& ymd) NOEXCEPT
{
    return ymd + dy;
}
CONSTCD11
inline
year_month_day
operator-(const year_month_day& ymd, const years& dy) NOEXCEPT
{
    return ymd + (-dy);
}
// year_month_weekday
CONSTCD11
inline
year_month_weekday::year_month_weekday(const date::year& y, const date::month& m,
                                       const date::weekday_indexed& wdi)
        NOEXCEPT
    : y_(y)
    , m_(m)
    , wdi_(wdi)
    {}
CONSTCD14
inline
year_month_weekday::year_month_weekday(const sys_days& dp) NOEXCEPT
    : year_month_weekday(from_days(dp.time_since_epoch()))
    {}
CONSTCD14
inline
year_month_weekday::year_month_weekday(const local_days& dp) NOEXCEPT
    : year_month_weekday(from_days(dp.time_since_epoch()))
    {}
CONSTCD14
inline
year_month_weekday&
year_month_weekday::operator+=(const months& m) NOEXCEPT
{
    *this = *this + m;
    return *this;
}
CONSTCD14
inline
year_month_weekday&
year_month_weekday::operator-=(const months& m) NOEXCEPT
{
    *this = *this - m;
    return *this;
}
CONSTCD14
inline
year_month_weekday&
year_month_weekday::operator+=(const years& y) NOEXCEPT
{
    *this = *this + y;
    return *this;
}
CONSTCD14
inline
year_month_weekday&
year_month_weekday::operator-=(const years& y) NOEXCEPT
{
    *this = *this - y;
    return *this;
}
CONSTCD11 inline year year_month_weekday::year() const NOEXCEPT {return y_;}
CONSTCD11 inline month year_month_weekday::month() const NOEXCEPT {return m_;}
CONSTCD11
inline
weekday
year_month_weekday::weekday() const NOEXCEPT
{
    return wdi_.weekday();
}
CONSTCD11
inline
unsigned
year_month_weekday::index() const NOEXCEPT
{
    return wdi_.index();
}
CONSTCD11
inline
weekday_indexed
year_month_weekday::weekday_indexed() const NOEXCEPT
{
    return wdi_;
}
CONSTCD14
inline
year_month_weekday::operator sys_days() const NOEXCEPT
{
    return sys_days{to_days()};
}
CONSTCD14
inline
year_month_weekday::operator local_days() const NOEXCEPT
{
    return local_days{to_days()};
}
CONSTCD14
inline
bool
year_month_weekday::ok() const NOEXCEPT
{
    if (!y_.ok() || !m_.ok() || !wdi_.weekday().ok() || wdi_.index() < 1)
        return false;
    if (wdi_.index() <= 4)
        return true;
    auto d2 = wdi_.weekday() - date::weekday(static_cast<sys_days>(y_/m_/1)) + days((wdi_.index()-1)*7 + 1);
    return static_cast<unsigned>(d2.count()) <= static_cast<unsigned>((y_/m_/last).day());
}
CONSTCD14
inline
year_month_weekday
year_month_weekday::from_days(days d) NOEXCEPT
{
    sys_days dp{d};
    auto const wd = date::weekday(dp);
    auto const ymd = year_month_day(dp);
    return {ymd.year(), ymd.month(), wd[(static_cast<unsigned>(ymd.day())-1)/7+1]};
}
CONSTCD14
inline
days
year_month_weekday::to_days() const NOEXCEPT
{
    auto d = sys_days(y_/m_/1);
    return (d + (wdi_.weekday() - date::weekday(d) + days{(wdi_.index()-1)*7})
           ).time_since_epoch();
}
CONSTCD11
inline
bool
operator==(const year_month_weekday& x, const year_month_weekday& y) NOEXCEPT
{
    return x.year() == y.year() && x.month() == y.month() &&
           x.weekday_indexed() == y.weekday_indexed();
}
CONSTCD11
inline
bool
operator!=(const year_month_weekday& x, const year_month_weekday& y) NOEXCEPT
{
    return !(x == y);
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const year_month_weekday& ymwdi)
{
    return os << ymwdi.year() << '/' << ymwdi.month()
              << '/' << ymwdi.weekday_indexed();
}
CONSTCD14
inline
year_month_weekday
operator+(const year_month_weekday& ymwd, const months& dm) NOEXCEPT
{
    return (ymwd.year() / ymwd.month() + dm) / ymwd.weekday_indexed();
}
CONSTCD14
inline
year_month_weekday
operator+(const months& dm, const year_month_weekday& ymwd) NOEXCEPT
{
    return ymwd + dm;
}
CONSTCD14
inline
year_month_weekday
operator-(const year_month_weekday& ymwd, const months& dm) NOEXCEPT
{
    return ymwd + (-dm);
}
CONSTCD11
inline
year_month_weekday
operator+(const year_month_weekday& ymwd, const years& dy) NOEXCEPT
{
    return {ymwd.year()+dy, ymwd.month(), ymwd.weekday_indexed()};
}
CONSTCD11
inline
year_month_weekday
operator+(const years& dy, const year_month_weekday& ymwd) NOEXCEPT
{
    return ymwd + dy;
}
CONSTCD11
inline
year_month_weekday
operator-(const year_month_weekday& ymwd, const years& dy) NOEXCEPT
{
    return ymwd + (-dy);
}
// year_month_weekday_last
CONSTCD11
inline
year_month_weekday_last::year_month_weekday_last(const date::year& y,
                                                 const date::month& m,
                                                 const date::weekday_last& wdl) NOEXCEPT
    : y_(y)
    , m_(m)
    , wdl_(wdl)
    {}
CONSTCD14
inline
year_month_weekday_last&
year_month_weekday_last::operator+=(const months& m) NOEXCEPT
{
    *this = *this + m;
    return *this;
}
CONSTCD14
inline
year_month_weekday_last&
year_month_weekday_last::operator-=(const months& m) NOEXCEPT
{
    *this = *this - m;
    return *this;
}
CONSTCD14
inline
year_month_weekday_last&
year_month_weekday_last::operator+=(const years& y) NOEXCEPT
{
    *this = *this + y;
    return *this;
}
CONSTCD14
inline
year_month_weekday_last&
year_month_weekday_last::operator-=(const years& y) NOEXCEPT
{
    *this = *this - y;
    return *this;
}
CONSTCD11 inline year year_month_weekday_last::year() const NOEXCEPT {return y_;}
CONSTCD11 inline month year_month_weekday_last::month() const NOEXCEPT {return m_;}
CONSTCD11
inline
weekday
year_month_weekday_last::weekday() const NOEXCEPT
{
    return wdl_.weekday();
}
CONSTCD11
inline
weekday_last
year_month_weekday_last::weekday_last() const NOEXCEPT
{
    return wdl_;
}
CONSTCD14
inline
year_month_weekday_last::operator sys_days() const NOEXCEPT
{
    return sys_days{to_days()};
}
CONSTCD14
inline
year_month_weekday_last::operator local_days() const NOEXCEPT
{
    return local_days{to_days()};
}
CONSTCD11
inline
bool
year_month_weekday_last::ok() const NOEXCEPT
{
    return y_.ok() && m_.ok() && wdl_.ok();
}
CONSTCD14
inline
days
year_month_weekday_last::to_days() const NOEXCEPT
{
    auto const d = sys_days(y_/m_/last);
    return (d - (date::weekday{d} - wdl_.weekday())).time_since_epoch();
}
CONSTCD11
inline
bool
operator==(const year_month_weekday_last& x, const year_month_weekday_last& y) NOEXCEPT
{
    return x.year() == y.year() && x.month() == y.month() &&
           x.weekday_last() == y.weekday_last();
}
CONSTCD11
inline
bool
operator!=(const year_month_weekday_last& x, const year_month_weekday_last& y) NOEXCEPT
{
    return !(x == y);
}
template<class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const year_month_weekday_last& ymwdl)
{
    return os << ymwdl.year() << '/' << ymwdl.month() << '/' << ymwdl.weekday_last();
}
CONSTCD14
inline
year_month_weekday_last
operator+(const year_month_weekday_last& ymwdl, const months& dm) NOEXCEPT
{
    return (ymwdl.year() / ymwdl.month() + dm) / ymwdl.weekday_last();
}
CONSTCD14
inline
year_month_weekday_last
operator+(const months& dm, const year_month_weekday_last& ymwdl) NOEXCEPT
{
    return ymwdl + dm;
}
CONSTCD14
inline
year_month_weekday_last
operator-(const year_month_weekday_last& ymwdl, const months& dm) NOEXCEPT
{
    return ymwdl + (-dm);
}
CONSTCD11
inline
year_month_weekday_last
operator+(const year_month_weekday_last& ymwdl, const years& dy) NOEXCEPT
{
    return {ymwdl.year()+dy, ymwdl.month(), ymwdl.weekday_last()};
}
CONSTCD11
inline
year_month_weekday_last
operator+(const years& dy, const year_month_weekday_last& ymwdl) NOEXCEPT
{
    return ymwdl + dy;
}
CONSTCD11
inline
year_month_weekday_last
operator-(const year_month_weekday_last& ymwdl, const years& dy) NOEXCEPT
{
    return ymwdl + (-dy);
}
// year_month from operator/()
CONSTCD11
inline
year_month
operator/(const year& y, const month& m) NOEXCEPT
{
    return {y, m};
}
CONSTCD11
inline
year_month
operator/(const year& y, int   m) NOEXCEPT
{
    return y / month(static_cast<unsigned>(m));
}
// month_day from operator/()
CONSTCD11
inline
month_day
operator/(const month& m, const day& d) NOEXCEPT
{
    return {m, d};
}
CONSTCD11
inline
month_day
operator/(const day& d, const month& m) NOEXCEPT
{
    return m / d;
}
CONSTCD11
inline
month_day
operator/(const month& m, int d) NOEXCEPT
{
    return m / day(static_cast<unsigned>(d));
}
CONSTCD11
inline
month_day
operator/(int m, const day& d) NOEXCEPT
{
    return month(static_cast<unsigned>(m)) / d;
}
CONSTCD11 inline month_day operator/(const day& d, int m) NOEXCEPT {return m / d;}
// month_day_last from operator/()
CONSTCD11
inline
month_day_last
operator/(const month& m, last_spec) NOEXCEPT
{
    return month_day_last{m};
}
CONSTCD11
inline
month_day_last
operator/(last_spec, const month& m) NOEXCEPT
{
    return m/last;
}
CONSTCD11
inline
month_day_last
operator/(int m, last_spec) NOEXCEPT
{
    return month(static_cast<unsigned>(m))/last;
}
CONSTCD11
inline
month_day_last
operator/(last_spec, int m) NOEXCEPT
{
    return m/last;
}
// month_weekday from operator/()
CONSTCD11
inline
month_weekday
operator/(const month& m, const weekday_indexed& wdi) NOEXCEPT
{
    return {m, wdi};
}
CONSTCD11
inline
month_weekday
operator/(const weekday_indexed& wdi, const month& m) NOEXCEPT
{
    return m / wdi;
}
CONSTCD11
inline
month_weekday
operator/(int m, const weekday_indexed& wdi) NOEXCEPT
{
    return month(static_cast<unsigned>(m)) / wdi;
}
CONSTCD11
inline
month_weekday
operator/(const weekday_indexed& wdi, int m) NOEXCEPT
{
    return m / wdi;
}
// month_weekday_last from operator/()
CONSTCD11
inline
month_weekday_last
operator/(const month& m, const weekday_last& wdl) NOEXCEPT
{
    return {m, wdl};
}
CONSTCD11
inline
month_weekday_last
operator/(const weekday_last& wdl, const month& m) NOEXCEPT
{
    return m / wdl;
}
CONSTCD11
inline
month_weekday_last
operator/(int m, const weekday_last& wdl) NOEXCEPT
{
    return month(static_cast<unsigned>(m)) / wdl;
}
CONSTCD11
inline
month_weekday_last
operator/(const weekday_last& wdl, int m) NOEXCEPT
{
    return m / wdl;
}
// year_month_day from operator/()
CONSTCD11
inline
year_month_day
operator/(const year_month& ym, const day& d) NOEXCEPT
{
    return {ym.year(), ym.month(), d};
}
CONSTCD11
inline
year_month_day
operator/(const year_month& ym, int d)  NOEXCEPT
{
    return ym / day(static_cast<unsigned>(d));
}
CONSTCD11
inline
year_month_day
operator/(const year& y, const month_day& md) NOEXCEPT
{
    return y / md.month() / md.day();
}
CONSTCD11
inline
year_month_day
operator/(int y, const month_day& md) NOEXCEPT
{
    return year(y) / md;
}
CONSTCD11
inline
year_month_day
operator/(const month_day& md, const year& y)  NOEXCEPT
{
    return y / md;
}
CONSTCD11
inline
year_month_day
operator/(const month_day& md, int y) NOEXCEPT
{
    return year(y) / md;
}
// year_month_day_last from operator/()
CONSTCD11
inline
year_month_day_last
operator/(const year_month& ym, last_spec) NOEXCEPT
{
    return {ym.year(), month_day_last{ym.month()}};
}
CONSTCD11
inline
year_month_day_last
operator/(const year& y, const month_day_last& mdl) NOEXCEPT
{
    return {y, mdl};
}
CONSTCD11
inline
year_month_day_last
operator/(int y, const month_day_last& mdl) NOEXCEPT
{
    return year(y) / mdl;
}
CONSTCD11
inline
year_month_day_last
operator/(const month_day_last& mdl, const year& y) NOEXCEPT
{
    return y / mdl;
}
CONSTCD11
inline
year_month_day_last
operator/(const month_day_last& mdl, int y) NOEXCEPT
{
    return year(y) / mdl;
}
// year_month_weekday from operator/()
CONSTCD11
inline
year_month_weekday
operator/(const year_month& ym, const weekday_indexed& wdi) NOEXCEPT
{
    return {ym.year(), ym.month(), wdi};
}
CONSTCD11
inline
year_month_weekday
operator/(const year& y, const month_weekday& mwd) NOEXCEPT
{
    return {y, mwd.month(), mwd.weekday_indexed()};
}
CONSTCD11
inline
year_month_weekday
operator/(int y, const month_weekday& mwd) NOEXCEPT
{
    return year(y) / mwd;
}
CONSTCD11
inline
year_month_weekday
operator/(const month_weekday& mwd, const year& y) NOEXCEPT
{
    return y / mwd;
}
CONSTCD11
inline
year_month_weekday
operator/(const month_weekday& mwd, int y) NOEXCEPT
{
    return year(y) / mwd;
}
// year_month_weekday_last from operator/()
CONSTCD11
inline
year_month_weekday_last
operator/(const year_month& ym, const weekday_last& wdl) NOEXCEPT
{
    return {ym.year(), ym.month(), wdl};
}
CONSTCD11
inline
year_month_weekday_last
operator/(const year& y, const month_weekday_last& mwdl) NOEXCEPT
{
    return {y, mwdl.month(), mwdl.weekday_last()};
}
CONSTCD11
inline
year_month_weekday_last
operator/(int y, const month_weekday_last& mwdl) NOEXCEPT
{
    return year(y) / mwdl;
}
CONSTCD11
inline
year_month_weekday_last
operator/(const month_weekday_last& mwdl, const year& y) NOEXCEPT
{
    return y / mwdl;
}
CONSTCD11
inline
year_month_weekday_last
operator/(const month_weekday_last& mwdl, int y) NOEXCEPT
{
    return year(y) / mwdl;
}
// time_of_day
enum {am = 1, pm};
namespace detail
{
// width<n>::value is the number of fractional decimal digits in 1/n
// width<0>::value and width<1>::value are defined to be 0
// If 1/n takes more than 18 fractional decimal digits,
//   the result is truncated to 19.
// Example:  width<2>::value    ==  1
// Example:  width<3>::value    == 19
// Example:  width<4>::value    ==  2
// Example:  width<10>::value   ==  1
// Example:  width<1000>::value ==  3
template <std::uint64_t n, std::uint64_t d = 10, unsigned w = 0,
          bool should_continue = !(n < 2) && d != 0 && (w < 19)>
struct width
{
    static CONSTDATA unsigned value = 1 + width<n, d%n*10, w+1>::value;
};
template <std::uint64_t n, std::uint64_t d, unsigned w>
struct width<n, d, w, false>
{
    static CONSTDATA unsigned value = 0;
};
template <unsigned exp>
struct static_pow10
{
private:
    static CONSTDATA std::uint64_t h = static_pow10<exp/2>::value;
public:
    static CONSTDATA std::uint64_t value = h * h * (exp % 2 ? 10 : 1);
};
template <>
struct static_pow10<0>
{
    static CONSTDATA std::uint64_t value = 1;
};
template <unsigned w, bool in_range = (w < 19)>
struct make_precision
{
    using type = std::chrono::duration<std::int64_t,
                                       std::ratio<1, static_pow10<w>::value>>;
    static CONSTDATA unsigned width = w;
};
template <unsigned w>
struct make_precision<w, false>
{
    using type = std::chrono::microseconds;
    static CONSTDATA unsigned width = 6;
};
template <class Duration,
          unsigned w = width<std::common_type<
                                 Duration,
                                 std::chrono::seconds>::type::period::den>::value>
class decimal_format_seconds
{
public:
    using precision = typename make_precision<w>::type;
    static auto CONSTDATA width = make_precision<w>::width;
private:
    std::chrono::seconds s_;
    precision            sub_s_;
public:
    CONSTCD11 explicit decimal_format_seconds(const Duration& d) NOEXCEPT
        : s_(std::chrono::duration_cast<std::chrono::seconds>(d))
        , sub_s_(std::chrono::duration_cast<precision>(d - s_))
        {}
    CONSTCD14 std::chrono::seconds& seconds() NOEXCEPT {return s_;}
    CONSTCD11 std::chrono::seconds seconds() const NOEXCEPT {return s_;}
    CONSTCD11 precision subseconds() const NOEXCEPT {return sub_s_;}
    CONSTCD14 precision to_duration() const NOEXCEPT
    {
        return s_ + sub_s_;
    }
    template <class CharT, class Traits>
    friend
    std::basic_ostream<CharT, Traits>&
    operator<<(std::basic_ostream<CharT, Traits>& os, const decimal_format_seconds& x)
    {
        date::detail::save_stream<CharT, Traits> _(os);
        os.fill('0');
        os.flags(std::ios::dec | std::ios::right);
        os.width(2);
        os << x.s_.count() <<
              std::use_facet<std::numpunct<char>>(os.getloc()).decimal_point();
        os.width(width);
        os << x.sub_s_.count();
        return os;
    }
};
template <class Duration>
class decimal_format_seconds<Duration, 0>
{
    static CONSTDATA unsigned w = 0;
public:
    using precision = std::chrono::seconds;
    static auto CONSTDATA width = make_precision<w>::width;
private:
    std::chrono::seconds s_;
public:
     ;
};
enum class classify
{
    not_valid,
    hour,
    minute,
    second,
    subsecond
};
template <class Duration>
struct classify_duration
{
    static CONSTDATA classify value =
        std::is_convertible<Duration, std::chrono::hours>::value
                ? classify::hour :
        std::is_convertible<Duration, std::chrono::minutes>::value
                ? classify::minute :
        std::is_convertible<Duration, std::chrono::seconds>::value
                ? classify::second :
        std::chrono::treat_as_floating_point<typename Duration::rep>::value
                ? classify::not_valid :
                classify::subsecond;
};
class time_of_day_base
{
protected:
    std::chrono::hours   h_;
    unsigned char mode_;
    bool          neg_;
    enum {is24hr};
    CONSTCD11 time_of_day_base(std::chrono::hours h, bool neg, unsigned m) 
        ;
    CONSTCD14 void make24() NOEXCEPT;
    CONSTCD14 void make12() NOEXCEPT;
    CONSTCD14 std::chrono::hours to24hr() const;
};
template <class Duration, detail::classify = detail::classify_duration<Duration>::value>
class time_of_day_storage;
template <class Rep, class Period>
class time_of_day_storage<std::chrono::duration<Rep, Period>, detail::classify::hour>
    : private detail::time_of_day_base
{
    using base = detail::time_of_day_base;
public:
    using precision = std::chrono::hours;
    CONSTCD11 explicit time_of_day_storage(std::chrono::hours since_midnight) NOEXCEPT
        : base(since_midnight, since_midnight < std::chrono::hours{0}, is24hr)
        {}
    CONSTCD11 explicit time_of_day_storage(std::chrono::hours h, unsigned md) NOEXCEPT
        : base(h, h < std::chrono::hours{0}, md)
        {}
    CONSTCD11 std::chrono::hours hours() const NOEXCEPT {return h_;}
    CONSTCD11 unsigned mode() const NOEXCEPT {return mode_;}
    CONSTCD14 explicit operator precision() const NOEXCEPT
    {
        auto p = to24hr();
        if (neg_)
            p = -p;
        return p;
    }
    CONSTCD14 precision to_duration() const NOEXCEPT
    {
        return static_cast<precision>(*this);
    }
    CONSTCD14 time_of_day_storage& make24() NOEXCEPT {base::make24(); return *this;}
    CONSTCD14 time_of_day_storage& make12() NOEXCEPT {base::make12(); return *this;}
    template<class CharT, class Traits>
    friend
    std::basic_ostream<CharT, Traits>&
    operator<<(std::basic_ostream<CharT, Traits>& os, const time_of_day_storage& t)
    {
        using namespace std;
        detail::save_stream<CharT, Traits> _(os);
        if (t.neg_)
            os << '-';
        os.fill('0');
        os.flags(std::ios::dec | std::ios::right);
        if (t.mode_ != am && t.mode_ != pm)
            os.width(2);
        os << t.h_.count();
        switch (t.mode_)
        {
        case time_of_day_storage::is24hr:
            os << "00";
            break;
        case am:
            os << "am";
            break;
        case pm:
            os << "pm";
            break;
        }
        return os;
    }
};
template <class Rep, class Period>
class time_of_day_storage<std::chrono::duration<Rep, Period>, detail::classify::minute>
    : private detail::time_of_day_base
{
    using base = detail::time_of_day_base;
    std::chrono::minutes m_;
public:
   using precision = std::chrono::minutes;
   CONSTCD11 explicit time_of_day_storage(std::chrono::minutes since_midnight) NOEXCEPT
        : base(std::chrono::duration_cast<std::chrono::hours>(since_midnight),
               since_midnight < std::chrono::minutes{0}, is24hr)
        , m_(abs(since_midnight) - h_)
        {}
    CONSTCD11 explicit time_of_day_storage(std::chrono::hours h, std::chrono::minutes m,
                                           unsigned md) NOEXCEPT
        : base(h, false, md)
        , m_(m)
        {}
    CONSTCD11 std::chrono::hours hours() const NOEXCEPT {return h_;}
    CONSTCD11 std::chrono::minutes minutes() const NOEXCEPT {return m_;}
    CONSTCD11 unsigned mode() const NOEXCEPT {return mode_;}
    CONSTCD14 explicit operator precision() const NOEXCEPT
    {
        auto p = to24hr() + m_;
        if (neg_)
            p = -p;
        return p;
    }
    CONSTCD14 precision to_duration() const NOEXCEPT
    {
        return static_cast<precision>(*this);
    }
    CONSTCD14 time_of_day_storage& make24() NOEXCEPT {base::make24(); return *this;}
    CONSTCD14 time_of_day_storage& make12() NOEXCEPT {base::make12(); return *this;}
    template<class CharT, class Traits>
    friend
    std::basic_ostream<CharT, Traits>&
    operator<<(std::basic_ostream<CharT, Traits>& os, const time_of_day_storage& t)
    {
        using namespace std;
        detail::save_stream<CharT, Traits> _(os);
        if (t.neg_)
            os << '-';
        os.fill('0');
        os.flags(std::ios::dec | std::ios::right);
        if (t.mode_ != am && t.mode_ != pm)
            os.width(2);
        os << t.h_.count() << ':';
        os.width(2);
        os << t.m_.count();
        switch (t.mode_)
        {
        case am:
            os << "am";
            break;
        case pm:
            os << "pm";
            break;
        }
        return os;
    }
};
template <class Rep, class Period>
class time_of_day_storage<std::chrono::duration<Rep, Period>, detail::classify::second>
    : private detail::time_of_day_base
{
    using base = detail::time_of_day_base;
    std::chrono::minutes m_;
    std::chrono::seconds s_;
public:
    using precision = std::chrono::seconds;
    CONSTCD11 explicit time_of_day_storage(std::chrono::seconds since_midnight) NOEXCEPT
        : base(std::chrono::duration_cast<std::chrono::hours>(since_midnight),
               since_midnight < std::chrono::seconds{0}, is24hr)
        , m_(std::chrono::duration_cast<std::chrono::minutes>(abs(since_midnight) - h_))
        , s_(abs(since_midnight) - h_ - m_)
        {}
    CONSTCD11 explicit time_of_day_storage(std::chrono::hours h, std::chrono::minutes m,
                                           std::chrono::seconds s, unsigned md) NOEXCEPT
        : base(h, false, md)
        , m_(m)
        , s_(s)
        {}
    CONSTCD11 std::chrono::hours hours() const NOEXCEPT {return h_;}
    CONSTCD11 std::chrono::minutes minutes() const NOEXCEPT {return m_;}
    CONSTCD14 std::chrono::seconds& seconds() NOEXCEPT {return s_;}
    CONSTCD11 std::chrono::seconds seconds() const NOEXCEPT {return s_;}
    CONSTCD11 unsigned mode() const NOEXCEPT {return mode_;}
    CONSTCD14 explicit operator precision() const NOEXCEPT
    {
        auto p = to24hr() + s_ + m_;
        if (neg_)
            p = -p;
        return p;
    }
    CONSTCD14 precision to_duration() const NOEXCEPT
    {
        return static_cast<precision>(*this);
    }
    CONSTCD14 time_of_day_storage& make24() NOEXCEPT {base::make24(); return *this;}
    CONSTCD14 time_of_day_storage& make12() NOEXCEPT {base::make12(); return *this;}
    template<class CharT, class Traits>
    friend
    std::basic_ostream<CharT, Traits>&
    operator<<(std::basic_ostream<CharT, Traits>& os, const time_of_day_storage& t)
    {
        using namespace std;
        detail::save_stream<CharT, Traits> _(os);
        if (t.neg_)
            os << '-';
        os.fill('0');
        os.flags(std::ios::dec | std::ios::right);
        if (t.mode_ != am && t.mode_ != pm)
            os.width(2);
        os << t.h_.count() << ':';
        os.width(2);
        os << t.m_.count() << ':';
        os.width(2);
        os << t.s_.count();
        switch (t.mode_)
        {
        case am:
            os << "am";
            break;
        case pm:
            os << "pm";
            break;
        }
        return os;
    }
};
template <class Rep, class Period>
class time_of_day_storage<std::chrono::duration<Rep, Period>, detail::classify::subsecond>
    : private detail::time_of_day_base
{
public:
    using Duration = std::chrono::duration<Rep, Period>;
    using dfs = decimal_format_seconds<typename std::common_type<Duration,
                    std::chrono::minutes>::type>;
    using precision = typename dfs::precision;
private:
    using base = detail::time_of_day_base;
    std::chrono::minutes m_;
    dfs                  s_;
public:
    CONSTCD11 explicit time_of_day_storage(Duration since_midnight) NOEXCEPT
        : base(std::chrono::duration_cast<std::chrono::hours>(since_midnight),
               since_midnight < Duration{0}, is24hr)
        , m_(std::chrono::duration_cast<std::chrono::minutes>(abs(since_midnight) - h_))
        , s_(abs(since_midnight) - h_ - m_)
        {}
    CONSTCD11 explicit time_of_day_storage(std::chrono::hours h, std::chrono::minutes m,
                                           std::chrono::seconds s, precision sub_s,
                                           unsigned md) NOEXCEPT
        : base(h, false, md)
        , m_(m)
        , s_(s + sub_s)
        {}
    CONSTCD11 std::chrono::hours hours() const NOEXCEPT {return h_;}
    CONSTCD11 std::chrono::minutes minutes() const NOEXCEPT {return m_;}
    CONSTCD14 std::chrono::seconds& seconds() NOEXCEPT {return s_.seconds();}
    CONSTCD11 std::chrono::seconds seconds() const NOEXCEPT {return s_.seconds();}
    CONSTCD11 precision subseconds() const NOEXCEPT {return s_.subseconds();}
    CONSTCD11 unsigned mode() const NOEXCEPT {return mode_;}
    CONSTCD14 explicit operator precision() const NOEXCEPT
    {
        auto p = to24hr() + s_.to_duration() + m_;
        if (neg_)
            p = -p;
        return p;
    }
    CONSTCD14 precision to_duration() const NOEXCEPT
    {
        return static_cast<precision>(*this);
    }
    CONSTCD14 time_of_day_storage& make24() NOEXCEPT {base::make24(); return *this;}
    CONSTCD14 time_of_day_storage& make12() NOEXCEPT {base::make12(); return *this;}
    template<class CharT, class Traits>
    friend
    std::basic_ostream<CharT, Traits>&
    operator<<(std::basic_ostream<CharT, Traits>& os, const time_of_day_storage& t)
    {
        using namespace std;
        detail::save_stream<CharT, Traits> _(os);
        if (t.neg_)
            os << '-';
        os.fill('0');
        os.flags(std::ios::dec | std::ios::right);
        if (t.mode_ != am && t.mode_ != pm)
            os.width(2);
        os << t.h_.count() << ':';
        os.width(2);
        os << t.m_.count() << ':' << t.s_;
        switch (t.mode_)
        {
        case am:
            os << "am";
            break;
        case pm:
            os << "pm";
            break;
        }
        return os;
    }
};
}  // namespace detail
template <class Duration>
class time_of_day
    : public detail::time_of_day_storage<Duration>
{
    using base = detail::time_of_day_storage<Duration>;
public:
#if !(defined(_MSC_VER) && !defined(__clang__))
    // C++11
    using base::base;
#else
    // MS cl compiler workaround.
    template <class ...Args>
    CONSTCD11
    explicit time_of_day(Args&& ...args) NOEXCEPT
        : base(std::forward<Args>(args)...)
        {}
#endif
};
template <class Rep, class Period,
          class = typename std::enable_if
              <!std::chrono::treat_as_floating_point<Rep>::value>::type>
CONSTCD11
inline
time_of_day<std::chrono::duration<Rep, Period>>
make_time(const std::chrono::duration<Rep, Period>& d)
{
    return time_of_day<std::chrono::duration<Rep, Period>>(d);
}
CONSTCD11
inline
time_of_day<std::chrono::hours>
make_time(const std::chrono::hours& h, unsigned md)
{
    return time_of_day<std::chrono::hours>(h, md);
}
CONSTCD11
inline
time_of_day<std::chrono::minutes>
make_time(const std::chrono::hours& h, const std::chrono::minutes& m,
          unsigned md)
{
    return time_of_day<std::chrono::minutes>(h, m, md);
}
CONSTCD11
inline
time_of_day<std::chrono::seconds>
make_time(const std::chrono::hours& h, const std::chrono::minutes& m,
          const std::chrono::seconds& s, unsigned md)
{
    return time_of_day<std::chrono::seconds>(h, m, s, md);
}
template <class Rep, class Period,
          class = typename std::enable_if<std::ratio_less<Period,
                                                          std::ratio<1>>::value>::type>
CONSTCD11
inline
time_of_day<std::chrono::duration<Rep, Period>>
make_time(const std::chrono::hours& h, const std::chrono::minutes& m,
          const std::chrono::seconds& s, const std::chrono::duration<Rep, Period>& sub_s,
          unsigned md)
{
    return time_of_day<std::chrono::duration<Rep, Period>>(h, m, s, sub_s, md);
}
template <class CharT, class Traits, class Duration>
inline
typename std::enable_if
<
    !std::chrono::treat_as_floating_point<typename Duration::rep>::value &&
        std::ratio_less<typename Duration::period, days::period>::value
    , std::basic_ostream<CharT, Traits>&
>::type
operator<<(std::basic_ostream<CharT, Traits>& os, const sys_time<Duration>& tp)
{
    auto const dp = floor<days>(tp);
    return os << year_month_day(dp) << ' ' << make_time(tp-dp);
}
template <class CharT, class Traits>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const sys_days& dp)
{
    return os << year_month_day(dp);
}
template <class CharT, class Traits, class Duration>
inline
std::basic_ostream<CharT, Traits>&
operator<<(std::basic_ostream<CharT, Traits>& os, const local_time<Duration>& ut)
{
    return os << sys_time<Duration>{ut.time_since_epoch()};
}
// to_stream
template <class CharT, class Traits, class Duration>
void
to_stream(std::basic_ostream<CharT, Traits>& os, const CharT* fmt,
          const local_time<Duration>& tp, const std::string* abbrev = nullptr,
          const std::chrono::seconds* offset_sec = nullptr)
{
    using namespace std;
    using namespace std::chrono;
    tm tm;
    auto& facet = use_facet<time_put<CharT>>(os.getloc());
    auto command = false;
    CharT modified = CharT{};
    for (; *fmt; ++fmt)
    {
        if (!command && modified != CharT{})
            throw std::logic_error("loop invariant broken: !command && modified");
        else if (modified != CharT{} && modified != CharT{'E'} && modified != CharT{'O'})
            throw std::logic_error(std::string("bad value for modified: ") + char(modified));
        switch (*fmt)
        {
        case 'a':
        case 'A':
            if (command)
            {
                if (modified == CharT{})
                {
                    tm.tm_wday = static_cast<int>(static_cast<unsigned>(
                                     weekday{floor<days>(tp)}));
                    const CharT f[] = {'%', *fmt};
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'b':
        case 'B':
        case 'h':
            if (command)
            {
                if (modified == CharT{})
                {
                    tm.tm_mon = static_cast<int>(static_cast<unsigned>(
                                    year_month_day{floor<days>(tp)}.month())) - 1;
                    const CharT f[] = {'%', *fmt};
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'c':
        case 'x':
        case 'X':
            if (command)
            {
                if (modified == CharT{'O'})
                    os << CharT{'%'} << modified << *fmt;
                else
                {
                    tm = std::tm{};
                    auto ld = floor<days>(tp);
                    auto ymd = year_month_day{ld};
                    auto hms = make_time(floor<seconds>(tp - ld));
                    tm.tm_sec = static_cast<int>(hms.seconds().count());
                    tm.tm_min = static_cast<int>(hms.minutes().count());
                    tm.tm_hour = static_cast<int>(hms.hours().count());
                    tm.tm_mday = static_cast<int>(static_cast<unsigned>(ymd.day()));
                    tm.tm_mon = static_cast<int>(static_cast<unsigned>(ymd.month()) - 1);
                    tm.tm_year = static_cast<int64_t>(ymd.year()) - 1900;
                    tm.tm_wday = static_cast<int>(static_cast<unsigned>(weekday{ld}));
                    tm.tm_yday = static_cast<int>((ld - local_days(ymd.year()/1/1)).count());
                    CharT f[3] = {'%'};
                    auto fe = begin(f) + 1;
                    if (modified == CharT{'E'})
                        *fe++ = modified;
                    *fe++ = *fmt;
                    facet.put(os, os, os.fill(), &tm, begin(f), fe);
                }
                command = false;
                modified = CharT{};
            }
            else
                os << *fmt;
            break;
        case 'C':
            if (command)
            {
                auto y = static_cast<int64_t>(year_month_day{floor<days>(tp)}.year());
                if (modified == CharT{'E'})
                {
                    tm.tm_year = y - 1900;
                    CharT f[3] = {'%', 'E', 'C'};
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                }
                else if (modified == CharT{})
                {
                    detail::save_stream<CharT, Traits> _(os);
                    os.fill('0');
                    os.flags(std::ios::dec | std::ios::right);
                    if (y >= 0)
                    {
                        os.width(2);
                        os << y/100;
                    }
                    else
                    {
                        os << CharT{'-'};
                        os.width(2);
                        os << -(y-99)/100;
                    }
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                }
                command = false;
                modified = CharT{};
            }
            else
                os << *fmt;
            break;
        case 'd':
        case 'e':
            if (command)
            {
                auto d = static_cast<int>(static_cast<unsigned>(
                             year_month_day{floor<days>(tp)}.day()));
                if (modified == CharT{'O'})
                {
                    tm.tm_mday = d;
                    CharT f[3] = {'%', 'O', *fmt};
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                }
                else if (modified == CharT{})
                {
                    detail::save_stream<CharT, Traits> _(os);
                    if (*fmt == CharT{'d'})
                        os.fill('0');
                    os.flags(std::ios::dec | std::ios::right);
                    os.width(2);
                    os << d;
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                }
                command = false;
                modified = CharT{};
            }
            else
                os << *fmt;
            break;
        case 'D':
            if (command)
            {
                if (modified == CharT{})
                {
                    auto ymd = year_month_day{floor<days>(tp)};
                    detail::save_stream<CharT, Traits> _(os);
                    os.fill('0');
                    os.flags(std::ios::dec | std::ios::right);
                    os.width(2);
                    os << static_cast<unsigned>(ymd.month()) << CharT{'/'};
                    os.width(2);
                    os << static_cast<unsigned>(ymd.day()) << CharT{'/'};
                    os.width(2);
                    os << static_cast<int64_t>(ymd.year()) % 100;
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'F':
            if (command)
            {
                if (modified == CharT{})
                    os << floor<days>(tp);
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'g':
        case 'G':
            if (command)
            {
                if (modified == CharT{})
                {
                    auto ld = floor<days>(tp);
                    auto y = year_month_day{ld + days{3}}.year();
                    auto start = local_days{(y - years{1})/date::dec/thu[last]} + (mon-thu);
                    if (ld < start)
                        --y;
                    if (*fmt == CharT{'G'})
                        os << y;
                    else
                    {
                        detail::save_stream<CharT, Traits> _(os);
                        os.fill('0');
                        os.flags(std::ios::dec | std::ios::right);
                        os.width(2);
                        os << static_cast<int64_t>(y) % 100;
                    }
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'H':
        case 'I':
            if (command)
            {
                auto hms = make_time(floor<hours>(tp - floor<days>(tp)));
                if (modified == CharT{'O'})
                {
                    const CharT f[] = {'%', modified, *fmt};
                    tm.tm_hour = static_cast<int>(hms.hours().count());
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                    modified = CharT{};
                }
                else if (modified == CharT{})
                {
                    if (*fmt == CharT{'I'})
                        hms.make12();
                    if (hms.hours() < hours{10})
                        os << CharT{'0'};
                    os << hms.hours().count();
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'j':
            if (command)
            {
                if (modified == CharT{})
                {
                    auto ld = floor<days>(tp);
                    auto y = year_month_day{ld}.year();
                    auto doy = ld - local_days{y/jan/1} + days{1};
                    detail::save_stream<CharT, Traits> _(os);
                    os.fill('0');
                    os.flags(std::ios::dec | std::ios::right);
                    os.width(3);
                    os << doy.count();
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'm':
            if (command)
            {
                auto m = static_cast<unsigned>(year_month_day{floor<days>(tp)}.month());
                if (modified == CharT{'O'})
                {
                    const CharT f[] = {'%', modified, *fmt};
                    tm.tm_mon = static_cast<int>(m-1);
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                    modified = CharT{};
                }
                else if (modified == CharT{})
                {
                    if (m < 10)
                        os << CharT{'0'};
                    os << m;
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'M':
            if (command)
            {
                auto hms = make_time(floor<minutes>(tp - floor<days>(tp)));
                if (modified == CharT{'O'})
                {
                    const CharT f[] = {'%', modified, *fmt};
                    tm.tm_min = static_cast<int>(hms.minutes().count());
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                    modified = CharT{};
                }
                else if (modified == CharT{})
                {
                    if (hms.minutes() < minutes{10})
                        os << CharT{'0'};
                    os << hms.minutes().count();
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'n':
            if (command)
            {
                if (modified == CharT{})
                    os << CharT{'\n'};
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'p':
            if (command)
            {
                if (modified == CharT{})
                {
                    auto h = floor<hours>(tp - floor<days>(tp));
                    const CharT f[] = {'%', *fmt};
                    tm.tm_hour = static_cast<int>(h.count());
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'r':
            if (command)
            {
                if (modified == CharT{})
                {
                    auto hms = make_time(floor<seconds>(tp - floor<days>(tp)));
                    const CharT f[] = {'%', *fmt};
                    tm.tm_hour = static_cast<int>(hms.hours().count());
                    tm.tm_min = static_cast<int>(hms.minutes().count());
                    tm.tm_sec = static_cast<int>(hms.seconds().count());
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'R':
            if (command)
            {
                if (modified == CharT{})
                {
                    auto hms = make_time(floor<minutes>(tp - floor<days>(tp)));
                    if (hms.hours() < hours{10})
                        os << CharT{'0'};
                    os << hms.hours().count() << CharT{':'};
                    if (hms.minutes() < minutes{10})
                        os << CharT{'0'};
                    os << hms.minutes().count();
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'S':
            if (command)
            {
                if (modified == CharT{'O'})
                {
                    const CharT f[] = {'%', modified, *fmt};
                    auto hms = make_time(floor<seconds>(tp - floor<days>(tp)));
                    tm.tm_sec = static_cast<int>(hms.seconds().count());
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                    modified = CharT{};
                }
                else if (modified == CharT{})
                {
                    auto fs = (tp - floor<days>(tp)) % minutes{1};
                    os << detail::decimal_format_seconds<decltype(fs)>(fs);
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 't':
            if (command)
            {
                if (modified == CharT{})
                    os << CharT{'\t'};
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'T':
            if (command)
            {
                if (modified == CharT{})
                {
                    using CT = typename common_type<seconds, Duration>::type;
                    os << time_of_day<CT>{tp - floor<days>(tp)};
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'u':
            if (command)
            {
                auto wd = static_cast<unsigned>(weekday{floor<days>(tp)});
                if (modified == CharT{'O'})
                {
                    const CharT f[] = {'%', modified, *fmt};
                    tm.tm_wday = static_cast<int>(wd);
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                    modified = CharT{};
                }
                else if (modified == CharT{})
                {
                    os << (wd != 0 ? wd : 7u);
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'U':
            if (command)
            {
                auto ld = floor<days>(tp);
                auto ymd = year_month_day{ld};
                if (modified == CharT{'O'})
                {
                    const CharT f[] = {'%', modified, *fmt};
                    tm.tm_year = static_cast<int64_t>(ymd.year()) - 1900;
                    tm.tm_wday = static_cast<int>(static_cast<unsigned>(weekday{ld}));
                    tm.tm_yday = static_cast<int>((ld - local_days(ymd.year()/1/1)).count());
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                    modified = CharT{};
                }
                else if (modified == CharT{})
                {
                    auto st = local_days{sun[1]/jan/ymd.year()};
                    if (ld < st)
                        os << CharT{'0'} << CharT{'0'};
                    else
                    {
                        auto wn = duration_cast<weeks>(ld - st).count() + 1;
                        if (wn < 10)
                            os << CharT{'0'};
                        os << wn;
                    }
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'V':
            if (command)
            {
                auto ld = floor<days>(tp);
                if (modified == CharT{'O'})
                {
                    const CharT f[] = {'%', modified, *fmt};
                    auto ymd = year_month_day{ld};
                    tm.tm_year = static_cast<int64_t>(ymd.year()) - 1900;
                    tm.tm_wday = static_cast<int>(static_cast<unsigned>(weekday{ld}));
                    tm.tm_yday = static_cast<int>((ld - local_days(ymd.year()/1/1)).count());
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                    modified = CharT{};
                }
                else if (modified == CharT{})
                {
                    auto y = year_month_day{ld + days{3}}.year();
                    auto st = local_days{(y - years{1})/12/thu[last]} + (mon-thu);
                    if (ld < st)
                    {
                        --y;
                        st = local_days{(y - years{1})/12/thu[last]} + (mon-thu);
                    }
                    auto wn = duration_cast<weeks>(ld - st).count() + 1;
                    if (wn < 10)
                        os << CharT{'0'};
                    os << wn;
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'w':
            if (command)
            {
                auto wd = static_cast<unsigned>(weekday{floor<days>(tp)});
                if (modified == CharT{'O'})
                {
                    const CharT f[] = {'%', modified, *fmt};
                    tm.tm_wday = static_cast<int>(wd);
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                    modified = CharT{};
                }
                else if (modified == CharT{})
                {
                    os << wd;
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'W':
            if (command)
            {
                auto ld = floor<days>(tp);
                auto ymd = year_month_day{ld};
                if (modified == CharT{'O'})
                {
                    const CharT f[] = {'%', modified, *fmt};
                    tm.tm_year = static_cast<int64_t>(ymd.year()) - 1900;
                    tm.tm_wday = static_cast<int>(static_cast<unsigned>(weekday{ld}));
                    tm.tm_yday = static_cast<int>((ld - local_days(ymd.year()/1/1)).count());
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                    modified = CharT{};
                }
                else if (modified == CharT{})
                {
                    auto st = local_days{mon[1]/jan/ymd.year()};
                    if (ld < st)
                        os << CharT{'0'} << CharT{'0'};
                    else
                    {
                        auto wn = duration_cast<weeks>(ld - st).count() + 1;
                        if (wn < 10)
                            os << CharT{'0'};
                        os << wn;
                    }
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'y':
            if (command)
            {
                auto y = static_cast<int64_t>(year_month_day{floor<days>(tp)}.year());
                if (modified != CharT{})
                {
                    const CharT f[] = {'%', modified, *fmt};
                    tm.tm_year = y - 1900;
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                    modified = CharT{};
                }
                else if (modified == CharT{})
                {
                    y = std::abs(y) % 100;
                    if (y < 10)
                        os << CharT{'0'};
                    os << y;
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'Y':
            if (command)
            {
                auto y = year_month_day{floor<days>(tp)}.year();
                if (modified == CharT{'E'})
                {
                    const CharT f[] = {'%', modified, *fmt};
                    tm.tm_year = static_cast<int64_t>(y) - 1900;
                    facet.put(os, os, os.fill(), &tm, begin(f), end(f));
                    modified = CharT{};
                }
                else if (modified == CharT{})
                {
                    os << y;
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'z':
            if (command)
            {
                if (offset_sec == nullptr)
                    throw std::runtime_error("Can not format local_time with %z");
                auto m = duration_cast<minutes>(*offset_sec);
                auto neg = m < minutes{0};
                m = abs(m);
                auto h = duration_cast<hours>(m);
                m -= h;
                if (neg)
                    os << CharT{'-'};
                else
                    os << CharT{'+'};
                if (h < hours{10})
                    os << CharT{'0'};
                os << h.count();
                if (modified != CharT{})
                    os << CharT{':'};
                if (m < minutes{10})
                    os << CharT{'0'};
                os << m.count();
                command = false;
                modified = CharT{};
            }
            else
                os << *fmt;
            break;
        case 'Z':
            if (command)
            {
                if (modified == CharT{})
                {
                    if (abbrev == nullptr)
                        throw std::runtime_error("Can not format local_time with %Z");
                    for (auto c : *abbrev)
                        os << CharT{c};
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    modified = CharT{};
                }
                command = false;
            }
            else
                os << *fmt;
            break;
        case 'E':
        case 'O':
            if (command)
            {
                if (modified == CharT{})
                {
                    modified = *fmt;
                }
                else
                {
                    os << CharT{'%'} << modified << *fmt;
                    command = false;
                    modified = CharT{};
                }
            }
            else
                os << *fmt;
            break;
        case '%':
            if (command)
            {
                if (modified == CharT{})
                {
                    os << CharT{'%'};
                    command = false;
                }
                else
                {
                    os << CharT{'%'} << modified << CharT{'%'};
                    command = false;
                    modified = CharT{};
                }
            }
            else
                command = true;
            break;
        default:
            if (command)
            {
                os << CharT{'%'};
                command = false;
            }
            if (modified != CharT{})
            {
                os << modified;
                modified = CharT{};
            }
            os << *fmt;
            break;
        }
    }
    if (command)
        os << CharT{'%'};
    if (modified != CharT{})
        os << modified;
}
template <class CharT, class Traits, class Duration>
void
to_stream(std::basic_ostream<CharT, Traits>& os, const CharT* fmt,
          const sys_time<Duration>& tp)
{
    const std::string abbrev("UTC");
    CONSTDATA std::chrono::seconds offset{0};
    to_stream(os, fmt, local_time<Duration>{tp.time_since_epoch()}, &abbrev, &offset);
}
// format
// const CharT* formats
template <class CharT, class Duration>
std::basic_string<CharT>
format(const std::locale& loc, const CharT* fmt, const local_time<Duration>& tp)
{
    std::basic_ostringstream<CharT> os;
    os.imbue(loc);
    to_stream(os, fmt, tp);
    return os.str();
}
template <class CharT, class Duration>
std::basic_string<CharT>
format(const CharT* fmt, const local_time<Duration>& tp)
{
    std::basic_ostringstream<CharT> os;
    to_stream(os, fmt, tp);
    return os.str();
}
template <class CharT, class Duration>
std::basic_string<CharT>
format(const std::locale& loc, const CharT* fmt, const sys_time<Duration>& tp)
{
    std::basic_ostringstream<CharT> os;
    os.imbue(loc);
    to_stream(os, fmt, tp);
    return os.str();
}
template <class CharT, class Duration>
std::basic_string<CharT>
format(const CharT* fmt, const sys_time<Duration>& tp)
{
    std::basic_ostringstream<CharT> os;
    to_stream(os, fmt, tp);
    return os.str();
}
// basic_string formats
template <class CharT, class Traits, class Duration>
std::basic_string<CharT, Traits>
format(const std::locale& loc, const std::basic_string<CharT, Traits>& fmt,
       const local_time<Duration>& tp)
{
    std::basic_ostringstream<CharT, Traits> os;
    os.imbue(loc);
    to_stream(os, fmt.c_str(), tp);
    return os.str();
}
template <class CharT, class Traits, class Duration>
std::basic_string<CharT, Traits>
format(const std::basic_string<CharT, Traits>& fmt, const local_time<Duration>& tp)
{
    std::basic_ostringstream<CharT, Traits> os;
    to_stream(os, fmt.c_str(), tp);
    return os.str();
}
template <class CharT, class Traits, class Duration>
std::basic_string<CharT, Traits>
format(const std::locale& loc, const std::basic_string<CharT, Traits>& fmt,
       const sys_time<Duration>& tp)
{
    std::basic_ostringstream<CharT, Traits> os;
    os.imbue(loc);
    to_stream(os, fmt.c_str(), tp);
    return os.str();
}
template <class CharT, class Traits, class Duration>
std::basic_string<CharT, Traits>
format(const std::basic_string<CharT, Traits>& fmt, const sys_time<Duration>& tp)
{
    std::basic_ostringstream<CharT, Traits> os;
    to_stream(os, fmt.c_str(), tp);
    return os.str();
}
// parse
namespace detail
{
template <class CharT, class Traits>
bool
read_char(std::basic_istream<CharT, Traits>& is, CharT fmt, std::ios::iostate& err)
{
    auto ic = is.get();
    if (Traits::eq_int_type(ic, Traits::eof()) ||
       !Traits::eq(Traits::to_char_type(ic), fmt))
    {
        err |= std::ios::failbit;
        is.setstate(std::ios::failbit);
        return false;
    }
    return true;
}
template <class CharT, class Traits>
unsigned
read_unsigned(std::basic_istream<CharT, Traits>& is, unsigned m = 1, unsigned M = 10)
{
    unsigned x = 0;
    unsigned count = 0;
    while (true)
    {
        auto ic = is.peek();
        if (Traits::eq_int_type(ic, Traits::eof()))
            break;
        auto c = static_cast<char>(Traits::to_char_type(ic));
        if (!('0' <= c && c <= '9'))
            break;
        (void)is.get();
        ++count;
        x = 10*x + (c - '0');
        if (count == M)
            break;
    }
    if (count < m)
        is.setstate(std::ios::failbit);
    return x;
}
template <class CharT, class Traits>
int
read_signed(std::basic_istream<CharT, Traits>& is, unsigned m = 1, unsigned M = 10)
{
    auto ic = is.peek();
    if (!Traits::eq_int_type(ic, Traits::eof()))
    {
        auto c = static_cast<char>(Traits::to_char_type(ic));
        if (('0' <= c && c <= '9') || c == '-' || c == '+')
        {
            if (c == '-' || c == '+')
                (void)is.get();
            auto x = static_cast<int>(read_unsigned(is, m, M));
            if (!is.fail())
            {
                if (c == '-')
                    x = -x;
                return x;
            }
        }
    }
    is.setstate(std::ios::failbit);
    return 0;
}
template <class CharT, class Traits>
long double
read_long_double(std::basic_istream<CharT, Traits>& is, unsigned m = 1, unsigned M = 10)
{
    using namespace std;
    unsigned count = 0;
    auto decimal_point = Traits::to_int_type(
        use_facet<numpunct<CharT>>(is.getloc()).decimal_point());
    string buf;
    while (true)
    {
        auto ic = is.peek();
        if (Traits::eq_int_type(ic, Traits::eof()))
            break;
        if (Traits::eq_int_type(ic, decimal_point))
        {
            buf += '.';
            decimal_point = Traits::eof();
            is.get();
        }
        else
        {
            auto c = static_cast<char>(Traits::to_char_type(ic));
            if (!('0' <= c && c <= '9'))
                break;
            buf += c;
            (void)is.get();
            ++count;
        }
        if (count == M)
            break;
    }
    if (count < m)
        is.setstate(std::ios::failbit);
    return std::stold(buf);
}
struct rs
{
    int& i;
    unsigned m;
    unsigned M;
};
struct ru
{
    int& i;
    unsigned m;
    unsigned M;
};
struct rld
{
    long double& i;
    unsigned m;
    unsigned M;
};
template <class CharT, class Traits>
void
read(std::basic_istream<CharT, Traits>&)
{
}
template <class CharT, class Traits, class ...Args>
void
read(std::basic_istream<CharT, Traits>& is, CharT a0, Args&& ...args);
template <class CharT, class Traits, class ...Args>
void
read(std::basic_istream<CharT, Traits>& is, rs a0, Args&& ...args);
template <class CharT, class Traits, class ...Args>
void
read(std::basic_istream<CharT, Traits>& is, ru a0, Args&& ...args);
template <class CharT, class Traits, class ...Args>
void
read(std::basic_istream<CharT, Traits>& is, int a0, Args&& ...args);
template <class CharT, class Traits, class ...Args>
void
read(std::basic_istream<CharT, Traits>& is, rld a0, Args&& ...args);
template <class CharT, class Traits, class ...Args>
void
read(std::basic_istream<CharT, Traits>& is, CharT a0, Args&& ...args)
{
    if (a0 != CharT{})
    {
        auto ic = is.peek();
        if (Traits::eq_int_type(ic, Traits::eof()))
            return;
        if (!Traits::eq(Traits::to_char_type(ic), a0))
        {
            is.setstate(std::ios::failbit);
            return;
        }
        (void)is.get();
    }
    else
    {
        while (isspace(is.peek()))
            (void)is.get();
    }
    read(is, std::forward<Args>(args)...);
}
template <class CharT, class Traits, class ...Args>
void
read(std::basic_istream<CharT, Traits>& is, rs a0, Args&& ...args)
{
    auto x = read_signed(is, a0.m, a0.M);
    if (is.fail())
        return;
    a0.i = x;
    read(is, std::forward<Args>(args)...);
}
template <class CharT, class Traits, class ...Args>
void
read(std::basic_istream<CharT, Traits>& is, ru a0, Args&& ...args)
{
    auto x = read_unsigned(is, a0.m, a0.M);
    if (is.fail())
        return;
    a0.i = static_cast<int>(x);
    read(is, std::forward<Args>(args)...);
}
template <class CharT, class Traits, class ...Args>
void
read(std::basic_istream<CharT, Traits>& is, int a0, Args&& ...args)
{
    if (a0 != -1)
    {
        auto u = static_cast<unsigned>(a0);
        CharT buf[std::numeric_limits<unsigned>::digits10+2] = {};
        auto e = buf;
        do
        {
            *e++ = CharT(u % 10) + CharT{'0'};
            u /= 10;
        } while (u > 0);
        std::reverse(buf, e);
        for (auto p = buf; p != e && is.rdstate() == std::ios::goodbit; ++p)
            read(is, *p);
    }
    if (is.rdstate() == std::ios::goodbit)
        read(is, std::forward<Args>(args)...);
}
template <class CharT, class Traits, class ...Args>
void
read(std::basic_istream<CharT, Traits>& is, rld a0, Args&& ...args)
{
    auto x = read_long_double(is, a0.m, a0.M);
    if (is.fail())
        return;
    a0.i = x;
    read(is, std::forward<Args>(args)...);
}
template <class CharT, class Traits, class Duration>
void
parse(std::basic_istream<CharT, Traits>& is,
      const CharT* fmt, local_time<Duration>& tp,
      std::basic_string<CharT, Traits>* abbrev = nullptr,
      std::chrono::minutes* offset = nullptr)
{
    using namespace std;
    using namespace std::chrono;
    typename basic_istream<CharT, Traits>::sentry ok{is};
    if (ok)
    {
        auto& f = use_facet<time_get<CharT>>(is.getloc());
        std::tm tm{};
        std::basic_string<CharT, Traits> temp_abbrev;
        minutes temp_offset{};
        const CharT* command = nullptr;
        auto modified = CharT{};
        auto width = -1;
        constexpr int not_a_year = 33000;
        int Y = not_a_year;
        constexpr int not_a_century = not_a_year / 100;
        int C = not_a_century;
        constexpr int not_a_2digit_year = 100;
        int y = not_a_2digit_year;
        int m{};
        int d{};
        int j{};
        constexpr int not_a_weekday = 7;
        int wd = not_a_weekday;
        constexpr int not_a_hour_12_value = 0;
        int I = not_a_hour_12_value;
        hours h{};
        minutes min{};
        Duration s{};
        int g = not_a_2digit_year;
        int G = not_a_year;
        constexpr int not_a_week_num = 100;
        int V = not_a_week_num;
        int U = not_a_week_num;
        int W = not_a_week_num;
        using detail::read;
        using detail::rs;
        using detail::ru;
        using detail::rld;
        for (; *fmt && is.rdstate() == std::ios::goodbit; ++fmt)
        {
            if (isspace(*fmt))
            {
                // space matches 0 or more white space characters
                ws(is);
                continue;
            }
            switch (*fmt)
            {
            case 'a':
            case 'A':
                if (command)
                {
                    ios_base::iostate err = ios_base::goodbit;
                    f.get(is, 0, is, err, &tm, command, fmt+1);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                    if ((err & ios::failbit) == 0)
                        wd = tm.tm_wday;
                    is.setstate(err);
                }
                else
                    read(is, *fmt);
                break;
            case 'b':
            case 'B':
            case 'h':
                if (command)
                {
                    ios_base::iostate err = ios_base::goodbit;
                    f.get(is, 0, is, err, &tm, command, fmt+1);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                    if ((err & ios::failbit) == 0)
                        m = tm.tm_mon + 1;
                    is.setstate(err);
                }
                else
                    read(is, *fmt);
                break;
            case 'c':
                if (command)
                {
                    ios_base::iostate err = ios_base::goodbit;
                    f.get(is, 0, is, err, &tm, command, fmt+1);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                    if ((err & ios::failbit) == 0)
                    {
                        Y = tm.tm_year + 1900;
                        m = tm.tm_mon + 1;
                        d = tm.tm_mday;
                        h = hours{tm.tm_hour};
                        min = minutes{tm.tm_min};
                        s = duration_cast<Duration>(seconds{tm.tm_sec});
                    }
                    is.setstate(err);
                }
                else
                    read(is, *fmt);
                break;
            case 'x':
                if (command)
                {
                    ios_base::iostate err = ios_base::goodbit;
                    f.get(is, 0, is, err, &tm, command, fmt+1);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                    if ((err & ios::failbit) == 0)
                    {
                        Y = tm.tm_year + 1900;
                        m = tm.tm_mon + 1;
                        d = tm.tm_mday;
                    }
                    is.setstate(err);
                }
                else
                    read(is, *fmt);
                break;
            case 'X':
                if (command)
                {
                    ios_base::iostate err = ios_base::goodbit;
                    f.get(is, 0, is, err, &tm, command, fmt+1);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                    if ((err & ios::failbit) == 0)
                    {
                        h = hours{tm.tm_hour};
                        min = minutes{tm.tm_min};
                        s = duration_cast<Duration>(seconds{tm.tm_sec});
                    }
                    is.setstate(err);
                }
                else
                    read(is, *fmt);
                break;
            case 'C':
                if (command)
                {
                    if (modified == CharT{})
                    {
                        read(is, rs{C, 1, width == -1 ? 2u : width});
                    }
                    else
                    {
                        ios_base::iostate err = ios_base::goodbit;
                        f.get(is, 0, is, err, &tm, command, fmt+1);
                        if ((err & ios::failbit) == 0)
                        {
                            auto tY = tm.tm_year + 1900;
                            C = (tY >= 0 ? tY : tY-99) / 100;
                        }
                        is.setstate(err);
                    }
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'D':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, ru{m, 1, 2}, CharT{'\0'}, CharT{'/'}, CharT{'\0'},
                                 ru{d, 1, 2}, CharT{'\0'}, CharT{'/'}, CharT{'\0'},
                                 rs{y, 1, 2});
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'F':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, rs{Y, 1, width == -1 ? 4u : width}, CharT{'-'},
                                 ru{m, 1, 2}, CharT{'-'}, ru{d, 1, 2});
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'd':
            case 'e':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, rs{d, 1, width == -1 ? 2u : width});
                    else if (modified == CharT{'O'})
                    {
                        ios_base::iostate err = ios_base::goodbit;
                        f.get(is, 0, is, err, &tm, command, fmt+1);
                        command = nullptr;
                        width = -1;
                        modified = CharT{};
                        if ((err & ios::failbit) == 0)
                            d = tm.tm_mday;
                        is.setstate(err);
                    }
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'H':
                if (command)
                {
                    if (modified == CharT{})
                    {
                        int H;
                        read(is, ru{H, 1, width == -1 ? 2u : width});
                        if (!is.fail())
                            h = hours{H};
                    }
                    else if (modified == CharT{'O'})
                    {
                        ios_base::iostate err = ios_base::goodbit;
                        f.get(is, 0, is, err, &tm, command, fmt+1);
                        if ((err & ios::failbit) == 0)
                            h = hours{tm.tm_hour};
                        is.setstate(err);
                    }
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'I':
                if (command)
                {
                    if (modified == CharT{})
                    {
                        // reads in an hour into I, but most be in [1, 12]
                        read(is, rs{I, 1, width == -1 ? 2u : width});
                        if (I != not_a_hour_12_value)
                        {
                            if (!(1 <= I && I <= 12))
                            {
                                I = not_a_hour_12_value;
                                goto broken;
                            }
                        }
                    }
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
               break;
            case 'j':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, ru{j, 1, width == -1 ? 3u : width});
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'M':
                if (command)
                {
                    if (modified == CharT{})
                    {
                        int M;
                        read(is, ru{M, 1, width == -1 ? 2u : width});
                        if (!is.fail())
                            min = minutes{M};
                    }
                    else if (modified == CharT{'O'})
                    {
                        ios_base::iostate err = ios_base::goodbit;
                        f.get(is, 0, is, err, &tm, command, fmt+1);
                        if ((err & ios::failbit) == 0)
                            min = minutes{tm.tm_min};
                        is.setstate(err);
                    }
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'm':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, rs{m, 1, width == -1 ? 2u : width});
                    else if (modified == CharT{'O'})
                    {
                        ios_base::iostate err = ios_base::goodbit;
                        f.get(is, 0, is, err, &tm, command, fmt+1);
                        command = nullptr;
                        width = -1;
                        modified = CharT{};
                        if ((err & ios::failbit) == 0)
                            m = tm.tm_mon + 1;
                        is.setstate(err);
                    }
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'n':
            case 't':
                if (command)
                {
                    // %n and %t match 1 or more white space characters
                    // consecutive %n and %t count as one 
                    auto ic = is.peek();
                    if (Traits::eq_int_type(ic, Traits::eof()))
                        break;
                    if (!isspace(ic))
                    {
                        is.setstate(ios::failbit);
                        break;
                    }
                    ws(is);
                    for (++fmt; *fmt == 'n' || *fmt == 't'; ++fmt)
                        ;
                    --fmt;
                }
                else
                    read(is, *fmt);
                break;
            case 'p':
                // Error if haven't yet seen %I
                if (command)
                {
                    if (modified == CharT{})
                    {
                        if (I == not_a_hour_12_value)
                            goto broken;
                        tm.tm_hour = I;
                        ios_base::iostate err = ios_base::goodbit;
                        f.get(is, 0, is, err, &tm, command, fmt+1);
                        if (!(err & ios::failbit))
                        {
                            h = hours{tm.tm_hour};
                            I = not_a_hour_12_value;
                        }
                    }
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
               break;
            case 'r':
                if (command)
                {
                    ios_base::iostate err = ios_base::goodbit;
                    f.get(is, 0, is, err, &tm, command, fmt+1);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                    if ((err & ios::failbit) == 0)
                    {
                        h = hours{tm.tm_hour};
                        min = minutes{tm.tm_min};
                        s = duration_cast<Duration>(seconds{tm.tm_sec});
                    }
                    is.setstate(err);
                }
                else
                    read(is, *fmt);
                break;
            case 'R':
                if (command)
                {
                    if (modified == CharT{})
                    {
                        int H, M;
                        read(is, ru{H, 1, 2}, CharT{'\0'}, CharT{':'}, CharT{'\0'},
                                 ru{M, 1, 2}, CharT{'\0'});
                        if (!is.fail())
                        {
                            h = hours{H};
                            min = minutes{M};
                        }
                    }
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'S':
                if (command)
                {
                    if (modified == CharT{})
                    {
                        using dfs = detail::decimal_format_seconds<Duration>;
                        CONSTDATA auto w = Duration::period::den == 1 ? 2 : 3 + dfs::width;
                        long double S;
                        read(is, rld{S, 1, width == -1 ? w : width});
                        if (!is.fail())
                            s = round<Duration>(duration<long double>{S});
                    }
                    else if (modified == CharT{'O'})
                    {
                        ios_base::iostate err = ios_base::goodbit;
                        f.get(is, 0, is, err, &tm, command, fmt+1);
                        if ((err & ios::failbit) == 0)
                            s = duration_cast<Duration>(seconds{tm.tm_sec});
                        is.setstate(err);
                    }
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'T':
                if (command)
                {
                    if (modified == CharT{})
                    {
                        using dfs = detail::decimal_format_seconds<Duration>;
                        CONSTDATA auto w = Duration::period::den == 1 ? 2 : 3 + dfs::width;
                        int H;
                        int M;
                        long double S;
                        read(is, ru{H, 1, 2}, CharT{':'}, ru{M, 1, 2},
                                              CharT{':'}, rld{S, 1, w});
                        if (!is.fail())
                        {
                            h = hours{H};
                            min = minutes{M};
                            s = round<Duration>(duration<long double>{S});
                        }
                    }
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'Y':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, rs{Y, 1, width == -1 ? 4u : width});
                    else if (modified == CharT{'E'})
                    {
                        ios_base::iostate err = ios_base::goodbit;
                        f.get(is, 0, is, err, &tm, command, fmt+1);
                        command = nullptr;
                        width = -1;
                        modified = CharT{};
                        if ((err & ios::failbit) == 0)
                            Y = tm.tm_year + 1900;
                        is.setstate(err);
                    }
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'y':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, ru{y, 1, width == -1 ? 2u : width});
                    else
                    {
                        ios_base::iostate err = ios_base::goodbit;
                        f.get(is, 0, is, err, &tm, command, fmt+1);
                        if ((err & ios::failbit) == 0)
                            Y = tm.tm_year + 1900;
                        is.setstate(err);
                    }
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'g':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, ru{g, 1, width == -1 ? 2u : width});
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'G':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, rs{G, 1, width == -1 ? 4u : width});
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'U':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, ru{U, 1, width == -1 ? 2u : width});
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'V':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, ru{V, 1, width == -1 ? 2u : width});
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'W':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, ru{W, 1, width == -1 ? 2u : width});
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'u':
            case 'w':
                if (command)
                {
                    if (modified == CharT{})
                    {
                        read(is, ru{wd, 1, width == -1 ? 1u : width});
                        if (!is.fail() && *fmt == 'u')
                        {
                            if (wd == 7)
                                wd = 0;
                        }
                    }
                    else if (modified == CharT{'O'})
                    {
                        ios_base::iostate err = ios_base::goodbit;
                        f.get(is, 0, is, err, &tm, command, fmt+1);
                        if ((err & ios::failbit) == 0)
                            wd = tm.tm_wday;
                        is.setstate(err);
                    }
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'E':
            case 'O':
                if (command)
                {
                    if (modified == CharT{})
                    {
                        modified = *fmt;
                    }
                    else
                    {
                        read(is, CharT{'%'}, width, modified, *fmt);
                        command = nullptr;
                        width = -1;
                        modified = CharT{};
                    }
                }
                else
                    read(is, *fmt);
                break;
            case '%':
                if (command)
                {
                    if (modified == CharT{})
                        read(is, *fmt);
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    command = fmt;
                break;
            case 'z':
                if (command)
                {
                    int H, M;
                    if (modified == CharT{})
                        read(is, rs{H, 2, 2}, ru{M, 2, 2});
                    else
                        read(is, rs{H, 2, 2}, CharT{':'}, ru{M, 2, 2});
                    if (!is.fail())
                        temp_offset = hours{H} + minutes{M};
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            case 'Z':
                if (command)
                {
                    if (modified == CharT{})
                        is >> temp_abbrev;
                    else
                        read(is, CharT{'%'}, width, modified, *fmt);
                    command = nullptr;
                    width = -1;
                    modified = CharT{};
                }
                else
                    read(is, *fmt);
                break;
            default:
                if (command)
                {
                    if (width == -1 && modified == CharT{} && '0' <= *fmt && *fmt <= '9')
                    {
                        width = static_cast<char>(*fmt) - '0';
                        while ('0' <= fmt[1] && fmt[1] <= '9')
                            width = 10*width + static_cast<char>(*++fmt) - '0';
                    }
                    else
                    {
                        if (modified == CharT{})
                            read(is, CharT{'%'}, width, *fmt);
                        else
                            read(is, CharT{'%'}, width, modified, *fmt);
                        command = nullptr;
                        width = -1;
                        modified = CharT{};
                    }
                }
                else
                    read(is, *fmt);
                break;
            }
        }
        // is.rdstate() != ios::goodbit || *fmt == CharT{}
        if (is.rdstate() == ios::goodbit && command)
        {
            if (modified == CharT{})
                read(is, CharT{'%'}, width);
            else
                read(is, CharT{'%'}, width, modified);
        }
        if (!is.fail())
        {
            if (y != not_a_2digit_year)
            {
                if (!(0 <= y && y <= 99))
                    goto broken;
                if (C == not_a_century)
                {
                    if (Y == not_a_year)
                    {
                        if (y >= 69)
                            C = 19;
                        else
                            C = 20;
                    }
                    else
                    {
                        C = (Y >= 0 ? Y : Y-100) / 100;
                    }
                }
                int tY;
                if (C >= 0)
                    tY = 100*C + y;
                else
                    tY = 100*(C+1) - (y == 0 ? 100 : y);
                if (Y != not_a_year && Y != tY)
                    goto broken;
                Y = tY;
            }
            if (g != not_a_2digit_year)
            {
                if (!(0 <= g && g <= 99))
                    goto broken;
                if (C == not_a_century)
                {
                    if (G == not_a_year)
                    {
                        if (g >= 69)
                            C = 19;
                        else
                            C = 20;
                    }
                    else
                    {
                        C = (G >= 0 ? G : G-100) / 100;
                    }
                }
                int tG;
                if (C >= 0)
                    tG = 100*C + g;
                else
                    tG = 100*(C+1) - (g == 0 ? 100 : g);
                if (G != not_a_year && G != tG)
                    goto broken;
                G = tG;
            }
            if (G != not_a_year)
            {
                if (V == not_a_week_num || wd == not_a_weekday)
                    goto broken;
                auto ymd = year_month_day{local_days{year{G-1}/dec/thu[last]} +
                                          (mon-thu) + weeks{V-1} +
                                          (weekday{static_cast<unsigned>(wd)}-mon)};
                if (Y == not_a_year)
                    Y = static_cast<int64_t>(ymd.year());
                else if (year{Y} != ymd.year())
                    goto broken;
                if (m == 0)
                    m = static_cast<int>(static_cast<unsigned>(ymd.month()));
                else if (month(m) != ymd.month())
                    goto broken;
                if (d == 0)
                    d = static_cast<int>(static_cast<unsigned>(ymd.day()));
                else if (day(d) != ymd.day())
                    goto broken;
            }
            if (Y != not_a_year)
            {
                if (!std::in_range<int64_t>(Y))
                    goto broken;
                if (j != 0)
                {
                    auto ymd = year_month_day{local_days{year{Y}/1/1} + days{j-1}};
                    if (m == 0)
                        m = static_cast<int>(static_cast<unsigned>(ymd.month()));
                    else if (month(m) != ymd.month())
                        goto broken;
                    if (d == 0)
                        d = static_cast<int>(static_cast<unsigned>(ymd.day()));
                    else if (day(d) != ymd.day())
                        goto broken;
                }
                if (U != not_a_week_num)
                {
                    if (wd == not_a_weekday)
                        goto broken;
                    sys_days sd;
                    if (U == 0)
                        sd = year{Y-1}/dec/weekday{static_cast<unsigned>(wd)}[last];
                    else
                        sd = sys_days{year{Y}/jan/sun[1]} + weeks{U-1} + 
                             (weekday{static_cast<unsigned>(wd)} - sun);
                    year_month_day ymd = sd;
                    if (year{Y} != ymd.year())
                        goto broken;
                    if (m == 0)
                        m = static_cast<int>(static_cast<unsigned>(ymd.month()));
                    else if (month(m) != ymd.month())
                        goto broken;
                    if (d == 0)
                        d = static_cast<int>(static_cast<unsigned>(ymd.day()));
                    else if (day(d) != ymd.day())
                        goto broken;
                }
                if (W != not_a_week_num)
                {
                    if (wd == not_a_weekday)
                        goto broken;
                    sys_days sd;
                    if (W == 0)
                        sd = year{Y-1}/dec/weekday{static_cast<unsigned>(wd)}[last];
                    else
                        sd = sys_days{year{Y}/jan/mon[1]} + weeks{W-1} + 
                             (weekday{static_cast<unsigned>(wd)} - mon);
                    year_month_day ymd = sd;
                    if (year{Y} != ymd.year())
                        goto broken;
                    if (m == 0)
                        m = static_cast<int>(static_cast<unsigned>(ymd.month()));
                    else if (month(m) != ymd.month())
                        goto broken;
                    if (d == 0)
                        d = static_cast<int>(static_cast<unsigned>(ymd.day()));
                    else if (day(d) != ymd.day())
                        goto broken;
                }
                if (m != 0 && d != 0)
                {
                    auto ymd = year{Y}/m/d;
                    if (!ymd.ok())
                        goto broken;
                    auto ld = local_days{ymd};
                    if (wd != not_a_weekday &&
                            weekday{static_cast<unsigned>(wd)} != weekday{ld})
                        goto broken;
                    tp = local_time<Duration>{floor<Duration>(ld + h + min + s)};
                }
                else
                    goto broken;
            }
            else  // did not parse a year
            {
                goto broken;
            }
            if (abbrev != nullptr)
                *abbrev = std::move(temp_abbrev);
            if (offset != nullptr)
                *offset = temp_offset;
        }
        return;
    }
broken:
    is.setstate(ios_base::failbit);
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is, const CharT* fmt, local_time<Duration>& tp,
      std::chrono::minutes* offset)
{
    parse(is, fmt, tp, static_cast<std::basic_string<CharT, Traits>*>(nullptr), offset);
}
template <class Duration, class CharT, class Traits = std::char_traits<CharT>>
struct parse_local_manip
{
    const std::basic_string<CharT, Traits> format_;
    local_time<Duration>&                  tp_;
    std::basic_string<CharT, Traits>*      abbrev_;
    std::chrono::minutes*                  offset_;
public:
    parse_local_manip(std::basic_string<CharT, Traits> format,
                      local_time<Duration>& tp, std::basic_string<CharT, Traits>* abbrev = nullptr,
                      std::chrono::minutes* offset = nullptr)
        : format_(std::move(format))
        , tp_(tp)
        , abbrev_(abbrev)
        , offset_(offset)
        {}
};
template <class Duration, class CharT, class Traits>
std::basic_istream<CharT, Traits>&
operator>>(std::basic_istream<CharT, Traits>& is,
           const parse_local_manip<Duration, CharT, Traits>& x)
{
    parse(is, x.format_.c_str(), x.tp_, x.abbrev_, x.offset_);
    return is;
}
template <class Duration, class CharT, class Traits = std::char_traits<CharT>>
struct parse_sys_manip
{
    const std::basic_string<CharT, Traits> format_;
    sys_time<Duration>&                    tp_;
    std::basic_string<CharT, Traits>*      abbrev_;
    std::chrono::minutes*                  offset_;
public:
    parse_sys_manip(std::basic_string<CharT, Traits> format,
                    sys_time<Duration>& tp, std::basic_string<CharT, Traits>* abbrev = nullptr,
                    std::chrono::minutes* offset = nullptr)
        : format_(std::move(format))
        , tp_(tp)
        , abbrev_(abbrev)
        , offset_(offset)
        {}
};
template <class Duration, class CharT, class Traits>
std::basic_istream<CharT, Traits>&
operator>>(std::basic_istream<CharT, Traits>& is,
           const parse_sys_manip<Duration, CharT, Traits>& x)
{
    std::chrono::minutes offset{};
    auto offptr = x.offset_ ? x.offset_ : &offset;
    local_time<Duration> lt;
    parse(is, x.format_.c_str(), lt, x.abbrev_, offptr);
    if (!is.fail())
        x.tp_ = sys_time<Duration>{floor<Duration>(lt - *offptr).time_since_epoch()};
    return is;
}
}  // namespace detail
template <class Duration, class CharT, class Traits>
inline
detail::parse_sys_manip<Duration, CharT, Traits>
parse(const std::basic_string<CharT, Traits>& format, sys_time<Duration>& tp)
{
    return {format, tp};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_sys_manip<Duration, CharT, Traits>
parse(const std::basic_string<CharT, Traits>& format, sys_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev)
{
    return {format, tp, &abbrev};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_sys_manip<Duration, CharT, Traits>
parse(const std::basic_string<CharT, Traits>& format, sys_time<Duration>& tp,
      std::chrono::minutes& offset)
{
    return {format, tp, nullptr, &offset};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_sys_manip<Duration, CharT, Traits>
parse(const std::basic_string<CharT, Traits>& format, sys_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev, std::chrono::minutes& offset)
{
    return {format, tp, &abbrev, &offset};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_sys_manip<Duration, CharT, Traits>
parse(const std::basic_string<CharT, Traits>& format, sys_time<Duration>& tp,
      std::chrono::minutes& offset, std::basic_string<CharT, Traits>& abbrev)
{
    return {format, tp, &abbrev, &offset};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_local_manip<Duration, CharT, Traits>
parse(const std::basic_string<CharT, Traits>& format, local_time<Duration>& tp)
{
    return {format, tp};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_local_manip<Duration, CharT, Traits>
parse(const std::basic_string<CharT, Traits>& format, local_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev)
{
    return {format, tp, &abbrev};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_local_manip<Duration, CharT, Traits>
parse(const std::basic_string<CharT, Traits>& format, local_time<Duration>& tp,
      std::chrono::minutes& offset)
{
    return {format, tp, nullptr, &offset};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_local_manip<Duration, CharT, Traits>
parse(const std::basic_string<CharT, Traits>& format, local_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev, std::chrono::minutes& offset)
{
    return {format, tp, &abbrev, &offset};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_local_manip<Duration, CharT, Traits>
parse(const std::basic_string<CharT, Traits>& format, local_time<Duration>& tp,
      std::chrono::minutes& offset, std::basic_string<CharT, Traits>& abbrev)
{
    return {format, tp, &abbrev, &offset};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is,
      const std::basic_string<CharT, Traits>& format, sys_time<Duration>& tp)
{
    std::chrono::minutes offset{};
    local_time<Duration> lt;
    detail::parse(is, format.c_str(), lt, &offset);
    if (!is.fail())
        tp = sys_time<Duration>{floor<Duration>(lt - offset).time_since_epoch()};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is,
      const std::basic_string<CharT, Traits>& format, sys_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev)
{
    std::chrono::minutes offset{};
    local_time<Duration> lt;
    detail::parse(is, format.c_str(), lt, &abbrev, &offset);
    if (!is.fail())
        tp = sys_time<Duration>{floor<Duration>(lt - offset).time_since_epoch()};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is,
      const std::basic_string<CharT, Traits>& format, sys_time<Duration>& tp,
      std::chrono::minutes& offset)
{
    local_time<Duration> lt;
    detail::parse(is, format.c_str(), lt, &offset);
    if (!is.fail())
        tp = sys_time<Duration>{floor<Duration>(lt - offset).time_since_epoch()};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is,
      const std::basic_string<CharT, Traits>& format, sys_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev, std::chrono::minutes& offset)
{
    local_time<Duration> lt;
    detail::parse(is, format.c_str(), lt, &abbrev, &offset);
    if (!is.fail())
        tp = sys_time<Duration>{floor<Duration>(lt - offset).time_since_epoch()};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is,
      const std::basic_string<CharT, Traits>& format, sys_time<Duration>& tp,
      std::chrono::minutes& offset, std::basic_string<CharT, Traits>& abbrev)
{
    local_time<Duration> lt;
    detail::parse(is, format.c_str(), lt, &abbrev, &offset);
    if (!is.fail())
        tp = sys_time<Duration>{floor<Duration>(lt - offset).time_since_epoch()};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is,
      const std::basic_string<CharT, Traits>& format, local_time<Duration>& tp)
{
    detail::parse(is, format.c_str(), tp);
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is,
      const std::basic_string<CharT, Traits>& format, local_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev)
{
    detail::parse(is, format.c_str(), tp, &abbrev);
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is,
      const std::basic_string<CharT, Traits>& format, local_time<Duration>& tp,
      std::chrono::minutes& offset)
{
    detail::parse(is, format.c_str(), tp, &offset);
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is,
      const std::basic_string<CharT, Traits>& format, local_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev, std::chrono::minutes& offset)
{
    detail::parse(is, format.c_str(), tp, &abbrev, &offset);
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is,
      const std::basic_string<CharT, Traits>& format, local_time<Duration>& tp,
      std::chrono::minutes& offset, std::basic_string<CharT, Traits>& abbrev)
{
    detail::parse(is, format.c_str(), tp, &abbrev, &offset);
}
// const CharT* formats
template <class Duration, class CharT>
inline
detail::parse_sys_manip<Duration, CharT>
parse(const CharT* format, sys_time<Duration>& tp)
{
    return {format, tp};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_sys_manip<Duration, CharT, Traits>
parse(const CharT* format, sys_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev)
{
    return {format, tp, &abbrev};
}
template <class Duration, class CharT>
inline
detail::parse_sys_manip<Duration, CharT>
parse(const CharT* format, sys_time<Duration>& tp, std::chrono::minutes& offset)
{
    return {format, tp, nullptr, &offset};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_sys_manip<Duration, CharT, Traits>
parse(const CharT* format, sys_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev, std::chrono::minutes& offset)
{
    return {format, tp, &abbrev, &offset};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_sys_manip<Duration, CharT, Traits>
parse(const CharT* format, sys_time<Duration>& tp,
      std::chrono::minutes& offset, std::basic_string<CharT, Traits>& abbrev)
{
    return {format, tp, &abbrev, &offset};
}
template <class Duration, class CharT>
inline
detail::parse_local_manip<Duration, CharT>
parse(const CharT* format, local_time<Duration>& tp)
{
    return {format, tp};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_local_manip<Duration, CharT, Traits>
parse(const CharT* format, local_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev)
{
    return {format, tp, &abbrev};
}
template <class Duration, class CharT>
inline
detail::parse_local_manip<Duration, CharT>
parse(const CharT* format, local_time<Duration>& tp, std::chrono::minutes& offset)
{
    return {format, tp, nullptr, &offset};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_local_manip<Duration, CharT, Traits>
parse(const CharT* format, local_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev, std::chrono::minutes& offset)
{
    return {format, tp, &abbrev, &offset};
}
template <class Duration, class CharT, class Traits>
inline
detail::parse_local_manip<Duration, CharT, Traits>
parse(const CharT* format, local_time<Duration>& tp, std::chrono::minutes& offset,
      std::basic_string<CharT, Traits>& abbrev)
{
    return {format, tp, &abbrev, &offset};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is, const CharT* format, sys_time<Duration>& tp)
{
    std::chrono::minutes offset{};
    local_time<Duration> lt;
    detail::parse(is, format, lt, &offset);
    if (!is.fail())
        tp = sys_time<Duration>{floor<Duration>(lt - offset).time_since_epoch()};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is, const CharT* format, sys_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev)
{
    std::chrono::minutes offset{};
    local_time<Duration> lt;
    detail::parse(is, format, lt, &abbrev, &offset);
    if (!is.fail())
        tp = sys_time<Duration>{floor<Duration>(lt - offset).time_since_epoch()};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is, const CharT* format, sys_time<Duration>& tp,
      std::chrono::minutes& offset)
{
    local_time<Duration> lt;
    detail::parse(is, format, lt, &offset);
    if (!is.fail())
        tp = sys_time<Duration>{floor<Duration>(lt - offset).time_since_epoch()};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is, const CharT* format, sys_time<Duration>& tp,
      std::basic_string<CharT, Traits>& abbrev, std::chrono::minutes& offset)
{
    local_time<Duration> lt;
    detail::parse(is, format, lt, &abbrev, &offset);
    if (!is.fail())
        tp = sys_time<Duration>{floor<Duration>(lt - offset).time_since_epoch()};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is, const CharT* format, sys_time<Duration>& tp,
      std::chrono::minutes& offset, std::basic_string<CharT, Traits>& abbrev)
{
    local_time<Duration> lt;
    detail::parse(is, format, lt, &abbrev, &offset);
    if (!is.fail())
        tp = sys_time<Duration>{floor<Duration>(lt - offset).time_since_epoch()};
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is, const CharT* format,
      local_time<Duration>& tp)
{
    detail::parse(is, format, tp);
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is, const CharT* format,
      local_time<Duration>& tp, std::basic_string<CharT, Traits>& abbrev)
{
    detail::parse(is, format, tp, &abbrev);
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is, const CharT* format,
      local_time<Duration>& tp, std::chrono::minutes& offset)
{
    detail::parse(is, format, tp, &offset);
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is, const CharT* format,
      local_time<Duration>& tp, std::basic_string<CharT, Traits>& abbrev,
      std::chrono::minutes& offset)
{
    detail::parse(is, format, tp, &abbrev, &offset);
}
template <class CharT, class Traits, class Duration>
inline
void
parse(std::basic_istream<CharT, Traits>& is, const CharT* format,
      local_time<Duration>& tp, std::chrono::minutes& offset,
      std::basic_string<CharT, Traits>& abbrev)
{
    detail::parse(is, format, tp, &abbrev, &offset);
}
}  // namespace date
#endif  // DATE_H
template <typename Dividend, typename Divisor>
// requires Integral<Dividend> && Integral<Divisor>
auto
div_ceil(Dividend dividend, Divisor divisor) ;
namespace utils {
class dynamic_bitset {
    using int_type = uint64_t;
    static constexpr size_t bits_per_int = std::numeric_limits<int_type>::digits;
    static constexpr int_type all_set = std::numeric_limits<int_type>::max();
    static constexpr unsigned level_shift = seastar::log2ceil(bits_per_int);
private:
    std::vector<std::vector<int_type>> _bits; // level n+1 = 64:1 summary of level n
    size_t _bits_count = 0;
private:
    // For n in range 0..(bits_per_int-1), produces a mask with all bits < n set
    static int_type mask_lower_bits(size_t n) noexcept {
        return (int_type(1) << n) - 1;
    }
    // For n in range 0..(bits_per_int-1), produces a mask with all bits >= n set
    static int_type mask_higher_bits(size_t n) noexcept ;
    // For bit n, produce index into _bits[level]
    static size_t level_idx(unsigned level, size_t n) noexcept {
        return n >> ((level + 1) * level_shift);
    }
    // For bit n, produce bit number in _bits[level][level_idx]
    static unsigned level_remainder(unsigned level, size_t n) noexcept {
        return (n >> (level * level_shift)) & (bits_per_int - 1);
    }
public:
    enum : size_t {
        npos = std::numeric_limits<size_t>::max()
    };
public:
    explicit dynamic_bitset(size_t nr_bits);
    // undefined if n >= size
    bool test(size_t n) const noexcept {
        auto idx = n / bits_per_int;
        return _bits[0][idx] & (int_type(1u) << (n % bits_per_int));
    }
    // undefined if n >= size
    void set(size_t n) noexcept;
    // undefined if n >= size
    void clear(size_t n) noexcept;
    size_t size() const noexcept ;
    size_t find_first_set() const noexcept;
    size_t find_next_set(size_t n) const noexcept;
    size_t find_last_set() const noexcept;
};
}
namespace utils
{
// Exception thrown by enabled error injection
class injected_error : public std::runtime_error
{
public:
    injected_error(const sstring &err_name)
        : runtime_error{err_name} {}
};
extern logging::logger errinj_logger;
template <bool injection_enabled>
class error_injection
{
    inline static thread_local error_injection _local;
    using handler_fun = std::function<void()>;
    // String cross-type comparator
    class str_less
    {
    public:
        using is_transparent = std::true_type;
        template <typename TypeLeft, typename TypeRight>
        bool operator()(const TypeLeft &left, const TypeRight &right) const;
    };
    // Map enabled-injection-name -> is-one-shot
    // TODO: change to unordered_set once we have heterogeneous lookups
    std::map<sstring, bool, str_less> _enabled;
    bool is_enabled(const std::string_view &injection_name) const;
    bool is_one_shot(const std::string_view &injection_name) const;
public:
    // \brief Enter into error injection if it's enabled
    // \param name error injection name to check
    bool enter(const std::string_view &name);
    void enable(const std::string_view &injection_name, bool one_shot = false);
    void disable(const std::string_view &injection_name);
    // \brief Inject a lambda call
    // \param f lambda to be run
    [[gnu::always_inline]] void inject(const std::string_view &name, handler_fun f)
    {
        if (!is_enabled(name))
        {
        return;
        }
        if (is_one_shot(name))
        {
        disable(name);
        }
        errinj_logger.debug("Triggering injection \"{}\"", name);
    }
    // \brief Inject a sleep for milliseconds
    [[gnu::always_inline]] future<> inject(const std::string_view &name,
                                           const std::chrono::milliseconds duration)
    {
        if (!is_enabled(name))
        {
        return make_ready_future<>();
        }
        if (is_one_shot(name))
        {
        disable(name);
        }
        errinj_logger.debug("Triggering sleep injection \"{}\" ({}ms)", name, duration.count());
        return seastar::sleep(duration);
    }
    // \brief Inject a sleep to deadline (timeout)
    template <typename Clock, typename Duration>
    [[gnu::always_inline]] future<> inject(const std::string_view &name, std::chrono::time_point<Clock, Duration> deadline)
    {
        if (!is_enabled(name))
        {
        return make_ready_future<>();
        }
        if (is_one_shot(name))
        {
        disable(name);
        }
        // Time left until deadline
        std::chrono::milliseconds duration = std::chrono::duration_cast<std::chrono::milliseconds>(deadline - Clock::now());
        errinj_logger.debug("Triggering sleep injection \"{}\" ({}ms)", name, duration.count());
        return seastar::sleep<Clock>(duration);
    }
    // \brief Inject a sleep to deadline with lambda(timeout)
    // Avoid adding a sleep continuation in the chain for disabled error injection
    template <typename Clock, typename Duration, typename Func>
    [[gnu::always_inline]] std::result_of_t<Func()> inject(const std::string_view &name, std::chrono::time_point<Clock, Duration> deadline,
                                                           Func &&func)
    {
        if (is_enabled(name))
        {
        if (is_one_shot(name))
        {
            disable(name);
        }
        std::chrono::milliseconds duration = std::chrono::duration_cast<std::chrono::milliseconds>(deadline - Clock::now());
        errinj_logger.debug("Triggering sleep injection \"{}\" ({}ms)", name, duration.count());
        return seastar::sleep<Clock>(duration).then([func = std::move(func)]
                                                    { return func(); });
        }
        else
        {
        return func();
        }
    }
    // \brief Inject exception
    // \param exception_factory function returning an exception pointer
    template <typename Func>
        requires std::is_invocable_r_v<std::exception_ptr, Func>
    [[gnu::always_inline]] future<>
    inject(const std::string_view &name,
           Func &&exception_factory)
    {
        if (!is_enabled(name))
        {
        return make_ready_future<>();
        }
        if (is_one_shot(name))
        {
        disable(name);
        }
        errinj_logger.debug("Triggering exception injection \"{}\"", name);
        return make_exception_future<>(exception_factory());
    }
    future<> enable_on_all(const std::string_view &injection_name, bool one_shot = false)
    {
        return smp::invoke_on_all([injection_name = sstring(injection_name), one_shot]
                                  {
            auto& errinj = _local;
            errinj.enable(injection_name, one_shot); });
    }
    static future<> disable_on_all(const std::string_view &injection_name)
    {
        return smp::invoke_on_all([injection_name = sstring(injection_name)]
                                  {
            auto& errinj = _local;
            errinj.disable(injection_name); });
    }
    static future<> disable_on_all()
    {
        return smp::invoke_on_all([]
                                  {
            auto& errinj = _local;
            errinj.disable_all(); });
    }
    static std::vector<sstring> enabled_injections_on_all()
    {
        // TODO: currently we always enable an injection on all shards at once,
        // so returning the list from the current shard will do.
        // In future different shards may have different enabled sets,
        // in which case we may want to extend the API.
        auto &errinj = _local;
        return errinj.enabled_injections();
    }
    static error_injection &get_local()
    {
        return _local;
    }
};
// no-op, should be optimized away
template <>
class error_injection<false>
{
    static thread_local error_injection _local;
    using handler_fun = std::function<void()>;
public:
    [[gnu::always_inline]] void enable(const std::string_view &injection_name, const bool one_shot = false) {}
    [[gnu::always_inline]] void disable(const std::string_view &injection_name) {}
    [[gnu::always_inline]] void disable_all() {}
    [[gnu::always_inline]] std::vector<sstring> enabled_injections() const { return {}; };
    // Inject a lambda call
    [[gnu::always_inline]] void inject(const std::string_view &name, handler_fun f) {}
    // Inject sleep
    [[gnu::always_inline]] future<> inject(const std::string_view &name,
                                           const std::chrono::milliseconds duration)
    {
        return make_ready_future<>();
    }
    // \brief Inject a sleep to deadline (timeout)
    template <typename Clock, typename Duration>
    [[gnu::always_inline]] future<> inject(const std::string_view &name, std::chrono::time_point<Clock, Duration> deadline)
    {
        return make_ready_future<>();
    }
    // \brief Inject a sleep to deadline (timeout) with lambda
    // Avoid adding a continuation in the chain for disabled error injections
    template <typename Clock, typename Duration, typename Func>
    [[gnu::always_inline]] std::result_of_t<Func()> inject(const std::string_view &name, std::chrono::time_point<Clock, Duration> deadline,
                                                           Func &&func)
    {
        return func();
    }
    // Inject exception
    template <typename Func>
        requires std::is_invocable_r_v<std::exception_ptr, Func>
    [[gnu::always_inline]] future<>
    inject(const std::string_view &name,
           Func &&exception_factory)
    {
        return make_ready_future<>();
    }
    [[gnu::always_inline]] static future<> enable_on_all(const std::string_view &injection_name, const bool one_shot = false)
    {
        return make_ready_future<>();
    }
    [[gnu::always_inline]] static future<> disable_on_all(const std::string_view &injection_name)
    {
        return make_ready_future<>();
    }
    [[gnu::always_inline]] static future<> disable_on_all()
    {
        return make_ready_future<>();
    }
    [[gnu::always_inline]] static std::vector<sstring> enabled_injections_on_all() { return {}; }
    static error_injection &get_local()
    {
        return _local;
    }
};
#ifdef SCYLLA_ENABLE_ERROR_INJECTION
using error_injection_type = error_injection<true>; // debug, dev
#else
using error_injection_type = error_injection<false>; // release
#endif
} // namespace utils
// compatibility between fmt < 8 (that doesn't have fmt::runtime())
// and fmt 8 (that requires it)
#if FMT_VERSION < 8'00'00
namespace fmt
{
// fmt 8 requires that non-constant format strings be wrapped with
// fmt::runtime(), supply a nop-op version for older fmt
;
}
#endif
namespace utils
{
struct human_readable_value
{
    uint16_t value; // [0, 1024)
    char suffix;    // 0 -> no suffix
};
/// Convert a size to a human readable representation.
///
/// The human-readable representation has at most 4 digits
/// and a letter appropriate to the power of two the number has to be multiplied
/// with to arrive to the original number (with some loss of precision).
/// The different powers of two are the conventional 2 ** (N * 10) variants:
/// * N=0: (B)ytes
/// * N=1: (K)bytes
/// * N=2: (M)bytes
/// * N=3: (G)bytes
/// * N=4: (T)bytes
///
/// Examples:
/// * 87665 will be converted to 87K
/// * 1024 will be converted to 1K
} // namespace utils
namespace fs = std::filesystem;
class lister final
{
public:
using listable_entry_types = super_enum<directory_entry_type, directory_entry_type::regular, directory_entry_type::directory>;
using dir_entry_types = enum_set<listable_entry_types>;
using walker_type = std::function<future<>(fs::path, directory_entry)>;
using filter_type = std::function<bool(const fs::path &, const directory_entry &)>;
struct show_hidden_tag
{
};
using show_hidden = bool_class<show_hidden_tag>;
private:
file _f;
walker_type _walker;
filter_type _filter;
dir_entry_types _expected_type;
future<> _listing_done;
fs::path _dir;
show_hidden _show_hidden;
public:
static future<> scan_dir(fs::path dir, dir_entry_types type, walker_type walker);
static future<> scan_dir(fs::path dir, dir_entry_types type, show_hidden do_show_hidden, walker_type walker);
private:
};
class directory_lister
{
fs::path _dir;
lister::dir_entry_types _type;
lister::filter_type _filter;
lister::show_hidden _do_show_hidden;
seastar::queue<std::optional<directory_entry>> _queue;
std::unique_ptr<lister> _lister;
std::optional<future<>> _opt_done_fut;
public:
// Get the next directory_entry from the lister,
// if available.  When the directory listing is done,
// a disengaged optional is returned.
//
// Caller should either drain all entries using get()
// until it gets a disengaged result or an error, or
// close() can be called to terminate the listing prematurely,
// and wait on any background work to complete.
//
// Calling get() after the listing is done and a disengaged
// result has been returned results in a broken_pipe_exception.
future<std::optional<directory_entry>> get();
// Close the directory_lister, ignoring any errors.
// Must be called after get() if not all entries were retrieved.
//
// Close aborts the lister, waking up get() if any is waiting,
// and waits for all background work to complete.
future<> close() noexcept;
};
static fs::path operator/(const fs::path &lhs, const char *rhs)
;


static fs::path operator/(const fs::path &lhs, const std::string &rhs);
namespace bi = boost::intrusive;
// Returns largest N such that 2^N <= v.
// Undefined for v == 0.
inline size_t pow2_rank(size_t v)
{
return std::numeric_limits<size_t>::digits - 1 - count_leading_zeros(v);
}
// Returns largest N such that 2^N <= v.
// Undefined for v == 0.
inline constexpr size_t pow2_rank_constexpr(size_t v)
{
return v <= 1 ? 1 : 1 + pow2_rank_constexpr(v >> 1);
}
// Configures log_heap.
// Only values <= max_size can be inserted into the histogram.
// log_heap::contains_above_min() returns true if and only if the histogram contains any value >= min_size.
struct log_heap_options
{
const size_t min_size;
const size_t sub_bucket_shift;
const size_t max_size;
constexpr log_heap_options(const size_t min_size, size_t sub_bucket_shift, size_t max_size)
    : min_size(min_size), sub_bucket_shift(sub_bucket_shift), max_size(max_size)
{
}
size_t bucket_of(size_t value) const
{
#ifdef SANITIZE
    // ubsan will otherwise complain about pow2_rank(0)
    if (value < min_size)
    {
        return 0;
    }
#endif
    const auto min_mask = -size_t(value >= min_size); // 0 when below min_size, all bits on otherwise
    value = value - min_size + 1;
    const auto pow2_index = pow2_rank(value);
    const auto bucket = (pow2_index + 1) & min_mask;
    const auto unmasked_sub_bucket_index = (value << sub_bucket_shift) >> pow2_index;
    const auto mask = ((1 << sub_bucket_shift) - 1) & min_mask;
    const auto sub_bucket_index = unmasked_sub_bucket_index & mask;
    return (bucket << sub_bucket_shift) - mask + sub_bucket_index;
}
constexpr size_t number_of_buckets() const
{
    const auto min_mask = -size_t(max_size >= min_size); // 0 when below min_size, all bits on otherwise
    const auto value = max_size - min_size + 1;
    const auto pow2_index = pow2_rank_constexpr(value);
    const auto bucket = (pow2_index + 1) & min_mask;
    const auto unmasked_sub_bucket_index = (value << sub_bucket_shift) >> pow2_index;
    const auto mask = ((1 << sub_bucket_shift) - 1) & min_mask;
    const auto sub_bucket_index = unmasked_sub_bucket_index & mask;
    const auto ret = (bucket << sub_bucket_shift) - mask + sub_bucket_index;
    return ret + 1;
}
};
template <const log_heap_options &opts>
struct log_heap_bucket_index
{
using type = std::conditional_t<(opts.number_of_buckets() > ((1 << 16) - 1)), uint32_t,
                                std::conditional_t<(opts.number_of_buckets() > ((1 << 8) - 1)), uint16_t, uint8_t>>;
};
template <const log_heap_options &opts>
struct log_heap_hook : public bi::list_base_hook<>
{
typename log_heap_bucket_index<opts>::type cached_bucket;
};
template <typename T>
size_t hist_key(const T &);
template <typename T, const log_heap_options &opts, bool = std::is_base_of<log_heap_hook<opts>, T>::value>
struct log_heap_element_traits
{
using bucket_type = bi::list<T, bi::constant_time_size<false>>;
static void cache_bucket(T &v, typename log_heap_bucket_index<opts>::type b)
{
    v.cached_bucket = b;
}
static size_t cached_bucket(const T &v)
{
    return v.cached_bucket;
}
static size_t hist_key(const T &v)
{
    return ::hist_key<T>(v);
}
};
template <typename T, const log_heap_options &opts>
struct log_heap_element_traits<T, opts, false>
{
using bucket_type = typename T::bucket_type;



};
template <typename T, const log_heap_options &opts>
    requires requires() {
        typename log_heap_element_traits<T, opts>;
    }
class log_heap final
{
// Ensure that (value << sub_bucket_index) in bucket_of() doesn't overflow
static_assert(pow2_rank_constexpr(opts.max_size - opts.min_size + 1) + opts.sub_bucket_shift < std::numeric_limits<size_t>::digits, "overflow");
private:
using traits = log_heap_element_traits<T, opts>;
using bucket = typename traits::bucket_type;
struct hist_size_less_compare
{
};
std::array<bucket, opts.number_of_buckets()> _buckets;
ssize_t _watermark = -1;
public:
template <bool IsConst>
class hist_iterator
{
public:
    using iterator_category = std::input_iterator_tag;
    using value_type = std::conditional_t<IsConst, const T, T>;
    using difference_type = std::ptrdiff_t;
    using pointer = std::conditional_t<IsConst, const T, T> *;
    using reference = std::conditional_t<IsConst, const T, T> &;
private:
    using hist_type = std::conditional_t<IsConst, const log_heap, log_heap>;
    using iterator_type = std::conditional_t<IsConst, typename bucket::const_iterator, typename bucket::iterator>;
    hist_type &_h;
    ssize_t _b;
    iterator_type _it;
public:
    struct end_tag
    {
    };
    
};
using iterator = hist_iterator<false>;
using const_iterator = hist_iterator<true>;
public:
bool empty() const noexcept
{
    return _watermark == -1;
}
// Returns true if and only if contains any value >= opts.min_size.
bool contains_above_min() const noexcept
{
    return _watermark > 0;
}
const_iterator begin() const noexcept;



// Returns a range of buckets starting from that with the smaller values.
// Each bucket is a range of const T&.

// Pops one of the largest elements in the histogram.
void pop_one_of_largest() noexcept
{
    _buckets[_watermark].pop_front();
    maybe_adjust_watermark();
}
// Returns one of the largest elements in the histogram.
const T &one_of_largest() const noexcept
{
    return _buckets[_watermark].front();
}
// Returns one of the largest elements in the histogram.
T &one_of_largest() noexcept
{
    return _buckets[_watermark].front();
}
// Pushes a new element onto the histogram.
void push(T &v)
{
    auto b = opts.bucket_of(traits::hist_key(v));
    traits::cache_bucket(v, b);
    _buckets[b].push_front(v);
    _watermark = std::max(ssize_t(b), _watermark);
}
// Adjusts the histogram when the specified element becomes larger.
void adjust_up(T &v)
;
// Removes the specified element from the histogram.
void erase(T &v) noexcept
{
    auto &b = _buckets[traits::cached_bucket(v)];
    b.erase(b.iterator_to(v));
    maybe_adjust_watermark();
}
// Merges the specified histogram, moving all elements from it into this.

private:
void maybe_adjust_watermark() noexcept
{
    while (_buckets[_watermark].empty() && --_watermark >= 0)
        ;
}
};
namespace cql3::statements
{
class select_statement;
}
class view_info final
{
const schema &_schema;
raw_view_info _raw;
// The following fields are used to select base table rows.
mutable shared_ptr<cql3::statements::select_statement> _select_statement;
mutable std::optional<query::partition_slice> _partition_slice;
db::view::base_info_ptr _base_info;
mutable bool _has_computed_column_depending_on_base_non_primary_key;
public:





bool has_base_non_pk_columns_in_view_pk() const;
bool has_computed_column_depending_on_base_non_primary_key() const;
/// Returns a pointer to the base_dependent_view_info which matches the current
/// schema of the base table.
///
/// base_dependent_view_info lives separately from the view schema.
/// It can change without the view schema changing its value.
/// This pointer is updated on base table schema changes as long as this view_info
/// corresponds to the current schema of the view. After that the pointer stops tracking
/// the base table schema.
///
/// The snapshot of both the view schema and base_dependent_view_info is represented
/// by view_and_base. See with_base_info_snapshot().
const db::view::base_info_ptr &base_info() const;
void set_base_info(db::view::base_info_ptr);
db::view::base_info_ptr make_base_dependent_view_info(const schema &base_schema) const;
};
// Accumulates data sent to the memory_data_sink allowing it
// to be examined later.
class memory_data_sink_buffers
{
using buffers_type = utils::small_vector<temporary_buffer<char>, 1>;
buffers_type _bufs;
size_t _size = 0;
public:
// Strong exception guarantees



};
class memory_data_sink : public data_sink_impl
{
memory_data_sink_buffers &_bufs;
public:
};
namespace runtime
{
/// Returns the uptime of the system.
}
using namespace seastar;
class memory_data_sink_buffers;
namespace s3
{
struct range
{
    uint64_t off;
    size_t len;
};
class client : public enable_shared_from_this<client>
{
    class upload_sink;
    class readable_file;
    std::string _host;
    endpoint_config_ptr _cfg;
    http::experimental::client _http;
    using global_factory = std::function<shared_ptr<client>(std::string)>;
    global_factory _gf;
    struct private_tag
    {
    };
public:
    struct stats
    {
        uint64_t size;
        std::time_t last_modified;
    };
    
    void update_config(endpoint_config_ptr);
    struct handle
    {
        std::string _host;
        global_factory _gf;
    public:
        handle(const client &cln)
            : _host(cln._host), _gf(cln._gf)
        {
        }
        shared_ptr<client> to_client() &&;
    };
    
};
} // s3 namespace
using namespace seastar;
namespace utils
{
// Similar to std::merge but it does not stall. Must run inside a seastar
// thread. It merges items from list2 into list1. Items from list2 can only be copied.
template <class T, class Compare>
    requires LessComparable<T, T, Compare>
void merge_to_gently(std::list<T> &list1, const std::list<T> &list2, Compare comp)
{
    auto first1 = list1.begin();
    auto first2 = list2.begin();
    auto last1 = list1.end();
    auto last2 = list2.end();
    while (first2 != last2)
    {
        seastar::thread::maybe_yield();
        if (first1 == last1)
        {
        // Copy remaining items of list2 into list1
        list1.insert(last1, *first2);
        ++first2;
        continue;
        }
        if (comp(*first2, *first1))
        {
        list1.insert(first1, *first2);
        ++first2;
        }
        else
        {
        ++first1;
        }
    }
}
// The clear_gently functions are meant for
// gently destroying the contents of containers.
// The containers can be re-used after clear_gently
// or may be destroyed.  But unlike e.g. std::vector::clear(),
// clear_gently will not necessarily keep the object allocation.
template <typename T>
concept HasClearGentlyMethod = requires(T x) {
    {
        x.clear_gently()
    } -> std::same_as<future<>>;
};
template <typename T>
concept SmartPointer = requires(T x) {
    {
        x.get()
    } -> std::same_as<typename T::element_type *>;
    {
        *x
    } -> std::same_as<typename T::element_type &>;
};
template <typename T>
concept SharedPointer = SmartPointer<T> && requires(T x) {
    {
        x.use_count()
    } -> std::convertible_to<long>;
};
template <typename T>
concept StringLike = requires(T x) {
    std::is_same_v<typename T::traits_type, std::char_traits<typename T::value_type>>;
};
template <typename T>
concept Iterable = requires(T x) {
    {
        x.empty()
    } -> std::same_as<bool>;
    {
        x.begin()
    } -> std::same_as<typename T::iterator>;
    {
        x.end()
    } -> std::same_as<typename T::iterator>;
};
template <typename T>
concept Sequence = Iterable<T> && requires(T x, size_t n) {
    {
        x.back()
    } -> std::same_as<typename T::value_type &>;
    {
        x.pop_back()
    } -> std::same_as<void>;
};
template <typename T>
concept TriviallyClearableSequence =
    Sequence<T> && std::is_trivially_destructible_v<typename T::value_type> && !HasClearGentlyMethod<typename T::value_type> && requires(T s) {
        {
            s.clear()
        } -> std::same_as<void>;
    };
template <typename T>
concept Container = Iterable<T> && requires(T x, typename T::iterator it) {
    {
        x.erase(it)
    } -> std::same_as<typename T::iterator>;
};
template <typename T>
concept MapLike = Container<T> && requires(T x) {
    std::is_same_v<typename T::value_type, std::pair<const typename T::key_type, typename T::mapped_type>>;
};
template <HasClearGentlyMethod T>
future<> clear_gently(T &o) noexcept;
;
template <SharedPointer T>
future<> clear_gently(T &o) noexcept;
template <SmartPointer T>
future<> clear_gently(T &o) noexcept;
template <typename T, std::size_t N>
future<> clear_gently(std::array<T, N> &a) noexcept;
template <typename T>
    requires(StringLike<T> || TriviallyClearableSequence<T>)
future<> clear_gently(T &s) noexcept;
template <Sequence T>
    requires(!StringLike<T> && !TriviallyClearableSequence<T>)
future<> clear_gently(T &v) noexcept;
template <MapLike T>
future<> clear_gently(T &c) noexcept;
template <Container T>
    requires(!StringLike<T> && !Sequence<T> && !MapLike<T>)
future<> clear_gently(T &c) noexcept;
template <typename T>
future<> clear_gently(std::optional<T> &opt) noexcept;
template <typename T>
future<> clear_gently(seastar::optimized_optional<T> &opt) noexcept;
namespace internal
{
    template <typename T>
    concept HasClearGentlyImpl = requires(T x) {
        {
            clear_gently(x)
        } -> std::same_as<future<>>;
    };
    template <typename T>
        requires HasClearGentlyImpl<T>
    future<> clear_gently(T &x) noexcept
    {
        return utils::clear_gently(x);
    }
    // This default implementation of clear_gently
    // is required to "terminate" recursive clear_gently calls
    // at trivial objects
    template <typename T>
    future<> clear_gently(T &) noexcept
    {
        return make_ready_future<>();
    }
} // namespace internal
template <HasClearGentlyMethod T>
future<> clear_gently(T &o) noexcept
{
    return futurize_invoke(std::bind(&T::clear_gently, &o));
}
;
template <SharedPointer T>
future<> clear_gently(T &o) noexcept
{
    if (o.use_count() == 1)
    {
        return internal::clear_gently(*o);
    }
    return make_ready_future<>();
}
template <SmartPointer T>
future<> clear_gently(T &o) noexcept
{
    if (auto p = o.get())
    {
        return internal::clear_gently(*p);
    }
    else
    {
        return make_ready_future<>();
    }
}
template <typename T, std::size_t N>
future<> clear_gently(std::array<T, N> &a) noexcept
{
    return do_for_each(a, [](T &o)
                       { return internal::clear_gently(o); });
}
// Trivially destructible elements can be safely cleared in bulk
template <typename T>
    requires(StringLike<T> || TriviallyClearableSequence<T>)
future<> clear_gently(T &s) noexcept
{
    // Note: clear() is pointless in this case since it keeps the allocation
    // and since the values are trivially destructible it achieves nothing.
    // `s = {}` will free the vector/string allocation.
    s = {};
    return make_ready_future<>();
}
// Clear the elements gently and destroy them one-by-one
// in reverse order, to avoid copying.
template <Sequence T>
    requires(!StringLike<T> && !TriviallyClearableSequence<T>)
future<> clear_gently(T &v) noexcept
{
    return do_until([&v]
                    { return v.empty(); },
                    [&v]
                    {
                        return internal::clear_gently(v.back()).then([&v]
                                                                     { v.pop_back(); });
                    });
    return make_ready_future<>();
}
template <MapLike T>
future<> clear_gently(T &c) noexcept
{
    return do_until([&c]
                    { return c.empty(); },
                    [&c]
                    {
                        auto it = c.begin();
                        return internal::clear_gently(it->second).then([&c, it = std::move(it)]() mutable
                                                                       { c.erase(it); });
                    });
}
template <Container T>
    requires(!StringLike<T> && !Sequence<T> && !MapLike<T>)
future<> clear_gently(T &c) noexcept
{
    return do_until([&c]
                    { return c.empty(); },
                    [&c]
                    {
                        auto it = c.begin();
                        return internal::clear_gently(*it).then([&c, it = std::move(it)]() mutable
                                                                { c.erase(it); });
                    });
}
template <typename T>
future<> clear_gently(std::optional<T> &opt) noexcept
{
    if (opt)
    {
        return utils::clear_gently(*opt);
    }
    else
    {
        return make_ready_future<>();
    }
}
template <typename T>
future<> clear_gently(seastar::optimized_optional<T> &opt) noexcept
{
    if (opt)
    {
        return utils::clear_gently(*opt);
    }
    else
    {
        return make_ready_future<>();
    }
}
}
;
template <typename Container>
typename Container::iterator
unconst(Container &c, typename Container::const_iterator i);
namespace utils
{
static constexpr size_t uleb64_express_bits = 12;
static constexpr uint32_t uleb64_express_supreme = 1 << uleb64_express_bits;
// Returns the number of bytes needed to encode the value
// The value cannot be 0 (not checked)
static inline size_t uleb64_encoded_size(uint32_t val) noexcept
{
    return seastar::log2floor(val) / 6 + 1;
}
template <typename Poison, typename Unpoison>
    requires std::is_invocable<Poison, const char *, size_t>::value && std::is_invocable<Unpoison, const char *, size_t>::value
static inline void uleb64_encode(char *&pos, uint32_t val, Poison &&poison, Unpoison &&unpoison) noexcept
{
    uint64_t b = 64;
    auto start = pos;
    do
    {
        b |= val & 63;
        val >>= 6;
        if (!val)
        {
        b |= 128;
        }
        unpoison(pos, 1);
        *pos++ = b;
        b = 0;
    } while (val);
    poison(start, pos - start);
}
template <typename Poison, typename Unpoison>
    requires std::is_invocable<Poison, const char *, size_t>::value && std::is_invocable<Unpoison, const char *, size_t>::value
static inline void uleb64_encode(char *&pos, uint32_t val, size_t encoded_size, Poison &&poison, Unpoison &&unpoison) noexcept
{
    uint64_t b = 64;
    auto start = pos;
    unpoison(start, encoded_size);
    do
    {
        b |= val & 63;
        val >>= 6;
        if (!--encoded_size)
        {
        b |= 128;
        }
        *pos++ = b;
        b = 0;
    } while (encoded_size);
    poison(start, pos - start);
}
#if !defined(SEASTAR_ASAN_ENABLED)
static inline void uleb64_express_encode_impl(char *&pos, uint64_t val, size_t size) noexcept
{
    static_assert(uleb64_express_bits == 12);
    if (size > sizeof(uint64_t))
    {
        static uint64_t zero = 0;
        std::copy_n(reinterpret_cast<char *>(&zero), sizeof(zero), pos + size - sizeof(uint64_t));
    }
    seastar::write_le(pos, uint64_t(((val & 0xfc0) << 2) | ((val & 0x3f) | 64)));
    pos += size;
    pos[-1] |= 0x80;
}
template <typename Poison, typename Unpoison>
    requires std::is_invocable<Poison, const char *, size_t>::value && std::is_invocable<Unpoison, const char *, size_t>::value
static inline void uleb64_express_encode(char *&pos, uint32_t val, size_t encoded_size, size_t gap, Poison &&poison, Unpoison &&unpoison) noexcept
{
    if (encoded_size + gap > sizeof(uint64_t))
    {
        uleb64_express_encode_impl(pos, val, encoded_size);
    }
    else
    {
        uleb64_encode(pos, val, encoded_size, poison, unpoison);
    }
}
#else
template <typename Poison, typename Unpoison>
    requires std::is_invocable<Poison, const char *, size_t>::value && std::is_invocable<Unpoison, const char *, size_t>::value
static inline void uleb64_express_encode(char *&pos, uint32_t val, size_t encoded_size, size_t gap, Poison &&poison, Unpoison &&unpoison) noexcept
{
    uleb64_encode(pos, val, encoded_size, poison, unpoison);
}
#endif
template <typename Poison, typename Unpoison>
    requires std::is_invocable<Poison, const char *, size_t>::value && std::is_invocable<Unpoison, const char *, size_t>::value
static inline uint32_t uleb64_decode_forwards(const char *&pos, Poison &&poison, Unpoison &&unpoison) noexcept
{
    uint32_t n = 0;
    unsigned shift = 0;
    auto p = pos; // avoid aliasing; p++ doesn't touch memory
    uint8_t b;
    do
    {
        unpoison(p, 1);
        b = *p++;
        if (shift < 32)
        {
        // non-canonical encoding can cause large shift; undefined in C++
        n |= uint32_t(b & 63) << shift;
        }
        shift += 6;
    } while ((b & 128) == 0);
    poison(pos, p - pos);
    pos = p;
    return n;
}
template <typename Poison, typename Unpoison>
    requires std::is_invocable<Poison, const char *, size_t>::value && std::is_invocable<Unpoison, const char *, size_t>::value
static inline uint32_t uleb64_decode_bacwards(const char *&pos, Poison &&poison, Unpoison &&unpoison) noexcept
{
    uint32_t n = 0;
    uint8_t b;
    auto p = pos; // avoid aliasing; --p doesn't touch memory
    do
    {
        --p;
        unpoison(p, 1);
        b = *p;
        n = (n << 6) | (b & 63);
    } while ((b & 64) == 0);
    poison(p, pos - p);
    pos = p;
    return n;
}
} // namespace utils
using vint_size_type = bytes::size_type;
static constexpr size_t max_vint_length = 9;
struct unsigned_vint final
{
using value_type = uint64_t;
static vint_size_type serialized_size(value_type) noexcept;
static vint_size_type serialize(value_type, bytes::iterator out);
static value_type deserialize(bytes_view v);
static vint_size_type serialized_size_from_first_byte(bytes::value_type first_byte);
};
struct signed_vint final
{
using value_type = int64_t;
static vint_size_type serialized_size(value_type) noexcept;
static vint_size_type serialize(value_type, bytes::iterator out);
static value_type deserialize(bytes_view v);
static vint_size_type serialized_size_from_first_byte(bytes::value_type first_byte);
};
namespace tests
{
}
void tests::random_schema::add_row(std::mt19937 &engine, data_model::mutation_description &md, data_model::mutation_description::key ckey, timestamp_generator ts_gen, expiry_generator exp_gen)
{
value_generator gen;
const auto &cdef = _schema->regular_columns()[0];
{
    auto value = gen.generate_value(engine, *cdef.type);
    md.add_clustered_cell(ckey, cdef.name_as_text(), std::move(value));
}
}
static auto ts_gen = tests::default_timestamp_generator();
static auto exp_gen = tests::no_expiry_expiry_generator();
future<> my_coroutine(uint32_t seed, tests::random_schema &random_schema)
{
auto engine = std::mt19937(seed);
const auto partition_count = 2;
std::vector<mutation> muts;
for (size_t pk = 0; pk != partition_count; ++pk)
{
    auto mut = random_schema.new_mutation(pk);
    const auto clustering_row_count = 1;
    const auto range_tombstone_count = 1;
    auto ckeys = random_schema.make_ckeys(std::max(clustering_row_count, range_tombstone_count));
    random_schema.add_row(engine, mut, ckeys[0], ts_gen, exp_gen);
    co_await coroutine::maybe_yield();
    muts.emplace_back(mut.build(random_schema.schema()));
}
}
SEASTAR_TEST_CASE(test_validate_checksums)
{
static auto random_spec = tests::make_random_schema_specification(get_name(), std::uniform_int_distribution<size_t>(1, 1), std::uniform_int_distribution<size_t>(1, 1), std::uniform_int_distribution<size_t>(1, 1), std::uniform_int_distribution<size_t>(1, 1));
static auto random_schema = tests::random_schema{0, *random_spec};
return my_coroutine(7, random_schema);
}
atomic_cell atomic_cell::make_live(const abstract_type &type, api::timestamp_type timestamp, bytes_view value, atomic_cell::collection_member cm) { return atomic_cell_type::make_live(timestamp, single_fragment_range(value)); }
atomic_cell atomic_cell::make_live(const abstract_type &type, api::timestamp_type timestamp, bytes_view value, gc_clock::time_point expiry, gc_clock::duration ttl, atomic_cell::collection_member cm) { return atomic_cell_type::make_live(timestamp, single_fragment_range(value), expiry, ttl); }
atomic_cell atomic_cell::make_live_counter_update(api::timestamp_type timestamp, int64_t value) { return atomic_cell_type::make_live_counter_update(timestamp, value); }
atomic_cell atomic_cell::make_live_uninitialized(const abstract_type &type, api::timestamp_type timestamp, size_t size) { return atomic_cell_type::make_live_uninitialized(timestamp, size); }
// Based on:
//  - org.apache.cassandra.db.AbstractCell#reconcile()
//  - org.apache.cassandra.db.BufferExpiringCell#reconcile()
//  - org.apache.cassandra.db.BufferDeletedCell#reconcile()
std::strong_ordering compare_atomic_cell_for_merge(atomic_cell_view left, atomic_cell_view right)
{
if (left.timestamp() != right.timestamp())
{
    return left.timestamp() <=> right.timestamp();
}
if (left.is_live() != right.is_live())
{
    return left.is_live() ? std::strong_ordering::less : std::strong_ordering::greater;
}
if (left.is_live())
{
    auto c = compare_unsigned(left.value(), right.value()) <=> 0;
    if (c != 0)
    {
        return c;
    }
    if (left.is_live_and_has_ttl() != right.is_live_and_has_ttl())
    { // prefer expiring cells.
        return left.is_live_and_has_ttl() ? std::strong_ordering::greater : std::strong_ordering::less;
    }
    if (left.is_live_and_has_ttl())
    {
        if (left.expiry() != right.expiry())
        {
        return left.expiry() <=> right.expiry();
        }
        else
        { // prefer the cell that was written later,
        // so it survives longer after it expires, until purged.
        return right.ttl() <=> left.ttl();
        }
    }
}
else
{ // Both are deleted
    // Origin compares big-endian serialized deletion time. That's because it
    // delegates to AbstractCell.reconcile() which compares values after
    // comparing timestamps, which in case of deleted cells will hold
    // serialized expiry.
    return (uint64_t)left.deletion_time().time_since_epoch().count() <=> (uint64_t)right.deletion_time().time_since_epoch().count();
}
return std::strong_ordering::equal;
}
std::ostream &operator<<(std::ostream &os, const atomic_cell_view &acv);




//
// Representation layout:
//
// <mutation> ::= <column-family-id> <schema-version> <partition-key> <partition>
//
using namespace db;
class fragmenting_mutation_freezer
{
const schema &_schema;
std::optional<partition_key> _key;
tombstone _partition_tombstone;
std::optional<static_row> _sr;
std::deque<clustering_row> _crs;
range_tombstone_list _rts;
frozen_mutation_consumer_fn _consumer;
bool _fragmented = false;
size_t _dirty_size = 0;
size_t _fragment_size;
range_tombstone_change _current_rtc;
private:
public:
};
mutation::data::data(partition_key &&key_, schema_ptr &&schema) : _schema(std::move(schema)), _dk(dht::decorate_key(*_schema, std::move(key_))), _p(_schema) {}
void mutation::set_static_cell(const column_definition &def, atomic_cell_or_collection &&value) { partition().static_row().apply(def, std::move(value)); }
void mutation::set_clustered_cell(const clustering_key &key, const column_definition &def, atomic_cell_or_collection &&value)
{
auto &row = partition().clustered_row(*schema(), key).cells();
row.apply(def, std::move(value));
}
boost::iterator_range<std::vector<mutation>::const_iterator> slice(const std::vector<mutation> &partitions, const dht::partition_range &r)
{
struct cmp
{
    bool operator()(const dht::ring_position &pos, const mutation &m) const { return m.decorated_key().tri_compare(*m.schema(), pos) > 0; };
    bool operator()(const mutation &m, const dht::ring_position &pos) const { return m.decorated_key().tri_compare(*m.schema(), pos) < 0; };
};
return boost::make_iterator_range(r.start() ? (r.start()->is_inclusive() ? std::lower_bound(partitions.begin(), partitions.end(), r.start()->value(), cmp()) : std::upper_bound(partitions.begin(), partitions.end(), r.start()->value(), cmp())) : partitions.cbegin(), r.end() ? (r.end()->is_inclusive() ? std::upper_bound(partitions.begin(), partitions.end(), r.end()->value(), cmp()) : std::lower_bound(partitions.begin(), partitions.end(), r.end()->value(), cmp())) : partitions.cend());
}
void mutation::upgrade(const schema_ptr &new_schema)
{
if (_ptr->_schema != new_schema)
{
    schema_ptr s = new_schema;
    partition().upgrade(*schema(), *new_schema);
    _ptr->_schema = std::move(s);
}
}
void mutation::apply(mutation &&m)
{
mutation_application_stats app_stats;
partition().apply(*schema(), std::move(m.partition()), *m.schema(), app_stats);
}
void mutation::apply(const mutation &m)
{
mutation_application_stats app_stats;
partition().apply(*schema(), m.partition(), *m.schema(), app_stats);
}
void mutation::apply(const mutation_fragment &mf) { partition().apply(*schema(), mf); }
mutation &mutation::operator=(const mutation &m) { return *this = mutation(m); }
mutation mutation::operator+(const mutation &other) const
{
auto m = *this;
m.apply(other);
return m;
}
mutation &mutation::operator+=(const mutation &other)
{
apply(other);
return *this;
}
mutation &mutation::operator+=(mutation &&other)
{
apply(std::move(other));
return *this;
}
mutation mutation::sliced(const query::clustering_row_ranges &ranges) const { return mutation(schema(), decorated_key(), partition().sliced(*schema(), ranges)); }
mutation mutation::compacted() const
{
auto m = *this;
m.partition().compact_for_compaction(*schema(), always_gc, m.decorated_key(), gc_clock::time_point::min(), tombstone_gc_state(nullptr));
return m;
}
mutation reverse(mutation mut)
{
auto reverse_schema = mut.schema()->make_reversed();
mutation_rebuilder_v2 reverse_rebuilder(reverse_schema);
return *std::move(mut).consume(reverse_rebuilder, consume_in_reverse::yes).result;
}
std::ostream &operator<<(std::ostream &os, const mutation &m)
{
const ::schema &s = *m.schema();
const auto &dk = m.decorated_key();
fmt::print(os, "{{table: '{}.{}', key: {{", s.ks_name(), s.cf_name());
auto type_iterator = dk._key.get_compound_type(s)->types().begin();
auto column_iterator = s.partition_key_columns().begin();
for (auto &&e : dk._key.components(s))
{
    os << "'" << column_iterator->name_as_text() << "': " << (*type_iterator)->to_string(to_bytes(e)) << ", ";
    ++type_iterator;
    ++column_iterator;
}
fmt::print(os, "token: {}}}, ", dk._token);
os << mutation_partition::printer(s, m.partition()) << "\n}";
return os;
}
std::ostream &operator<<(std::ostream &os, const clustering_row::printer &p)
{
auto &row = p._clustering_row;
return os << "{clustering_row: ck " << row._ck << " dr " << deletable_row::printer(p._schema, row._row) << "}";
}
std::ostream &operator<<(std::ostream &os, const static_row::printer &p) { return os << "{static_row: " << row::printer(p._schema, column_kind::static_column, p._static_row._cells) << "}"; }
std::ostream &operator<<(std::ostream &os, const partition_start &ph)
{
fmt::print(os, "{{partition_start: pk {} partition_tombstone {}}}", ph._key, ph._partition_tombstone);
return os;
}
std::ostream &operator<<(std::ostream &os, const partition_end &eop) { return os << "{partition_end}"; }
partition_region parse_partition_region(std::string_view s)
{
if (s == "partition_start")
{
    return partition_region::partition_start;
}
else if (s == "static_row")
{
    return partition_region::static_row;
}
else if (s == "clustered")
{
    return partition_region::clustered;
}
else if (s == "partition_end")
{
    return partition_region::partition_end;
}
else
{
    throw std::runtime_error(fmt::format("Invalid value for partition_region: {}", s));
}
}
std::ostream &operator<<(std::ostream &out, position_in_partition_view pos)
{
fmt::print(out, "{}", pos);
return out;
}
std::ostream &operator<<(std::ostream &out, const position_in_partition &pos)
{
fmt::print(out, "{}", pos);
return out;
}
std::ostream &operator<<(std::ostream &out, const position_range &range) { return out << "{" << range.start() << ", " << range.end() << "}"; }
mutation_fragment::mutation_fragment(const schema &s, reader_permit permit, static_row &&r) : _kind(kind::static_row), _data(std::make_unique<data>(std::move(permit)))
{
new (&_data->_static_row) static_row(std::move(r));
reset_memory(s);
}
mutation_fragment::mutation_fragment(const schema &s, reader_permit permit, clustering_row &&r) : _kind(kind::clustering_row), _data(std::make_unique<data>(std::move(permit)))
{
new (&_data->_clustering_row) clustering_row(std::move(r));
reset_memory(s);
}
mutation_fragment::mutation_fragment(const schema &s, reader_permit permit, range_tombstone &&r) : _kind(kind::range_tombstone), _data(std::make_unique<data>(std::move(permit)))
{
new (&_data->_range_tombstone) range_tombstone(std::move(r));
reset_memory(s);
}
mutation_fragment::mutation_fragment(const schema &s, reader_permit permit, partition_start &&r) : _kind(kind::partition_start), _data(std::make_unique<data>(std::move(permit)))
{
new (&_data->_partition_start) partition_start(std::move(r));
reset_memory(s);
}
mutation_fragment::mutation_fragment(const schema &s, reader_permit permit, partition_end &&r) : _kind(kind::partition_end), _data(std::make_unique<data>(std::move(permit)))
{
new (&_data->_partition_end) partition_end(std::move(r));
reset_memory(s);
}
void mutation_fragment::reset_memory(const schema &s, std::optional<reader_resources> res)
{
try
{
    _data->_memory.reset_to(res ? *res : reader_resources::with_memory(calculate_memory_usage(s)));
}
catch (...)
{
    destroy_data();
    throw;
}
}
void mutation_fragment::destroy_data() noexcept
{
switch (_kind)
{
case kind::static_row:
    _data->_static_row.~static_row();
    break;
case kind::clustering_row:
    _data->_clustering_row.~clustering_row();
    break;
case kind::range_tombstone:
    _data->_range_tombstone.~range_tombstone();
    break;
case kind::partition_start:
    _data->_partition_start.~partition_start();
    break;
case kind::partition_end:
    _data->_partition_end.~partition_end();
    break;
}
}
mutation_fragment_v2::mutation_fragment_v2(const schema &s, reader_permit permit, static_row &&r) : _kind(kind::static_row), _data(std::make_unique<data>(std::move(permit)))
{
new (&_data->_static_row) static_row(std::move(r));
reset_memory(s);
}
mutation_fragment_v2::mutation_fragment_v2(const schema &s, reader_permit permit, clustering_row &&r) : _kind(kind::clustering_row), _data(std::make_unique<data>(std::move(permit)))
{
new (&_data->_clustering_row) clustering_row(std::move(r));
reset_memory(s);
}
mutation_fragment_v2::mutation_fragment_v2(const schema &s, reader_permit permit, range_tombstone_change &&r) : _kind(kind::range_tombstone_change), _data(std::make_unique<data>(std::move(permit)))
{
new (&_data->_range_tombstone_chg) range_tombstone_change(std::move(r));
reset_memory(s);
}
mutation_fragment_v2::mutation_fragment_v2(const schema &s, reader_permit permit, partition_start &&r) : _kind(kind::partition_start), _data(std::make_unique<data>(std::move(permit)))
{
new (&_data->_partition_start) partition_start(std::move(r));
reset_memory(s);
}
mutation_fragment_v2::mutation_fragment_v2(const schema &s, reader_permit permit, partition_end &&r) : _kind(kind::partition_end), _data(std::make_unique<data>(std::move(permit)))
{
new (&_data->_partition_end) partition_end(std::move(r));
reset_memory(s);
}
void mutation_fragment_v2::destroy_data() noexcept
{
switch (_kind)
{
case kind::static_row:
    _data->_static_row.~static_row();
    break;
case kind::clustering_row:
    _data->_clustering_row.~clustering_row();
    break;
case kind::range_tombstone_change:
    _data->_range_tombstone_chg.~range_tombstone_change();
    break;
case kind::partition_start:
    _data->_partition_start.~partition_start();
    break;
case kind::partition_end:
    _data->_partition_end.~partition_end();
    break;
}
}
void mutation_fragment_v2::reset_memory(const schema &s, std::optional<reader_resources> res)
{
try
{
    _data->_memory.reset_to(res ? *res : reader_resources::with_memory(calculate_memory_usage(s)));
}
catch (...)
{
    destroy_data();
    throw;
}
}
namespace
{
struct get_key_visitor
{
    const clustering_key_prefix &operator()(const clustering_row &cr) { return cr.key(); }
    const clustering_key_prefix &operator()(const range_tombstone &rt) { return rt.start; }
    const clustering_key_prefix &operator()(const range_tombstone_change &rt) { return rt.position().key(); }
    template <typename T>
    const clustering_key_prefix &operator()(const T &) { abort(); }
};
}
const clustering_key_prefix &mutation_fragment::key() const
{
assert(has_key());
return visit(get_key_visitor());
}
void mutation_fragment::apply(const schema &s, mutation_fragment &&mf)
{
assert(mergeable_with(mf));
switch (_kind)
{
case mutation_fragment::kind::partition_start:
    _data->_partition_start.partition_tombstone().apply(mf._data->_partition_start.partition_tombstone());
    mf._data->_partition_start.~partition_start();
    break;
case kind::static_row:
    _data->_static_row.apply(s, std::move(mf._data->_static_row));
    mf._data->_static_row.~static_row();
    break;
case kind::clustering_row:
    _data->_clustering_row.apply(s, std::move(mf._data->_clustering_row));
    mf._data->_clustering_row.~clustering_row();
    break;
case mutation_fragment::kind::partition_end: // Nothing to do for this guy.
    mf._data->_partition_end.~partition_end();
    break;
default:
    abort();
}
mf._data.reset();
reset_memory(s);
}
position_in_partition_view mutation_fragment::position() const
{
return visit([](auto &mf) -> position_in_partition_view
             { return mf.position(); });
}
position_range mutation_fragment::range(const schema &s) const
{
switch (_kind)
{
case kind::static_row:
    return position_range::for_static_row();
case kind::clustering_row:
    return position_range(position_in_partition(position()), position_in_partition::after_key(s, key()));
case kind::partition_start:
    return position_range(position_in_partition(position()), position_in_partition::for_static_row());
case kind::partition_end:
    return position_range(position_in_partition(position()), position_in_partition::after_all_clustered_rows());
case kind::range_tombstone:
    auto &&rt = as_range_tombstone();
    return position_range(position_in_partition(rt.position()), position_in_partition(rt.end_position()));
}
abort();
}
std::ostream &operator<<(std::ostream &os, mutation_fragment::kind k)
{
switch (k)
{
case mutation_fragment::kind::static_row:
    return os << "static row";
case mutation_fragment::kind::clustering_row:
    return os << "clustering row";
case mutation_fragment::kind::range_tombstone:
    return os << "range tombstone";
case mutation_fragment::kind::partition_start:
    return os << "partition start";
case mutation_fragment::kind::partition_end:
    return os << "partition end";
}
abort();
}
std::ostream &operator<<(std::ostream &os, const mutation_fragment::printer &p)
{
auto &mf = p._mutation_fragment;
os << "{mutation_fragment: " << mf._kind << " " << mf.position() << " ";
mf.visit(make_visitor([&](const clustering_row &cr)
                      { os << clustering_row::printer(p._schema, cr); },
                      [&](const static_row &sr)
                      { os << static_row::printer(p._schema, sr); },
                      [&](const auto &what) -> void
                      { fmt::print(os, "{}", what); }));
os << "}";
return os;
}
const clustering_key_prefix &mutation_fragment_v2::key() const
{
assert(has_key());
return visit(get_key_visitor());
}
void mutation_fragment_v2::apply(const schema &s, mutation_fragment_v2 &&mf)
{
assert(mergeable_with(mf));
switch (_kind)
{
case mutation_fragment_v2::kind::partition_start:
    _data->_partition_start.partition_tombstone().apply(mf._data->_partition_start.partition_tombstone());
    mf._data->_partition_start.~partition_start();
    break;
case kind::static_row:
    _data->_static_row.apply(s, std::move(mf._data->_static_row));
    mf._data->_static_row.~static_row();
    break;
case kind::clustering_row:
    _data->_clustering_row.apply(s, std::move(mf._data->_clustering_row));
    mf._data->_clustering_row.~clustering_row();
    break;
case mutation_fragment_v2::kind::partition_end: // Nothing to do for this guy.
    mf._data->_partition_end.~partition_end();
    break;
default:
    abort();
}
mf._data.reset();
reset_memory(s);
}
position_in_partition_view mutation_fragment_v2::position() const
{
return visit([](auto &mf) -> position_in_partition_view
             { return mf.position(); });
}
std::ostream &operator<<(std::ostream &os, mutation_fragment_v2::kind k)
{
switch (k)
{
case mutation_fragment_v2::kind::static_row:
    return os << "static row";
case mutation_fragment_v2::kind::clustering_row:
    return os << "clustering row";
case mutation_fragment_v2::kind::range_tombstone_change:
    return os << "range tombstone change";
case mutation_fragment_v2::kind::partition_start:
    return os << "partition start";
case mutation_fragment_v2::kind::partition_end:
    return os << "partition end";
}
abort();
}
std::ostream &operator<<(std::ostream &os, const mutation_fragment_v2::printer &p)
{
auto &mf = p._mutation_fragment;
os << "{mutation_fragment: " << mf._kind << " " << mf.position() << " ";
mf.visit(make_visitor([&](const clustering_row &cr)
                      { os << clustering_row::printer(p._schema, cr); },
                      [&](const static_row &sr)
                      { os << static_row::printer(p._schema, sr); },
                      [&](const auto &what) -> void
                      { fmt::print(os, "{}", what); }));
os << "}";
return os;
}
mutation_fragment_opt range_tombstone_stream::do_get_next() { return mutation_fragment(_schema, _permit, _list.pop(_list.begin())); }
mutation_fragment_opt range_tombstone_stream::get_next(const rows_entry &re)
{
if (!_list.empty())
{
    return !_cmp(re.position(), _list.begin()->position()) ? do_get_next() : mutation_fragment_opt();
}
return {};
}
mutation_fragment_opt range_tombstone_stream::get_next(const mutation_fragment &mf)
{
if (!_list.empty())
{
    return !_cmp(mf.position(), _list.begin()->position()) ? do_get_next() : mutation_fragment_opt();
}
return {};
}
mutation_fragment_opt range_tombstone_stream::get_next(position_in_partition_view upper_bound)
{
if (!_list.empty())
{
    return _cmp(_list.begin()->position(), upper_bound) ? do_get_next() : mutation_fragment_opt();
}
return {};
}
mutation_fragment_opt range_tombstone_stream::get_next()
{
if (!_list.empty())
{
    return do_get_next();
}
return {};
}
const range_tombstone &range_tombstone_stream::peek_next() const { return _list.begin()->tombstone(); }
void range_tombstone_stream::forward_to(position_in_partition_view pos)
{
_list.erase_where([this, &pos](const range_tombstone &rt)
                  { return !_cmp(pos, rt.end_position()); });
}
void range_tombstone_stream::reset() { _list.clear(); }
bool range_tombstone_stream::empty() const { return _list.empty(); }
position_range position_range::from_range(const query::clustering_range &range)
{
auto bv_range = bound_view::from_range(range);
return {position_in_partition(position_in_partition::range_tag_t(), bv_range.first), position_in_partition(position_in_partition::range_tag_t(), bv_range.second)};
}
position_range::position_range(const query::clustering_range &range) : position_range(from_range(range)) {}
position_range::position_range(query::clustering_range &&range) : position_range(range) // FIXME: optimize
{
}
bool mutation_fragment::relevant_for_range(const schema &s, position_in_partition_view pos) const
{
position_in_partition::less_compare cmp(s);
if (!cmp(position(), pos))
{
    return true;
}
return relevant_for_range_assuming_after(s, pos);
}
bool mutation_fragment::relevant_for_range_assuming_after(const schema &s, position_in_partition_view pos) const
{
position_in_partition::less_compare cmp(s); // Range tombstones overlapping with the new range are let in
return is_range_tombstone() && cmp(pos, as_range_tombstone().end_position());
}
bool mutation_fragment_v2::relevant_for_range(const schema &s, position_in_partition_view pos) const
{
position_in_partition::less_compare less(s);
if (!less(position(), pos))
{
    return true;
}
return false;
}
std::ostream &operator<<(std::ostream &out, const range_tombstone_stream &rtl)
{
fmt::print(out, "{}", rtl._list);
return out;
}
std::ostream &operator<<(std::ostream &out, const clustering_interval_set &set)
{
fmt::print(out, "{{{}}}", fmt::join(set, ",\n  "));
return out;
}
template <typename Hasher>
void appending_hash<mutation_fragment>::operator()(Hasher &h, const mutation_fragment &mf, const schema &s) const
{
auto hash_cell = [&](const column_definition &col, const atomic_cell_or_collection &cell)
{         feed_hash(h, col.kind);         feed_hash(h, col.id);         feed_hash(h, cell, col); };
mf.visit(seastar::make_visitor([&](const clustering_row &cr)
                               {             feed_hash(h, cr.key(), s);             feed_hash(h, cr.tomb());             feed_hash(h, cr.marker());             cr.cells().for_each_cell([&] (column_id id, const atomic_cell_or_collection& cell) {                 auto&& col = s.regular_column_at(id);                 hash_cell(col, cell);             }); },
                               [&](const static_row &sr)
                               { sr.cells().for_each_cell([&](column_id id, const atomic_cell_or_collection &cell)
                                                          {                 auto&& col = s.static_column_at(id);                 hash_cell(col, cell); }); },
                               [&](const range_tombstone &rt)
                               {             feed_hash(h, rt.start, s);             feed_hash(h, rt.start_kind);             feed_hash(h, rt.tomb);             feed_hash(h, rt.end, s);             feed_hash(h, rt.end_kind); },
                               [&](const partition_start &ps)
                               {             feed_hash(h, ps.key().key(), s);             if (ps.partition_tombstone()) {                 feed_hash(h, ps.partition_tombstone());             } },
                               [&](const partition_end &pe)
                               { throw std::runtime_error("partition_end is not expected"); }));
}
// Instantiation for repair/row_level.cc
template void appending_hash<mutation_fragment>::operator()<xx_hasher>(xx_hasher &h, const mutation_fragment &cells, const schema &s) const;
logging::logger validator_log("mutation_fragment_stream_validator");
static mutation_fragment_v2::kind to_mutation_fragment_kind_v2(mutation_fragment::kind k);


namespace
{
[[noreturn]] void on_validation_error(seastar::logger &l, const mutation_fragment_stream_validating_filter &zis, mutation_fragment_stream_validator::validation_result res)
{
    try
    {
        on_internal_error(l, format("[validator {} for {}] {}", fmt::ptr(&zis), zis.full_name(), res.what()));
    }
    catch (std::runtime_error &e)
    {
        throw invalid_mutation_fragment_stream(e);
    }
}
}
logging::logger mplog("mutation_partition");
template <bool reversed>
struct reversal_traits;
template <>
struct reversal_traits<false>
{
template <typename Container>
static auto begin(Container &c) { return c.begin(); }
template <typename Container>
static auto end(Container &c) { return c.end(); }
template <typename Container, typename Disposer>
static typename Container::iterator erase_and_dispose(Container &c, typename Container::iterator begin, typename Container::iterator end, Disposer disposer);
template <typename Container, typename Disposer>
static typename Container::iterator erase_dispose_and_update_end(Container &c, typename Container::iterator it, Disposer &&disposer, typename Container::iterator &);
template <typename Container>
static typename Container::iterator maybe_reverse(Container &, typename Container::iterator r) ;
};
template <>
struct reversal_traits<true>
{
template <typename Container>
static auto begin(Container &c) { return c.rbegin(); }
template <typename Container>
static auto end(Container &c) { return c.rend(); }
template <typename Container, typename Disposer>
static typename Container::reverse_iterator erase_and_dispose(Container &c, typename Container::reverse_iterator begin, typename Container::reverse_iterator end, Disposer disposer) ; // Erases element pointed to by it and makes sure than iterator end is not
// invalidated.
template <typename Container, typename Disposer>
static typename Container::reverse_iterator erase_dispose_and_update_end(Container &c, typename Container::reverse_iterator it, Disposer &&disposer, typename Container::reverse_iterator &end);
template <typename Container>
static typename Container::reverse_iterator maybe_reverse(Container &, typename Container::iterator r);
};

struct mutation_fragment_applier
{
const schema &_s;
mutation_partition &_mp;



};
void mutation_partition::apply_row_tombstone(const schema &schema, clustering_key_prefix prefix, tombstone t)
{
check_schema(schema);
assert(!prefix.is_full(schema));
auto start = prefix;
_row_tombstones.apply(schema, {std::move(start), std::move(prefix), std::move(t)});
}
void mutation_partition::apply_row_tombstone(const schema &schema, range_tombstone rt)
{
check_schema(schema);
_row_tombstones.apply(schema, std::move(rt));
}
void mutation_partition::apply_delete(const schema &schema, range_tombstone rt)
{
check_schema(schema);
if (range_tombstone::is_single_clustering_row_tombstone(schema, rt.start, rt.start_kind, rt.end, rt.end_kind))
{
    apply_delete(schema, std::move(rt.start), std::move(rt.tomb));
    return;
}
apply_row_tombstone(schema, std::move(rt));
}
void mutation_partition::apply_delete(const schema &schema, clustering_key &&prefix, tombstone t)
{
check_schema(schema);
if (prefix.is_empty(schema))
{
    apply(t);
}
else if (prefix.is_full(schema))
{
    clustered_row(schema, std::move(prefix)).apply(t);
}
else
{
    apply_row_tombstone(schema, std::move(prefix), t);
}
}
deletable_row &mutation_partition::clustered_row(const schema &s, clustering_key &&key)
{
check_schema(s);
auto i = _rows.find(key, rows_entry::tri_compare(s));
if (i == _rows.end())
{
    auto e = alloc_strategy_unique_ptr<rows_entry>(current_allocator().construct<rows_entry>(std::move(key)));
    i = _rows.insert_before_hint(i, std::move(e), rows_entry::tri_compare(s)).first;
}
return i->row();
}
deletable_row &mutation_partition::clustered_row(const schema &s, const clustering_key &key)
{
check_schema(s);
auto i = _rows.find(key, rows_entry::tri_compare(s));
if (i == _rows.end())
{
    auto e = alloc_strategy_unique_ptr<rows_entry>(current_allocator().construct<rows_entry>(key));
    i = _rows.insert_before_hint(i, std::move(e), rows_entry::tri_compare(s)).first;
}
return i->row();
}
;
;
;
;
template <typename RowWriter>
static void get_compacted_row_slice(const schema &s, const query::partition_slice &slice, column_kind kind, const row &cells, const query::column_id_vector &columns, RowWriter &writer);


// Transforms given range of printable into a range of strings where each element
// in the original range is prefxied with given string.
template <typename RangeOfPrintable>
static auto prefixed(const sstring &prefix, const RangeOfPrintable &r);
constexpr gc_clock::duration row_marker::no_ttl;
constexpr gc_clock::duration row_marker::dead;
int compare_row_marker_for_merge(const row_marker &left, const row_marker &right) noexcept
{
if (left.timestamp() != right.timestamp())
{
    return left.timestamp() > right.timestamp() ? 1 : -1;
}
if (left.is_live() != right.is_live())
{
    return left.is_live() ? -1 : 1;
}
if (left.is_live())
{
    if (left.is_expiring() != right.is_expiring())
    { // prefer expiring cells.
        return left.is_expiring() ? 1 : -1;
    }
    if (left.is_expiring() && left.expiry() != right.expiry())
    {
        return left.expiry() < right.expiry() ? -1 : 1;
    }
}
else
{ // Both are either deleted or missing
    if (left.deletion_time() != right.deletion_time())
    { // Origin compares big-endian serialized deletion time. That's because it
        // delegates to AbstractCell.reconcile() which compares values after
        // comparing timestamps, which in case of deleted cells will hold
        // serialized expiry.
        return (uint64_t)left.deletion_time().time_since_epoch().count() < (uint64_t)right.deletion_time().time_since_epoch().count() ? -1 : 1;
    }
}
return 0;
}
static void apply_monotonically(const column_definition &def, cell_and_hash &dst, atomic_cell_or_collection &src, cell_hash_opt src_hash)
{
if (def.is_atomic())
{
    if (def.is_counter())
    {
        counter_cell_view::apply(def, dst.cell, src); // FIXME: Optimize
        dst.hash = {};
    }
    else if (compare_atomic_cell_for_merge(dst.cell.as_atomic_cell(def), src.as_atomic_cell(def)) < 0)
    {
        using std::swap;
        swap(dst.cell, src);
        dst.hash = std::move(src_hash);
    }
}
else
{
    dst.cell = merge(*def.type, dst.cell.as_collection_mutation(), src.as_collection_mutation());
    dst.hash = {};
}
}
void row::apply(const column_definition &column, atomic_cell_or_collection &&value, cell_hash_opt hash) { apply_monotonically(column, std::move(value), std::move(hash)); }
template <typename Func>
void row::consume_with(Func &&func)
{
_cells.weed([func, this](column_id id, cell_and_hash &cah)
            {         func(id, cah);         _size--;         return true; });
}
void row::apply_monotonically(const column_definition &column, atomic_cell_or_collection &&value, cell_hash_opt hash)
{
static_assert(std::is_nothrow_move_constructible<atomic_cell_or_collection>::value && std::is_nothrow_move_assignable<atomic_cell_or_collection>::value, "noexcept required for atomicity"); // our mutations are not yet immutable
auto id = column.id;
cell_and_hash *cah = _cells.get(id);
if (cah == nullptr)
{ // FIXME -- add .locate method to radix_tree to find or allocate a spot
    _cells.emplace(id, std::move(value), std::move(hash));
    _size++;
}
else
{
    ::apply_monotonically(column, *cah, value, std::move(hash));
}
}
template <bool reversed, typename Func>
    requires std::is_invocable_r_v<stop_iteration, Func, rows_entry &>
void mutation_partition::trim_rows(const schema &s, const std::vector<query::clustering_range> &row_ranges, Func &&func)
{
check_schema(s);
stop_iteration stop = stop_iteration::no;
auto last = reversal_traits<reversed>::begin(_rows);
auto deleter = current_deleter<rows_entry>();
auto range_begin = [this, &s](const query::clustering_range &range)
{ return reversed ? upper_bound(s, range) : lower_bound(s, range); };
auto range_end = [this, &s](const query::clustering_range &range)
{ return reversed ? lower_bound(s, range) : upper_bound(s, range); };
for (auto &&row_range : row_ranges)
{
    if (stop)
    {
        break;
    }
    last = reversal_traits<reversed>::erase_and_dispose(_rows, last, reversal_traits<reversed>::maybe_reverse(_rows, range_begin(row_range)), deleter);
    auto end = reversal_traits<reversed>::maybe_reverse(_rows, range_end(row_range));
    while (last != end && !stop)
    {
        rows_entry &e = *last;
        stop = func(e);
        if (e.empty())
        {
        last = reversal_traits<reversed>::erase_dispose_and_update_end(_rows, last, deleter, end);
        }
        else
        {
        ++last;
        }
    }
}
reversal_traits<reversed>::erase_and_dispose(_rows, last, reversal_traits<reversed>::end(_rows), deleter);
}
uint32_t mutation_partition::do_compact(const schema &s, const dht::decorated_key &dk, gc_clock::time_point query_time, const std::vector<query::clustering_range> &row_ranges, bool always_return_static_content, bool reverse, uint64_t row_limit, can_gc_fn &can_gc, bool drop_tombstones_unconditionally, const tombstone_gc_state &gc_state)
{
check_schema(s);
assert(row_limit > 0);
auto gc_before = drop_tombstones_unconditionally ? gc_clock::time_point::max() : gc_state.get_gc_before_for_key(s.shared_from_this(), dk, query_time);
auto should_purge_tombstone = [&](const tombstone &t)
{ return t.deletion_time < gc_before && can_gc(t); };
bool static_row_live = _static_row.compact_and_expire(s, column_kind::static_column, row_tombstone(_tombstone), query_time, can_gc, gc_before);
uint64_t row_count = 0;
auto row_callback = [&](rows_entry &e)
{         if (e.dummy()) {             return stop_iteration::no;         }         deletable_row& row = e.row();         tombstone tomb = range_tombstone_for_row(s, e.key());         bool is_live = row.compact_and_expire(s, tomb, query_time, can_gc, gc_before, nullptr);         return stop_iteration(is_live && ++row_count == row_limit); };
if (reverse)
{
    trim_rows<true>(s, row_ranges, row_callback);
}
else
{
    trim_rows<false>(s, row_ranges, row_callback);
} // #589 - Do not add extra row for statics unless we did a CK range-less query.
// See comment in query
bool return_static_content_on_partition_with_no_rows = always_return_static_content || !has_ck_selector(row_ranges);
if (row_count == 0 && static_row_live && return_static_content_on_partition_with_no_rows)
{
    ++row_count;
}
_row_tombstones.erase_where([&](auto &&rt)
                            { return should_purge_tombstone(rt.tomb) || rt.tomb <= _tombstone; });
if (should_purge_tombstone(_tombstone))
{
    _tombstone = tombstone();
} // FIXME: purge unneeded prefix tombstones based on row_ranges
return row_count;
}
uint64_t mutation_partition::compact_for_query(const schema &s, const dht::decorated_key &dk, gc_clock::time_point query_time, const std::vector<query::clustering_range> &row_ranges, bool always_return_static_content, bool reverse, uint64_t row_limit)
{
check_schema(s);
bool drop_tombstones_unconditionally = false; // Replicas should only send non-purgeable tombstones already,
// so we can expect to not have to actually purge any tombstones here.
return do_compact(s, dk, query_time, row_ranges, always_return_static_content, reverse, row_limit, always_gc, drop_tombstones_unconditionally, tombstone_gc_state(nullptr));
}
void mutation_partition::compact_for_compaction(const schema &s, can_gc_fn &can_gc, const dht::decorated_key &dk, gc_clock::time_point compaction_time, const tombstone_gc_state &gc_state)
{
check_schema(s);
static const std::vector<query::clustering_range> all_rows = {query::clustering_range::make_open_ended_both_sides()};
bool drop_tombstones_unconditionally = false;
do_compact(s, dk, compaction_time, all_rows, true, false, query::partition_max_rows, can_gc, drop_tombstones_unconditionally, gc_state);
}
void mutation_partition::compact_for_compaction_drop_tombstones_unconditionally(const schema &s, const dht::decorated_key &dk)
{
check_schema(s);
static const std::vector<query::clustering_range> all_rows = {query::clustering_range::make_open_ended_both_sides()};
bool drop_tombstones_unconditionally = true;
auto compaction_time = gc_clock::time_point::max();
do_compact(s, dk, compaction_time, all_rows, true, false, query::partition_max_rows, always_gc, drop_tombstones_unconditionally, tombstone_gc_state(nullptr));
}
// Returns true if the mutation_partition represents no writes.
bool mutation_partition::empty() const
{
if (_tombstone.timestamp != api::missing_timestamp)
{
    return false;
}
return !_static_row.size() && _rows.empty() && _row_tombstones.empty();
}
bool deletable_row::is_live(const schema &s, column_kind kind, tombstone base_tombstone, gc_clock::time_point query_time) const
{ // _created_at corresponds to the row marker cell, present for rows
// created with the 'insert' statement. If row marker is live, we know the
// row is live. Otherwise, a row is considered live if it has any cell
// which is live.
base_tombstone.apply(_deleted_at.tomb());
return _marker.is_live(base_tombstone, query_time) || _cells.is_live(s, kind, base_tombstone, query_time);
}
bool row::is_live(const schema &s, column_kind kind, tombstone base_tombstone, gc_clock::time_point query_time) const { return has_any_live_data(s, kind, *this, base_tombstone, query_time); }
bool mutation_partition::is_static_row_live(const schema &s, gc_clock::time_point query_time) const
{
check_schema(s);
return has_any_live_data(s, column_kind::static_column, static_row().get(), _tombstone, query_time);
}
uint64_t mutation_partition::live_row_count(const schema &s, gc_clock::time_point query_time) const
{
check_schema(s);
uint64_t count = 0;
for (const rows_entry &e : non_dummy_rows())
{
    tombstone base_tombstone = range_tombstone_for_row(s, e.key());
    if (e.row().is_live(s, column_kind::regular_column, base_tombstone, query_time))
    {
        ++count;
    }
}
if (count == 0 && is_static_row_live(s, query_time))
{
    return 1;
}
return count;
}
uint64_t mutation_partition::row_count() const { return _rows.calculate_size(); }
rows_entry::rows_entry(rows_entry &&o) noexcept : evictable(std::move(o)), _link(std::move(o._link)), _key(std::move(o._key)), _row(std::move(o._row)), _range_tombstone(std::move(o._range_tombstone)), _flags(std::move(o._flags)) {}
void rows_entry::compact(const schema &s, tombstone t)
{
can_gc_fn never_gc = [](tombstone)
{ return false; };
row().compact_and_expire(s, t + _range_tombstone, gc_clock::time_point::min(), // no TTL expiration
                         never_gc,                                             // no GC
                         gc_clock::time_point::min());                         // no GC
// FIXME: Purge redundant _range_tombstone
}
void rows_entry::replace_with(rows_entry &&o) noexcept
{
swap(o);
_range_tombstone = std::move(o._range_tombstone);
_row = std::move(o._row);
}
row::row(const schema &s, column_kind kind, const row &o) : _size(o._size)
{
auto clone_cell_and_hash = [&s, &kind](column_id id, const cell_and_hash &cah)
{         auto& cdef = s.column_at(kind, id);         return cell_and_hash(cah.cell.copy(*cdef.type), cah.hash); };
_cells.clone_from(o._cells, clone_cell_and_hash);
}
row::~row() {}
const atomic_cell_or_collection &row::cell_at(column_id id) const
{
auto &&cell = find_cell(id);
if (!cell)
{
    throw_with_backtrace<std::out_of_range>(format("Column not found for id = {:d}", id));
}
return *cell;
}
bool row::equal(column_kind kind, const schema &this_schema, const row &other, const schema &other_schema) const
{
if (size() != other.size())
{
    return false;
}
auto cells_equal = [&](column_id id1, const atomic_cell_or_collection &c1, column_id id2, const atomic_cell_or_collection &c2)
{         static_assert(schema::row_column_ids_are_ordered_by_name::value, "Relying on column ids being ordered by name");         auto& at1 = *this_schema.column_at(kind, id1).type;         auto& at2 = *other_schema.column_at(kind, id2).type;         return at1 == at2                && this_schema.column_at(kind, id1).name() == other_schema.column_at(kind, id2).name()                && c1.equals(at1, c2); };
auto i1 = _cells.begin();
auto i1_end = _cells.end();
auto i2 = other._cells.begin();
auto i2_end = other._cells.end();
while (true)
{
    if (i1 == i1_end)
    {
        return i2 == i2_end;
    }
    if (i2 == i2_end)
    {
        return i1 == i1_end;
    }
    if (!cells_equal(i1.key(), i1->cell, i2.key(), i2->cell))
    {
        return false;
    }
    i1++;
    i2++;
}
}
row::row() {}
row::row(row &&other) noexcept : _size(other._size), _cells(std::move(other._cells)) { other._size = 0; }
row &row::operator=(row &&other) noexcept
{
if (this != &other)
{
    this->~row();
    new (this) row(std::move(other));
}
return *this;
}
void row::apply(const schema &s, column_kind kind, const row &other)
{
if (other.empty())
{
    return;
}
other.for_each_cell([&](column_id id, const cell_and_hash &c_a_h)
                    { apply(s.column_at(kind, id), c_a_h.cell, c_a_h.hash); });
}
void row::apply(const schema &s, column_kind kind, row &&other) { apply_monotonically(s, kind, std::move(other)); }
void row::apply_monotonically(const schema &s, column_kind kind, row &&other)
{
if (other.empty())
{
    return;
}
other.consume_with([&](column_id id, cell_and_hash &c_a_h)
                   { apply_monotonically(s.column_at(kind, id), std::move(c_a_h.cell), std::move(c_a_h.hash)); });
}
// When views contain a primary key column that is not part of the base table primary key,
// that column determines whether the row is live or not. We need to ensure that when that
// cell is dead, and thus the derived row marker, either by normal deletion of by TTL, so
// is the rest of the row. To ensure that none of the regular columns keep the row alive,
// we erase the live cells according to the shadowable_tombstone rules.
static bool dead_marker_shadows_row(const schema &s, column_kind kind, const row_marker &marker)
{
return s.is_view() && s.view_info()->has_base_non_pk_columns_in_view_pk() && !marker.is_live() && kind == column_kind::regular_column; // not applicable to static rows
}
bool row::compact_and_expire(const schema &s, column_kind kind, row_tombstone tomb, gc_clock::time_point query_time, can_gc_fn &can_gc, gc_clock::time_point gc_before, const row_marker &marker, compaction_garbage_collector *collector)
{
if (dead_marker_shadows_row(s, kind, marker))
{
    tomb.apply(shadowable_tombstone(api::max_timestamp, gc_clock::time_point::max()), row_marker());
}
bool any_live = false;
remove_if([&](column_id id, atomic_cell_or_collection &c)
          {         bool erase = false;         const column_definition& def = s.column_at(kind, id);         if (def.is_atomic()) {             atomic_cell_view cell = c.as_atomic_cell(def);             auto can_erase_cell = [&] {                 return cell.deletion_time() < gc_before && can_gc(tombstone(cell.timestamp(), cell.deletion_time()));             };             if (cell.is_covered_by(tomb.regular(), def.is_counter())) {                 erase = true;             } else if (cell.is_covered_by(tomb.shadowable().tomb(), def.is_counter())) {                 erase = true;             } else if (cell.has_expired(query_time)) {                 erase = can_erase_cell();                 if (!erase) {                     c = atomic_cell::make_dead(cell.timestamp(), cell.deletion_time());                 } else if (collector) {                     collector->collect(id, atomic_cell::make_dead(cell.timestamp(), cell.deletion_time()));                 }             } else if (!cell.is_live()) {                 erase = can_erase_cell();                 if (erase && collector) {                     collector->collect(id, atomic_cell::make_dead(cell.timestamp(), cell.deletion_time()));                 }             } else {                 any_live = true;             }         } else {             c.as_collection_mutation().with_deserialized(*def.type, [&] (collection_mutation_view_description m_view) {                 auto m = m_view.materialize(*def.type);                 any_live |= m.compact_and_expire(id, tomb, query_time, can_gc, gc_before, collector);                 if (m.cells.empty() && m.tomb <= tomb.tomb()) {                     erase = true;                 } else {                     c = m.serialize(*def.type);                 }             });         }         return erase; });
return any_live;
}
bool row::compact_and_expire(const schema &s, column_kind kind, row_tombstone tomb, gc_clock::time_point query_time, can_gc_fn &can_gc, gc_clock::time_point gc_before, compaction_garbage_collector *collector)
{
row_marker m;
return compact_and_expire(s, kind, tomb, query_time, can_gc, gc_before, m, collector);
}
bool lazy_row::compact_and_expire(const schema &s, column_kind kind, row_tombstone tomb, gc_clock::time_point query_time, can_gc_fn &can_gc, gc_clock::time_point gc_before, const row_marker &marker, compaction_garbage_collector *collector)
{
if (!_row)
{
    return false;
}
return _row->compact_and_expire(s, kind, tomb, query_time, can_gc, gc_before, marker, collector);
}
bool lazy_row::compact_and_expire(const schema &s, column_kind kind, row_tombstone tomb, gc_clock::time_point query_time, can_gc_fn &can_gc, gc_clock::time_point gc_before, compaction_garbage_collector *collector)
{
if (!_row)
{
    return false;
}
return _row->compact_and_expire(s, kind, tomb, query_time, can_gc, gc_before, collector);
}
std::ostream &operator<<(std::ostream &os, const lazy_row::printer &p) { return os << row::printer(p._schema, p._kind, p._row.get()); }
bool deletable_row::compact_and_expire(const schema &s, tombstone tomb, gc_clock::time_point query_time, can_gc_fn &can_gc, gc_clock::time_point gc_before, compaction_garbage_collector *collector)
{
auto should_purge_row_tombstone = [&](const row_tombstone &t)
{ return t.max_deletion_time() < gc_before && can_gc(t.tomb()); };
apply(tomb);
bool is_live = marker().compact_and_expire(deleted_at().tomb(), query_time, can_gc, gc_before);
is_live |= cells().compact_and_expire(s, column_kind::regular_column, deleted_at(), query_time, can_gc, gc_before, marker(), collector);
if (deleted_at().tomb() <= tomb || should_purge_row_tombstone(deleted_at()))
{
    remove_tombstone();
}
return is_live;
}
deletable_row deletable_row::difference(const schema &s, column_kind kind, const deletable_row &other) const
{
deletable_row dr;
if (_deleted_at > other._deleted_at)
{
    dr.apply(_deleted_at);
}
if (compare_row_marker_for_merge(_marker, other._marker) > 0)
{
    dr.apply(_marker);
}
dr._cells = _cells.difference(s, kind, other._cells);
return dr;
}
row row::difference(const schema &s, column_kind kind, const row &other) const
{
row r;
auto c = _cells.begin();
auto c_end = _cells.end();
auto it = other._cells.begin();
auto it_end = other._cells.end();
while (c != c_end)
{
    while (it != it_end && it.key() < c.key())
    {
        ++it;
    }
    auto &cdef = s.column_at(kind, c.key());
    if (it == it_end || it.key() != c.key())
    {
        r.append_cell(c.key(), c->cell.copy(*cdef.type));
    }
    else if (cdef.is_counter())
    {
        auto cell = counter_cell_view::difference(c->cell.as_atomic_cell(cdef), it->cell.as_atomic_cell(cdef));
        if (cell)
        {
        r.append_cell(c.key(), std::move(*cell));
        }
    }
    else if (s.column_at(kind, c.key()).is_atomic())
    {
        if (compare_atomic_cell_for_merge(c->cell.as_atomic_cell(cdef), it->cell.as_atomic_cell(cdef)) > 0)
        {
        r.append_cell(c.key(), c->cell.copy(*cdef.type));
        }
    }
    else
    {
        auto diff = ::difference(*s.column_at(kind, c.key()).type, c->cell.as_collection_mutation(), it->cell.as_collection_mutation());
        if (!static_cast<collection_mutation_view>(diff).is_empty())
        {
        r.append_cell(c.key(), std::move(diff));
        }
    }
    c++;
}
return r;
}
bool row_marker::compact_and_expire(tombstone tomb, gc_clock::time_point now, can_gc_fn &can_gc, gc_clock::time_point gc_before, compaction_garbage_collector *collector)
{
if (is_missing())
{
    return false;
}
if (_timestamp <= tomb.timestamp)
{
    _timestamp = api::missing_timestamp;
    return false;
}
if (_ttl > no_ttl && _expiry <= now)
{
    _expiry -= _ttl;
    _ttl = dead;
}
if (_ttl == dead && _expiry < gc_before && can_gc(tombstone(_timestamp, _expiry)))
{
    if (collector)
    {
        collector->collect(*this);
    }
    _timestamp = api::missing_timestamp;
}
return !is_missing() && _ttl != dead;
}
mutation_partition mutation_partition::difference(schema_ptr s, const mutation_partition &other) const
{
check_schema(*s);
mutation_partition mp(s);
if (_tombstone > other._tombstone)
{
    mp.apply(_tombstone);
}
mp._static_row = _static_row.difference(*s, column_kind::static_column, other._static_row);
mp._row_tombstones = _row_tombstones.difference(*s, other._row_tombstones);
auto it_r = other._rows.begin();
rows_entry::compare cmp_r(*s);
for (auto &&r : _rows)
{
    if (r.dummy())
    {
        continue;
    }
    while (it_r != other._rows.end() && (it_r->dummy() || cmp_r(*it_r, r)))
    {
        ++it_r;
    }
    if (it_r == other._rows.end() || !it_r->key().equal(*s, r.key()))
    {
        mp.insert_row(*s, r.key(), r.row());
    }
    else
    {
        auto dr = r.row().difference(*s, column_kind::regular_column, it_r->row());
        if (!dr.empty())
        {
        mp.insert_row(*s, r.key(), std::move(dr));
        }
    }
}
return mp;
}
void mutation_partition::accept(const schema &s, mutation_partition_visitor &v) const
{
check_schema(s);
v.accept_partition_tombstone(_tombstone);
_static_row.for_each_cell([&](column_id id, const atomic_cell_or_collection &cell)
                          {         const column_definition& def = s.static_column_at(id);         if (def.is_atomic()) {             v.accept_static_cell(id, cell.as_atomic_cell(def));         } else {             v.accept_static_cell(id, cell.as_collection_mutation());         } });
for (const auto &rt : _row_tombstones)
{
    v.accept_row_tombstone(rt.tombstone());
}
for (const rows_entry &e : _rows)
{
    const deletable_row &dr = e.row();
    v.accept_row(e.position(), dr.deleted_at(), dr.marker(), e.dummy(), e.continuous());
    dr.cells().for_each_cell([&](column_id id, const atomic_cell_or_collection &cell)
                             {             const column_definition& def = s.regular_column_at(id);             if (def.is_atomic()) {                 v.accept_row_cell(id, cell.as_atomic_cell(def));             } else {                 v.accept_row_cell(id, cell.as_collection_mutation());             } });
}
}
void mutation_partition::upgrade(const schema &old_schema, const schema &new_schema)
{ // We need to copy to provide strong exception guarantees.
mutation_partition tmp(new_schema.shared_from_this());
tmp.set_static_row_continuous(_static_row_continuous);
converting_mutation_partition_applier v(old_schema.get_column_mapping(), new_schema, tmp);
accept(old_schema, v);
*this = std::move(tmp);
}
mutation_querier::mutation_querier(const schema &s, query::result::partition_writer pw, query::result_memory_accounter &memory_accounter) : _schema(s), _memory_accounter(memory_accounter), _pw(std::move(pw)), _static_cells_wr(pw.start().start_static_row().start_cells()) {}
void mutation_querier::query_static_row(const row &r, tombstone current_tombstone)
{
const query::partition_slice &slice = _pw.slice();
if (!slice.static_columns.empty())
{
    if (_pw.requested_result())
    {
        auto start = _static_cells_wr._out.size();
        get_compacted_row_slice(_schema, slice, column_kind::static_column, r, slice.static_columns, _static_cells_wr);
        _memory_accounter.update(_static_cells_wr._out.size() - start);
    }
    else
    {
        seastar::measuring_output_stream stream;
        ser::qr_partition__static_row__cells<seastar::measuring_output_stream> out(stream, {});
        auto start = stream.size();
        get_compacted_row_slice(_schema, slice, column_kind::static_column, r, slice.static_columns, out);
        _memory_accounter.update(stream.size() - start);
    }
    if (_pw.requested_digest())
    {
        max_timestamp max_ts{_pw.last_modified()};
        _pw.digest().feed_hash(current_tombstone);
        max_ts.update(current_tombstone.timestamp);
        _pw.digest().feed_hash(r, _schema, column_kind::static_column, slice.static_columns, max_ts);
        _pw.last_modified() = max_ts.max;
    }
}
_rows_wr.emplace(std::move(_static_cells_wr).end_cells().end_static_row().start_rows());
}
stop_iteration mutation_querier::consume(static_row &&sr, tombstone current_tombstone)
{
query_static_row(sr.cells(), current_tombstone);
_live_data_in_static_row = true;
return stop_iteration::no;
}
void mutation_querier::prepare_writers()
{
if (!_rows_wr)
{
    row empty_row;
    query_static_row(empty_row, {});
    _live_data_in_static_row = false;
}
}
stop_iteration mutation_querier::consume(clustering_row &&cr, row_tombstone current_tombstone)
{
prepare_writers();
const query::partition_slice &slice = _pw.slice();
if (_pw.requested_digest())
{
    _pw.digest().feed_hash(cr.key(), _schema);
    _pw.digest().feed_hash(current_tombstone);
    max_timestamp max_ts{_pw.last_modified()};
    max_ts.update(current_tombstone.tomb().timestamp);
    _pw.digest().feed_hash(cr.cells(), _schema, column_kind::regular_column, slice.regular_columns, max_ts);
    _pw.last_modified() = max_ts.max;
}
auto write_row = [&](auto &rows_writer)
{         auto cells_wr = [&] {             if (slice.options.contains(query::partition_slice::option::send_clustering_key)) {                 return rows_writer.add().write_key(cr.key()).start_cells().start_cells();             } else {                 return rows_writer.add().skip_key().start_cells().start_cells();             }         }();         get_compacted_row_slice(_schema, slice, column_kind::regular_column, cr.cells(), slice.regular_columns, cells_wr);         std::move(cells_wr).end_cells().end_cells().end_qr_clustered_row(); };
auto stop = stop_iteration::no;
if (_pw.requested_result())
{
    auto start = _rows_wr->_out.size();
    write_row(*_rows_wr);
    stop = _memory_accounter.update_and_check(_rows_wr->_out.size() - start);
}
else
{
    seastar::measuring_output_stream stream;
    ser::qr_partition__rows<seastar::measuring_output_stream> out(stream, {});
    auto start = stream.size();
    write_row(out);
    stop = _memory_accounter.update_and_check(stream.size() - start);
}
_live_clustering_rows++;
return stop;
}
uint64_t mutation_querier::consume_end_of_stream()
{
prepare_writers(); // If we got no rows, but have live static columns, we should only
// give them back IFF we did not have any CK restrictions.
// #589
// If ck:s exist, and we do a restriction on them, we either have maching
// rows, or return nothing, since cql does not allow "is null".
bool return_static_content_on_partition_with_no_rows = _pw.slice().options.contains(query::partition_slice::option::always_return_static_content) || !has_ck_selector(_pw.ranges());
if (!_live_clustering_rows && (!return_static_content_on_partition_with_no_rows || !_live_data_in_static_row))
{
    _pw.retract();
    return 0;
}
else
{
    auto live_rows = std::max(_live_clustering_rows, uint64_t(1));
    _pw.row_count() += live_rows;
    _pw.partition_count() += 1;
    std::move(*_rows_wr).end_rows().end_qr_partition();
    return live_rows;
}
}
query_result_builder::query_result_builder(const schema &s, query::result::builder &rb) noexcept : _schema(s), _rb(rb) {}
void query_result_builder::consume_new_partition(const dht::decorated_key &dk) { _mutation_consumer.emplace(mutation_querier(_schema, _rb.add_partition(_schema, dk.key()), _rb.memory_accounter())); }
void query_result_builder::consume(tombstone t)
{
_mutation_consumer->consume(t);
_stop = _rb.bump_and_check_tombstone_limit();
}
stop_iteration query_result_builder::consume(static_row &&sr, tombstone t, bool is_live)
{
if (!is_live)
{
    _stop = _rb.bump_and_check_tombstone_limit();
    return _stop;
}
_stop = _mutation_consumer->consume(std::move(sr), t);
return _stop;
}
stop_iteration query_result_builder::consume(clustering_row &&cr, row_tombstone t, bool is_live)
{
if (!is_live)
{
    _stop = _rb.bump_and_check_tombstone_limit();
    return _stop;
}
_stop = _mutation_consumer->consume(std::move(cr), t);
return _stop;
}
stop_iteration query_result_builder::consume(range_tombstone_change &&rtc)
{
_stop = _rb.bump_and_check_tombstone_limit();
return _stop;
}
stop_iteration query_result_builder::consume_end_of_partition()
{
auto live_rows_in_partition = _mutation_consumer->consume_end_of_stream();
if (live_rows_in_partition > 0 && !_stop)
{
    _stop = _rb.memory_accounter().check();
}
if (_stop)
{
    _rb.mark_as_short_read();
}
return _stop;
}
void query_result_builder::consume_end_of_stream() {}
stop_iteration query::result_memory_accounter::check_local_limit() const
{
if (_short_read_allowed)
{
    return stop_iteration(_total_used_memory > _maximum_result_size.get_page_size());
}
else
{
    if (_total_used_memory > _maximum_result_size.hard_limit)
    {
        throw std::runtime_error(fmt::format("Memory usage of unpaged query exceeds hard limit of {} (configured via max_memory_for_unlimited_query_hard_limit)", _maximum_result_size.hard_limit));
    }
    if (_below_soft_limit && _total_used_memory > _maximum_result_size.soft_limit)
    {
        mplog.warn("Memory usage of unpaged query exceeds soft limit of {} (configured via max_memory_for_unlimited_query_soft_limit)", _maximum_result_size.soft_limit);
        _below_soft_limit = false;
    }
}
return stop_iteration::no;
}
void reconcilable_result_builder::consume_new_partition(const dht::decorated_key &dk)
{
_rt_assembler.reset();
_return_static_content_on_partition_with_no_rows = _slice.options.contains(query::partition_slice::option::always_return_static_content) || !has_ck_selector(_slice.row_ranges(_schema, dk.key()));
_static_row_is_alive = false;
_live_rows = 0;
_mutation_consumer.emplace(streamed_mutation_freezer(_schema, dk.key(), _reversed));
}
void reconcilable_result_builder::consume(tombstone t) { _mutation_consumer->consume(t); }
stop_iteration reconcilable_result_builder::consume(static_row &&sr, tombstone, bool is_alive)
{
_static_row_is_alive = is_alive;
_memory_accounter.update(sr.memory_usage(_schema));
return _mutation_consumer->consume(std::move(sr));
}
stop_iteration reconcilable_result_builder::consume(clustering_row &&cr, row_tombstone, bool is_alive)
{
if (_rt_assembler.needs_flush())
{
    if (auto rt_opt = _rt_assembler.flush(_schema, position_in_partition::after_key(_schema, cr.key())))
    {
        consume(std::move(*rt_opt));
    }
}
_live_rows += is_alive;
auto stop = _memory_accounter.update_and_check(cr.memory_usage(_schema));
if (is_alive)
{ // We are considering finishing current read only after consuming a
    // live clustering row. While sending a single live row is enough to
    // guarantee progress, not ending the result on a live row would
    // mean that the next page fetch will read all tombstones after the
    // last live row again.
    _stop = stop;
}
return _mutation_consumer->consume(std::move(cr)) || _stop;
}
stop_iteration reconcilable_result_builder::consume(range_tombstone &&rt)
{
_memory_accounter.update(rt.memory_usage(_schema));
if (_reversed)
{ // undo reversing done for the native reversed format, coordinator still uses old reversing format
    rt.reverse();
}
return _mutation_consumer->consume(std::move(rt));
}
stop_iteration reconcilable_result_builder::consume(range_tombstone_change &&rtc)
{
if (auto rt_opt = _rt_assembler.consume(_schema, std::move(rtc)))
{
    return consume(std::move(*rt_opt));
}
return stop_iteration::no;
}
stop_iteration reconcilable_result_builder::consume_end_of_partition()
{
_rt_assembler.on_end_of_stream();
if (_live_rows == 0 && _static_row_is_alive && _return_static_content_on_partition_with_no_rows)
{
    ++_live_rows; // Normally we count only live clustering rows, to guarantee that
    // the next page fetch won't ask for the same range. However,
    // if we return just a single static row we can stop the result as
    // well. Next page fetch will ask for the next partition and if we
    // don't do that we could end up with an unbounded number of
    // partitions with only a static row.
    _stop = _stop || _memory_accounter.check();
}
_total_live_rows += _live_rows;
_result.emplace_back(partition{_live_rows, _mutation_consumer->consume_end_of_stream()});
return _stop;
}
reconcilable_result reconcilable_result_builder::consume_end_of_stream() { return reconcilable_result(_total_live_rows, std::move(_result), query::short_read(bool(_stop)), std::move(_memory_accounter).done()); }
future<query::result> to_data_query_result(const reconcilable_result &r, schema_ptr s, const query::partition_slice &slice, uint64_t max_rows, uint32_t max_partitions, query::result_options opts)
{ // This result was already built with a limit, don't apply another one.
query::result::builder builder(slice, opts, query::result_memory_accounter{query::result_memory_limiter::unlimited_result_size}, query::max_tombstones);
auto consumer = compact_for_query_v2<query_result_builder>(*s, gc_clock::time_point::min(), slice, max_rows, max_partitions, query_result_builder(*s, builder));
auto compaction_state = consumer.get_state();
const auto reverse = slice.options.contains(query::partition_slice::option::reversed) ? consume_in_reverse::yes : consume_in_reverse::no; // FIXME: frozen_mutation::consume supports only forward consumers
if (reverse == consume_in_reverse::no)
{
    frozen_mutation_consumer_adaptor adaptor(s, consumer);
    for (const partition &p : r.partitions())
    {
        const auto res = co_await p.mut().consume_gently(s, adaptor);
        if (res.stop == stop_iteration::yes)
        {
        break;
        }
    }
}
else
{
    for (const partition &p : r.partitions())
    {
        auto m = co_await p.mut().unfreeze_gently(s);
        const auto res = co_await std::move(m).consume_gently(consumer, reverse);
        if (res.stop == stop_iteration::yes)
        {
        break;
        }
    }
}
if (r.is_short_read())
{
    builder.mark_as_short_read();
}
co_return builder.build(compaction_state->current_full_position());
}
query::result query_mutation(mutation &&m, const query::partition_slice &slice, uint64_t row_limit, gc_clock::time_point now, query::result_options opts)
{
query::result::builder builder(slice, opts, query::result_memory_accounter{query::result_memory_limiter::unlimited_result_size}, query::max_tombstones);
auto consumer = compact_for_query_v2<query_result_builder>(*m.schema(), now, slice, row_limit, query::max_partitions, query_result_builder(*m.schema(), builder));
auto compaction_state = consumer.get_state();
const auto reverse = slice.options.contains(query::partition_slice::option::reversed) ? consume_in_reverse::yes : consume_in_reverse::no;
std::move(m).consume(consumer, reverse);
return builder.build(compaction_state->current_full_position());
}
class counter_write_query_result_builder
{
const schema &_schema;
mutation_opt _mutation;
public:
counter_write_query_result_builder(const schema &s) : _schema(s) {}
void consume_new_partition(const dht::decorated_key &dk) { _mutation = mutation(_schema.shared_from_this(), dk); }
void consume(tombstone) {}
stop_iteration consume(static_row &&sr, tombstone, bool is_live)
{
    if (!is_live)
    {
        return stop_iteration::no;
    }
    _mutation->partition().static_row().maybe_create() = std::move(sr.cells());
    return stop_iteration::no;
}
stop_iteration consume(clustering_row &&cr, row_tombstone, bool is_live)
{
    if (!is_live)
    {
        return stop_iteration::no;
    }
    _mutation->partition().insert_row(_schema, cr.key(), std::move(cr).as_deletable_row());
    return stop_iteration::no;
}
stop_iteration consume(range_tombstone_change &&rtc) { return stop_iteration::no; }
stop_iteration consume_end_of_partition() { return stop_iteration::no; }
mutation_opt consume_end_of_stream() { return std::move(_mutation); }
};
can_gc_fn always_gc = [](tombstone)
{ return true; };
logging::logger compound_logger("compound");
extern logging::logger mplog;
template <>
struct fmt::formatter<apply_resume> : fmt::formatter<std::string_view>
{
template <typename FormatContext>
auto format(const apply_resume &res, FormatContext &ctx) const { return fmt::format_to(ctx.out(), "{{{}, {}}}", int(res._stage), res._pos); }
};

// Returns true if the mutation_partition_v2 represents no writes.
using namespace db;
static_assert(MutationViewVisitor<mutation_partition_view_virtual_visitor>);
mutation_partition_view_virtual_visitor::~mutation_partition_view_virtual_visitor() = default;
namespace
{
using atomic_cell_variant = boost::variant<ser::live_cell_view, ser::expiring_cell_view, ser::dead_cell_view, ser::counter_cell_view, ser::unknown_variant_type>;

;
row_marker read_row_marker(boost::variant<ser::live_marker_view, ser::expiring_marker_view, ser::dead_marker_view, ser::no_marker_view, ser::unknown_variant_type> rmv)
{
    struct row_marker_visitor : boost::static_visitor<row_marker>
    {
        row_marker operator()(ser::live_marker_view &lmv) const { return row_marker(lmv.created_at()); }
        row_marker operator()(ser::expiring_marker_view &emv) const { return row_marker(emv.lm().created_at(), emv.ttl(), emv.expiry()); }
        row_marker operator()(ser::dead_marker_view &dmv) const { return row_marker(dmv.tomb()); }
        row_marker operator()(ser::no_marker_view &) const { return row_marker(); }
        row_marker operator()(ser::unknown_variant_type &) const { throw std::runtime_error("Trying to deserialize unknown row marker type"); }
    };
    return boost::apply_visitor(row_marker_visitor(), rmv);
}
}
using namespace db;
namespace
{
;
;
;
;
;
;
;
}
;
;
;
namespace
{ // A functor which transforms objects from Domain into objects from CoDomain
template <typename U, typename Domain, typename CoDomain>
concept Mapper = requires(U obj, const Domain &src) {         { obj(src) } -> std::convertible_to<const CoDomain&>; }; // A functor which merges two objects from Domain into one. The result is stored in the first argument.
template <typename U, typename Domain>
concept Reducer = requires(U obj, Domain &dst, const Domain &src) {         { obj(dst, src) } -> std::same_as<void>; }; // Calculates the value of particular part of mutation_partition represented by
// the version chain starting from v.
// |map| extracts the part from each version.
// |reduce| Combines parts from the two versions.
template <typename Result, typename Map, typename Initial, typename Reduce>
    requires Mapper<Map, mutation_partition_v2, Result> && Reducer<Reduce, Result>
inline Result squashed(const partition_version_ref &v, Map &&map, Initial &&initial, Reduce &&reduce)
{
    const partition_version *this_v = &*v;
    partition_version *it = v->last();
    Result r = initial(map(it->partition()));
    while (it != this_v)
    {
        it = it->prev();
        reduce(r, map(it->partition()));
    }
    return r;
}
template <typename Result, typename Map, typename Reduce>
    requires Mapper<Map, mutation_partition_v2, Result> && Reducer<Reduce, Result>
inline Result squashed(const partition_version_ref &v, Map &&map, Reduce &&reduce)
{
    return squashed<Result>(
        v, map, [](auto &&o) -> decltype(auto)
        { return std::forward<decltype(o)>(o); },
        reduce);
}
}
std::ostream &operator<<(std::ostream &out, const partition_entry::printer &p);
position_in_partition_view range_tombstone::position() const { return position_in_partition_view(position_in_partition_view::range_tombstone_tag_t(), start_bound()); }
position_in_partition_view range_tombstone::end_position() const { return position_in_partition_view(position_in_partition_view::range_tombstone_tag_t(), end_bound()); }
range_tombstone_list::~range_tombstone_list() { _tombstones.clear_and_dispose(current_deleter<range_tombstone_entry>()); }
template <typename... Args>
static auto construct_range_tombstone_entry(Args &&...args) { return alloc_strategy_unique_ptr<range_tombstone_entry>(current_allocator().construct<range_tombstone_entry>(range_tombstone(std::forward<Args>(args)...))); }
void range_tombstone_list::apply_reversibly(const schema &s, clustering_key_prefix start_key, bound_kind start_kind, clustering_key_prefix end_key, bound_kind end_kind, tombstone tomb, reverter &rev)
{
position_in_partition::less_compare less(s);
position_in_partition start(position_in_partition::range_tag_t(), bound_view(std::move(start_key), start_kind));
position_in_partition end(position_in_partition::range_tag_t(), bound_view(std::move(end_key), end_kind));
if (!less(start, end))
{
    return;
}
if (!_tombstones.empty())
{
    auto last = --_tombstones.end();
    range_tombstones_type::iterator it;
    if (less(start, last->end_position()))
    {
        it = _tombstones.upper_bound(start, [less](auto &&sb, auto &&rt)
                                     { return less(sb, rt.end_position()); });
    }
    else
    {
        it = _tombstones.end();
    }
    insert_from(s, std::move(it), std::move(start), std::move(end), std::move(tomb), rev);
    return;
}
auto rt = construct_range_tombstone_entry(std::move(start), std::move(end), std::move(tomb));
rev.insert(_tombstones.end(), *rt);
rt.release();
}
void range_tombstone_list::insert_from(const schema &s, range_tombstones_type::iterator it, position_in_partition start, position_in_partition end, tombstone tomb, reverter &rev)
{
position_in_partition::tri_compare cmp(s);
if (it != _tombstones.begin())
{
    auto prev = std::prev(it);
    if (prev->tombstone().tomb == tomb && cmp(prev->end_position(), start) == 0)
    {
        start = prev->position();
        rev.erase(prev);
    }
}
while (it != _tombstones.end())
{
    if (cmp(end, start) <= 0)
    {
        return;
    }
    if (cmp(end, it->position()) < 0)
    { // not overlapping
        if (it->tombstone().tomb == tomb && cmp(end, it->position()) == 0)
        {
        rev.update(it, {std::move(start), std::move(end), tomb});
        }
        else
        {
        auto rt = construct_range_tombstone_entry(std::move(start), std::move(end), tomb);
        rev.insert(it, *rt);
        rt.release();
        }
        return;
    }
    auto c = tomb <=> it->tombstone().tomb;
    if (c == 0)
    { // same timestamp, overlapping or adjacent, so merge.
        if (cmp(it->position(), start) < 0)
        {
        start = it->position();
        }
        if (cmp(end, it->end_position()) < 0)
        {
        end = it->end_position();
        }
        it = rev.erase(it);
    }
    else if (c > 0)
    { // We overwrite the current tombstone.
        if (cmp(it->position(), start) < 0)
        {
        {
            auto rt = construct_range_tombstone_entry(it->position(), start, it->tombstone().tomb);
            rev.update(it, {start, it->end_position(), it->tombstone().tomb});
            rev.insert(it, *rt);
            rt.release();
        }
        }
        if (cmp(end, it->end_position()) < 0)
        { // Here start <= it->start and end < it->end.
        auto rt = construct_range_tombstone_entry(std::move(start), end, std::move(tomb));
        rev.update(it, {std::move(end), it->end_position(), it->tombstone().tomb});
        rev.insert(it, *rt);
        rt.release();
        return;
        } // Here start <= it->start and end >= it->end.
        it = rev.erase(it);
    }
    else
    { // We don't overwrite the current tombstone.
        if (cmp(start, it->position()) < 0)
        { // The new tombstone starts before the current one.
        if (cmp(it->position(), end) < 0)
        { // Here start < it->start and it->start < end.
            {
                auto rt = construct_range_tombstone_entry(std::move(start), it->position(), tomb);
                it = rev.insert(it, *rt);
                rt.release();
                ++it;
            }
        }
        else
        { // Here start < it->start and end <= it->start, so just insert the new tombstone.
            auto rt = construct_range_tombstone_entry(std::move(start), std::move(end), std::move(tomb));
            rev.insert(it, *rt);
            rt.release();
            return;
        }
        }
        if (cmp(it->end_position(), end) < 0)
        { // Here the current tombstone overwrites a range of the new one.
        start = it->end_position();
        ++it;
        }
        else
        { // Here the current tombstone completely overwrites the new one.
        return;
        }
    }
} // If we got here, then just insert the remainder at the end.
auto rt = construct_range_tombstone_entry(std::move(start), std::move(end), std::move(tomb));
rev.insert(it, *rt);
rt.release();
}

// See reversibly_mergeable.hh
namespace
{
struct bv_order_by_end
{
    bound_view::compare less;
    
    bool operator()(bound_view v, const range_tombstone_entry &rt) const;
};
struct bv_order_by_start
{
    bound_view::compare less;
};
struct pos_order_by_end
{
    position_in_partition::less_compare less;
};
struct pos_order_by_start
{
    position_in_partition::less_compare less;
};
}
// namespace
range_tombstone_list::range_tombstones_type::iterator range_tombstone_list::reverter::insert(range_tombstones_type::iterator it, range_tombstone_entry &new_rt)
{
_ops.emplace_back(insert_undo_op(new_rt));
return _dst._tombstones.insert_before(it, new_rt);
}
range_tombstone_list::range_tombstones_type::iterator range_tombstone_list::reverter::erase(range_tombstones_type::iterator it)
{
_ops.emplace_back(std::in_place_type<erase_undo_op>, *it);
return _dst._tombstones.erase(it);
}
void range_tombstone_list::reverter::update(range_tombstones_type::iterator it, range_tombstone &&new_rt)
{
_ops.emplace_back(std::in_place_type<update_undo_op>, std::move(it->tombstone()), *it);
it->tombstone() = std::move(new_rt);
}
void range_tombstone_list::reverter::revert() noexcept
{
for (auto &&rt : _ops | boost::adaptors::reversed)
{
    seastar::visit(rt, [this](auto &op)
                   { op.undo(_s, _dst); });
}
cancel();
}
range_tombstone_list::range_tombstones_type::iterator range_tombstone_list::nop_reverter::insert(range_tombstones_type::iterator it, range_tombstone_entry &new_rt) { return _dst._tombstones.insert_before(it, new_rt); }
range_tombstone_list::range_tombstones_type::iterator range_tombstone_list::nop_reverter::erase(range_tombstones_type::iterator it) { return _dst._tombstones.erase_and_dispose(it, alloc_strategy_deleter<range_tombstone_entry>()); }
void range_tombstone_list::nop_reverter::update(range_tombstones_type::iterator it, range_tombstone &&new_rt) { *it = std::move(new_rt); }
void range_tombstone_list::insert_undo_op::undo(const schema &s, range_tombstone_list &rt_list) noexcept
{
auto it = rt_list.find(s, _new_rt);
assert(it != rt_list.end());
rt_list._tombstones.erase_and_dispose(it, current_deleter<range_tombstone_entry>());
}
void range_tombstone_list::erase_undo_op::undo(const schema &s, range_tombstone_list &rt_list) noexcept { rt_list._tombstones.insert(*_rt.release()); }
void range_tombstone_list::update_undo_op::undo(const schema &s, range_tombstone_list &rt_list) noexcept
{
auto it = rt_list.find(s, _new_rt);
assert(it != rt_list.end());
*it = std::move(_old_rt);
}
bytes_view collection_mutation_input_stream::read_linearized(size_t n)
{
managed_bytes_view mbv = ::read_simple_bytes(_src, n);
if (mbv.is_linearized())
{
    return mbv.current_fragment();
}
else
{
    return _linearized.emplace_front(linearized(mbv));
}
}
managed_bytes_view collection_mutation_input_stream::read_fragmented(size_t n) { return ::read_simple_bytes(_src, n); }
bool collection_mutation_input_stream::empty() const { return _src.empty(); }
collection_mutation::collection_mutation(const abstract_type &type, managed_bytes data) : _data(std::move(data)) {}
collection_mutation_view atomic_cell_or_collection::as_collection_mutation() const { return collection_mutation_view{managed_bytes_view(_data)}; }
std::ostream &operator<<(std::ostream &os, const collection_mutation_view::printer &cmvp);
template <typename Iterator>
static collection_mutation serialize_collection_mutation(const abstract_type &type, const tombstone &tomb, boost::iterator_range<Iterator> cells)
{
auto element_size = [](size_t c, auto &&e) -> size_t
{ return c + 8 + e.first.size() + e.second.serialize().size(); };
auto size = accumulate(cells, (size_t)4, element_size);
size += 1;
if (tomb)
{
    size += sizeof(int64_t) + sizeof(int64_t);
}
managed_bytes ret(managed_bytes::initialized_later(), size);
managed_bytes_mutable_view out(ret);
write<uint8_t>(out, uint8_t(bool(tomb)));
if (tomb)
{
    write<int64_t>(out, tomb.timestamp);
    write<int64_t>(out, tomb.deletion_time.time_since_epoch().count());
}
auto writek = [&out](bytes_view v)
{         write<int32_t>(out, v.size());         write_fragmented(out, single_fragmented_view(v)); };
auto writev = [&out](managed_bytes_view v)
{         write<int32_t>(out, v.size());         write_fragmented(out, v); }; // FIXME: overflow?
write<int32_t>(out, boost::distance(cells));
for (auto &&kv : cells)
{
    auto &&k = kv.first;
    auto &&v = kv.second;
    writek(k);
    writev(v.serialize());
}
return collection_mutation(type, ret);
}
collection_mutation collection_mutation_description::serialize(const abstract_type &type) const { return serialize_collection_mutation(type, tomb, boost::make_iterator_range(cells.begin(), cells.end())); }
collection_mutation collection_mutation_view_description::serialize(const abstract_type &type) const { return serialize_collection_mutation(type, tomb, boost::make_iterator_range(cells.begin(), cells.end())); }
template <typename C>
    requires std::is_base_of_v<abstract_type, std::remove_reference_t<C>>
static collection_mutation_view_description merge(collection_mutation_view_description a, collection_mutation_view_description b, C &&key_type)
{
using element_type = std::pair<bytes_view, atomic_cell_view>;
auto compare = [&](const element_type &e1, const element_type &e2)
{ return key_type.less(e1.first, e2.first); };
auto merge = [](const element_type &e1, const element_type &e2) { // FIXME: use std::max()?
    return std::make_pair(e1.first, compare_atomic_cell_for_merge(e1.second, e2.second) > 0 ? e1.second : e2.second);
}; // applied to a tombstone, returns a predicate checking whether a cell is killed by
// the tombstone
auto cell_killed = [](const std::optional<tombstone> &t)
{ return [&t](const element_type &e)
  {
      if (!t)
      {
          return false;
      } // tombstone wins if timestamps equal here, unlike row tombstones
      if (t->timestamp < e.second.timestamp())
      {
          return false;
      }
      return true; // FIXME: should we consider TTLs too?
  }; };
collection_mutation_view_description merged;
merged.cells.reserve(a.cells.size() + b.cells.size());
combine(a.cells.begin(), std::remove_if(a.cells.begin(), a.cells.end(), cell_killed(b.tomb)), b.cells.begin(), std::remove_if(b.cells.begin(), b.cells.end(), cell_killed(a.tomb)), std::back_inserter(merged.cells), compare, merge);
merged.tomb = std::max(a.tomb, b.tomb);
return merged;
}
collection_mutation merge(const abstract_type &type, collection_mutation_view a, collection_mutation_view b)
{
return a.with_deserialized(type, [&](collection_mutation_view_description a_view)
                           { return b.with_deserialized(type, [&](collection_mutation_view_description b_view)
                                                        { return visit(type, make_visitor([&](const collection_type_impl &ctype)
                                                                                          { return merge(std::move(a_view), std::move(b_view), *ctype.name_comparator()); },
                                                                                          [&](const user_type_impl &utype)
                                                                                          { return merge(std::move(a_view), std::move(b_view), *short_type); },
                                                                                          [](const abstract_type &o) -> collection_mutation_view_description
                                                                                          { throw std::runtime_error(format("collection_mutation merge: unknown type: {}", o.name())); }))
                                                              .serialize(type); }); });
}
template <typename C>
    requires std::is_base_of_v<abstract_type, std::remove_reference_t<C>>
static collection_mutation_view_description difference(collection_mutation_view_description a, collection_mutation_view_description b, C &&key_type)
{
collection_mutation_view_description diff;
diff.cells.reserve(std::max(a.cells.size(), b.cells.size()));
auto it = b.cells.begin();
for (auto &&c : a.cells)
{
    while (it != b.cells.end() && key_type.less(it->first, c.first))
    {
        ++it;
    }
    if (it == b.cells.end() || !key_type.equal(it->first, c.first) || compare_atomic_cell_for_merge(c.second, it->second) > 0)
    {
        auto cell = std::make_pair(c.first, c.second);
        diff.cells.emplace_back(std::move(cell));
    }
}
if (a.tomb > b.tomb)
{
    diff.tomb = a.tomb;
}
return diff;
}
collection_mutation difference(const abstract_type &type, collection_mutation_view a, collection_mutation_view b)
{
return a.with_deserialized(type, [&](collection_mutation_view_description a_view)
                           { return b.with_deserialized(type, [&](collection_mutation_view_description b_view)
                                                        { return visit(type, make_visitor([&](const collection_type_impl &ctype)
                                                                                          { return difference(std::move(a_view), std::move(b_view), *ctype.name_comparator()); },
                                                                                          [&](const user_type_impl &utype)
                                                                                          { return difference(std::move(a_view), std::move(b_view), *short_type); },
                                                                                          [](const abstract_type &o) -> collection_mutation_view_description
                                                                                          { throw std::runtime_error(format("collection_mutation difference: unknown type: {}", o.name())); }))
                                                              .serialize(type); }); });
}
template <typename F>
    requires std::is_invocable_r_v<std::pair<bytes_view, atomic_cell_view>, F, collection_mutation_input_stream &>
static collection_mutation_view_description deserialize_collection_mutation(collection_mutation_input_stream &in, F &&read_kv)
{
collection_mutation_view_description ret;
auto has_tomb = in.read_trivial<uint8_t>();
if (has_tomb)
{
    auto ts = in.read_trivial<api::timestamp_type>();
    auto ttl = in.read_trivial<gc_clock::duration::rep>();
    ret.tomb = tombstone{ts, gc_clock::time_point(gc_clock::duration(ttl))};
}
auto nr = in.read_trivial<uint32_t>();
ret.cells.reserve(nr);
for (uint32_t i = 0; i != nr; ++i)
{
    ret.cells.push_back(read_kv(in));
}
assert(in.empty());
return ret;
}
collection_mutation_view_description deserialize_collection_mutation(const abstract_type &type, collection_mutation_input_stream &in)
{
return visit(type, make_visitor([&](const collection_type_impl &ctype) {                                     // value_comparator(), ugh
                 return deserialize_collection_mutation(in, [&ctype](collection_mutation_input_stream &in) { // FIXME: we could probably avoid the need for size
                     auto ksize = in.read_trivial<uint32_t>();
                     auto key = in.read_linearized(ksize);
                     auto vsize = in.read_trivial<uint32_t>();
                     auto value = atomic_cell_view::from_bytes(*ctype.value_comparator(), in.read_fragmented(vsize));
                     return std::make_pair(key, value);
                 });
             },
                                [&](const user_type_impl &utype)
                                { return deserialize_collection_mutation(in, [&utype](collection_mutation_input_stream &in) { // FIXME: we could probably avoid the need for size
                                      auto ksize = in.read_trivial<uint32_t>();
                                      auto key = in.read_linearized(ksize);
                                      auto vsize = in.read_trivial<uint32_t>();
                                      auto value = atomic_cell_view::from_bytes(*utype.type(deserialize_field_index(key)), in.read_fragmented(vsize));
                                      return std::make_pair(key, value);
                                  }); },
                                [&](const abstract_type &o) -> collection_mutation_view_description
                                { throw std::runtime_error(format("deserialize_collection_mutation: unknown type {}", o.name())); }));
}
caching_options::caching_options(sstring k, sstring r, bool enabled) : _key_cache(k), _row_cache(r), _enabled(enabled)
{
if ((k != "ALL") && (k != "NONE"))
{
    throw exceptions::configuration_exception("Invalid key value: " + k);
}
if ((r == "ALL") || (r == "NONE"))
{
    return;
}
else
{
    try
    {
        boost::lexical_cast<unsigned long>(r);
    }
    catch (boost::bad_lexical_cast &e)
    {
        throw exceptions::configuration_exception("Invalid key value: " + r);
    }
}
}
caching_options::caching_options() : _key_cache(default_key), _row_cache(default_row) {}
std::map<sstring, sstring> caching_options::to_map() const
{
std::map<sstring, sstring> res = {{"keys", _key_cache}, {"rows_per_partition", _row_cache}};
if (!_enabled)
{
    res.insert({"enabled", "false"});
}
return res;
}
sstring caching_options::to_sstring() const { return rjson::print(rjson::from_string_map(to_map())); }
caching_options caching_options::get_disabled_caching_options() { return caching_options("NONE", "NONE", false); }
caching_options caching_options::from_map(const std::map<sstring, sstring> &map)
{
sstring k = default_key;
sstring r = default_row;
bool e = true;
for (auto &p : map)
{
    if (p.first == "keys")
    {
        k = p.second;
    }
    else if (p.first == "rows_per_partition")
    {
        r = p.second;
    }
    else if (p.first == "enabled")
    {
        e = p.second == "true";
    }
    else
    {
        throw exceptions::configuration_exception(format("Invalid caching option: {}", p.first));
    }
}
return caching_options(k, r, e);
}
caching_options caching_options::from_sstring(const sstring &str) { return from_map(rjson::parse_to_map<std::map<sstring, sstring>>(str)); }
constexpr int32_t schema::NAME_LENGTH;
extern logging::logger dblog;
template <typename Sequence>
std::vector<data_type> get_column_types(const Sequence &column_definitions)
{
std::vector<data_type> result;
for (auto &&col : column_definitions)
{
    result.push_back(col.type);
}
return result;
}
thread_local std::map<sstring, std::unique_ptr<dht::i_partitioner>> partitioners;
thread_local std::map<std::pair<unsigned, unsigned>, std::unique_ptr<dht::sharder>> sharders;
sstring default_partitioner_name = "org.apache.cassandra.dht.Murmur3Partitioner";
unsigned default_partitioner_ignore_msb = 12;
static const dht::i_partitioner &get_partitioner(const sstring &name)
{
auto it = partitioners.find(name);
if (it == partitioners.end())
{
    auto p = dht::make_partitioner(name);
    it = partitioners.insert({name, std::move(p)}).first;
}
return *it->second;
}
static const dht::sharder &get_sharder(unsigned shard_count, unsigned ignore_msb)
{
auto it = sharders.find({shard_count, ignore_msb});
if (it == sharders.end())
{
    auto sharder = std::make_unique<dht::sharder>(shard_count, ignore_msb);
    it = sharders.emplace(std::make_pair(shard_count, ignore_msb), std::move(sharder)).first;
}
return *it->second;
}
const dht::i_partitioner &schema::get_partitioner() const { return _raw._partitioner.get(); }
lw_shared_ptr<cql3::column_specification> schema::make_column_specification(const column_definition &def) const
{
auto id = ::make_shared<cql3::column_identifier>(def.name(), column_name_type(def));
return make_lw_shared<cql3::column_specification>(_raw._ks_name, _raw._cf_name, std::move(id), def.type);
}
v3_columns::v3_columns(std::vector<column_definition> cols, bool is_dense, bool is_compound) : _is_dense(is_dense), _is_compound(is_compound), _columns(std::move(cols))
{
for (column_definition &def : _columns)
{
    _columns_by_name[def.name()] = &def;
}
}
v3_columns v3_columns::from_v2_schema(const schema &s)
{
data_type static_column_name_type = utf8_type;
std::vector<column_definition> cols;
if (s.is_static_compact_table())
{
    if (s.has_static_columns())
    {
        throw std::runtime_error(format("v2 static compact table should not have static columns: {}.{}", s.ks_name(), s.cf_name()));
    }
    if (s.clustering_key_size())
    {
        throw std::runtime_error(format("v2 static compact table should not have clustering columns: {}.{}", s.ks_name(), s.cf_name()));
    }
    static_column_name_type = s.regular_column_name_type();
    for (auto &c : s.all_columns())
    { // Note that for "static" no-clustering compact storage we use static for the defined columns
        if (c.kind == column_kind::regular_column)
        {
        auto new_def = c;
        new_def.kind = column_kind::static_column;
        cols.push_back(new_def);
        }
        else
        {
        cols.push_back(c);
        }
    }
    schema_builder::default_names names(s._raw);
    cols.emplace_back(to_bytes(names.clustering_name()), static_column_name_type, column_kind::clustering_key, 0);
    cols.emplace_back(to_bytes(names.compact_value_name()), s.make_legacy_default_validator(), column_kind::regular_column, 0);
}
else
{
    cols = s.all_columns();
}
for (column_definition &def : cols)
{
    data_type name_type = def.is_static() ? static_column_name_type : utf8_type;
    auto id = ::make_shared<cql3::column_identifier>(def.name(), name_type);
    def.column_specification = make_lw_shared<cql3::column_specification>(s.ks_name(), s.cf_name(), std::move(id), def.type);
}
return v3_columns(std::move(cols), s.is_dense(), s.is_compound());
}
void schema::rebuild()
{
_partition_key_type = make_lw_shared<compound_type<>>(get_column_types(partition_key_columns()));
_clustering_key_type = make_lw_shared<compound_prefix>(get_column_types(clustering_key_columns()));
_clustering_key_size = column_offset(column_kind::static_column) - column_offset(column_kind::clustering_key);
_regular_column_count = _raw._columns.size() - column_offset(column_kind::regular_column);
_static_column_count = column_offset(column_kind::regular_column) - column_offset(column_kind::static_column);
_columns_by_name.clear();
for (const column_definition &def : all_columns())
{
    _columns_by_name[def.name()] = &def;
}
static_assert(row_column_ids_are_ordered_by_name::value, "row columns don't need to be ordered by name");
if (!std::is_sorted(regular_columns().begin(), regular_columns().end(), column_definition::name_comparator(regular_column_name_type())))
{
    throw std::runtime_error("Regular columns should be sorted by name");
}
if (!std::is_sorted(static_columns().begin(), static_columns().end(), column_definition::name_comparator(static_column_name_type())))
{
    throw std::runtime_error("Static columns should be sorted by name");
}
{
    std::vector<column_mapping_entry> cm_columns;
    for (const column_definition &def : boost::range::join(static_columns(), regular_columns()))
    {
        cm_columns.emplace_back(column_mapping_entry{def.name(), def.type});
    }
    _column_mapping = column_mapping(std::move(cm_columns), static_columns_count());
}
thrift()._compound = is_compound();
thrift()._is_dynamic = clustering_key_size() > 0;
if (is_counter())
{
    for (auto &&cdef : boost::range::join(static_columns(), regular_columns()))
    {
        if (!cdef.type->is_counter())
        {
        throw exceptions::configuration_exception(format("Cannot add a non counter column ({}) in a counter column family", cdef.name_as_text()));
        }
    }
}
else
{
    for (auto &&cdef : all_columns())
    {
        if (cdef.type->is_counter())
        {
        throw exceptions::configuration_exception(format("Cannot add a counter column ({}) in a non counter column family", cdef.name_as_text()));
        }
    }
}
_v3_columns = v3_columns::from_v2_schema(*this);
_full_slice = make_shared<query::partition_slice>(partition_slice_builder(*this).build());
}
schema::raw_schema::raw_schema(table_id id) : _id(id), _partitioner(::get_partitioner(default_partitioner_name)), _sharder(::get_sharder(smp::count, default_partitioner_ignore_msb)) {}
schema::schema(private_tag, const raw_schema &raw, std::optional<raw_view_info> raw_view_info, const schema_static_props &props) : _raw(raw), _static_props(props), _offsets([this]
                                                                                                                                                                             {         if (_raw._columns.size() > std::numeric_limits<column_count_type>::max()) {             throw std::runtime_error(format("Column count limit ({:d}) overflowed: {:d}",                                             std::numeric_limits<column_count_type>::max(), _raw._columns.size()));         }         auto& cols = _raw._columns;         std::array<column_count_type, 4> count = { 0, 0, 0, 0 };         auto i = cols.begin();         auto e = cols.end();         for (auto k : { column_kind::partition_key, column_kind::clustering_key, column_kind::static_column, column_kind::regular_column }) {             auto j = std::stable_partition(i, e, [k](const auto& c) {                 return c.kind == k;             });             count[column_count_type(k)] = std::distance(i, j);             i = j;         }         return std::array<column_count_type, 3> {                 count[0],                 count[0] + count[1],                 count[0] + count[1] + count[2],         }; }())
{
std::sort(_raw._columns.begin() + column_offset(column_kind::static_column), _raw._columns.begin() + column_offset(column_kind::regular_column), column_definition::name_comparator(static_column_name_type()));
std::sort(_raw._columns.begin() + column_offset(column_kind::regular_column), _raw._columns.end(), column_definition::name_comparator(regular_column_name_type()));
std::stable_sort(_raw._columns.begin(), _raw._columns.begin() + column_offset(column_kind::clustering_key), [](auto x, auto y)
                 { return x.id < y.id; });
std::stable_sort(_raw._columns.begin() + column_offset(column_kind::clustering_key), _raw._columns.begin() + column_offset(column_kind::static_column), [](auto x, auto y)
                 { return x.id < y.id; });
column_id id = 0;
for (auto &def : _raw._columns)
{
    def.column_specification = make_column_specification(def);
    assert(!def.id || def.id == id - column_offset(def.kind));
    def.ordinal_id = static_cast<ordinal_column_id>(id);
    def.id = id - column_offset(def.kind);
    auto dropped_at_it = _raw._dropped_columns.find(def.name_as_text());
    if (dropped_at_it != _raw._dropped_columns.end())
    {
        def._dropped_at = std::max(def._dropped_at, dropped_at_it->second.timestamp);
    }
    def._thrift_bits = column_definition::thrift_bits();
    { // is_on_all_components
        // TODO : In origin, this predicate is "componentIndex == null", which is true in
        // a number of cases, some of which I've most likely missed...
        switch (def.kind)
        {
        case column_kind::partition_key: // In origin, ci == null is true for a PK column where CFMetaData "keyValidator" is non-composite.
        // Which is true of #pk == 1
        def._thrift_bits.is_on_all_components = partition_key_size() == 1;
        break;
        case column_kind::regular_column:
        if (_raw._is_dense)
        { // regular values in dense tables are alone, so they have no index
            def._thrift_bits.is_on_all_components = true;
            break;
        }
        default: // Or any other column where "comparator" is not compound
        def._thrift_bits.is_on_all_components = !thrift().has_compound_comparator();
        break;
        }
    }
    ++id;
}
rebuild();
}
lw_shared_ptr<const schema> make_shared_schema(std::optional<table_id> id, std::string_view ks_name, std::string_view cf_name, std::vector<schema::column> partition_key, std::vector<schema::column> clustering_key, std::vector<schema::column> regular_columns, std::vector<schema::column> static_columns, data_type regular_column_name_type, sstring comment)
{
schema_builder builder(std::move(ks_name), std::move(cf_name), std::move(id), std::move(regular_column_name_type));
for (auto &&column : partition_key)
{
    builder.with_column(std::move(column.name), std::move(column.type), column_kind::partition_key);
}
for (auto &&column : clustering_key)
{
    builder.with_column(std::move(column.name), std::move(column.type), column_kind::clustering_key);
}
for (auto &&column : regular_columns)
{
    builder.with_column(std::move(column.name), std::move(column.type));
}
for (auto &&column : static_columns)
{
    builder.with_column(std::move(column.name), std::move(column.type), column_kind::static_column);
}
builder.set_comment(comment);
return builder.build();
}
schema::~schema()
{
if (_registry_entry)
{
    _registry_entry->detach_schema();
}
}
schema_registry_entry *schema::registry_entry() const noexcept { return _registry_entry; }
sstring schema::thrift_key_validator() const
{
if (partition_key_size() == 1)
{
    return partition_key_columns().begin()->type->name();
}
else
{
    auto type_params = fmt::join(partition_key_columns() | boost::adaptors::transformed(std::mem_fn(&column_definition::type)) | boost::adaptors::transformed(std::mem_fn(&abstract_type::name)), ", ");
    return format("org.apache.cassandra.db.marshal.CompositeType({})", type_params);
}
}
bool schema::has_multi_cell_collections() const
{
return boost::algorithm::any_of(all_columns(), [](const column_definition &cdef)
                                { return cdef.type->is_collection() && cdef.type->is_multi_cell(); });
}
bool operator==(const schema &x, const schema &y) { return x._raw._id == y._raw._id && x._raw._ks_name == y._raw._ks_name && x._raw._cf_name == y._raw._cf_name && x._raw._columns == y._raw._columns && x._raw._comment == y._raw._comment && x._raw._default_time_to_live == y._raw._default_time_to_live && x._raw._regular_column_name_type == y._raw._regular_column_name_type && x._raw._bloom_filter_fp_chance == y._raw._bloom_filter_fp_chance && x._raw._compressor_params == y._raw._compressor_params && x._raw._is_dense == y._raw._is_dense && x._raw._is_compound == y._raw._is_compound && x._raw._type == y._raw._type && x._raw._gc_grace_seconds == y._raw._gc_grace_seconds && x.paxos_grace_seconds() == y.paxos_grace_seconds() && x._raw._dc_local_read_repair_chance == y._raw._dc_local_read_repair_chance && x._raw._read_repair_chance == y._raw._read_repair_chance && x._raw._min_compaction_threshold == y._raw._min_compaction_threshold && x._raw._max_compaction_threshold == y._raw._max_compaction_threshold && x._raw._min_index_interval == y._raw._min_index_interval && x._raw._max_index_interval == y._raw._max_index_interval && x._raw._memtable_flush_period == y._raw._memtable_flush_period && x._raw._speculative_retry == y._raw._speculative_retry && x._raw._compaction_strategy == y._raw._compaction_strategy && x._raw._compaction_strategy_options == y._raw._compaction_strategy_options && x._raw._compaction_enabled == y._raw._compaction_enabled && x.cdc_options() == y.cdc_options() && x.tombstone_gc_options() == y.tombstone_gc_options() && x._raw._caching_options == y._raw._caching_options && x._raw._dropped_columns == y._raw._dropped_columns && x._raw._collections == y._raw._collections && x._raw._indices_by_name == y._raw._indices_by_name && x._raw._is_counter == y._raw._is_counter; }
index_metadata::index_metadata(const sstring &name, const index_options_map &options, index_metadata_kind kind, is_local_index local) : _id{utils::UUID_gen::get_name_UUID(name)}, _name{name}, _kind{kind}, _options{options}, _local{bool(local)}
{
}
bool index_metadata::operator==(const index_metadata &other) const { return _id == other._id && _name == other._name && _kind == other._kind && _options == other._options; }
bool index_metadata::equals_noname(const index_metadata &other) const { return _kind == other._kind && _options == other._options; }
const table_id &index_metadata::id() const { return _id; }
const sstring &index_metadata::name() const { return _name; }
const index_metadata_kind index_metadata::kind() const { return _kind; }
const index_options_map &index_metadata::options() const { return _options; }
bool index_metadata::local() const { return _local; }
sstring index_metadata::get_default_index_name(const sstring &cf_name, std::optional<sstring> root)
{
if (root)
{ // As noted in issue #3403, because table names in CQL only use word
    // characters [A-Za-z0-9_], the default index name should drop other
    // characters from the column name ("root").
    sstring name = root.value();
    name.erase(std::remove_if(name.begin(), name.end(), [](char c)
                              { return !((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z') || (c >= '0' && c <= '9') || (c == '_')); }),
               name.end());
    return cf_name + "_" + name + "_idx";
}
return cf_name + "_idx";
}
column_definition::column_definition(bytes name, data_type type, column_kind kind, column_id component_index, column_view_virtual is_view_virtual, column_computation_ptr computation, api::timestamp_type dropped_at) : _name(std::move(name)), _dropped_at(dropped_at), _is_atomic(type->is_atomic()), _is_counter(type->is_counter()), _is_view_virtual(is_view_virtual), _computation(std::move(computation)), type(std::move(type)), id(component_index), kind(kind) {}
std::ostream &operator<<(std::ostream &os, const column_definition &cd)
{
os << "ColumnDefinition{";
os << "name=" << cd.name_as_text();
os << ", type=" << cd.type->name();
os << ", kind=" << to_sstring(cd.kind);
if (cd.is_view_virtual())
{
    os << ", view_virtual";
}
if (cd.is_computed())
{
    os << ", computed:" << cd.get_computation().serialize();
}
os << ", componentIndex=" << (cd.has_component_index() ? std::to_string(cd.component_index()) : "null");
os << ", droppedAt=" << cd._dropped_at;
os << "}";
return os;
}
const column_definition *schema::get_column_definition(const bytes &name) const
{
auto i = _columns_by_name.find(name);
if (i == _columns_by_name.end())
{
    return nullptr;
}
return i->second;
}
const column_definition &schema::column_at(column_kind kind, column_id id) const { return column_at(static_cast<ordinal_column_id>(column_offset(kind) + id)); }
const column_definition &schema::column_at(ordinal_column_id ordinal_id) const
{
if (size_t(ordinal_id) >= _raw._columns.size()) [[unlikely]]
{
    on_internal_error(dblog, format("{}.{}@{}: column id {:d} >= {:d}", ks_name(), cf_name(), version(), size_t(ordinal_id), _raw._columns.size()));
}
return _raw._columns.at(static_cast<column_count_type>(ordinal_id));
}
std::ostream &operator<<(std::ostream &os, const schema &s)
{
os << "org.apache.cassandra.config.CFMetaData@" << &s << "[";
os << "cfId=" << s._raw._id;
os << ",ksName=" << s._raw._ks_name;
os << ",cfName=" << s._raw._cf_name;
os << ",cfType=" << cf_type_to_sstring(s._raw._type);
os << ",comparator=" << cell_comparator::to_sstring(s);
os << ",comment=" << s._raw._comment;
os << ",readRepairChance=" << s._raw._read_repair_chance;
os << ",dcLocalReadRepairChance=" << s._raw._dc_local_read_repair_chance;
os << ",tombstoneGcOptions=" << s.tombstone_gc_options().to_sstring();
os << ",gcGraceSeconds=" << s._raw._gc_grace_seconds;
os << ",keyValidator=" << s.thrift_key_validator();
os << ",minCompactionThreshold=" << s._raw._min_compaction_threshold;
os << ",maxCompactionThreshold=" << s._raw._max_compaction_threshold;
os << ",columnMetadata=[";
int n = 0;
for (auto &cdef : s._raw._columns)
{
    if (n++ != 0)
    {
        os << ", ";
    }
    os << cdef;
}
os << "]";
os << ",compactionStrategyClass=class org.apache.cassandra.db.compaction." << sstables::compaction_strategy::name(s._raw._compaction_strategy);
os << ",compactionStrategyOptions={";
n = 0;
for (auto &p : s._raw._compaction_strategy_options)
{
    os << p.first << "=" << p.second;
    os << ", ";
}
os << "enabled=" << std::boolalpha << s._raw._compaction_enabled;
os << "}";
os << ",compressionParameters={";
n = 0;
for (auto &p : s._raw._compressor_params.get_options())
{
    if (n++ != 0)
    {
        os << ", ";
    }
    os << p.first << "=" << p.second;
}
os << "}";
os << ",bloomFilterFpChance=" << s._raw._bloom_filter_fp_chance;
os << ",memtableFlushPeriod=" << s._raw._memtable_flush_period;
os << ",caching=" << s._raw._caching_options.to_sstring();
os << ",cdc=" << s.cdc_options().to_sstring();
os << ",defaultTimeToLive=" << s._raw._default_time_to_live.count();
os << ",minIndexInterval=" << s._raw._min_index_interval;
os << ",maxIndexInterval=" << s._raw._max_index_interval;
os << ",speculativeRetry=" << s._raw._speculative_retry.to_sstring();
os << ",triggers=[]";
os << ",isDense=" << std::boolalpha << s._raw._is_dense;
os << ",version=" << s.version();
os << ",droppedColumns={";
n = 0;
for (auto &dc : s._raw._dropped_columns)
{
    if (n++ != 0)
    {
        os << ", ";
    }
    os << dc.first << " : { " << dc.second.type->name() << ", " << dc.second.timestamp << " }";
}
os << "}";
os << ",collections={";
n = 0;
for (auto &c : s._raw._collections)
{
    if (n++ != 0)
    {
        os << ", ";
    }
    os << c.first << " : " << c.second->name();
}
os << "}";
os << ",indices={";
n = 0;
for (auto &c : s._raw._indices_by_name)
{
    if (n++ != 0)
    {
        os << ", ";
    }
    os << c.first << " : " << c.second.id();
}
os << "}";
os << "]";
return os;
}
static std::ostream &map_as_cql_param(std::ostream &os, const std::map<sstring, sstring> &map, bool first = true)
{
for (auto i : map)
{
    if (first)
    {
        first = false;
    }
    else
    {
        os << ",";
    }
    os << "'" << i.first << "': '" << i.second << "'";
}
return os;
}
static std::ostream &column_definition_as_cql_key(std::ostream &os, const column_definition &cd)
{
os << cd.name_as_cql_string();
os << " " << cd.type->cql3_type_name();
if (cd.kind == column_kind::static_column)
{
    os << " STATIC";
}
return os;
}
static bool is_global_index(replica::database &db, const table_id &id, const schema &s) { return false; }
static bool is_index(replica::database &db, const table_id &id, const schema &s) { return false; }
sstring schema::element_type(replica::database &db) const { return "table"; }
std::ostream &schema::describe(replica::database &db, std::ostream &os, bool with_internals) const { return os; }
const sstring &column_definition::name_as_text() const { return column_specification->name->text(); }
const bytes &column_definition::name() const { return _name; }
sstring column_definition::name_as_cql_string() const { return cql3::util::maybe_quote(name_as_text()); }
bool column_definition::is_on_all_components() const { return _thrift_bits.is_on_all_components; }
bool operator==(const column_definition &x, const column_definition &y) { return x._name == y._name && x.type == y.type && x.id == y.id && x.kind == y.kind && x._dropped_at == y._dropped_at; }
// Based on org.apache.cassandra.config.CFMetaData#generateLegacyCfId
table_id generate_legacy_id(const sstring &ks_name, const sstring &cf_name) { return table_id(utils::UUID_gen::get_name_UUID(ks_name + cf_name)); }
bool thrift_schema::has_compound_comparator() const { return _compound; }
bool thrift_schema::is_dynamic() const { return _is_dynamic; }
schema_builder &schema_builder::set_compaction_strategy_options(std::map<sstring, sstring> &&options)
{
_raw._compaction_strategy_options = std::move(options);
return *this;
}
schema_builder &schema_builder::with_partitioner(sstring name)
{
_raw._partitioner = get_partitioner(name);
return *this;
}
schema_builder &schema_builder::with_sharder(unsigned shard_count, unsigned sharding_ignore_msb_bits)
{
_raw._sharder = get_sharder(shard_count, sharding_ignore_msb_bits);
return *this;
}
schema_builder::schema_builder(std::string_view ks_name, std::string_view cf_name, std::optional<table_id> id, data_type rct) : _raw(id ? *id : table_id(utils::UUID_gen::get_time_UUID()))
{ // Various schema-creation commands (creating tables, indexes, etc.)
// usually place limits on which characters are allowed in keyspace or
// table names. But in case we have a hole in those defences (see issue
// #3403, for example), let's prevent at least the characters "/" and
// null from being in the keyspace or table name, because those will
// surely cause serious problems when materialized to directory names.
// We throw a logic_error because we expect earlier defences to have
// avoided this case in the first place.
if (ks_name.find_first_of('/') != std::string_view::npos || ks_name.find_first_of('\0') != std::string_view::npos)
{
    throw std::logic_error(format("Tried to create a schema with illegal characters in keyspace name: {}", ks_name));
}
if (cf_name.find_first_of('/') != std::string_view::npos || cf_name.find_first_of('\0') != std::string_view::npos)
{
    throw std::logic_error(format("Tried to create a schema with illegal characters in table name: {}", cf_name));
}
_raw._ks_name = sstring(ks_name);
_raw._cf_name = sstring(cf_name);
_raw._regular_column_name_type = rct;
}
schema_builder::schema_builder(const schema_ptr s) : schema_builder(s->_raw) {}
schema_builder::schema_builder(const schema::raw_schema &raw) : _raw(raw)
{
static_assert(schema::row_column_ids_are_ordered_by_name::value, "row columns don't need to be ordered by name"); // Schema builder may add or remove columns and their ids need to be
// recomputed in build().
for (auto &def : _raw._columns | boost::adaptors::filtered([](auto &def)
                                                           { return !def.is_primary_key(); }))
{
    def.id = 0;
    def.ordinal_id = static_cast<ordinal_column_id>(0);
}
}
schema_builder::schema_builder(std::optional<table_id> id, std::string_view ks_name, std::string_view cf_name, std::vector<schema::column> partition_key, std::vector<schema::column> clustering_key, std::vector<schema::column> regular_columns, std::vector<schema::column> static_columns, data_type regular_column_name_type, sstring comment) : schema_builder(ks_name, cf_name, std::move(id), std::move(regular_column_name_type))
{
for (auto &&column : partition_key)
{
    with_column(std::move(column.name), std::move(column.type), column_kind::partition_key);
}
for (auto &&column : clustering_key)
{
    with_column(std::move(column.name), std::move(column.type), column_kind::clustering_key);
}
for (auto &&column : regular_columns)
{
    with_column(std::move(column.name), std::move(column.type));
}
for (auto &&column : static_columns)
{
    with_column(std::move(column.name), std::move(column.type), column_kind::static_column);
}
set_comment(comment);
}
column_definition &schema_builder::find_column(const cql3::column_identifier &c)
{
auto i = std::find_if(_raw._columns.begin(), _raw._columns.end(), [c](auto &p)
                      { return p.name() == c.name(); });
if (i != _raw._columns.end())
{
    return *i;
}
throw std::invalid_argument(format("No such column {}", c.name()));
}
bool schema_builder::has_column(const cql3::column_identifier &c)
{
auto i = std::find_if(_raw._columns.begin(), _raw._columns.end(), [c](auto &p)
                      { return p.name() == c.name(); });
return i != _raw._columns.end();
}
schema_builder &schema_builder::with_column_ordered(const column_definition &c) { return with_column(bytes(c.name()), data_type(c.type), column_kind(c.kind), c.position(), c.view_virtual(), c.get_computation_ptr()); }
schema_builder &schema_builder::with_column(bytes name, data_type type, column_kind kind, column_view_virtual is_view_virtual)
{ // component_index will be determined by schema cosntructor
return with_column(name, type, kind, 0, is_view_virtual);
}
schema_builder &schema_builder::with_column(bytes name, data_type type, column_kind kind, column_id component_index, column_view_virtual is_view_virtual, column_computation_ptr computation)
{
_raw._columns.emplace_back(name, type, kind, component_index, is_view_virtual, std::move(computation));
if (type->is_multi_cell())
{
    with_collection(name, type);
}
else if (type->is_counter())
{
    set_is_counter(true);
}
return *this;
}
schema_builder &schema_builder::with_computed_column(bytes name, data_type type, column_kind kind, column_computation_ptr computation) { return with_column(name, type, kind, 0, column_view_virtual::no, std::move(computation)); }
schema_builder &schema_builder::remove_column(bytes name)
{
auto it = boost::range::find_if(_raw._columns, [&](auto &column)
                                { return column.name() == name; });
if (it == _raw._columns.end())
{
    throw std::out_of_range(format("Cannot remove: column {} not found.", name));
}
auto name_as_text = it->column_specification ? it->name_as_text() : schema::column_name_type(*it, _raw._regular_column_name_type)->get_string(it->name());
without_column(name_as_text, it->type, api::new_timestamp());
_raw._columns.erase(it);
return *this;
}
schema_builder &schema_builder::without_column(sstring name, api::timestamp_type timestamp) { return without_column(std::move(name), bytes_type, timestamp); }
schema_builder &schema_builder::without_column(sstring name, data_type type, api::timestamp_type timestamp)
{
auto ret = _raw._dropped_columns.emplace(name, schema::dropped_column{type, timestamp});
if (!ret.second && ret.first->second.timestamp < timestamp)
{
    ret.first->second.type = type;
    ret.first->second.timestamp = timestamp;
}
return *this;
}
schema_builder &schema_builder::rename_column(bytes from, bytes to)
{
auto it = std::find_if(_raw._columns.begin(), _raw._columns.end(), [&](auto &col)
                       { return col.name() == from; });
assert(it != _raw._columns.end());
auto &def = *it;
column_definition new_def(to, def.type, def.kind, def.component_index());
_raw._columns.erase(it);
return with_column_ordered(new_def);
}
schema_builder &schema_builder::alter_column_type(bytes name, data_type new_type)
{
auto it = boost::find_if(_raw._columns, [&name](auto &c)
                         { return c.name() == name; });
assert(it != _raw._columns.end());
it->type = new_type;
if (new_type->is_multi_cell())
{
    auto c_it = _raw._collections.find(name);
    assert(c_it != _raw._collections.end());
    c_it->second = new_type;
}
return *this;
}
schema_builder &schema_builder::mark_column_computed(bytes name, column_computation_ptr computation)
{
auto it = boost::find_if(_raw._columns, [&name](const column_definition &c)
                         { return c.name() == name; });
assert(it != _raw._columns.end());
it->set_computed(std::move(computation));
return *this;
}
schema_builder &schema_builder::with_collection(bytes name, data_type type)
{
_raw._collections.emplace(name, type);
return *this;
}
schema_builder &schema_builder::with(compact_storage cs)
{
_compact_storage = cs;
return *this;
}
schema_builder &schema_builder::with_version(table_schema_version v)
{
_version = v;
return *this;
}
static const sstring default_partition_key_name = "key";
static const sstring default_clustering_name = "column";
static const sstring default_compact_value_name = "value";
schema_builder::default_names::default_names(const schema::raw_schema &raw) : _raw(raw), _partition_index(0), _clustering_index(1), _compact_index(0) {}
sstring schema_builder::default_names::unique_name(const sstring &base, size_t &idx, size_t off) const
{
for (;;)
{
    auto candidate = idx == 0 ? base : base + std::to_string(idx + off);
    ++idx;
    auto i = std::find_if(_raw._columns.begin(), _raw._columns.end(), [b = to_bytes(candidate)](const column_definition &c)
                          { return c.name() == b; });
    if (i == _raw._columns.end())
    {
        return candidate;
    }
}
}
sstring schema_builder::default_names::clustering_name() { return unique_name(default_clustering_name, _clustering_index, 0); }
sstring schema_builder::default_names::compact_value_name() { return unique_name(default_compact_value_name, _compact_index, 0); }
void schema_builder::prepare_dense_schema(schema::raw_schema &raw)
{
auto is_dense = raw._is_dense;
auto is_compound = raw._is_compound;
auto is_compact_table = is_dense || !is_compound;
if (is_compact_table)
{
    auto count_kind = [&raw](column_kind kind)
    { return std::count_if(raw._columns.begin(), raw._columns.end(), [kind](const column_definition &c)
                           { return c.kind == kind; }); };
    default_names names(raw);
    if (is_dense)
    {
        auto regular_cols = count_kind(column_kind::regular_column); // In Origin, dense CFs always have at least one regular column
        if (regular_cols == 0)
        {
        raw._columns.emplace_back(to_bytes(names.compact_value_name()), empty_type, column_kind::regular_column, 0);
        }
        else if (regular_cols > 1)
        {
        throw exceptions::configuration_exception(format("Expecting exactly one regular column. Found {:d}", regular_cols));
        }
    }
}
}
schema_ptr schema_builder::build()
{
schema::raw_schema new_raw = _raw; // Copy so that build() remains idempotent.
schema_static_props static_props{};
for (const auto &c : static_configurators())
{
    c(new_raw._ks_name, new_raw._cf_name, static_props);
}
if (_version)
{
    new_raw._version = *_version;
}
else
{
    new_raw._version = table_schema_version(utils::UUID_gen::get_time_UUID());
}
if (new_raw._is_counter)
{
    new_raw._default_validation_class = counter_type;
}
if (_compact_storage)
{ // Dense means that no part of the comparator stores a CQL column name. This means
    // COMPACT STORAGE with at least one columnAliases (otherwise it's a thrift "static" CF).
    auto clustering_key_size = std::count_if(new_raw._columns.begin(), new_raw._columns.end(), [](auto &&col)
                                             { return col.kind == column_kind::clustering_key; });
    new_raw._is_dense = (*_compact_storage == compact_storage::yes) && (clustering_key_size > 0);
    if (clustering_key_size == 0)
    {
        if (*_compact_storage == compact_storage::yes)
        {
        new_raw._is_compound = false;
        }
        else
        {
        new_raw._is_compound = true;
        }
    }
    else
    {
        if ((*_compact_storage == compact_storage::yes) && clustering_key_size == 1)
        {
        new_raw._is_compound = false;
        }
        else
        {
        new_raw._is_compound = true;
        }
    }
}
prepare_dense_schema(new_raw); // cache `paxos_grace_seconds` value for fast access through the schema object, which is immutable
if (auto it = new_raw._extensions.find(db::paxos_grace_seconds_extension::NAME); it != new_raw._extensions.end())
{
    new_raw._paxos_grace_seconds = dynamic_pointer_cast<db::paxos_grace_seconds_extension>(it->second)->get_paxos_grace_seconds();
} // cache the `per_partition_rate_limit` parameters for fast access through the schema object.
if (auto it = new_raw._extensions.find(db::per_partition_rate_limit_extension::NAME); it != new_raw._extensions.end())
{
    new_raw._per_partition_rate_limit_options = dynamic_pointer_cast<db::per_partition_rate_limit_extension>(it->second)->get_options();
}
if (static_props.use_null_sharder)
{
    new_raw._sharder = get_sharder(1, 0);
}
return make_lw_shared<schema>(schema::private_tag{}, new_raw, _view_info, static_props);
}
auto schema_builder::static_configurators() -> std::vector<static_configurator> &
{
static std::vector<static_configurator> result{};
return result;
}
// Useful functions to manipulate the schema's comparator field
namespace cell_comparator
{
static constexpr auto _composite_str = "org.apache.cassandra.db.marshal.CompositeType";
static constexpr auto _collection_str = "org.apache.cassandra.db.marshal.ColumnToCollectionType";
}
struct column_less_comparator
{
bool operator()(const column_definition &def, const bytes &name);
bool operator()(const bytes &name, const column_definition &def);
};
data_type schema::column_name_type(const column_definition &def, const data_type &regular_column_name_type)
{
if (def.kind == column_kind::regular_column)
{
    return regular_column_name_type;
}
return utf8_type;
}
data_type schema::column_name_type(const column_definition &def) const { return column_name_type(def, _raw._regular_column_name_type); }
bool schema::has_static_columns() const { return !static_columns().empty(); }
column_count_type schema::partition_key_size() const { return column_offset(column_kind::clustering_key); }
schema::const_iterator_range_type schema::partition_key_columns() const { return boost::make_iterator_range(_raw._columns.begin() + column_offset(column_kind::partition_key), _raw._columns.begin() + column_offset(column_kind::clustering_key)); }
schema::const_iterator_range_type schema::clustering_key_columns() const { return boost::make_iterator_range(_raw._columns.begin() + column_offset(column_kind::clustering_key), _raw._columns.begin() + column_offset(column_kind::static_column)); }
schema::const_iterator_range_type schema::static_columns() const { return boost::make_iterator_range(_raw._columns.begin() + column_offset(column_kind::static_column), _raw._columns.begin() + column_offset(column_kind::regular_column)); }
schema::const_iterator_range_type schema::regular_columns() const { return boost::make_iterator_range(_raw._columns.begin() + column_offset(column_kind::regular_column), _raw._columns.end()); }
data_type schema::make_legacy_default_validator() const { return _raw._default_validation_class; }
void collection_column_computation::operate_on_collection_entries(std::invocable<collection_kv *, collection_kv *, tombstone> auto &&old_and_new_row_func, const schema &schema, const partition_key &key, const db::view::clustering_or_static_row &update, const std::optional<db::view::clustering_or_static_row> &existing) const
{
const column_definition *cdef = schema.get_column_definition(_collection_name);
decltype(collection_mutation_view_description::cells) update_cells, existing_cells;
const auto *update_cell = update.cells().find_cell(cdef->id);
tombstone update_tombstone = update.tomb().tomb();
if (update_cell)
{
    collection_mutation_view update_col_view = update_cell->as_collection_mutation();
    update_col_view.with_deserialized(*(cdef->type), [&update_cells, &update_tombstone](collection_mutation_view_description descr)
                                      {             update_tombstone.apply(descr.tomb);             update_cells = descr.cells; });
}
if (existing)
{
    const auto *existing_cell = existing->cells().find_cell(cdef->id);
    if (existing_cell)
    {
        collection_mutation_view existing_col_view = existing_cell->as_collection_mutation();
        existing_col_view.with_deserialized(*(cdef->type), [&existing_cells](collection_mutation_view_description descr)
                                            { existing_cells = descr.cells; });
    }
}
auto compare = [](const collection_kv &p1, const collection_kv &p2)
{ return p1.first <=> p2.first; }; // Both collections are assumed to be sorted by the keys.
auto existing_it = existing_cells.begin();
auto update_it = update_cells.begin();
auto is_existing_end = [&]
{ return existing_it == existing_cells.end(); };
auto is_update_end = [&]
{ return update_it == update_cells.end(); };
while (!(is_existing_end() && is_update_end()))
{
    std::strong_ordering cmp = [&]
    {             if (is_existing_end()) {                 return std::strong_ordering::greater;             } else if (is_update_end()) {                 return std::strong_ordering::less;             }             return compare(*existing_it, *update_it); }();
    auto existing_ptr = [&]() -> collection_kv *
    { return (!is_existing_end() && cmp <= 0) ? &*existing_it : nullptr; };
    auto update_ptr = [&]() -> collection_kv *
    { return (!is_update_end() && cmp >= 0) ? &*update_it : nullptr; };
    old_and_new_row_func(existing_ptr(), update_ptr(), update_tombstone);
    if (cmp <= 0)
    {
        ++existing_it;
    }
    if (cmp >= 0)
    {
        ++update_it;
    }
}
}
static logging::logger slogger("schema_registry");
static thread_local schema_registry registry;
schema_registry::~schema_registry() = default;
schema_registry_entry::erase_clock::duration schema_registry::grace_period() const { return std::chrono::seconds(_ctxt->schema_registry_grace_period()); }
void schema_registry_entry::detach_schema() noexcept
{
slogger.trace("Deactivating {}", _version);
_schema = nullptr;
_erase_timer.arm(_registry.grace_period());
}
future<> schema_registry_entry::maybe_sync(std::function<future<>()> syncer)
{
switch (_sync_state)
{
case schema_registry_entry::sync_state::SYNCED:
    return make_ready_future<>();
case schema_registry_entry::sync_state::SYNCING:
    return _synced_promise.get_shared_future();
case schema_registry_entry::sync_state::NOT_SYNCED:
{
    slogger.debug("Syncing {}", _version);
    _synced_promise = {};
    auto f = do_with(std::move(syncer), [](auto &syncer)
                     { return syncer(); });
    auto sf = _synced_promise.get_shared_future();
    _sync_state = schema_registry_entry::sync_state::SYNCING; // Move to background.
    (void)f.then_wrapped([this, self = shared_from_this()](auto &&f)
                         {                 if (_sync_state != sync_state::SYNCING) {                     f.ignore_ready_future();                     return;                 }                 if (f.failed()) {                     slogger.debug("Syncing of {} failed", _version);                     _sync_state = schema_registry_entry::sync_state::NOT_SYNCED;                     _synced_promise.set_exception(f.get_exception());                 } else {                     slogger.debug("Synced {}", _version);                     _sync_state = schema_registry_entry::sync_state::SYNCED;                     _synced_promise.set_value();                 } });
    return sf;
}
}
abort();
}
bool schema_registry_entry::is_synced() const { return _sync_state == sync_state::SYNCED; }
void schema_registry_entry::mark_synced()
{
if (_sync_state == sync_state::SYNCING)
{
    _synced_promise.set_value();
}
_sync_state = sync_state::SYNCED;
slogger.debug("Marked {} as synced", _version);
}
global_schema_ptr::global_schema_ptr(const global_schema_ptr &o) : global_schema_ptr(o.get()) {}
global_schema_ptr::global_schema_ptr(global_schema_ptr &&o) noexcept
{
auto current = this_shard_id();
assert(o._cpu_of_origin == current);
_ptr = std::move(o._ptr);
_cpu_of_origin = current;
_base_schema = std::move(o._base_schema);
}
schema_ptr global_schema_ptr::get() const
{
if (this_shard_id() == _cpu_of_origin)
{
    return _ptr;
}
else
{
    auto registered_schema = [](const schema_registry_entry &e)
    {             schema_ptr ret = local_schema_registry().get_or_null(e.version());             if (!ret) {                 ret = local_schema_registry().get_or_load(e.version(), [&e](table_schema_version) {                     return e.frozen();                 });             }             return ret; };
    schema_ptr registered_bs; // the following code contains registry entry dereference of a foreign shard
    // however, it is guarantied to succeed since we made sure in the constructor
    // that _bs_schema and _ptr will have a registry on the foreign shard where this
    // object originated so as long as this object lives the registry entries lives too
    // and it is safe to reference them on foreign shards.
    if (_base_schema)
    {
        registered_bs = registered_schema(*_base_schema->registry_entry());
        if (_base_schema->registry_entry()->is_synced())
        {
        registered_bs->registry_entry()->mark_synced();
        }
    }
    schema_ptr s = registered_schema(*_ptr->registry_entry());
    if (s->is_view())
    {
        if (!s->view_info()->base_info())
        { // we know that registered_bs is valid here because we make sure of it in the constructors.
        s->view_info()->set_base_info(s->view_info()->make_base_dependent_view_info(*registered_bs));
        }
    }
    if (_ptr->registry_entry()->is_synced())
    {
        s->registry_entry()->mark_synced();
    }
    return s;
}
}
global_schema_ptr::global_schema_ptr(const schema_ptr &ptr) : _cpu_of_origin(this_shard_id())
{ // _ptr must always have an associated registry entry,
// if ptr doesn't, we need to load it into the registry.
auto ensure_registry_entry = [](const schema_ptr &s)
{         schema_registry_entry* e = s->registry_entry();         if (e) {             return s;         } else {             return local_schema_registry().get_or_load(s->version(), [&s] (table_schema_version) {                 return frozen_schema(s);             });         } };
schema_ptr s = ensure_registry_entry(ptr);
if (s->is_view())
{
    if (s->view_info()->base_info())
    {
        _base_schema = ensure_registry_entry(s->view_info()->base_info()->base_schema());
    }
    else if (ptr->view_info()->base_info())
    {
        _base_schema = ensure_registry_entry(ptr->view_info()->base_info()->base_schema());
    }
    else
    {
        on_internal_error(slogger, format("Tried to build a global schema for view {}.{} with an uninitialized base info", s->ks_name(), s->cf_name()));
    }
    if (!s->view_info()->base_info() || !s->view_info()->base_info()->base_schema()->registry_entry())
    {
        s->view_info()->set_base_info(s->view_info()->make_base_dependent_view_info(*_base_schema));
    }
}
_ptr = s;
}
frozen_schema::frozen_schema(const schema_ptr &s) : _data([&s]
                                                          {         schema_mutations sm = db::schema_tables::make_schema_mutations(s, api::new_timestamp(), true);         bytes_ostream out;         ser::writer_of_schema<bytes_ostream> wr(out);         std::move(wr).write_version(s->version())                      .write_mutations(sm)                      .end_schema();         return out; }()) {}
schema_ptr frozen_schema::unfreeze(const db::schema_ctxt &ctxt) const
{
auto in = ser::as_input_stream(_data);
auto sv = ser::deserialize(in, boost::type<ser::schema_view>());
return sv.mutations().is_view() ? db::schema_tables::create_view_from_mutations(ctxt, sv.mutations(), sv.version()) : db::schema_tables::create_table_from_mutations(ctxt, sv.mutations(), sv.version());
}
frozen_schema::frozen_schema(bytes_ostream b) : _data(std::move(b)) {}
const bytes_ostream &frozen_schema::representation() const { return _data; }
#define arch_target(name) [[gnu::target(name)]]
namespace utils
{
arch_target("default") int array_search_gt_impl(int64_t val, const int64_t *array, const int capacity, const int size)
{
    int i;
    for (i = 0; i < size; i++)
    {
        if (val < array[i])
        break;
    }
    return i;
}
static inline unsigned array_search_eq_impl(uint8_t val, const uint8_t *arr, unsigned len)
{
    unsigned i;
    for (i = 0; i < len; i++)
    {
        if (arr[i] == val)
        {
        break;
        }
    }
    return i;
}
arch_target("default") unsigned array_search_16_eq_impl(uint8_t val, const uint8_t *arr) { return array_search_eq_impl(val, arr, 16); }
arch_target("default") unsigned array_search_32_eq_impl(uint8_t val, const uint8_t *arr) { return array_search_eq_impl(val, arr, 32); }
arch_target("default") unsigned array_search_x32_eq_impl(uint8_t val, const uint8_t *arr, int nr) { return array_search_eq_impl(val, arr, 32 * nr); }
arch_target("avx2") int array_search_gt_impl(int64_t val, const int64_t *array, const int capacity, const int size)
{
    int cnt = 0; // 0. Load key into 256-bit ymm
    __m256i k = _mm256_set1_epi64x(val);
    for (int i = 0; i < capacity; i += 4)
    {                                      // 4. Count the number of 1-s, each gt match gives 8 bits
        cnt += _mm_popcnt_u32(             // 3. Pack result into 4 bytes -- 1 byte from each comparison
                   _mm256_movemask_epi8(   // 2. Compare array[i] > key, 4 elements in one go
                       _mm256_cmpgt_epi64( // 1. Load next 4 elements into ymm
                           _mm256_lddqu_si256((__m256i *)&array[i]), k))) /
               8;
    }
    return size - cnt;
}
arch_target("sse") unsigned array_search_16_eq_impl(uint8_t val, const uint8_t *arr)
{
    auto a = _mm_set1_epi8(val);
    auto b = _mm_lddqu_si128((__m128i *)arr);
    auto c = _mm_cmpeq_epi8(a, b);
    unsigned int m = _mm_movemask_epi8(c);
    return __builtin_ctz(m | 0x10000);
}
arch_target("avx2") unsigned array_search_32_eq_impl(uint8_t val, const uint8_t *arr)
{
    auto a = _mm256_set1_epi8(val);
    auto b = _mm256_lddqu_si256((__m256i *)arr);
    auto c = _mm256_cmpeq_epi8(a, b);
    unsigned long long m = _mm256_movemask_epi8(c);
    return __builtin_ctzll(m | 0x100000000ull);
}
arch_target("avx2") unsigned array_search_x32_eq_impl(uint8_t val, const uint8_t *arr, int nr)
{
    unsigned len = 32 * nr;
    auto a = _mm256_set1_epi8(val);
    for (unsigned off = 0; off < len; off += 32)
    {
        auto b = _mm256_lddqu_si256((__m256i *)arr);
        auto c = _mm256_cmpeq_epi8(a, b);
        unsigned m = _mm256_movemask_epi8(c);
        if (m != 0)
        {
        return __builtin_ctz(m) + off;
        }
    }
    return len;
}
unsigned array_search_16_eq(uint8_t val, const uint8_t *arr) { return array_search_16_eq_impl(val, arr); }
unsigned array_search_32_eq(uint8_t val, const uint8_t *array) { return array_search_32_eq_impl(val, array); }
}
// Arrays for quickly converting to and from an integer between 0 and 63,
// and the character used in base64 encoding to represent it.
static class base64_chars
{
public:
static constexpr const char to[] = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/";
static constexpr uint8_t invalid_char = 255;
uint8_t from[255];
base64_chars()
{
    static_assert(sizeof(to) == 64 + 1);
    for (int i = 0; i < 255; i++)
    {
        from[i] = invalid_char; // signal invalid character
    }
    for (int i = 0; i < 64; i++)
    {
        from[(unsigned)to[i]] = i;
    }
}
} base64_chars;
using namespace std::chrono_literals;
// For each aligned 8 byte segment, the algorithm used by address
// sanitizer can represent any addressable prefix followd by a
// poisoned suffix. The details are at:
// https://github.com/google/sanitizers/wiki/AddressSanitizerAlgorithm
// For us this means that:
// * The descriptor must be 8 byte aligned. If it was not, making the
//   descriptor addressable would also make the end of the previous
//   value addressable.
// * Each value must be at least 8 byte aligned. If it was not, making
//   the value addressable would also make the end of the descriptor
//   addressable.
template <typename T>
[[nodiscard]] static T align_up_for_asan(T val) { return align_up(val, size_t(8)); }
template <typename T>
void poison(const T *addr, size_t size)
{                                 // Both values and descriptors must be aligned.
assert(uintptr_t(addr) % 8 == 0); // This can be followed by
// * 8 byte aligned descriptor (this is a value)
// * 8 byte aligned value
// * dead value
// * end of segment
// In all cases, we can align up the size to guarantee that asan
// is able to poison this.
ASAN_POISON_MEMORY_REGION(addr, align_up_for_asan(size));
}
void unpoison(const char *addr, size_t size) { ASAN_UNPOISON_MEMORY_REGION(addr, size); }
namespace bi = boost::intrusive;
standard_allocation_strategy standard_allocation_strategy_instance;
namespace
{
class migrators_base
{
protected:
    std::vector<const migrate_fn_type *> _migrators;
};
class migrators : public migrators_base, public enable_lw_shared_from_this<migrators>
{
private:
    struct backtrace_entry
    {
        saved_backtrace _registration;
        saved_backtrace _deregistration;
    };
    std::vector<std::unique_ptr<backtrace_entry>> _backtraces;
    static logging::logger _logger;
private:
    void on_error() { abort(); }
public:
    uint32_t add(const migrate_fn_type *m)
    {
        _migrators.push_back(m);
        _backtraces.push_back(std::make_unique<backtrace_entry>(backtrace_entry{current_backtrace(), {}}));
        return _migrators.size() - 1;
    }
    void remove(uint32_t idx)
    {
        if (idx >= _migrators.size())
        {
        _logger.error("Attempting to deregister migrator id {} which was never registered:\n{}", idx, current_backtrace());
        on_error();
        }
        if (!_migrators[idx])
        {
        _logger.error("Attempting to double deregister migrator id {}:\n{}\n"
                      "Previously deregistered at:\n{}\nRegistered at:\n{}",
                      idx, current_backtrace(), _backtraces[idx]->_deregistration, _backtraces[idx]->_registration);
        on_error();
        }
        _migrators[idx] = nullptr;
        _backtraces[idx]->_deregistration = current_backtrace();
    }
    const migrate_fn_type *&operator[](uint32_t idx)
    {
        if (idx >= _migrators.size())
        {
        _logger.error("Attempting to use migrator id {} that was never registered:\n{}", idx, current_backtrace());
        on_error();
        }
        if (!_migrators[idx])
        {
        _logger.error("Attempting to use deregistered migrator id {}:\n{}\n"
                      "Deregistered at:\n{}\nRegistered at:\n{}",
                      idx, current_backtrace(), _backtraces[idx]->_deregistration, _backtraces[idx]->_registration);
        on_error();
        }
        return _migrators[idx];
    }
};
logging::logger migrators::_logger("lsa-migrator-sanitizer");
static migrators &static_migrators() noexcept
{
    memory::scoped_critical_alloc_section dfg;
    static thread_local lw_shared_ptr<migrators> obj = make_lw_shared<migrators>();
    return *obj;
}
}
namespace debug
{
thread_local migrators *static_migrators = &::static_migrators();
}
uint32_t migrate_fn_type::register_migrator(migrate_fn_type *m)
{
auto &migrators = *debug::static_migrators;
auto idx = migrators.add(m); // object_descriptor encodes 2 * index() + 1
assert(idx * 2 + 1 < utils::uleb64_express_supreme);
m->_migrators = migrators.shared_from_this();
return idx;
}
void migrate_fn_type::unregister_migrator(uint32_t index) { static_migrators().remove(index); }
namespace logalloc
{
class region_sanitizer
{
    struct allocation
    {
        size_t size;
        saved_backtrace backtrace;
    };
private:
    static logging::logger logger;
    const bool *_report_backtrace = nullptr;
    bool _broken = false;
    std::unordered_map<const void *, allocation> _allocations;
private:
    template <typename Function>
    void run_and_handle_errors(Function &&fn) noexcept
    {
        memory::scoped_critical_alloc_section dfg;
        if (_broken)
        {
        return;
        }
        try
        {
        fn();
        }
        catch (...)
        {
        logger.error("Internal error, disabling the sanitizer: {}", std::current_exception());
        _broken = true;
        _allocations.clear();
        }
    }
private:
    void on_error() { abort(); }
public:
    region_sanitizer(const bool &report_backtrace);
    void on_region_destruction() noexcept;
    
    
    void on_migrate(const void *src, size_t size, const void *dst) noexcept
    {
        run_and_handle_errors([&]
                              {             auto it_src = _allocations.find(src);             if (it_src == _allocations.end()) {                 logger.error("Attempting to migrate an object at {} (size: {}) that does not exist",                              src, size);                 on_error();             }             if (it_src->second.size != size) {                 logger.error("Mismatch between allocation and migration size of object at {}: {} vs. {}\n"                              "Allocated at:\n{}",                              src, it_src->second.size, size, it_src->second.backtrace);                 on_error();             }             auto [ it_dst, success ] = _allocations.emplace(dst, std::move(it_src->second));             if (!success) {                 logger.error("Attempting to migrate an {} byte object to an already occupied address {}:\n"                              "Migrated object allocated from:\n{}\n"                              "Previous allocation of {} bytes at the destination:\n{}",                              size, dst, it_src->second.backtrace, it_dst->second.size, it_dst->second.backtrace);                 on_error();             }             _allocations.erase(it_src); });
    }
    void merge(region_sanitizer &other) noexcept
    ;
};
logging::logger region_sanitizer::logger("lsa-sanitizer");
struct segment;
static logging::logger llogger("lsa");
static logging::logger timing_logger("lsa-timing");
static tracker &get_tracker_instance() noexcept
{
    memory::scoped_critical_alloc_section dfg;
    static thread_local tracker obj;
    return obj;
}
static thread_local tracker &tracker_instance = get_tracker_instance();
using clock = std::chrono::steady_clock;
class background_reclaimer
{
    scheduling_group _sg;
    noncopyable_function<void(size_t target)> _reclaim;
    timer<lowres_clock> _adjust_shares_timer; // If engaged, main loop is not running, set_value() to wake it.
    promise<> *_main_loop_wait = nullptr;
    future<> _done;
    bool _stopping = false;
    static constexpr size_t free_memory_threshold = 60'000'000;
private:
    
    
    
    
public:
};
class segment_pool;
struct reclaim_timer;
class tracker::impl
{
    std::unique_ptr<logalloc::segment_pool> _segment_pool;
    std::optional<background_reclaimer> _background_reclaimer;
    std::vector<region::impl *> _regions;
    seastar::metrics::metric_groups _metrics;
    unsigned _reclaiming_disabled_depth = 0;
    size_t _reclamation_step = 1;
    bool _abort_on_bad_alloc = false;
    bool _sanitizer_report_backtrace = false;
    reclaim_timer *_active_timer = nullptr;
private: // Prevents tracker's reclaimer from running while live. Reclaimer may be
    // invoked synchronously with allocator. This guard ensures that this
    // object is not re-entered while inside one of the tracker's methods.
    struct reclaiming_lock
    {
        impl &_ref;
        reclaiming_lock(impl &ref) noexcept : _ref(ref) { _ref.disable_reclaim(); }
        ~reclaiming_lock() { _ref.enable_reclaim(); }
    };
    friend class tracker_reclaimer_lock;
public:
    ;
    ;
    void disable_reclaim() noexcept { ++_reclaiming_disabled_depth; }
    void enable_reclaim() noexcept { --_reclaiming_disabled_depth; }
    logalloc::segment_pool &segment_pool() { return *_segment_pool; }
    void register_region(region::impl *);
    void unregister_region(region::impl *) noexcept;
    size_t reclaim(size_t bytes, is_preemptible p); // Compacts one segment at a time from sparsest segment to least sparse until work_waiting_on_reactor returns true
    // or there are no more segments to compact.
    idle_cpu_handler_result compact_on_idle(work_waiting_on_reactor check_for_work); // Releases whole segments back to the segment pool.
    // After the call, if there is enough evictable memory, the amount of free segments in the pool
    // will be at least reserve_segments + div_ceil(bytes, segment::size).
    // Returns the amount by which segment_pool.total_memory_in_use() has decreased.
    size_t compact_and_evict(size_t reserve_segments, size_t bytes, is_preemptible p);
    void full_compaction();
    
    
    occupancy_stats region_occupancy() const noexcept;
    
     // Set the minimum number of segments reclaimed during single reclamation cycle.
    void set_reclamation_step(size_t step_in_segments) noexcept ;
    size_t reclamation_step() const noexcept { return _reclamation_step; } // Abort on allocation failure from LSA
    
    
     // const bool&, so interested parties can save a reference and see updates.
    const bool &sanitizer_report_backtrace() const;
    
    bool try_set_active_timer(reclaim_timer &timer)
    {
        if (_active_timer)
        {
        return false;
        }
        _active_timer = &timer;
        return true;
    }
    bool try_reset_active_timer(reclaim_timer &timer)
    ;
private:                                                                                            // Like compact_and_evict() but assumes that reclaim_lock is held around the operation.
    size_t compact_and_evict_locked(size_t reserve_segments, size_t bytes, is_preemptible preempt); // Like reclaim() but assumes that reclaim_lock is held around the operation.
    size_t reclaim_locked(size_t bytes, is_preemptible p);
};
tracker_reclaimer_lock::tracker_reclaimer_lock(tracker::impl &impl) noexcept : _tracker_impl(impl) { _tracker_impl.disable_reclaim(); }
tracker_reclaimer_lock::~tracker_reclaimer_lock() { _tracker_impl.enable_reclaim(); }
tracker::tracker() : _impl(std::make_unique<impl>()), _reclaimer([this](seastar::memory::reclaimer::request r)
                                                                 { return reclaim(r); },
                                                                 memory::reclaimer_scope::sync) {}
tracker::~tracker() {}
size_t tracker::reclaim(size_t bytes) { return _impl->reclaim(bytes, is_preemptible::no); }
struct alignas(segment_size) segment
{
    static constexpr int size_shift = segment_size_shift;
    static constexpr int size_mask = segment_size | (segment_size - 1);
    using size_type = std::conditional_t<(size_shift < 16), uint16_t, uint32_t>;
    static constexpr size_t size = segment_size;
    uint8_t data[size];
    segment() noexcept {}
    template <typename T = void>
    const T *at(size_t offset) const noexcept { return reinterpret_cast<const T *>(data + offset); }
    template <typename T = void>
    T *at(size_t offset) noexcept { return reinterpret_cast<T *>(data + offset); }
    static void *operator new(size_t size) = delete;
    static void *operator new(size_t, void *ptr) noexcept { return ptr; }
    static void operator delete(void *ptr) = delete;
};
static constexpr size_t max_managed_object_size = segment_size * 0.1;
static constexpr auto max_used_space_ratio_for_compaction = 0.85;
static constexpr size_t max_used_space_for_compaction = segment_size * max_used_space_ratio_for_compaction;
static constexpr size_t min_free_space_for_compaction = segment_size - max_used_space_for_compaction;
struct [[gnu::packed]] non_lsa_object_cookie
{
    uint64_t value = 0xbadcaffe;
};
static_assert(min_free_space_for_compaction >= max_managed_object_size, "Segments which cannot fit max_managed_object_size must not be considered compactible for the sake of forward progress of compaction"); // Since we only compact if there's >= min_free_space_for_compaction of free space,
// we use min_free_space_for_compaction as the histogram's minimum size and put
// everything below that value in the same bucket.
extern constexpr log_heap_options segment_descriptor_hist_options(min_free_space_for_compaction, 3, segment_size);
enum segment_kind : int
{
    regular = 0, // Holds objects allocated with region_impl::alloc_small()
    bufs = 1     // Holds objects allocated with region_impl::alloc_buf()
};
struct segment_descriptor : public log_heap_hook<segment_descriptor_hist_options>
{
    static constexpr segment::size_type free_space_mask = segment::size_mask;
    static constexpr unsigned bits_for_free_space = segment::size_shift + 1;
    static constexpr segment::size_type segment_kind_mask = 1 << bits_for_free_space;
    static constexpr unsigned bits_for_segment_kind = 1;
    static constexpr unsigned shift_for_segment_kind = bits_for_free_space;
    static_assert(sizeof(segment::size_type) * 8 >= bits_for_free_space + bits_for_segment_kind);
    segment::size_type _free_space;
    region::impl *_region;
    segment::size_type free_space() const noexcept { return _free_space & free_space_mask; }
    void set_free_space(segment::size_type free_space) noexcept { _free_space = (_free_space & ~free_space_mask) | free_space; }
    segment_kind kind() const noexcept { return static_cast<segment_kind>((_free_space & segment_kind_mask) >> shift_for_segment_kind); }
    void set_kind(segment_kind kind) noexcept { _free_space = (_free_space & ~segment_kind_mask) | static_cast<segment::size_type>(kind) << shift_for_segment_kind; } // Valid if kind() == segment_kind::bufs.
    //
    // _buf_pointers holds links to lsa_buffer objects (paired with lsa_buffer::_link)
    // of live objects in the segment. The purpose of this is so that segment compaction
    // can update the pointers when it moves the objects.
    // The order of entangled objects in the vector is irrelevant.
    // Also, not all entangled objects may be engaged.
    std::vector<entangled> _buf_pointers;
    bool is_empty() const noexcept ;
    occupancy_stats occupancy() const noexcept { return {free_space(), segment::size}; }
    void record_alloc(segment::size_type size) noexcept { _free_space -= size; }
    void record_free(segment::size_type size) noexcept ;
};
using segment_descriptor_hist = log_heap<segment_descriptor, segment_descriptor_hist_options>;
class segment_store_backend
{
protected:
    memory::memory_layout _layout;                             // Whether freeing segments actually increases availability of non-lsa memory.
    bool _freed_segment_increases_general_memory_availability; // Aligned (to segment::size) address of the first segment.
    uintptr_t _segments_base;
public:
    explicit segment_store_backend(memory::memory_layout layout, bool freed_segment_increases_general_memory_availability) noexcept : _layout(layout), _freed_segment_increases_general_memory_availability(freed_segment_increases_general_memory_availability), _segments_base(align_up(_layout.start, static_cast<uintptr_t>(segment::size))) {}
    memory::memory_layout memory_layout() const noexcept { return _layout; }
    uintptr_t segments_base() const noexcept { return _segments_base; }
    virtual void *alloc_segment_memory() noexcept = 0;
    virtual void free_segment_memory(void *seg) noexcept = 0;
    virtual size_t free_memory() const noexcept = 0;
    bool can_allocate_more_segments(size_t non_lsa_reserve) const noexcept
    {
        if (_freed_segment_increases_general_memory_availability)
        {
        return free_memory() >= non_lsa_reserve + segment::size;
        }
        else
        {
        return free_memory() >= segment::size;
        }
    }
}; // Segments are allocated from the seastar allocator.
// The entire memory area of the local shard is used as a segment store, i.e.
// segments are allocated from the same memory area regular objeces are.
class seastar_memory_segment_store_backend : public segment_store_backend
{
public:
    seastar_memory_segment_store_backend() : segment_store_backend(memory::get_memory_layout(), true) {}
    virtual void *alloc_segment_memory() noexcept override ;
    virtual void free_segment_memory(void *seg) noexcept override ;
    virtual size_t free_memory() const noexcept override ;
}; // Segments storage is allocated via `mmap()`.
// This area cannot be shrunk or enlarged, so freeing segments doesn't increase
// memory availability.
class standard_memory_segment_store_backend : public segment_store_backend
{
    struct free_segment
    {
        free_segment *next = nullptr;
    };
private:
    uintptr_t _segments_offset = 0;
    free_segment *_freelist = nullptr;
    size_t _available_segments; // for fast free_memory()
private:
    static memory::memory_layout allocate_memory(size_t segments)
    ;
public:
    
    
    
    
};
static constexpr size_t segment_npos = size_t(-1); // Segments are allocated from a large contiguous memory area.
class contiguous_memory_segment_store
{
    std::unique_ptr<segment_store_backend> _backend;
public:
    size_t non_lsa_reserve = 0;
    
    struct with_standard_memory_backend
    {
    };
    
    const segment *segment_from_idx(size_t idx) const noexcept;
    segment *segment_from_idx(size_t idx) noexcept { return reinterpret_cast<segment *>(_backend->segments_base()) + idx; }
    size_t idx_from_segment(const segment *seg) const noexcept
    {
        const auto seg_uint = reinterpret_cast<uintptr_t>(seg);
        if (seg_uint < _backend->memory_layout().start || seg_uint > _backend->memory_layout().end) [[unlikely]]
        {
        return segment_npos;
        }
        return seg - reinterpret_cast<segment *>(_backend->segments_base());
    }
    std::pair<segment *, size_t> allocate_segment() noexcept
    {
        auto p = _backend->alloc_segment_memory();
        if (!p)
        {
        return {nullptr, 0};
        }
        auto seg = new (p) segment;
        poison(seg, sizeof(segment));
        return {seg, idx_from_segment(seg)};
    }
    void free_segment(segment *seg) noexcept
    {
        seg->~segment();
        _backend->free_segment_memory(seg);
    }
    size_t max_segments() const noexcept ;
    bool can_allocate_more_segments() const noexcept { return _backend->can_allocate_more_segments(non_lsa_reserve); }
};
using segment_store = contiguous_memory_segment_store; // Segment pool implementation for the seastar allocator.
// Stores segment descriptors in a vector which is indexed using most significant
// bits of segment address.
//
// We prefer using high-address segments, and returning low-address segments to the seastar
// allocator in order to segregate lsa and non-lsa memory, to reduce fragmentation.
class segment_pool
{
    logalloc::tracker::impl &_tracker;
    segment_store _store;
    std::vector<segment_descriptor> _segments;
    size_t _segments_in_use{};
    utils::dynamic_bitset _lsa_owned_segments_bitmap; // owned by this
    utils::dynamic_bitset _lsa_free_segments_bitmap;  // owned by this, but not in use
    size_t _free_segments = 0;
    size_t _current_emergency_reserve_goal = 1;
    size_t _emergency_reserve_max = 30;
    bool _allocation_failure_flag = false;
    bool _allocation_enabled = true;
    struct allocation_lock
    {
        segment_pool &_pool;
        bool _prev;
        allocation_lock(segment_pool &p) noexcept : _pool(p), _prev(p._allocation_enabled) { _pool._allocation_enabled = false; }
        ~allocation_lock() { _pool._allocation_enabled = _prev; }
    };
    size_t _non_lsa_memory_in_use = 0; // Invariants - a segment is in one of the following states:
    //   In use by some region
    //     - set in _lsa_owned_segments_bitmap
    //     - clear in _lsa_free_segments_bitmap
    //     - counted in _segments_in_use
    //   Free:
    //     - set in _lsa_owned_segments_bitmap
    //     - set in _lsa_free_segments_bitmap
    //     - counted in _unreserved_free_segments
    //   Non-lsa:
    //     - clear everywhere
private:
    segment *allocate_segment(size_t reserve);
    void deallocate_segment(segment *seg) noexcept;
    friend void *segment::operator new(size_t);
    friend void segment::operator delete(void *);
    segment *allocate_or_fallback_to_reserve();
    segment *segment_from_idx(size_t idx) noexcept { return _store.segment_from_idx(idx); }
    size_t idx_from_segment(const segment *seg) const noexcept { return _store.idx_from_segment(seg); }
    
    bool can_allocate_more_segments() const noexcept { return _allocation_enabled && _store.can_allocate_more_segments(); }
    bool compact_segment(segment *seg);
public:
    
    logalloc::tracker::impl &tracker() { return _tracker; }
    
    
    segment *new_segment(region::impl *r);
    segment_descriptor &descriptor(segment *seg) noexcept
    {
        uintptr_t index = idx_from_segment(seg);
        return _segments[index];
    } // Returns segment containing given object or nullptr.
    segment *containing_segment(const void *obj) noexcept;
    segment *segment_from(const segment_descriptor &desc) noexcept;
    void free_segment(segment *) noexcept;
    void free_segment(segment *, segment_descriptor &) noexcept;
    size_t current_emergency_reserve_goal() const noexcept { return _current_emergency_reserve_goal; }
    void set_emergency_reserve_max(size_t new_size) noexcept ;
    size_t emergency_reserve_max() const noexcept { return _emergency_reserve_max; }
    void set_current_emergency_reserve_goal(size_t goal) noexcept { _current_emergency_reserve_goal = goal; }
    
    
    
    
    
    
    size_t total_memory_in_use() const noexcept { return _non_lsa_memory_in_use + _segments_in_use * segment::size; }
    size_t total_free_memory() const noexcept ;
    struct reservation_goal;
    
    
    size_t reclaim_segments(size_t target, is_preemptible preempt);
    void reclaim_all_free_segments();
private:
    tracker::stats _stats{};
public:
    const tracker::stats &statistics() const noexcept { return _stats; }
    inline void on_segment_compaction(size_t used_size) noexcept;
    inline void on_memory_allocation(size_t size) noexcept;
    inline void on_memory_deallocation(size_t size) noexcept;
    inline void on_memory_eviction(size_t size) noexcept;
    size_t unreserved_free_segments() const noexcept ;
    size_t free_segments() const noexcept { return _free_segments; }
};
struct reclaim_timer
{
    using extra_logger = noncopyable_function<void(log_level)>;
private: // CLOCK_MONOTONIC_COARSE is not quite what we want -- to look for stalls,
    // we want thread time, not wall time. Wall time will give false positives
    // if the process is descheduled.
    // For this reason Seastar uses CLOCK_THREAD_CPUTIME_ID in its stall detector.
    // Unfortunately, CLOCK_THREAD_CPUTIME_ID_COARSE does not exist.
    // It's not an important problem, though.
    using clock = utils::coarse_steady_clock;
    struct stats
    {
        occupancy_stats region_occupancy;
        tracker::stats pool_stats;
        
        
    };
    clock::duration _duration_threshold;
    const char *_name;
    const is_preemptible _preemptible;
    const size_t _memory_to_release;
    const size_t _segments_to_release;
    const size_t _reserve_goal, _reserve_max;
    tracker::impl &_tracker;
    segment_pool &_segment_pool;
    extra_logger _extra_logs;
    const bool _debug_enabled;
    bool _stall_detected = false;
    size_t _memory_released = 0;
    clock::time_point _start;
    stats _start_stats, _end_stats, _stat_diff;
    clock::duration _duration;
    inline reclaim_timer(const char *name, is_preemptible preemptible, size_t memory_to_release, size_t segments_to_release, tracker::impl &tracker, segment_pool &segment_pool, extra_logger extra_logs);
public:
    inline reclaim_timer(
        const char *name, is_preemptible preemptible, size_t memory_to_release, size_t segments_to_release, tracker::impl &tracker, extra_logger extra_logs = [](log_level) {}) : reclaim_timer(name, preemptible, memory_to_release, segments_to_release, tracker, tracker.segment_pool(), std::move(extra_logs))
    {
    }
    inline reclaim_timer(
        const char *name, is_preemptible preemptible, size_t memory_to_release, size_t segments_to_release, segment_pool &segment_pool, extra_logger extra_logs = [](log_level) {}) : reclaim_timer(name, preemptible, memory_to_release, segments_to_release, segment_pool.tracker(), segment_pool, std::move(extra_logs)){};
    size_t set_memory_released(size_t memory_released) noexcept { return this->_memory_released = memory_released; }
private:
    void sample_stats(stats &data);
    
    ;
    ;
    template <typename T>
    void log_if_any_mem(log_level level, const char *name, T value) const noexcept;
    ;
};
size_t segment_pool::reclaim_segments(size_t target, is_preemptible preempt)
{ // Reclaimer tries to release segments occupying lower parts of the address
    // space.
    llogger.debug("Trying to reclaim {} segments", target); // Reclamation. Migrate segments to higher addresses and shrink segment pool.
    size_t reclaimed_segments = 0;
    reclaim_timer timing_guard("reclaim_segments", preempt, target * segment::size, target, *this, [&](log_level level)
                               { timing_logger.log(level, "- reclaimed {} out of requested {} segments", reclaimed_segments, target); }); // We may fail to reclaim because a region has reclaim disabled (usually because
    // it is in an allocating_section. Failed reclaims can cause high CPU usage
    // if all of the lower addresses happen to be in a reclaim-disabled region (this
    // is somewhat mitigated by the fact that checking for reclaim disabled is very
    // cheap), but worse, failing a segment reclaim can lead to reclaimed memory
    // being fragmented.  This results in the original allocation continuing to fail.
    //
    // To combat that, we limit the number of failed reclaims. If we reach the limit,
    // we fail the reclaim.  The surrounding allocating_section will release the
    // reclaim_lock, and increase reserves, which will result in reclaim being
    // retried with all regions being reclaimable, and succeed in allocating
    // contiguous memory.
    size_t failed_reclaims_allowance = 10;
    for (size_t src_idx = _lsa_owned_segments_bitmap.find_first_set(); reclaimed_segments != target && src_idx != utils::dynamic_bitset::npos && _free_segments > _current_emergency_reserve_goal; src_idx = _lsa_owned_segments_bitmap.find_next_set(src_idx))
    {
        auto src = segment_from_idx(src_idx);
        if (!_lsa_free_segments_bitmap.test(src_idx))
        {
        if (!compact_segment(src))
        {
            if (--failed_reclaims_allowance == 0)
            {
                break;
            }
            continue;
        }
        }
        _lsa_free_segments_bitmap.clear(src_idx);
        _lsa_owned_segments_bitmap.clear(src_idx);
        _store.free_segment(src);
        ++reclaimed_segments;
        --_free_segments;
        if (preempt && need_preempt())
        {
        break;
        }
    }
    llogger.debug("Reclaimed {} segments (requested {})", reclaimed_segments, target);
    timing_guard.set_memory_released(reclaimed_segments * segment::size);
    return reclaimed_segments;
}
segment *segment_pool::allocate_segment(size_t reserve)
{ //
    // When allocating a segment we want to avoid:
    //  - LSA and general-purpose allocator shouldn't constantly fight each
    //    other for every last bit of memory
    //
    // allocate_segment() always works with LSA reclaimer disabled.
    // 1. Firstly, the algorithm tries to allocate an lsa-owned but free segment
    // 2. If no free segmented is available, a new segment is allocated from the
    //    system allocator. However, if the free memory is below set threshold
    //    this step is skipped.
    // 3. Finally, the algorithm ties to compact and evict data stored in LSA
    //    memory in order to reclaim enough segments.
    //
    do
    {
        tracker_reclaimer_lock rl(_tracker);
        if (_free_segments > reserve)
        {
        auto free_idx = _lsa_free_segments_bitmap.find_last_set();
        _lsa_free_segments_bitmap.clear(free_idx);
        auto seg = segment_from_idx(free_idx);
        --_free_segments;
        return seg;
        }
        if (can_allocate_more_segments())
        {
        memory::disable_abort_on_alloc_failure_temporarily dfg;
        auto [seg, idx] = _store.allocate_segment();
        if (!seg)
        {
            continue;
        }
        _lsa_owned_segments_bitmap.set(idx);
        return seg;
        }
    } while (_tracker.compact_and_evict(reserve, _tracker.reclamation_step() * segment::size, is_preemptible::no));
    return nullptr;
}
void segment_pool::deallocate_segment(segment *seg) noexcept
{
    assert(_lsa_owned_segments_bitmap.test(idx_from_segment(seg)));
    _lsa_free_segments_bitmap.set(idx_from_segment(seg));
    _free_segments++;
}
segment *segment_pool::segment_from(const segment_descriptor &desc) noexcept
{
    assert(desc._region);
    auto index = &desc - &_segments[0];
    return segment_from_idx(index);
}
segment *segment_pool::allocate_or_fallback_to_reserve()
{
    auto seg = allocate_segment(_current_emergency_reserve_goal);
    if (!seg)
    {
        _allocation_failure_flag = true;
        throw std::bad_alloc();
    }
    return seg;
}
segment *segment_pool::new_segment(region::impl *r)
{
    auto seg = allocate_or_fallback_to_reserve();
    ++_segments_in_use;
    segment_descriptor &desc = descriptor(seg);
    desc.set_free_space(segment::size);
    desc.set_kind(segment_kind::regular);
    desc._region = r;
    return seg;
}
void segment_pool::free_segment(segment *seg, segment_descriptor &desc) noexcept
{
    llogger.trace("Releasing segment {}", fmt::ptr(seg));
    desc._region = nullptr;
    deallocate_segment(seg);
    --_segments_in_use;
}
inline void segment_pool::on_segment_compaction(size_t used_size) noexcept
{
    _stats.segments_compacted++;
    _stats.memory_compacted += used_size;
}
inline void segment_pool::on_memory_eviction(size_t size) noexcept { _stats.memory_evicted += size; } // RAII wrapper to maintain segment_pool::current_emergency_reserve_goal()
class segment_pool::reservation_goal
{
    segment_pool &_sp;
    size_t _old_goal;
public:
    reservation_goal(segment_pool &sp, size_t goal) noexcept : _sp(sp), _old_goal(_sp.current_emergency_reserve_goal()) { _sp.set_current_emergency_reserve_goal(goal); }
};
reclaim_timer::reclaim_timer(const char *name, is_preemptible preemptible, size_t memory_to_release, size_t segments_to_release, tracker::impl &tracker, segment_pool &segment_pool, extra_logger extra_logs) : _duration_threshold( // We only report reclaim stalls when their measured duration is
                                                                                                                                                                                                                    // bigger than the threshold by at least one measurement error
                                                                                                                                                                                                                    // (clock resolution). This prevents false positives.
                                                                                                                                                                                                                    //
                                                                                                                                                                                                                    // Explanation for the 10us: The clock value is not always an
                                                                                                                                                                                                                    // integral multiply of its resolution. In the case of coarse
                                                                                                                                                                                                                    // clocks, resolution only describes the frequency of syncs with
                                                                                                                                                                                                                    // the hardware clock -- no effort is made to round the values to
                                                                                                                                                                                                                    // resolution. Therefore, tick durations vary slightly in both
                                                                                                                                                                                                                    // directions. We subtract something slightly bigger than these
                                                                                                                                                                                                                    // variations, to accomodate blocked-reactor-notify-ms values which
                                                                                                                                                                                                                    // are multiplies of resolution.
                                                                                                                                                                                                                    // E.g. with kernel CONFIG_HZ=250, coarse clock resolution is 4ms.
                                                                                                                                                                                                                    // If also we also have blocked-reactor-notify-ms=4, then we would
                                                                                                                                                                                                                    // like to report two-tick stalls, since they have durations of
                                                                                                                                                                                                                    // 4ms-8ms. But two-tick durations can be just slightly smaller
                                                                                                                                                                                                                    // than 8ms (e.g. 7999us) due to the inaccuracy. So we set the
                                                                                                                                                                                                                    // threshold not to (blocked_reactor_notify_ms + resolution) = 8000us,
                                                                                                                                                                                                                    // but to (blocked_reactor_notify_ms + resolution - 10us) = 7990us,
                                                                                                                                                                                                                    // to account for this.
                                                                                                                                                                                                                    engine().get_blocked_reactor_notify_ms() + std::max(0ns, clock::get_resolution() - 10us)),
                                                                                                                                                                                                                _name(name), _preemptible(preemptible), _memory_to_release(memory_to_release), _segments_to_release(segments_to_release), _reserve_goal(segment_pool.current_emergency_reserve_goal()), _reserve_max(segment_pool.emergency_reserve_max()), _tracker(tracker), _segment_pool(segment_pool), _extra_logs(std::move(extra_logs)), _debug_enabled(timing_logger.is_enabled(logging::log_level::debug))
{
    if (!_tracker.try_set_active_timer(*this))
    {
        return;
    }
    _start = clock::now();
    sample_stats(_start_stats);
}
void reclaim_timer::sample_stats(stats &data)
{
    if (_debug_enabled)
    {
        data.region_occupancy = _tracker.region_occupancy();
    }
    data.pool_stats = _segment_pool.statistics();
}
region_listener::~region_listener() = default; //
// For interface documentation see logalloc::region and allocation_strategy.
//
// Allocation dynamics.
//
// Objects are allocated inside fixed-size segments. Objects don't cross
// segment boundary. Active allocations are served from a single segment using
// bump-the-pointer method. That segment is called the active segment. When
// active segment fills up, it is closed. Closed segments are kept in a heap
// which orders them by occupancy. As objects are freed, the segment become
// sparser and are eventually released. Objects which are too large are
// allocated using standard allocator.
//
// Segment layout.
//
// Objects in a segment are laid out sequentially. Each object is preceded by
// a descriptor (see object_descriptor). Object alignment is respected, so if
// there is a gap between the end of current object and the next object's
// descriptor, a trunk of the object descriptor is left right after the
// current object with the flags byte indicating the amount of padding.
//
// Per-segment metadata is kept in a separate array, managed by segment_pool
// object.
//
class region_impl final : public basic_region_impl
{ // Serialized object descriptor format:
    //  byte0 byte1 ... byte[n-1]
    //  bit0-bit5: ULEB64 significand
    //  bit6: 1 iff first byte
    //  bit7: 1 iff last byte
    // This format allows decoding both forwards and backwards (by scanning for bit7/bit6 respectively);
    // backward decoding is needed to recover the descriptor from the object pointer when freeing.
    //
    // Significand interpretation (value = n):
    //     even:  dead object, size n/2 (including descriptor)
    //     odd:   migrate_fn_type at index n/2, from static_migrators()
    class object_descriptor
    {
    private:
        uint32_t _n;
    private:
        explicit object_descriptor(uint32_t n) noexcept : _n(n) {}
    public:
        object_descriptor(allocation_strategy::migrate_fn migrator) noexcept : _n(migrator->index() * 2 + 1) {}
        static object_descriptor make_dead(size_t size) noexcept { return object_descriptor(size * 2); }
        allocation_strategy::migrate_fn migrator() const noexcept { return static_migrators()[_n / 2]; }
        uint8_t alignment() const noexcept { return migrator()->align(); }                             // excluding descriptor
        segment::size_type live_size(const void *obj) const noexcept { return migrator()->size(obj); } // including descriptor
        segment::size_type dead_size() const noexcept { return _n / 2; }
        bool is_live() const noexcept { return (_n & 1) == 1; }
        segment::size_type encoded_size() const noexcept
        {
        return utils::uleb64_encoded_size(_n); // 0 is illegal
        }
        void encode(char *&pos) const noexcept { utils::uleb64_encode(pos, _n, poison<char>, unpoison); } // non-canonical encoding to allow padding (for alignment); encoded_size must be
        // sufficient (greater than this->encoded_size()), _n must be the migrator's
        // index() (i.e. -- suitable for express encoding)
        void encode(char *&pos, size_t encoded_size, size_t size) const noexcept { utils::uleb64_express_encode(pos, _n, encoded_size, size, poison<char>, unpoison); }
        static object_descriptor decode_forwards(const char *&pos) noexcept { return object_descriptor(utils::uleb64_decode_forwards(pos, poison<char>, unpoison)); }
        static object_descriptor decode_backwards(const char *&pos) noexcept ;
    };
private: // lsa_buffer allocator
    segment *_buf_active = nullptr;
    size_t _buf_active_offset;
    static constexpr size_t buf_align = 4096; // All lsa_buffer:s will have addresses aligned to this value.
    // Emergency storage to ensure forward progress during segment compaction,
    // by ensuring that _buf_pointers allocation inside new_buf_active() does not fail.
    std::vector<entangled> _buf_ptrs_for_compact_segment;
private:
    region *_region = nullptr;
    region_listener *_listener = nullptr;
    segment *_active = nullptr;
    size_t _active_offset;
    segment_descriptor_hist _segment_descs; // Contains only closed segments
    occupancy_stats _closed_occupancy;
    occupancy_stats _non_lsa_occupancy; // This helps us updating out region_listener*. That's because we call update before
    // we have a chance to update the occupancy stats - mainly because at this point we don't know
    // what will we do with the new segment. Also, because we are not ever interested in the
    // fraction used, we'll keep it as a scalar and convert when we need to present it as an
    // occupancy. We could actually just present this as a scalar as well and never use occupancies,
    // but consistency is good.
    size_t _evictable_space = 0; // This is a mask applied to _evictable_space with bitwise-and before it's returned from evictable_space().
    // Used for forcing the result to zero without using conditionals.
    size_t _evictable_space_mask = std::numeric_limits<size_t>::max();
    bool _evictable = false;
    region_sanitizer _sanitizer;
    uint64_t _id;
    eviction_fn _eviction_fn;
private:
    struct compaction_lock
    {
        region_impl &_region;
        bool _prev;
        compaction_lock(region_impl &r) noexcept : _region(r), _prev(r._reclaiming_enabled) { _region._reclaiming_enabled = false; }
        ~compaction_lock() { _region._reclaiming_enabled = _prev; }
    };
    void *alloc_small(const object_descriptor &desc, segment::size_type size, size_t alignment)
    {
        if (!_active)
        {
        _active = new_segment();
        _active_offset = 0;
        }
        auto desc_encoded_size = desc.encoded_size();
        size_t obj_offset = align_up_for_asan(align_up(_active_offset + desc_encoded_size, alignment));
        if (obj_offset + size > segment::size)
        {
        close_and_open();
        return alloc_small(desc, size, alignment);
        }
        auto old_active_offset = _active_offset;
        auto pos = _active->at<char>(_active_offset); // Use non-canonical encoding to allow for alignment pad
        desc.encode(pos, obj_offset - _active_offset, size);
        unpoison(pos, size);
        _active_offset = obj_offset + size; // Align the end of the value so that the next descriptor is aligned
        _active_offset = align_up_for_asan(_active_offset);
        segment_pool().descriptor(_active).record_alloc(_active_offset - old_active_offset);
        return pos;
    }
    template <typename Func>
        requires std::is_invocable_r_v<void, Func, const object_descriptor *, void *, size_t>
    void for_each_live(segment *seg, Func &&func)
    { // scylla-gdb.py:scylla_lsa_segment is coupled with this implementation.
        auto pos = align_up_for_asan(seg->at<const char>(0));
        while (pos < seg->at<const char>(segment::size))
        {
        auto old_pos = pos;
        const auto desc = object_descriptor::decode_forwards(pos);
        if (desc.is_live())
        {
            auto size = desc.live_size(pos);
            func(&desc, const_cast<char *>(pos), size);
            pos += size;
        }
        else
        {
            pos = old_pos + desc.dead_size();
        }
        pos = align_up_for_asan(pos);
        }
    }
    void close_active()
    {
        if (!_active)
        {
        return;
        }
        if (_active_offset < segment::size)
        {
        auto desc = object_descriptor::make_dead(segment::size - _active_offset);
        auto pos = _active->at<char>(_active_offset);
        desc.encode(pos);
        }
        auto &desc = segment_pool().descriptor(_active);
        llogger.trace("Closing segment {}, used={}, waste={} [B]", fmt::ptr(_active), desc.occupancy(), segment::size - _active_offset);
        _closed_occupancy += desc.occupancy();
        _segment_descs.push(desc);
        _active = nullptr;
    }
    void close_buf_active()
    {
        if (!_buf_active)
        {
        return;
        }
        auto &desc = segment_pool().descriptor(_buf_active);
        llogger.trace("Closing buf segment {}, used={}, waste={} [B]", fmt::ptr(_buf_active), desc.occupancy(), segment::size - _buf_active_offset);
        _closed_occupancy += desc.occupancy();
        _segment_descs.push(desc);
        _buf_active = nullptr;
    }
    void free_segment(segment_descriptor &desc) noexcept { free_segment(segment_pool().segment_from(desc), desc); }
    void free_segment(segment *seg) noexcept { free_segment(seg, segment_pool().descriptor(seg)); }
    void free_segment(segment *seg, segment_descriptor &desc) noexcept
    {
        segment_pool().free_segment(seg, desc);
        if (_listener)
        {
        _evictable_space -= segment_size;
        _listener->decrease_usage(_region, -segment::size);
        }
    }
    segment *new_segment()
    {
        segment *seg = segment_pool().new_segment(this);
        if (_listener)
        {
        _evictable_space += segment_size;
        _listener->increase_usage(_region, segment::size);
        }
        return seg;
    }
    lsa_buffer alloc_buf(size_t buf_size)
    { // Note: Can be re-entered from allocation sites below due to memory reclamation which
        // invokes segment compaction.
        static_assert(segment::size % buf_align == 0);
        if (buf_size > segment::size)
        {
        throw_with_backtrace<std::runtime_error>(format("Buffer size {} too large", buf_size));
        }
        if (_buf_active_offset + buf_size > segment::size)
        {
        close_buf_active();
        }
        if (!_buf_active)
        {
        new_buf_active();
        }
        lsa_buffer ptr;
        ptr._buf = _buf_active->at<char>(_buf_active_offset);
        ptr._size = buf_size;
        unpoison(ptr._buf, buf_size);
        segment_descriptor &desc = segment_pool().descriptor(_buf_active);
        ptr._desc = &desc;
        desc._buf_pointers.emplace_back(entangled::make_paired_with(ptr._link));
        auto alloc_size = align_up(buf_size, buf_align);
        desc.record_alloc(alloc_size);
        _buf_active_offset += alloc_size;
        return ptr;
    }
    void free_buf(lsa_buffer &buf) noexcept
    {
        segment_descriptor &desc = *buf._desc;
        segment *seg = segment_pool().segment_from(desc);
        if (seg != _buf_active)
        {
        _closed_occupancy -= desc.occupancy();
        }
        auto alloc_size = align_up(buf._size, buf_align);
        desc.record_free(alloc_size);
        poison(buf._buf, buf._size); // Pack links so that segment compaction only has to walk live objects.
        // This procedure also ensures that the link for buf is destroyed, either
        // by replacing it with the last entangled, or by popping it from the back
        // if it is the last element.
        // Moving entangled links around is fine so we can move last_link.
        entangled &last_link = desc._buf_pointers.back();
        entangled &buf_link = *buf._link.get();
        std::swap(last_link, buf_link);
        desc._buf_pointers.pop_back();
        if (seg != _buf_active)
        {
        if (desc.is_empty())
        {
            assert(desc._buf_pointers.empty());
            _segment_descs.erase(desc);
            desc._buf_pointers = std::vector<entangled>();
            free_segment(seg, desc);
        }
        else
        {
            _segment_descs.adjust_up(desc);
            _closed_occupancy += desc.occupancy();
        }
        }
    }
    void compact_segment_locked(segment *seg, segment_descriptor &desc) noexcept
    {
        auto seg_occupancy = desc.occupancy();
        llogger.debug("Compacting segment {} from region {}, {}", fmt::ptr(seg), id(), seg_occupancy);
        ++_invalidate_counter;
        if (desc.kind() == segment_kind::bufs)
        { // This will free the storage of _buf_ptrs_for_compact_segment
        // making sure that alloc_buf() makes progress.
        // Also, empties desc._buf_pointers, making it back a generic segment, which
        // we need to do before freeing it.
        _buf_ptrs_for_compact_segment = std::move(desc._buf_pointers);
        for (entangled &e : _buf_ptrs_for_compact_segment)
        {
            if (e)
            {
                lsa_buffer *old_ptr = e.get(&lsa_buffer::_link);
                assert(&desc == old_ptr->_desc);
                lsa_buffer dst = alloc_buf(old_ptr->_size);
                memcpy(dst._buf, old_ptr->_buf, dst._size);
                old_ptr->_link = std::move(dst._link);
                old_ptr->_buf = dst._buf;
                old_ptr->_desc = dst._desc;
            }
        }
        }
        else
        {
        for_each_live(seg, [this](const object_descriptor *desc, void *obj, size_t size)
                      {                 auto dst = alloc_small(*desc, size, desc->alignment());                 _sanitizer.on_migrate(obj, size, dst);                 desc->migrator()->migrate(obj, dst, size); });
        }
        free_segment(seg, desc);
        segment_pool().on_segment_compaction(seg_occupancy.used_space());
    }
    void close_and_open()
    {
        segment *new_active = new_segment();
        close_active();
        _active = new_active;
        _active_offset = 0;
    }
    void new_buf_active()
    {
        std::vector<entangled> ptrs;
        ptrs.reserve(segment::size / buf_align);
        segment *new_active = new_segment();
        if (_buf_active) [[unlikely]]
        { // Memory allocation above could allocate active buffer during segment compaction.
        close_buf_active();
        }
        assert((uintptr_t)new_active->at(0) % buf_align == 0);
        segment_descriptor &desc = segment_pool().descriptor(new_active);
        desc._buf_pointers = std::move(ptrs);
        desc.set_kind(segment_kind::bufs);
        _buf_active = new_active;
        _buf_active_offset = 0;
    }
    static uint64_t next_id() noexcept
    {
        static std::atomic<uint64_t> id{0};
        return id.fetch_add(1);
    }
    struct unlisten_temporarily
    {
        region_impl *impl;
        region_listener *listener;
        explicit unlisten_temporarily(region_impl *impl) : impl(impl), listener(impl->_listener)
        {
        if (listener)
        {
            listener->del(impl->_region);
        }
        }
        ~unlisten_temporarily()
        {
        if (listener)
        {
            listener->add(impl->_region);
        }
        }
    };
public:
    explicit region_impl(tracker &tracker, region *region) : basic_region_impl(tracker), _region(region), _sanitizer(tracker.get_impl().sanitizer_report_backtrace()), _id(next_id())
    {
        _buf_ptrs_for_compact_segment.reserve(segment::size / buf_align);
        _preferred_max_contiguous_allocation = max_managed_object_size;
        tracker_instance._impl->register_region(this);
    }
    logalloc::segment_pool &segment_pool() const { return _tracker.get_impl().segment_pool(); }
     // Note: allocation is disallowed in this path
    // since we don't instantiate reclaiming_lock
    // while traversing _regions
    occupancy_stats occupancy() const noexcept
    {
        occupancy_stats total = _non_lsa_occupancy;
        total += _closed_occupancy;
        if (_active)
        {
        total += segment_pool().descriptor(_active).occupancy();
        }
        if (_buf_active)
        {
        total += segment_pool().descriptor(_buf_active).occupancy();
        }
        return total;
    }
    occupancy_stats compactible_occupancy() const noexcept { return _closed_occupancy; }
     //
    // Returns true if this region can be compacted and compact() will make forward progress,
    // so that this will eventually stop:
    //
    //    while (is_compactible()) { compact(); }
    //
    bool is_compactible() const noexcept { return _reclaiming_enabled // We require 2 segments per allocation segregation group to ensure forward progress during compaction.
                                                  // There are currently two fixed groups, one for the allocation_strategy implementation and one for lsa_buffer:s.
                                                  && (_closed_occupancy.free_space() >= 4 * segment::size) && _segment_descs.contains_above_min(); }
private:
public:
     // Merges another region into this region. The other region is made
    // to refer to this region.
    // Doesn't invalidate references to allocated objects.
     // Returns occupancy of the sparsest compactible segment.
    occupancy_stats min_occupancy() const noexcept
    {
        if (_segment_descs.empty())
        {
        return {};
        }
        return _segment_descs.one_of_largest().occupancy();
    } // Compacts a single segment, most appropriate for it
    void compact() noexcept
    {
        compaction_lock _(*this);
        auto &desc = _segment_descs.one_of_largest();
        _segment_descs.pop_one_of_largest();
        _closed_occupancy -= desc.occupancy();
        segment *seg = segment_pool().segment_from(desc);
        compact_segment_locked(seg, desc);
    } // Compacts everything. Mainly for testing.
    // Invalidates references to allocated objects.
    void compact_segment(segment *seg, segment_descriptor &desc)
    {
        compaction_lock _(*this);
        if (_active == seg)
        {
        close_active();
        }
        else if (_buf_active == seg)
        {
        close_buf_active();
        }
        _segment_descs.erase(desc);
        _closed_occupancy -= desc.occupancy();
        compact_segment_locked(seg, desc);
    }
    uint64_t id() const noexcept { return _id; } // Returns true if this pool is evictable, so that evict_some() can be called.
    bool is_evictable() const noexcept { return _evictable && _reclaiming_enabled; }
    memory::reclaiming_result evict_some()
    {
        ++_invalidate_counter;
        auto &pool = segment_pool();
        auto freed = pool.statistics().memory_freed;
        auto ret = _eviction_fn();
        pool.on_memory_eviction(pool.statistics().memory_freed - freed);
        return ret;
    }
    
    
    friend class region;
    friend class lsa_buffer;
    friend class region_evictable_occupancy_ascending_less_comparator;
};
memory::reclaiming_result tracker::reclaim(seastar::memory::reclaimer::request r) { return reclaim(std::max(r.bytes_to_reclaim, _impl->reclamation_step() * segment::size)) ? memory::reclaiming_result::reclaimed_something : memory::reclaiming_result::reclaimed_nothing; }
std::ostream &operator<<(std::ostream &out, const occupancy_stats &stats) { return out << format("{:.2f}%, {:d} / {:d} [B]", stats.used_fraction() * 100, stats.used_space(), stats.total_space()); }
 // Note: allocation is disallowed in this path
// since we don't instantiate reclaiming_lock
// while traversing _regions
occupancy_stats tracker::impl::region_occupancy() const noexcept
{
    occupancy_stats total{};
    for (auto &&r : _regions)
    {
        total += r->occupancy();
    }
    return total;
}
static void reclaim_from_evictable(region::impl &r, size_t target_mem_in_use, is_preemptible preempt)
{
    llogger.debug("reclaim_from_evictable: total_memory_in_use={} target={}", r.segment_pool().total_memory_in_use(), target_mem_in_use); // Before attempting segment compaction, try to evict at least deficit and one segment more so that
    // for workloads in which eviction order matches allocation order we will reclaim full segments
    // without needing to perform expensive compaction.
    auto deficit = r.segment_pool().total_memory_in_use() - target_mem_in_use;
    auto used = r.occupancy().used_space();
    auto used_target = used - std::min(used, deficit + segment::size);
    while (r.segment_pool().total_memory_in_use() > target_mem_in_use)
    {
        used = r.occupancy().used_space();
        if (used > used_target)
        {
        llogger.debug("Evicting {} bytes from region {}, occupancy={} in advance", used - used_target, r.id(), r.occupancy());
        }
        else
        {
        llogger.debug("Evicting from region {}, occupancy={} until it's compactible", r.id(), r.occupancy());
        }
        while (r.occupancy().used_space() > used_target || !r.is_compactible())
        {
        if (r.evict_some() == memory::reclaiming_result::reclaimed_nothing)
        {
            if (r.is_compactible())
            { // Need to make forward progress in case there is nothing to evict.
                break;
            }
            llogger.debug("Unable to evict more, evicted {} bytes", used - r.occupancy().used_space());
            return;
        }
        if (r.segment_pool().total_memory_in_use() <= target_mem_in_use)
        {
            llogger.debug("Target met after evicting {} bytes", used - r.occupancy().used_space());
            return;
        }
        if (preempt && need_preempt())
        {
            llogger.debug("reclaim_from_evictable preempted");
            return;
        }
        } // If there are many compactible segments, we will keep compacting without
        // entering the eviction loop above. So the preemption check there is not
        // sufficient and we also need to check here.
        //
        // Note that a preemptible reclaim_from_evictable may not do any real progress,
        // but it doesn't need to. Preemptible (background) reclaim is an optimization.
        // If the system is overwhelmed, and reclaim_from_evictable keeps getting
        // preempted without doing any useful work, then eventually memory will be
        // exhausted and reclaim will be called synchronously, without preemption.
        if (preempt && need_preempt())
        {
        llogger.debug("reclaim_from_evictable preempted");
        return;
        }
        llogger.debug("Compacting after evicting {} bytes", used - r.occupancy().used_space());
        r.compact();
    }
}
size_t tracker::impl::reclaim(size_t memory_to_release, is_preemptible preempt)
{
    if (_reclaiming_disabled_depth)
    {
        return 0;
    }
    reclaiming_lock rl(*this);
    reclaim_timer timing_guard("reclaim", preempt, memory_to_release, 0, *this);
    return timing_guard.set_memory_released(reclaim_locked(memory_to_release, preempt));
}
size_t tracker::impl::reclaim_locked(size_t memory_to_release, is_preemptible preempt)
{
    llogger.debug("reclaim_locked({}, preempt={})", memory_to_release, int(bool(preempt))); // Reclamation steps:
    // 1. Try to release free segments from segment pool and emergency reserve.
    // 2. Compact used segments and/or evict data.
    constexpr auto max_bytes = std::numeric_limits<size_t>::max() - segment::size;
    auto segments_to_release = align_up(std::min(max_bytes, memory_to_release), segment::size) >> segment::size_shift;
    auto nr_released = _segment_pool->reclaim_segments(segments_to_release, preempt);
    size_t mem_released = nr_released * segment::size;
    if (mem_released >= memory_to_release)
    {
        llogger.debug("reclaim_locked() = {}", memory_to_release);
        return memory_to_release;
    }
    if (preempt && need_preempt())
    {
        llogger.debug("reclaim_locked() = {}", mem_released);
        return mem_released;
    }
    auto compacted = compact_and_evict_locked(_segment_pool->current_emergency_reserve_goal(), memory_to_release - mem_released, preempt);
    if (compacted == 0)
    {
        llogger.debug("reclaim_locked() = {}", mem_released);
        return mem_released;
    } // compact_and_evict_locked() will not return segments to the standard allocator,
    // so do it here:
    nr_released = _segment_pool->reclaim_segments(compacted / segment::size, preempt);
    mem_released += nr_released * segment::size;
    llogger.debug("reclaim_locked() = {}", mem_released);
    return mem_released;
}
size_t tracker::impl::compact_and_evict(size_t reserve_segments, size_t memory_to_release, is_preemptible preempt)
{
    if (_reclaiming_disabled_depth)
    {
        return 0;
    }
    reclaiming_lock rl(*this);
    return compact_and_evict_locked(reserve_segments, memory_to_release, preempt);
}
size_t tracker::impl::compact_and_evict_locked(size_t reserve_segments, size_t memory_to_release, is_preemptible preempt)
{
    llogger.debug("compact_and_evict_locked({}, {}, {})", reserve_segments, memory_to_release, int(bool(preempt))); //
    // Algorithm outline.
    //
    // Regions are kept in a max-heap ordered so that regions with
    // sparser segments are picked first. Non-compactible regions will be
    // picked last. In each iteration we try to release one whole segment from
    // the region which has the sparsest segment. We do it until we released
    // enough segments or there are no more regions we can compact.
    //
    // When compaction is not sufficient to reclaim space, we evict data from
    // evictable regions.
    //
    // This may run synchronously with allocation, so we should not allocate
    // memory, otherwise we may get std::bad_alloc. Currently we only allocate
    // in the logger when debug level is enabled. It's disabled during normal
    // operation. Having it is still valuable during testing and in most cases
    // should work just fine even if allocates.
    size_t mem_released = 0;
    size_t mem_in_use = _segment_pool->total_memory_in_use();
    memory_to_release += (reserve_segments - std::min(reserve_segments, _segment_pool->free_segments())) * segment::size;
    auto target_mem = mem_in_use - std::min(mem_in_use, memory_to_release - mem_released);
    llogger.debug("Compacting, requested {} bytes, {} bytes in use, target is {}", memory_to_release, mem_in_use, target_mem); // Allow dipping into reserves while compacting
    segment_pool::reservation_goal open_emergency_pool(*_segment_pool, 0);
    auto cmp = [](region::impl *c1, region::impl *c2)
    {         if (c1->is_compactible() != c2->is_compactible()) {             return !c1->is_compactible();         }         return c2->min_occupancy() < c1->min_occupancy(); };
    boost::range::make_heap(_regions, cmp);
    if (llogger.is_enabled(logging::log_level::debug))
    {
        llogger.debug("Occupancy of regions:");
        for (region::impl *r : _regions)
        {
        llogger.debug(" - {}: min={}, avg={}", r->id(), r->min_occupancy(), r->compactible_occupancy());
        }
    }
    {
        int regions = 0, evictable_regions = 0;
        reclaim_timer timing_guard("compact", preempt, memory_to_release, reserve_segments, *this, [&](log_level level)
                                   { timing_logger.log(level, "- processed {} regions: reclaimed from {}, compacted {}", regions, evictable_regions, regions - evictable_regions); });
        while (_segment_pool->total_memory_in_use() > target_mem)
        {
        boost::range::pop_heap(_regions, cmp);
        region::impl *r = _regions.back();
        if (!r->is_compactible())
        {
            llogger.trace("Unable to release segments, no compactible pools.");
            break;
        }
        ++regions; // Prefer evicting if average occupancy ratio is above the compaction threshold to avoid
        // overhead of compaction in workloads where allocation order matches eviction order, where
        // we can reclaim memory by eviction only. In some cases the cost of compaction on allocation
        // would be higher than the cost of repopulating the region with evicted items.
        if (r->is_evictable() && r->occupancy().used_space() >= max_used_space_ratio_for_compaction * r->occupancy().total_space())
        {
            reclaim_from_evictable(*r, target_mem, preempt);
            ++evictable_regions;
        }
        else
        {
            r->compact();
        }
        boost::range::push_heap(_regions, cmp);
        if (preempt && need_preempt())
        {
            break;
        }
        }
    }
    auto released_during_compaction = mem_in_use - _segment_pool->total_memory_in_use();
    if (_segment_pool->total_memory_in_use() > target_mem)
    {
        int regions = 0, evictable_regions = 0;
        reclaim_timer timing_guard("evict", preempt, memory_to_release, reserve_segments, *this, [&](log_level level)
                                   { timing_logger.log(level, "- processed {} regions, reclaimed from {}", regions, evictable_regions); });
        llogger.debug("Considering evictable regions."); // FIXME: Fair eviction
        for (region::impl *r : _regions)
        {
        if (preempt && need_preempt())
        {
            break;
        }
        ++regions;
        if (r->is_evictable())
        {
            ++evictable_regions;
            reclaim_from_evictable(*r, target_mem, preempt);
            if (_segment_pool->total_memory_in_use() <= target_mem)
            {
                break;
            }
        }
        }
    }
    mem_released += mem_in_use - _segment_pool->total_memory_in_use();
    llogger.debug("Released {} bytes (wanted {}), {} during compaction", mem_released, memory_to_release, released_during_compaction);
    return mem_released;
}
bool segment_pool::compact_segment(segment *seg)
{
    auto &desc = descriptor(seg);
    if (!desc._region->reclaiming_enabled())
    {
        return false;
    } // Called with emergency reserve, open one for
    // region::alloc_small not to throw if it needs
    // one more segment
    reservation_goal open_emergency_pool(*this, 0);
    allocation_lock no_alloc(*this);
    tracker_reclaimer_lock no_reclaim(_tracker);
    desc._region->compact_segment(seg, desc);
    return true;
}
}
// Orders segments by free space, assuming all segments have the same size.
// This avoids using the occupancy, which entails extra division operations.
template <>
size_t hist_key<logalloc::segment_descriptor>(const logalloc::segment_descriptor &desc) { return desc.free_space(); }
using namespace seastar;
using namespace seastar;
class buffer_data_source_impl : public data_source_impl
{
private:
temporary_buffer<char> _buf;
public:
buffer_data_source_impl(temporary_buffer<char> &&buf) : _buf(std::move(buf)) {}
buffer_data_source_impl(buffer_data_source_impl &&) noexcept = default;
virtual future<temporary_buffer<char>> get() override;
};
input_stream<char> make_buffer_input_stream(temporary_buffer<char> &&buf, seastar::noncopyable_function<size_t()> &&limit_generator)
{
auto res = data_source{std::make_unique<buffer_data_source_impl>(std::move(buf))};
return input_stream<char>{make_limiting_data_source(std::move(res), std::move(limit_generator))};
}
using namespace seastar;
class limiting_data_source_impl final : public data_source_impl
{
data_source _src;
seastar::noncopyable_function<size_t()> _limit_generator;
temporary_buffer<char> _buf;
future<temporary_buffer<char>> do_get();
public:
limiting_data_source_impl(data_source &&src, seastar::noncopyable_function<size_t()> &&limit_generator) : _src(std::move(src)), _limit_generator(std::move(limit_generator)) {}
virtual future<temporary_buffer<char>> get() override;
};
data_source make_limiting_data_source(data_source &&src, seastar::noncopyable_function<size_t()> &&limit_generator);
namespace utils
{
void updateable_value_source_base::for_each_ref(std::function<void(updateable_value_base *ref)> func)
{
    for (auto ref : _refs)
    {
        func(ref);
    }
}
updateable_value_source_base::~updateable_value_source_base()
{
    for (auto ref : _refs)
    {
        ref->_source = nullptr;
    }
}
void updateable_value_source_base::add_ref(updateable_value_base *ref) const { _refs.push_back(ref); }
void updateable_value_source_base::del_ref(updateable_value_base *ref) const { _refs.erase(std::remove(_refs.begin(), _refs.end(), ref), _refs.end()); }
void updateable_value_source_base::update_ref(updateable_value_base *old_ref, updateable_value_base *new_ref) const { std::replace(_refs.begin(), _refs.end(), old_ref, new_ref); }
}
namespace utils
{
static future<> disk_sanity(fs::path path, bool developer_mode);
;
static future<file_lock> touch_and_lock(fs::path path);
void directories::set::add(std::vector<sstring> paths)
{
    for (auto &path : paths)
    {
        add(path);
    }
}
void directories::set::add_sharded(sstring p)
{
    fs::path path(p);
    for (unsigned i = 0; i < smp::count; i++)
    {
        add(path / seastar::to_sstring(i).c_str());
    }
}
directories::directories(bool developer_mode) : _developer_mode(developer_mode) {}
future<> directories::create_and_verify(directories::set dir_set)
{
    return do_with(std::vector<file_lock>(), [this, dir_set = std::move(dir_set)](std::vector<file_lock> &locks)
                   { return parallel_for_each(dir_set.get_paths(), [this, &locks](fs::path path)
                                              { return touch_and_lock(path).then([path = std::move(path), developer_mode = _developer_mode, &locks](file_lock lock)
                                                                                 {                 locks.emplace_back(std::move(lock));                 return disk_sanity(path, developer_mode).then([path = std::move(path)] {                     return directories::verify_owner_and_mode(path).handle_exception([](auto ep) {                         startlog.error("Failed owner and mode verification: {}", ep);                         return make_exception_future<>(ep);                     });                 }); }); })
                         .then([this, &locks]
                               { std::move(locks.begin(), locks.end(), std::back_inserter(_locks)); }); });
}
template <typename... Args>
static inline future<> verification_error(fs::path path, const char *fstr, Args &&...args)
{
    auto emsg = fmt::format(fmt::runtime(fstr), std::forward<Args>(args)...);
    startlog.error("{}: {}", path.string(), emsg);
    return make_exception_future<>(std::runtime_error(emsg));
} // Verify that all files and directories are owned by current uid
// and that files can be read and directories can be read, written, and looked up (execute)
// No other file types may exist.
future<> directories::verify_owner_and_mode(fs::path path)
{
    return file_stat(path.string(), follow_symlink::no).then([path = std::move(path)](stat_data sd) { // Under docker, we run with euid 0 and there is no reasonable way to enforce that the
        // in-container uid will have the same uid as files mounted from outside the container. So
        // just allow euid 0 as a special case. It should survive the file_accessible() checks below.
        // See #4823.
        if (geteuid() != 0 && sd.uid != geteuid())
        {
            return verification_error(std::move(path), "File not owned by current euid: {}. Owner is: {}", geteuid(), sd.uid);
        }
        switch (sd.type)
        {
        case directory_entry_type::regular:
        {
            auto f = file_accessible(path.string(), access_flags::read);
            return f.then([path = std::move(path)](bool can_access)
                          {                 if (!can_access) {                     return verification_error(std::move(path), "File cannot be accessed for read");                 }                 return make_ready_future<>(); });
            break;
        }
        case directory_entry_type::directory:
        {
            auto f = file_accessible(path.string(), access_flags::read | access_flags::write | access_flags::execute);
            return f.then([path = std::move(path)](bool can_access)
                          {                 if (!can_access) {                     return verification_error(std::move(path), "Directory cannot be accessed for read, write, and execute");                 }                 return lister::scan_dir(path, {}, [] (fs::path dir, directory_entry de) {                     return verify_owner_and_mode(dir / de.name);                 }); });
            break;
        }
        default:
            return verification_error(std::move(path), "Must be either a regular file or a directory (type={})", static_cast<int>(sd.type));
        }
    });
};
}
// namespace utils
namespace rjson
{
allocator the_allocator; // chunked_content_stream is a wrapper of a chunked_content which
// presents the Stream concept that the rapidjson library expects as input
// for its parser (https://rapidjson.org/classrapidjson_1_1_stream.html).
// This wrapper owns the chunked_content, so it can free each chunk as
// soon as it's parsed.
class chunked_content_stream
{
private:
    chunked_content _content;
    chunked_content::iterator _current_chunk; // _count only needed for Tell(). 32 bits is enough, we don't allow
    // more than 16 MB requests anyway.
    unsigned _count;
public:
    typedef char Ch;
     // Methods needed by rapidjson's Stream concept (see
    // https://rapidjson.org/classrapidjson_1_1_stream.html):
     // Not used in input streams, but unfortunately we still need to implement
    Ch *PutBegin();
};
template <typename Handler, bool EnableYield, typename Buffer = string_buffer>
struct guarded_yieldable_json_handler : public Handler
{
    size_t _nested_level = 0;
    size_t _max_nested_level;
public:
    using handler_base = Handler;
     // Parse any stream fitting https://rapidjson.org/classrapidjson_1_1_stream.html
    ;
protected:
};
rjson::value from_string(std::string_view view) { return rjson::value(view.data(), view.size(), the_allocator); }
const rjson::value *find(const rjson::value &value, std::string_view name)
{ // Although FindMember() has a variant taking a StringRef, it ignores the
    // given length (see https://github.com/Tencent/rapidjson/issues/1649).
    // Luckily, the variant taking a GenericValue doesn't share this bug,
    // and we can create a string GenericValue without copying the string.
    auto member_it = value.FindMember(rjson::value(name.data(), name.size()));
    return member_it != value.MemberEnd() ? &member_it->value : nullptr;
}
rjson::value *find(rjson::value &value, std::string_view name)
{
    auto member_it = value.FindMember(rjson::value(name.data(), name.size()));
    return member_it != value.MemberEnd() ? &member_it->value : nullptr;
}
bool remove_member(rjson::value &value, std::string_view name)
{ // Although RemoveMember() has a variant taking a StringRef, it ignores
    // given length (see https://github.com/Tencent/rapidjson/issues/1649).
    // Luckily, the variant taking a GenericValue doesn't share this bug,
    // and we can create a string GenericValue without copying the string.
    return value.RemoveMember(rjson::value(name.data(), name.size()));
}
void add_with_string_name(rjson::value &base, std::string_view name, rjson::value &&member) { base.AddMember(rjson::value(name.data(), name.size(), the_allocator), std::move(member), the_allocator); }
void add_with_string_name(rjson::value &base, std::string_view name, rjson::string_ref_type member) { base.AddMember(rjson::value(name.data(), name.size(), the_allocator), rjson::value(member), the_allocator); }
void add(rjson::value &base, rjson::string_ref_type name, rjson::value &&member) { base.AddMember(name, std::move(member), the_allocator); }
void add(rjson::value &base, rjson::string_ref_type name, rjson::string_ref_type member) { base.AddMember(name, rjson::value(member), the_allocator); }
void replace_with_string_name(rjson::value &base, const std::string_view name, rjson::value &&member)
{
    rjson::value *m = rjson::find(base, name);
    if (m)
    {
        *m = std::move(member);
    }
    else
    {
        add_with_string_name(base, name, std::move(member));
    }
}
void push_back(rjson::value &base_array, rjson::value &&item) { base_array.PushBack(std::move(item), the_allocator); }
bool single_value_comp::operator()(const rjson::value &r1, const rjson::value &r2) const
{
    auto r1_type = r1.GetType();
    auto r2_type = r2.GetType(); // null is the smallest type and compares with every other type, nothing is lesser than null
    if (r1_type == rjson::type::kNullType || r2_type == rjson::type::kNullType)
    {
        return r1_type < r2_type;
    } // only null, true, and false are comparable with each other, other types are not compatible
    if (r1_type != r2_type)
    {
        if (r1_type > rjson::type::kTrueType || r2_type > rjson::type::kTrueType)
        {
        throw rjson::error(format("Types are not comparable: {} {}", r1, r2));
        }
    }
    switch (r1_type)
    {
    case rjson::type::kNullType:  // fall-through
    case rjson::type::kFalseType: // fall-through
    case rjson::type::kTrueType:
        return r1_type < r2_type;
    case rjson::type::kObjectType:
        throw rjson::error("Object type comparison is not supported");
    case rjson::type::kArrayType:
        throw rjson::error("Array type comparison is not supported");
    case rjson::type::kStringType:
    {
        const size_t r1_len = r1.GetStringLength();
        const size_t r2_len = r2.GetStringLength();
        size_t len = std::min(r1_len, r2_len);
        int result = std::strncmp(r1.GetString(), r2.GetString(), len);
        return result < 0 || (result == 0 && r1_len < r2_len);
    }
    case rjson::type::kNumberType:
    {
        if (r1.IsInt() && r2.IsInt())
        {
        return r1.GetInt() < r2.GetInt();
        }
        else if (r1.IsUint() && r2.IsUint())
        {
        return r1.GetUint() < r2.GetUint();
        }
        else if (r1.IsInt64() && r2.IsInt64())
        {
        return r1.GetInt64() < r2.GetInt64();
        }
        else if (r1.IsUint64() && r2.IsUint64())
        {
        return r1.GetUint64() < r2.GetUint64();
        }
        else
        { // it's safe to call GetDouble() on any number type
        return r1.GetDouble() < r2.GetDouble();
        }
    }
    default:
        return false;
    }
}
rjson::value from_string_map(const std::map<sstring, sstring> &map)
{
    rjson::value v = rjson::empty_object();
    for (auto &entry : map)
    {
        rjson::add_with_string_name(v, std::string_view(entry.first), rjson::from_string(entry.second));
    }
    return v;
}
static inline bool is_control_char(char c) { return c >= 0 && c <= 0x1F; }
static inline bool needs_escaping(const sstring &s)
{
    return std::any_of(s.begin(), s.end(), [](char c)
                       { return is_control_char(c) || c == '"' || c == '\\'; });
}
sstring quote_json_string(const sstring &value)
{
    if (!needs_escaping(value))
    {
        return format("\"{}\"", value);
    }
    std::ostringstream oss;
    oss << std::hex << std::uppercase << std::setfill('0');
    oss.put('"');
    for (char c : value)
    {
        switch (c)
        {
        case '"':
        oss.put('\\').put('"');
        break;
        case '\\':
        oss.put('\\').put('\\');
        break;
        case '\b':
        oss.put('\\').put('b');
        break;
        case '\f':
        oss.put('\\').put('f');
        break;
        case '\n':
        oss.put('\\').put('n');
        break;
        case '\r':
        oss.put('\\').put('r');
        break;
        case '\t':
        oss.put('\\').put('t');
        break;
        default:
        if (is_control_char(c))
        {
            oss.put('\\').put('u') << std::setw(4) << static_cast<int>(c);
        }
        else
        {
            oss.put(c);
        }
        break;
        }
    }
    oss.put('"');
    return oss.str();
}
}
// end namespace rjson
namespace utils
{
}
// namespace utils
logging::logger klog("keys");
const legacy_compound_view<partition_key_view::c_type> partition_key_view::legacy_form(const schema &s) const { return {*get_compound_type(s), _bytes}; }
int32_t weight(bound_kind k)
{
switch (k)
{
case bound_kind::excl_end:
    return -2;
case bound_kind::incl_start:
    return -1;
case bound_kind::incl_end:
    return 1;
case bound_kind::excl_start:
    return 2;
}
abort();
}
const thread_local clustering_key_prefix bound_view::_empty_prefix = clustering_key::make_empty();
static bool apply_in_place(const column_definition &cdef, atomic_cell_mutable_view dst, atomic_cell_mutable_view src)
{
auto dst_ccmv = counter_cell_mutable_view(dst);
auto src_ccmv = counter_cell_mutable_view(src);
auto dst_shards = dst_ccmv.shards();
auto src_shards = src_ccmv.shards();
auto dst_it = dst_shards.begin();
auto src_it = src_shards.begin();
while (src_it != src_shards.end())
{
    while (dst_it != dst_shards.end() && dst_it->id() < src_it->id())
    {
        ++dst_it;
    }
    if (dst_it == dst_shards.end() || dst_it->id() != src_it->id())
    { // Fast-path failed. Revert and fall back to the slow path.
        if (dst_it == dst_shards.end())
        {
        --dst_it;
        }
        while (src_it != src_shards.begin())
        {
        --src_it;
        while (dst_it->id() != src_it->id())
        {
            --dst_it;
        }
        src_it->swap_value_and_clock(*dst_it);
        }
        return false;
    }
    if (dst_it->logical_clock() < src_it->logical_clock())
    {
        dst_it->swap_value_and_clock(*src_it);
    }
    else
    {
        src_it->set_value_and_clock(*dst_it);
    }
    ++src_it;
}
auto dst_ts = dst_ccmv.timestamp();
auto src_ts = src_ccmv.timestamp();
dst_ccmv.set_timestamp(std::max(dst_ts, src_ts));
src_ccmv.set_timestamp(dst_ts);
return true;
}
void counter_cell_view::apply(const column_definition &cdef, atomic_cell_or_collection &dst, atomic_cell_or_collection &src)
{
auto dst_ac = dst.as_atomic_cell(cdef);
auto src_ac = src.as_atomic_cell(cdef);
if (!dst_ac.is_live() || !src_ac.is_live())
{
    if (dst_ac.is_live() || (!src_ac.is_live() && compare_atomic_cell_for_merge(dst_ac, src_ac) < 0))
    {
        std::swap(dst, src);
    }
    return;
}
if (dst_ac.is_counter_update() && src_ac.is_counter_update())
{
    auto src_v = src_ac.counter_update_value();
    auto dst_v = dst_ac.counter_update_value();
    dst = atomic_cell::make_live_counter_update(std::max(dst_ac.timestamp(), src_ac.timestamp()), src_v + dst_v);
    return;
}
assert(!dst_ac.is_counter_update());
assert(!src_ac.is_counter_update());
auto src_ccv = counter_cell_view(src_ac);
auto dst_ccv = counter_cell_view(dst_ac);
if (dst_ccv.shard_count() >= src_ccv.shard_count())
{
    auto dst_amc = dst.as_mutable_atomic_cell(cdef);
    auto src_amc = src.as_mutable_atomic_cell(cdef);
    if (apply_in_place(cdef, dst_amc, src_amc))
    {
        return;
    }
}
auto dst_shards = dst_ccv.shards();
auto src_shards = src_ccv.shards();
counter_cell_builder result;
combine(dst_shards.begin(), dst_shards.end(), src_shards.begin(), src_shards.end(), result.inserter(), counter_shard_view::less_compare_by_id(), [](auto &x, auto &y)
        { return x.logical_clock() < y.logical_clock() ? y : x; });
auto cell = result.build(std::max(dst_ac.timestamp(), src_ac.timestamp()));
src = std::exchange(dst, atomic_cell_or_collection(std::move(cell)));
}
namespace runtime
{
static std::chrono::steady_clock::time_point boot_time;
}
namespace utils
{
namespace murmur_hash
{
    static inline uint64_t getblock(bytes_view key, uint32_t index)
    {
        uint32_t i_8 = index << 3;
        auto p = reinterpret_cast<const uint8_t *>(key.data() + i_8);
        return uint64_t(p[0]) | (uint64_t(p[1]) << 8) | (uint64_t(p[2]) << 16) | (uint64_t(p[3]) << 24) | (uint64_t(p[4]) << 32) | (uint64_t(p[5]) << 40) | (uint64_t(p[6]) << 48) | (uint64_t(p[7]) << 56);
    }
    void hash3_x64_128(bytes_view key, uint64_t seed, std::array<uint64_t, 2> &result)
    {
        uint32_t length = key.size();
        const uint32_t nblocks = length >> 4; // Process as 128-bit blocks.
        uint64_t h1 = seed;
        uint64_t h2 = seed;
        uint64_t c1 = 0x87c37b91114253d5L;
        uint64_t c2 = 0x4cf5ad432745937fL; //----------
        // body
        for (uint32_t i = 0; i < nblocks; i++)
        {
        uint64_t k1 = getblock(key, i * 2 + 0);
        uint64_t k2 = getblock(key, i * 2 + 1);
        k1 *= c1;
        k1 = rotl64(k1, 31);
        k1 *= c2;
        h1 ^= k1;
        h1 = rotl64(h1, 27);
        h1 += h2;
        h1 = h1 * 5 + 0x52dce729;
        k2 *= c2;
        k2 = rotl64(k2, 33);
        k2 *= c1;
        h2 ^= k2;
        h2 = rotl64(h2, 31);
        h2 += h1;
        h2 = h2 * 5 + 0x38495ab5;
        } //----------
        // tail
        // Advance offset to the unprocessed tail of the data.
        key.remove_prefix(nblocks * 16);
        uint64_t k1 = 0;
        uint64_t k2 = 0;
        switch (length & 15)
        {
        case 15:
        k2 ^= ((uint64_t)key[14]) << 48;
        case 14:
        k2 ^= ((uint64_t)key[13]) << 40;
        case 13:
        k2 ^= ((uint64_t)key[12]) << 32;
        case 12:
        k2 ^= ((uint64_t)key[11]) << 24;
        case 11:
        k2 ^= ((uint64_t)key[10]) << 16;
        case 10:
        k2 ^= ((uint64_t)key[9]) << 8;
        case 9:
        k2 ^= ((uint64_t)key[8]) << 0;
        k2 *= c2;
        k2 = rotl64(k2, 33);
        k2 *= c1;
        h2 ^= k2;
        case 8:
        k1 ^= ((uint64_t)key[7]) << 56;
        case 7:
        k1 ^= ((uint64_t)key[6]) << 48;
        case 6:
        k1 ^= ((uint64_t)key[5]) << 40;
        case 5:
        k1 ^= ((uint64_t)key[4]) << 32;
        case 4:
        k1 ^= ((uint64_t)key[3]) << 24;
        case 3:
        k1 ^= ((uint64_t)key[2]) << 16;
        case 2:
        k1 ^= ((uint64_t)key[1]) << 8;
        case 1:
        k1 ^= ((uint64_t)key[0]);
        k1 *= c1;
        k1 = rotl64(k1, 31);
        k1 *= c2;
        h1 ^= k1;
        }; //----------
        // finalization
        h1 ^= length;
        h2 ^= length;
        h1 += h2;
        h2 += h1;
        h1 = fmix(h1);
        h2 = fmix(h2);
        h1 += h2;
        h2 += h1;
        result[0] = h1;
        result[1] = h2;
    }
} // namespace murmur_hash
}
// namespace utils
namespace utils
{
UUID make_random_uuid() noexcept
{
    static thread_local std::mt19937_64 engine(std::random_device().operator()());
    static thread_local std::uniform_int_distribution<uint64_t> dist;
    uint64_t msb, lsb;
    msb = dist(engine);
    lsb = dist(engine);
    msb &= ~uint64_t(0x0f << 12);
    msb |= 0x4 << 12; // version 4
    lsb &= ~(uint64_t(0x3) << 62);
    lsb |= uint64_t(0x2) << 62; // IETF variant
    return UUID(msb, lsb);
}
}
// Clang or boost have a problem navigating the enable_if maze
// that is cpp_int's constructor. It ends up treating the
// string_view as binary and "0" ends up 48.
// Work around by casting to string.
using string_view_workaround = std::string;
static logging::logger tlogger("types");
bytes_view_opt read_collection_value(bytes_view &in);
void on_types_internal_error(std::exception_ptr ex) { on_internal_error(tlogger, std::move(ex)); }
template <typename T>
    requires requires {         typename T::duration;         requires std::same_as<typename T::duration, std::chrono::milliseconds>; }
sstring time_point_to_string(const T &tp);
static const char *byte_type_name = "org.apache.cassandra.db.marshal.ByteType";
static const char *short_type_name = "org.apache.cassandra.db.marshal.ShortType";
static const char *int32_type_name = "org.apache.cassandra.db.marshal.Int32Type";
static const char *long_type_name = "org.apache.cassandra.db.marshal.LongType";
static const char *ascii_type_name = "org.apache.cassandra.db.marshal.AsciiType";
static const char *utf8_type_name = "org.apache.cassandra.db.marshal.UTF8Type";
static const char *bytes_type_name = "org.apache.cassandra.db.marshal.BytesType";
static const char *boolean_type_name = "org.apache.cassandra.db.marshal.BooleanType";
static const char *timeuuid_type_name = "org.apache.cassandra.db.marshal.TimeUUIDType";
static const char *timestamp_type_name = "org.apache.cassandra.db.marshal.TimestampType";
static const char *date_type_name = "org.apache.cassandra.db.marshal.DateType";
static const char *simple_date_type_name = "org.apache.cassandra.db.marshal.SimpleDateType";
static const char *time_type_name = "org.apache.cassandra.db.marshal.TimeType";
static const char *uuid_type_name = "org.apache.cassandra.db.marshal.UUIDType";
static const char *inet_addr_type_name = "org.apache.cassandra.db.marshal.InetAddressType";
static const char *double_type_name = "org.apache.cassandra.db.marshal.DoubleType";
static const char *float_type_name = "org.apache.cassandra.db.marshal.FloatType";
static const char *counter_type_name = "org.apache.cassandra.db.marshal.CounterColumnType";
static const char *duration_type_name = "org.apache.cassandra.db.marshal.DurationType";
static const char *empty_type_name = "org.apache.cassandra.db.marshal.EmptyType";
template <typename T>
struct simple_type_traits
{
static constexpr size_t serialized_size = sizeof(T);
static T read_nonempty(managed_bytes_view v) { return read_simple_exactly<T>(v); }
};
template <>
struct simple_type_traits<bool>
{
static constexpr size_t serialized_size = 1;
static bool read_nonempty(managed_bytes_view v) { return read_simple_exactly<int8_t>(v) != 0; }
};
template <>
struct simple_type_traits<db_clock::time_point>
{
static constexpr size_t serialized_size = sizeof(uint64_t);
static db_clock::time_point read_nonempty(managed_bytes_view v) { return db_clock::time_point(db_clock::duration(read_simple_exactly<int64_t>(v))); }
};
template <typename T>
simple_type_impl<T>::simple_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed) : concrete_type<T>(k, std::move(name), std::move(value_length_if_fixed)) {}
template <typename T>
integer_type_impl<T>::integer_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed) : simple_type_impl<T>(k, name, std::move(value_length_if_fixed)) {}
;
;
// Note that although byte_type is of a fixed size,
// Cassandra (erroneously) treats it as a variable-size
// so we have to pass disengaged optional for the value size
byte_type_impl::byte_type_impl() : integer_type_impl{kind::byte, byte_type_name, {}}
{
}
short_type_impl::short_type_impl() : integer_type_impl{kind::short_kind, short_type_name, {}}
{
}
int32_type_impl::int32_type_impl() : integer_type_impl{kind::int32, int32_type_name, 4}
{
}
long_type_impl::long_type_impl() : integer_type_impl{kind::long_kind, long_type_name, 8}
{
}
string_type_impl::string_type_impl(kind k, sstring name) : concrete_type(k, name, {}) {}
ascii_type_impl::ascii_type_impl() : string_type_impl(kind::ascii, ascii_type_name) {}
utf8_type_impl::utf8_type_impl() : string_type_impl(kind::utf8, utf8_type_name) {}
bytes_type_impl::bytes_type_impl() : concrete_type(kind::bytes, bytes_type_name, {}) {}
boolean_type_impl::boolean_type_impl() : simple_type_impl<bool>(kind::boolean, boolean_type_name, 1) {}
date_type_impl::date_type_impl() : concrete_type(kind::date, date_type_name, 8) {}
timeuuid_type_impl::timeuuid_type_impl() : concrete_type<utils::UUID>(kind::timeuuid, timeuuid_type_name, 16) {}
timestamp_type_impl::timestamp_type_impl() : simple_type_impl(kind::timestamp, timestamp_type_name, 8) {}
static boost::posix_time::ptime get_time(const boost::smatch &sm)
;


simple_date_type_impl::simple_date_type_impl() : simple_type_impl{kind::simple_date, simple_date_type_name, {}}
{
}


time_type_impl::time_type_impl() : simple_type_impl{kind::time, time_type_name, {}}
{
}
uuid_type_impl::uuid_type_impl() : concrete_type(kind::uuid, uuid_type_name, 16) {}
using inet_address = seastar::net::inet_address;
inet_addr_type_impl::inet_addr_type_impl() : concrete_type<inet_address>(kind::inet, inet_addr_type_name, {}) {}
// Integer of same length of a given type. This is useful because our
// ntoh functions only know how to operate on integers.
template <typename T>
struct int_of_size;
template <typename D, typename I>
struct int_of_size_impl
{
using dtype = D;
using itype = I;
static_assert(sizeof(dtype) == sizeof(itype), "size mismatch");
static_assert(alignof(dtype) == alignof(itype), "align mismatch");
};
template <>
struct int_of_size<double> : public int_of_size_impl<double, uint64_t>
{
};
template <>
struct int_of_size<float> : public int_of_size_impl<float, uint32_t>
{
};
template <typename T>
struct float_type_traits
{
static constexpr size_t serialized_size = sizeof(typename int_of_size<T>::itype);
static double read_nonempty(managed_bytes_view v) { return std::bit_cast<T>(read_simple_exactly<typename int_of_size<T>::itype>(v)); }
};
template <>
struct simple_type_traits<float> : public float_type_traits<float>
{
};
template <>
struct simple_type_traits<double> : public float_type_traits<double>
{
};
template <typename T>
floating_type_impl<T>::floating_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed) : simple_type_impl<T>(k, std::move(name), std::move(value_length_if_fixed)) {}
double_type_impl::double_type_impl() : floating_type_impl{kind::double_kind, double_type_name, 8}
{
}
float_type_impl::float_type_impl() : floating_type_impl{kind::float_kind, float_type_name, 4}
{
}
counter_type_impl::counter_type_impl() : abstract_type{kind::counter, counter_type_name, {}}
{
}
// TODO(jhaberku): Move this to Seastar.
template <size_t... Ts, class Function>
auto generate_tuple_from_index(std::index_sequence<Ts...>, Function &&f)
{ // To ensure that tuple is constructed in the correct order (because the evaluation order of the arguments to
// `std::make_tuple` is unspecified), use braced initialization  (which does define the order). However, we still
// need to figure out the type.
using result_type = decltype(std::make_tuple(f(Ts)...));
return result_type{f(Ts)...};
}
duration_type_impl::duration_type_impl() : concrete_type(kind::duration, duration_type_name, {}) {}
using common_counter_type = cql_duration::common_counter_type;
static std::tuple<common_counter_type, common_counter_type, common_counter_type> deserialize_counters(bytes_view v)
{
auto deserialize_and_advance = [&v](auto &&i)
{         auto len = signed_vint::serialized_size_from_first_byte(v.front());         const auto d = signed_vint::deserialize(v);         v.remove_prefix(len);         if (v.empty() && (i != 2)) {             throw marshal_exception("Cannot deserialize duration");         }         return static_cast<common_counter_type>(d); };
return generate_tuple_from_index(std::make_index_sequence<3>(), std::move(deserialize_and_advance));
}
empty_type_impl::empty_type_impl() : abstract_type(kind::empty, empty_type_name, 0) {}
logging::logger collection_type_impl::_logger("collection_type_impl");
const size_t collection_type_impl::max_elements;
void listlike_collection_type_impl::validate_for_storage(const FragmentedView auto &value) const
{
for (auto val_opt : partially_deserialize_listlike(value))
{
    if (!val_opt)
    {
        throw exceptions::invalid_request_exception("Cannot store NULL in list or set");
    }
}
}
template void listlike_collection_type_impl::validate_for_storage(const managed_bytes_view &value) const;
template void listlike_collection_type_impl::validate_for_storage(const fragmented_temporary_buffer::view &value) const;
static bool is_compatible_with_aux(const collection_type_impl &t, const abstract_type &previous)
;
size_t collection_size_len() { return sizeof(int32_t); }
size_t collection_value_len() { return sizeof(int32_t); }
void write_collection_size(bytes::iterator &out, int size) { serialize_int32(out, size); }
void write_collection_value(bytes::iterator &out, std::optional<bytes_view> val_bytes_opt)
{
if (!val_bytes_opt)
{
    serialize_int32(out, int32_t(-1));
    return;
}
auto &val_bytes = *val_bytes_opt;
serialize_int32(out, int32_t(val_bytes.size()));
out = std::copy_n(val_bytes.begin(), val_bytes.size(), out);
}
// Passing the wrong integer type to a generic serialization function is a particularly
// easy mistake to do, so we want to disable template parameter deduction here.
// Hence std::type_identity.
template <typename T>
void write_simple(bytes_ostream &out, std::type_identity_t<T> val)
{
auto val_be = net::hton(val);
auto val_ptr = reinterpret_cast<const bytes::value_type *>(&val_be);
out.write(bytes_view(val_ptr, sizeof(T)));
}
void write_collection_value(bytes_ostream &out, atomic_cell_value_view val)
{
write_simple<int32_t>(out, int32_t(val.size_bytes()));
for (auto &&frag : fragment_range(val))
{
    out.write(frag);
}
}
void write_fragmented(managed_bytes_mutable_view &out, std::string_view val)
{
while (val.size() > 0)
{
    size_t current_n = std::min(val.size(), out.current_fragment().size());
    memcpy(out.current_fragment().data(), val.data(), current_n);
    val.remove_prefix(current_n);
    out.remove_prefix(current_n);
}
}
template <std::integral T>
void write_simple(managed_bytes_mutable_view &out, std::type_identity_t<T> val)
{
val = net::hton(val);
if (out.current_fragment().size() >= sizeof(T)) [[likely]]
{
    auto p = out.current_fragment().data();
    out.remove_prefix(sizeof(T)); // FIXME use write_unaligned after it's merged.
    write_unaligned<T>(p, val);
}
else if (out.size_bytes() >= sizeof(T))
{
    write_fragmented(out, std::string_view(reinterpret_cast<const char *>(&val), sizeof(T)));
}
else
{
    on_internal_error(tlogger, format("write_simple: attempted write of size {} to buffer of size {}", sizeof(T), out.size_bytes()));
}
}
void write_collection_size(managed_bytes_mutable_view &out, int size) { write_simple<uint32_t>(out, uint32_t(size)); }
void write_collection_value(managed_bytes_mutable_view &out, bytes_view val)
{
write_simple<int32_t>(out, int32_t(val.size()));
write_fragmented(out, single_fragmented_view(val));
}
void write_collection_value(managed_bytes_mutable_view &out, const managed_bytes_view_opt &val_opt)
{
if (!val_opt)
{
    write_simple<int32_t>(out, int32_t(-1));
    return;
}
auto &val = *val_opt;
write_simple<int32_t>(out, int32_t(val.size_bytes()));
write_fragmented(out, val);
}
void write_int32(bytes::iterator &out, int32_t value) { return serialize_int32(out, value); }
shared_ptr<const abstract_type> abstract_type::underlying_type() const
{
struct visitor
{
    shared_ptr<const abstract_type> operator()(const abstract_type &t) { return t.shared_from_this(); }
    shared_ptr<const abstract_type> operator()(const reversed_type_impl &r) { return r.underlying_type(); }
};
return visit(*this, visitor{});
}
bool abstract_type::is_counter() const
{
struct visitor
{
    bool operator()(const reversed_type_impl &r) { return r.underlying_type()->is_counter(); }
    bool operator()(const abstract_type &) { return false; }
    bool operator()(const counter_type_impl &) { return true; }
};
return visit(*this, visitor{});
}
bool abstract_type::is_collection() const
{
struct visitor
{
    bool operator()(const reversed_type_impl &r) { return r.underlying_type()->is_collection(); }
    bool operator()(const abstract_type &) { return false; }
    bool operator()(const collection_type_impl &) { return true; }
};
return visit(*this, visitor{});
}
bool abstract_type::is_tuple() const
{
struct visitor
{
    bool operator()(const abstract_type &) { return false; }
    bool operator()(const reversed_type_impl &t) { return t.underlying_type()->is_tuple(); }
    bool operator()(const tuple_type_impl &) { return true; }
};
return visit(*this, visitor{});
}
bool abstract_type::is_multi_cell() const
{
struct visitor
{
    bool operator()(const abstract_type &) { return false; }
    bool operator()(const reversed_type_impl &t) { return t.underlying_type()->is_multi_cell(); }
    bool operator()(const collection_type_impl &c) { return c.is_multi_cell(); }
    bool operator()(const user_type_impl &u) { return u.is_multi_cell(); }
};
return visit(*this, visitor{});
}
bool abstract_type::is_native() const { return !is_collection() && !is_tuple(); }
bool abstract_type::is_string() const
{
struct visitor
{
    bool operator()(const abstract_type &) { return false; }
    bool operator()(const reversed_type_impl &t) { return t.underlying_type()->is_string(); }
    bool operator()(const string_type_impl &) { return true; }
};
return visit(*this, visitor{});
}
template <typename Predicate>
    requires CanHandleAllTypes<Predicate>
static bool find(const abstract_type &t, const Predicate &f)
{
struct visitor
{
    const Predicate &f;
    bool operator()(const abstract_type &) { return false; }
    bool operator()(const reversed_type_impl &r) { return find(*r.underlying_type(), f); }
    bool operator()(const tuple_type_impl &t)
    {
        return boost::algorithm::any_of(t.all_types(), [&](const data_type &dt)
                                        { return find(*dt, f); });
    }
    bool operator()(const map_type_impl &m) { return find(*m.get_keys_type(), f) || find(*m.get_values_type(), f); }
    bool operator()(const listlike_collection_type_impl &l) { return find(*l.get_elements_type(), f); }
};
return visit(t, [&](const auto &t)
             {         if (f(t)) {             return true;         }         return visitor{f}(t); });
}
bool abstract_type::references_duration() const
{
struct visitor
{
    bool operator()(const abstract_type &) const { return false; }
    bool operator()(const duration_type_impl &) const { return true; }
};
return find(*this, visitor{});
}
bool abstract_type::references_user_type(const sstring &keyspace, const bytes &name) const
{
struct visitor
{
    const sstring &keyspace;
    const bytes &name;
    bool operator()(const abstract_type &) const { return false; }
    bool operator()(const user_type_impl &u) const { return u._keyspace == keyspace && u._name == name; }
};
return find(*this, visitor{keyspace, name});
}
namespace
{
struct is_byte_order_equal_visitor
{
    template <typename T>
    bool operator()(const simple_type_impl<T> &) { return true; }
    bool operator()(const concrete_type<utils::UUID> &) { return true; }
    bool operator()(const abstract_type &) { return false; }
    bool operator()(const reversed_type_impl &t) { return t.underlying_type()->is_byte_order_equal(); }
    bool operator()(const string_type_impl &) { return true; }
    bool operator()(const bytes_type_impl &) { return true; }
    bool operator()(const timestamp_date_base_class &) { return true; }
    bool operator()(const inet_addr_type_impl &) { return true; }
    bool operator()(const duration_type_impl &) { return true; } // FIXME: origin returns false for list.  Why?
    bool operator()(const set_type_impl &s) { return s.get_elements_type()->is_byte_order_equal(); }
};
}
bool abstract_type::is_byte_order_equal() const { return visit(*this, is_byte_order_equal_visitor{}); }
static bool check_compatibility(const tuple_type_impl &t, const abstract_type &previous, bool (abstract_type::*predicate)(const abstract_type &) const);


void write_collection_value(bytes::iterator &out, data_type type, const data_value &value)
{
if (value.is_null())
{
    auto val_len = -1;
    serialize_int32(out, val_len);
    return;
}
size_t val_len = value.serialized_size();
serialize_int32(out, val_len);
value.serialize(out);
}
sstring make_map_type_name(data_type keys, data_type values, bool is_multi_cell)
;
map_type_impl::map_type_impl(data_type keys, data_type values, bool is_multi_cell) : concrete_type(kind::map, make_map_type_name(keys, values, is_multi_cell), is_multi_cell), _keys(std::move(keys)), _values(std::move(values)) { _contains_set_or_map = true; }
data_type map_type_impl::freeze() const
{
if (_is_multi_cell)
{
    return get_instance(_keys, _values, false);
}
else
{
    return shared_from_this();
}
}
std::strong_ordering map_type_impl::compare_maps(data_type keys, data_type values, managed_bytes_view o1, managed_bytes_view o2)
{
if (o1.empty())
{
    return o2.empty() ? std::strong_ordering::equal : std::strong_ordering::less;
}
else if (o2.empty())
{
    return std::strong_ordering::greater;
}
int size1 = read_collection_size(o1);
int size2 = read_collection_size(o2); // FIXME: use std::lexicographical_compare()
for (int i = 0; i < std::min(size1, size2); ++i)
{
    auto k1 = read_collection_key(o1);
    auto k2 = read_collection_key(o2);
    auto cmp = keys->compare(k1, k2);
    if (cmp != 0)
    {
        return cmp;
    }
    auto v1 = read_collection_value_nonnull(o1);
    auto v2 = read_collection_value_nonnull(o2);
    cmp = values->compare(v1, v2);
    if (cmp != 0)
    {
        return cmp;
    }
}
return size1 <=> size2;
}
static size_t map_serialized_size(const map_type_impl::native_type *m)
{
size_t len = collection_size_len();
size_t psz = collection_value_len();
for (auto &&kv : *m)
{
    len += psz + kv.first.serialized_size();
    len += psz + kv.second.serialized_size();
}
return len;
}
static void serialize_map(const map_type_impl &t, const void *value, bytes::iterator &out)
{
auto &m = t.from_value(value);
write_collection_size(out, m.size());
for (auto &&kv : m)
{
    write_collection_value(out, t.get_keys_type(), kv.first);
    write_collection_value(out, t.get_values_type(), kv.second);
}
}
template <FragmentedView View>
data_value map_type_impl::deserialize(View in) const
{
native_type m;
auto size = read_collection_size(in);
for (int i = 0; i < size; ++i)
{
    auto k = _keys->deserialize(read_collection_key(in));
    auto v = _values->deserialize(read_collection_value_nonnull(in));
    m.insert(m.end(), std::make_pair(std::move(k), std::move(v)));
}
return make_value(std::move(m));
}
template data_value map_type_impl::deserialize<>(ser::buffer_view<bytes_ostream::fragment_iterator>) const;
static sstring map_to_string(const std::vector<std::pair<data_value, data_value>> &v, bool include_frozen_type)
{
std::ostringstream out;
if (include_frozen_type)
{
    out << "(";
}
fmt::print(out, "{}", fmt::join(v | boost::adaptors::transformed([](const std::pair<data_value, data_value> &p)
                                                                 {         std::ostringstream out;         const auto& k = p.first;         const auto& v = p.second;         out << "{" << k.type()->to_string_impl(k) << " : ";         out << v.type()->to_string_impl(v) << "}";         return out.str(); }),
                                ", "));
if (include_frozen_type)
{
    out << ")";
}
return out.str();
}
static std::optional<data_type> update_user_type_aux(const map_type_impl &m, const shared_ptr<const user_type_impl> updated)
;
static void serialize(const abstract_type &t, const void *value, bytes::iterator &out);
sstring make_set_type_name(data_type elements, bool is_multi_cell)
;
set_type_impl::set_type_impl(data_type elements, bool is_multi_cell) : concrete_type(kind::set, make_set_type_name(elements, is_multi_cell), elements, is_multi_cell) { _contains_set_or_map = true; }
data_type set_type_impl::value_comparator() const { return empty_type; }
static size_t listlike_serialized_size(const std::vector<data_value> *s)
{
size_t len = collection_size_len();
size_t psz = collection_value_len();
for (auto &&e : *s)
{
    len += psz + e.serialized_size();
}
return len;
}
static void serialize_set(const set_type_impl &t, const void *value, bytes::iterator &out)
{
auto &s = t.from_value(value);
write_collection_size(out, s.size());
for (auto &&e : s)
{
    write_collection_value(out, t.get_elements_type(), e);
}
}
template <FragmentedView View>
data_value set_type_impl::deserialize(View in) const
{
auto nr = read_collection_size(in);
native_type s;
s.reserve(nr);
for (int i = 0; i != nr; ++i)
{
    auto e = _elements->deserialize(read_collection_value_nonnull(in));
    if (e.is_null())
    {
        throw marshal_exception("Cannot deserialize a set");
    }
    s.push_back(std::move(e));
}
return make_value(std::move(s));
}
template data_value set_type_impl::deserialize<>(ser::buffer_view<bytes_ostream::fragment_iterator>) const;
template <FragmentedView View>
utils::chunked_vector<managed_bytes_opt> partially_deserialize_listlike(View in)
{
auto nr = read_collection_size(in);
utils::chunked_vector<managed_bytes_opt> elements;
elements.reserve(nr);
for (int i = 0; i != nr; ++i)
{
    elements.emplace_back(read_collection_value(in));
}
return elements;
}
template utils::chunked_vector<managed_bytes_opt> partially_deserialize_listlike(managed_bytes_view in);
template utils::chunked_vector<managed_bytes_opt> partially_deserialize_listlike(fragmented_temporary_buffer::view in);
template <FragmentedView View>
std::vector<std::pair<managed_bytes, managed_bytes>> partially_deserialize_map(View in)
{
auto nr = read_collection_size(in);
std::vector<std::pair<managed_bytes, managed_bytes>> elements;
elements.reserve(nr);
for (int i = 0; i != nr; ++i)
{
    auto key = managed_bytes(read_collection_key(in));
    auto value = managed_bytes_opt(read_collection_value_nonnull(in));
    if (!value)
    {
        on_internal_error(tlogger, "NULL value in map");
    }
    elements.emplace_back(std::move(key), std::move(*value));
}
return elements;
}
template std::vector<std::pair<managed_bytes, managed_bytes>> partially_deserialize_map(managed_bytes_view in);
template std::vector<std::pair<managed_bytes, managed_bytes>> partially_deserialize_map(fragmented_temporary_buffer::view in);
sstring make_list_type_name(data_type elements, bool is_multi_cell)
;
list_type_impl::list_type_impl(data_type elements, bool is_multi_cell) : concrete_type(kind::list, make_list_type_name(elements, is_multi_cell), elements, is_multi_cell) { _contains_set_or_map = _elements->contains_set_or_map(); }
data_type list_type_impl::name_comparator() const { return timeuuid_type; }
data_type list_type_impl::value_comparator() const { return _elements; }
static bool is_value_compatible_with_internal(const abstract_type &t, const abstract_type &other);
static void serialize_list(const list_type_impl &t, const void *value, bytes::iterator &out)
{
auto &s = t.from_value(value);
write_collection_size(out, s.size());
for (auto &&e : s)
{
    write_collection_value(out, t.get_elements_type(), e);
}
}
template <FragmentedView View>
data_value list_type_impl::deserialize(View in) const
{
auto nr = read_collection_size(in);
native_type s;
s.reserve(nr);
for (int i = 0; i != nr; ++i)
{
    auto serialized_value_opt = read_collection_value(in);
    if (serialized_value_opt)
    {
        auto e = _elements->deserialize(*serialized_value_opt);
        s.push_back(std::move(e));
    }
    else
    {
        s.push_back(data_value::make_null(data_type(shared_from_this())));
    }
}
return make_value(std::move(s));
}
template data_value list_type_impl::deserialize<>(ser::buffer_view<bytes_ostream::fragment_iterator>) const;
static sstring vector_to_string(const std::vector<data_value> &v, std::string_view sep)
{
return fmt::to_string(fmt::join(v | boost::adaptors::transformed([](const data_value &e)
                                                                 { return e.type()->to_string_impl(e); }),
                                sep));
}
template <typename F>
static std::optional<data_type> update_listlike(const listlike_collection_type_impl &c, F &&f, shared_ptr<const user_type_impl> updated)
;
shared_ptr<const tuple_type_impl> tuple_type_impl::get_instance(std::vector<data_type> types) { return intern::get_instance(std::move(types)); }
template <FragmentedView View>
static void validate_aux(const tuple_type_impl &t, View v)
{
auto ti = t.all_types().begin();
while (ti != t.all_types().end() && v.size_bytes())
{
    std::optional<View> e = read_tuple_element(v);
    if (e)
    {
        (*ti)->validate(*e);
    }
    ++ti;
}
size_t extra_elements = 0;
while (!v.empty())
{
    read_tuple_element(v);
    extra_elements += 1;
}
if (extra_elements > 0)
{ // This function is called for both tuple and user_type, print the name too
    throw marshal_exception(format("Value of type {} contained too many fields (expected {}, got {})", t.name(), t.size(), t.size() + extra_elements));
}
}
namespace
{
template <FragmentedView View>
struct validate_visitor
{
    const View &v;
    ;
    void operator()(const abstract_type &) {}
    template <typename T>
    void operator()(const integer_type_impl<T> &t)
    {
        if (v.empty())
        {
        return;
        }
        if (v.size_bytes() != sizeof(T))
        {
        throw marshal_exception(format("Validation failed for type {}: got {:d} bytes", t.name(), v.size_bytes()));
        }
    }
    template <typename T>
    void operator()(const floating_type_impl<T> &t)
    {
        if (v.empty())
        {
        return;
        }
        if (v.size_bytes() != sizeof(T))
        {
        throw marshal_exception(format("Expected {:d} bytes for a floating type, got {:d}", sizeof(T), v.size_bytes()));
        }
    }
    void operator()(const simple_date_type_impl &t)
    {
        if (v.empty())
        {
        return;
        }
        if (v.size_bytes() != 4)
        {
        throw marshal_exception(format("Expected 4 byte long for date ({:d})", v.size_bytes()));
        }
    }
};
}
template <FragmentedView View>
void abstract_type::validate(const View &view) const { visit(*this, validate_visitor<View>{view}); }
// Explicit instantiation.
template void abstract_type::validate<>(const single_fragmented_view &) const;
template void abstract_type::validate<>(const fragmented_temporary_buffer::view &) const;
template void abstract_type::validate<>(const managed_bytes_view &) const;
void abstract_type::validate(bytes_view v) const { visit(*this, validate_visitor<single_fragmented_view>{single_fragmented_view(v)}); }
static void serialize_aux(const tuple_type_impl &type, const tuple_type_impl::native_type *val, bytes::iterator &out)
{
assert(val);
auto &elems = *val;
assert(elems.size() <= type.size());
for (size_t i = 0; i < elems.size(); ++i)
{
    const abstract_type &t = type.type(i)->without_reversed();
    const data_value &v = elems[i];
    if (!v.is_null() && t != *v.type())
    {
        throw std::runtime_error(format("tuple element type mismatch: expected {}, got {}", t.name(), v.type()->name()));
    }
    if (v.is_null())
    {
        write(out, int32_t(-1));
    }
    else
    {
        write(out, int32_t(v.serialized_size()));
        v.serialize(out);
    }
}
}
namespace
{
struct serialize_visitor
{
    bytes::iterator &out;
    ;
    void operator()(const reversed_type_impl &t, const void *v) { return serialize(*t.underlying_type(), v, out); }
    template <typename T>
    void operator()(const integer_type_impl<T> &t, const typename integer_type_impl<T>::native_type *v1)
    {
        if (v1->empty())
        {
        return;
        }
        auto v = v1->get();
        auto u = net::hton(v);
        out = std::copy_n(reinterpret_cast<const char *>(&u), sizeof(u), out);
    }
    void operator()(const string_type_impl &t, const string_type_impl::native_type *v) { out = std::copy(v->begin(), v->end(), out); }
    void operator()(const bytes_type_impl &t, const bytes *v) { out = std::copy(v->begin(), v->end(), out); }
    void operator()(const boolean_type_impl &t, const boolean_type_impl::native_type *v)
    {
        if (!v->empty())
        {
        *out++ = char(*v);
        }
    }
    void operator()(const timestamp_date_base_class &t, const timestamp_date_base_class::native_type *v1)
    {
        if (v1->empty())
        {
        return;
        }
        uint64_t v = v1->get().time_since_epoch().count();
        v = net::hton(v);
        out = std::copy_n(reinterpret_cast<const char *>(&v), sizeof(v), out);
    }
    void operator()(const timeuuid_type_impl &t, const timeuuid_type_impl::native_type *uuid1)
    {
        if (uuid1->empty())
        {
        return;
        }
        auto uuid = uuid1->get();
        uuid.serialize(out);
    }
    void operator()(const simple_date_type_impl &t, const simple_date_type_impl::native_type *v1)
    {
        if (v1->empty())
        {
        return;
        }
        uint32_t v = v1->get();
        v = net::hton(v);
        out = std::copy_n(reinterpret_cast<const char *>(&v), sizeof(v), out);
    }
    void operator()(const time_type_impl &t, const time_type_impl::native_type *v1)
    {
        if (v1->empty())
        {
        return;
        }
        uint64_t v = v1->get();
        v = net::hton(v);
        out = std::copy_n(reinterpret_cast<const char *>(&v), sizeof(v), out);
    }
    void operator()(const empty_type_impl &t, const void *) {}
    void operator()(const uuid_type_impl &t, const uuid_type_impl::native_type *value)
    {
        if (value->empty())
        {
        return;
        }
        value->get().serialize(out);
    }
    void operator()(const inet_addr_type_impl &t, const inet_addr_type_impl::native_type *ipv)
    {
        if (ipv->empty())
        {
        return;
        }
        auto &ip = ipv->get();
        switch (ip.in_family())
        {
        case inet_address::family::INET:
        {
        const ::in_addr &in = ip;
        out = std::copy_n(reinterpret_cast<const char *>(&in.s_addr), sizeof(in.s_addr), out);
        break;
        }
        case inet_address::family::INET6:
        {
        const ::in6_addr &i6 = ip;
        out = std::copy_n(i6.s6_addr, ip.size(), out);
        break;
        }
        }
    }
    template <typename T>
    void operator()(const floating_type_impl<T> &t, const typename floating_type_impl<T>::native_type *value)
    {
        if (value->empty())
        {
        return;
        }
        T d = *value;
        if (std::isnan(d))
        { // Java's Double.doubleToLongBits() documentation specifies that
        // any nan must be serialized to the same specific value
        d = std::numeric_limits<T>::quiet_NaN();
        }
        typename int_of_size<T>::itype i;
        memcpy(&i, &d, sizeof(T));
        auto u = net::hton(i);
        out = std::copy_n(reinterpret_cast<const char *>(&u), sizeof(u), out);
    }
    void operator()(const counter_type_impl &t, const void *) { fail(unimplemented::cause::COUNTERS); }
    void operator()(const duration_type_impl &t, const duration_type_impl::native_type *m)
    {
        if (m->empty())
        {
        return;
        }
        const auto &d = m->get();
        out += signed_vint::serialize(d.months, out);
        out += signed_vint::serialize(d.days, out);
        out += signed_vint::serialize(d.nanoseconds, out);
    }
    void operator()(const list_type_impl &t, const void *value) { serialize_list(t, value, out); }
    void operator()(const map_type_impl &t, const void *value) { serialize_map(t, value, out); }
    void operator()(const set_type_impl &t, const void *value) { serialize_set(t, value, out); }
    void operator()(const tuple_type_impl &t, const tuple_type_impl::native_type *value) { return serialize_aux(t, value, out); }
};
}
static void serialize(const abstract_type &t, const void *value, bytes::iterator &out) { visit(t, value, serialize_visitor{out}); }
template <FragmentedView View>
data_value collection_type_impl::deserialize_impl(View v) const
{
struct visitor
{
    View v;
    ;
    data_value operator()(const abstract_type &) ;
    data_value operator()(const list_type_impl &t);
    data_value operator()(const map_type_impl &t);
    data_value operator()(const set_type_impl &t);
};
return ::visit(*this, visitor{v});
}
// Explicit instantiation.
// This should be repeated for every View type passed to collection_type_impl::deserialize.
template data_value collection_type_impl::deserialize_impl<>(ser::buffer_view<bytes_ostream::fragment_iterator>) const;
template data_value collection_type_impl::deserialize_impl<>(fragmented_temporary_buffer::view) const;
template data_value collection_type_impl::deserialize_impl<>(single_fragmented_view) const;
template data_value collection_type_impl::deserialize_impl<>(managed_bytes_view) const;
template int read_collection_size(ser::buffer_view<bytes_ostream::fragment_iterator> &in);
template ser::buffer_view<bytes_ostream::fragment_iterator> read_collection_value_nonnull(ser::buffer_view<bytes_ostream::fragment_iterator> &in);
template <FragmentedView View>
data_value deserialize_aux(const tuple_type_impl &t, View v)
{
tuple_type_impl::native_type ret;
ret.reserve(t.all_types().size());
auto ti = t.all_types().begin();
while (ti != t.all_types().end() && v.size_bytes())
{
    data_value obj = data_value::make_null(*ti);
    std::optional<View> e = read_tuple_element(v);
    if (e)
    {
        obj = (*ti)->deserialize(*e);
    }
    ret.push_back(std::move(obj));
    ++ti;
}
while (ti != t.all_types().end())
{
    ret.push_back(data_value::make_null(*ti++));
}
return data_value::make(t.shared_from_this(), std::make_unique<tuple_type_impl::native_type>(std::move(ret)));
}
template <typename T, FragmentedView View>
T deserialize_value(const floating_type_impl<T> &, View v)
{
typename int_of_size<T>::itype i = read_simple<typename int_of_size<T>::itype>(v);
if (v.size_bytes())
{
    throw marshal_exception(format("cannot deserialize floating - {:d} bytes left", v.size_bytes()));
}
T d;
memcpy(&d, &i, sizeof(T));
return d;
}
template <FragmentedView View>
cql_duration deserialize_value(const duration_type_impl &t, View v)
{
common_counter_type months, days, nanoseconds;
std::tie(months, days, nanoseconds) = with_linearized(v, [](bytes_view bv)
                                                      { return deserialize_counters(bv); });
return cql_duration(months_counter(months), days_counter(days), nanoseconds_counter(nanoseconds));
}
template <FragmentedView View>
inet_address deserialize_value(const inet_addr_type_impl &, View v)
{
switch (v.size_bytes())
{
case 4: // gah. read_simple_be, please...
    return inet_address(::in_addr{net::hton(read_simple<uint32_t>(v))});
case 16:;
    ::in6_addr buf;
    read_fragmented(v, sizeof(buf), reinterpret_cast<bytes::value_type *>(&buf));
    return inet_address(buf);
default:
    throw marshal_exception(format("cannot deserialize inet_address, unsupported size {:d} bytes", v.size_bytes()));
}
}
template <FragmentedView View>
utils::UUID deserialize_value(const uuid_type_impl &, View v)
{
auto msb = read_simple<uint64_t>(v);
auto lsb = read_simple<uint64_t>(v);
if (v.size_bytes())
{
    throw marshal_exception(format("cannot deserialize uuid, {:d} bytes left", v.size_bytes()));
}
return utils::UUID(msb, lsb);
}
template <FragmentedView View>
utils::UUID deserialize_value(const timeuuid_type_impl &, View v) { return deserialize_value(static_cast<const uuid_type_impl &>(*uuid_type), v); }
template <FragmentedView View>
db_clock::time_point deserialize_value(const timestamp_date_base_class &, View v)
{
auto v2 = read_simple_exactly<uint64_t>(v);
return db_clock::time_point(db_clock::duration(v2));
}
template <FragmentedView View>
uint32_t deserialize_value(const simple_date_type_impl &, View v) { return read_simple_exactly<uint32_t>(v); }
template <FragmentedView View>
int64_t deserialize_value(const time_type_impl &, View v) { return read_simple_exactly<int64_t>(v); }
template <FragmentedView View>
bool deserialize_value(const boolean_type_impl &, View v)
{
if (v.size_bytes() != 1)
{
    throw marshal_exception(format("cannot deserialize boolean, size mismatch ({:d})", v.size_bytes()));
}
skip_empty_fragments(v);
return v.current_fragment().front() != 0;
}
template <typename T, FragmentedView View>
T deserialize_value(const integer_type_impl<T> &t, View v) { return read_simple_exactly<T>(v); }
template <FragmentedView View>
sstring deserialize_value(const string_type_impl &, View v)
{ // FIXME: validation?
sstring buf(sstring::initialized_later(), v.size_bytes());
auto out = buf.begin();
while (v.size_bytes())
{
    out = std::copy(v.current_fragment().begin(), v.current_fragment().end(), out);
    v.remove_current();
}
return buf;
}
template <typename T>
decltype(auto) deserialize_value(const T &t, bytes_view v) { return deserialize_value(t, single_fragmented_view(v)); }
namespace
{
template <FragmentedView View>
struct deserialize_visitor
{
    View v;
    data_value operator()(const reversed_type_impl &t) { return t.underlying_type()->deserialize(v); }
    template <typename T>
    data_value operator()(const T &t)
    {
        if (!v.size_bytes())
        {
        return t.make_empty();
        }
        return t.make_value(deserialize_value(t, v));
    }
    data_value operator()(const ascii_type_impl &t) { return t.make_value(deserialize_value(t, v)); }
    data_value operator()(const utf8_type_impl &t) { return t.make_value(deserialize_value(t, v)); }
    data_value operator()(const bytes_type_impl &t) { return t.make_value(std::make_unique<bytes_type_impl::native_type>(linearized(v))); }
    data_value operator()(const counter_type_impl &t) { return static_cast<const long_type_impl &>(*long_type).make_value(read_simple_exactly<int64_t>(v)); }
    data_value operator()(const list_type_impl &t) { return t.deserialize(v); }
    data_value operator()(const map_type_impl &t) { return t.deserialize(v); }
    data_value operator()(const set_type_impl &t) { return t.deserialize(v); }
    data_value operator()(const tuple_type_impl &t) { return deserialize_aux(t, v); }
    data_value operator()(const user_type_impl &t) { return deserialize_aux(t, v); }
    data_value operator()(const empty_type_impl &t) { return data_value(empty_type_representation()); }
};
}
template <FragmentedView View>
data_value abstract_type::deserialize_impl(View v) const { return visit(*this, deserialize_visitor<View>{v}); }
// Explicit instantiation.
// This should be repeated for every type passed to deserialize().
template data_value abstract_type::deserialize_impl<>(fragmented_temporary_buffer::view) const;
template data_value abstract_type::deserialize_impl<>(single_fragmented_view) const;
template data_value abstract_type::deserialize_impl<>(ser::buffer_view<bytes_ostream::fragment_iterator>) const;
template data_value abstract_type::deserialize_impl<>(managed_bytes_view) const;
std::strong_ordering compare_aux(const tuple_type_impl &t, const managed_bytes_view &v1, const managed_bytes_view &v2)
{ // This is a slight modification of lexicographical_tri_compare:
// when the only difference between the tuples is that one of them has additional trailing nulls,
// we consider them equal. For example, in the following CQL scenario:
// 1. create type ut (a int);
// 2. create table cf (a int primary key, b frozen<ut>);
// 3. insert into cf (a, b) values (0, (0));
// 4. alter type ut add b int;
// 5. select * from cf where b = {a:0,b:null};
// the row with a = 0 should be returned, even though the value stored in the database is shorter
// (by one null) than the value given by the user.
auto types_first = t.all_types().begin();
auto types_last = t.all_types().end();
auto first1 = tuple_deserializing_iterator::start(v1);
auto last1 = tuple_deserializing_iterator::finish(v1);
auto first2 = tuple_deserializing_iterator::start(v2);
auto last2 = tuple_deserializing_iterator::finish(v2);
while (types_first != types_last && first1 != last1 && first2 != last2)
{
    if (auto c = tri_compare_opt(*types_first, *first1, *first2); c != 0)
    {
        return c;
    }
    ++first1;
    ++first2;
    ++types_first;
}
while (types_first != types_last && first1 != last1)
{
    if (*first1)
    {
        return std::strong_ordering::greater;
    }
    ++first1;
    ++types_first;
}
while (types_first != types_last && first2 != last2)
{
    if (*first2)
    {
        return std::strong_ordering::less;
    }
    ++first2;
    ++types_first;
}
return std::strong_ordering::equal;
}
namespace
{
struct compare_visitor
{
    managed_bytes_view v1;
    managed_bytes_view v2;
    template <std::invocable<> Func>
        requires std::same_as<std::strong_ordering, std::invoke_result_t<Func>>
    std::strong_ordering with_empty_checks(Func func)
    {
        if (v1.empty())
        {
        return v2.empty() ? std::strong_ordering::equal : std::strong_ordering::less;
        }
        if (v2.empty())
        {
        return std::strong_ordering::greater;
        }
        return func();
    }
    template <typename T>
    std::strong_ordering operator()(const simple_type_impl<T> &)
    {
        return with_empty_checks([&]
                                 {         T a = simple_type_traits<T>::read_nonempty(v1);         T b = simple_type_traits<T>::read_nonempty(v2);         return a <=> b; });
    }
    std::strong_ordering operator()(const string_type_impl &) { return compare_unsigned(v1, v2); }
    std::strong_ordering operator()(const bytes_type_impl &) { return compare_unsigned(v1, v2); }
    std::strong_ordering operator()(const duration_type_impl &) { return compare_unsigned(v1, v2); }
    std::strong_ordering operator()(const inet_addr_type_impl &) { return compare_unsigned(v1, v2); }
    std::strong_ordering operator()(const date_type_impl &)
    { // This is not the same behaviour as timestamp_type_impl
        return compare_unsigned(v1, v2);
    }
    std::strong_ordering operator()(const timeuuid_type_impl &)
    {
        return with_empty_checks([&]
                                 { return with_linearized(v1, [&](bytes_view v1)
                                                          { return with_linearized(v2, [&](bytes_view v2)
                                                                                   { return utils::timeuuid_tri_compare(v1, v2); }); }); });
    }
    std::strong_ordering operator()(const listlike_collection_type_impl &l)
    {
        using llpdi = listlike_partial_deserializing_iterator;
        return with_empty_checks([&]
                                 { return std::lexicographical_compare_three_way(llpdi::begin(v1), llpdi::end(v1), llpdi::begin(v2), llpdi::end(v2), [&](const managed_bytes_view_opt &o1, const managed_bytes_view_opt &o2)
                                                                                 {                         if (!o1.has_value() || !o2.has_value()) {                             return o1.has_value() <=> o2.has_value();                         } else {                             return l.get_elements_type()->compare(*o1, *o2);                         } }); });
    }
    std::strong_ordering operator()(const map_type_impl &m) { return map_type_impl::compare_maps(m.get_keys_type(), m.get_values_type(), v1, v2); }
    std::strong_ordering operator()(const uuid_type_impl &)
    {
        if (v1.size() < 16)
        {
        return v2.size() < 16 ? std::strong_ordering::equal : std::strong_ordering::less;
        }
        if (v2.size() < 16)
        {
        return std::strong_ordering::greater;
        }
        auto c1 = (v1[6] >> 4) & 0x0f;
        auto c2 = (v2[6] >> 4) & 0x0f;
        if (c1 != c2)
        {
        return c1 <=> c2;
        }
        if (c1 == 1)
        {
        return with_linearized(v1, [&](bytes_view v1)
                               { return with_linearized(v2, [&](bytes_view v2)
                                                        { return utils::uuid_tri_compare_timeuuid(v1, v2); }); });
        }
        return compare_unsigned(v1, v2);
    }
    std::strong_ordering operator()(const empty_type_impl &) { return std::strong_ordering::equal; }
    std::strong_ordering operator()(const tuple_type_impl &t) { return compare_aux(t, v1, v2); }
    std::strong_ordering operator()(const counter_type_impl &)
    { // untouched (empty) counter evaluates as 0
        const auto a = v1.empty() ? 0 : simple_type_traits<int64_t>::read_nonempty(v1);
        const auto b = v2.empty() ? 0 : simple_type_traits<int64_t>::read_nonempty(v2);
        return a <=> b;
    }
    template <typename T>
    std::strong_ordering operator()(const floating_type_impl<T> &)
    {
        return with_empty_checks([&]
                                 {         T a = simple_type_traits<T>::read_nonempty(v1);         T b = simple_type_traits<T>::read_nonempty(v2);         // in java world NaN == NaN and NaN is greater than anything else
        if (std::isnan(a) && std::isnan(b)) {             return std::strong_ordering::equal;         } else if (std::isnan(a)) {             return std::strong_ordering::greater;         } else if (std::isnan(b)) {             return std::strong_ordering::less;         }         // also -0 < 0
        if (std::signbit(a) && !std::signbit(b)) {             return std::strong_ordering::less;         } else if (!std::signbit(a) && std::signbit(b)) {             return std::strong_ordering::greater;         }         // note: float <=> returns std::partial_ordering
        return a == b ? std::strong_ordering::equal : a < b ? std::strong_ordering::less : std::strong_ordering::greater; });
    }
    std::strong_ordering operator()(const reversed_type_impl &r) { return r.underlying_type()->compare(v2, v1); }
};
}
std::strong_ordering abstract_type::compare(bytes_view v1, bytes_view v2) const { return compare(managed_bytes_view(v1), managed_bytes_view(v2)); }
std::strong_ordering abstract_type::compare(managed_bytes_view v1, managed_bytes_view v2) const
{
try
{
    return visit(*this, compare_visitor{v1, v2});
}
catch (const marshal_exception &)
{
    on_types_internal_error(std::current_exception());
}
}
// Count number of ':' which are not preceded by '\'.

// Split on ':', unless it's preceded by '\'.

// Replace "\:" with ":" and "\@" with "@".

// Replace ":" with "\:" and "@" with "\@".

// Concat list of bytes into a single bytes.
static bytes concat_fields(const std::vector<bytes> &fields, const std::vector<int32_t> field_len);
static size_t concrete_serialized_size(const byte_type_impl::native_type &) { return sizeof(int8_t); }
static size_t concrete_serialized_size(const short_type_impl::native_type &) { return sizeof(int16_t); }
static size_t concrete_serialized_size(const int32_type_impl::native_type &) { return sizeof(int32_t); }
static size_t concrete_serialized_size(const long_type_impl::native_type &) { return sizeof(int64_t); }
static size_t concrete_serialized_size(const float_type_impl::native_type &) { return sizeof(float); }
static size_t concrete_serialized_size(const double_type_impl::native_type &) { return sizeof(double); }
static size_t concrete_serialized_size(const boolean_type_impl::native_type &) { return 1; }
static size_t concrete_serialized_size(const timestamp_date_base_class::native_type &) { return 8; }
static size_t concrete_serialized_size(const timeuuid_type_impl::native_type &) { return 16; }
static size_t concrete_serialized_size(const simple_date_type_impl::native_type &) { return 4; }
static size_t concrete_serialized_size(const string_type_impl::native_type &v) { return v.size(); }
static size_t concrete_serialized_size(const bytes_type_impl::native_type &v) { return v.size(); }
static size_t concrete_serialized_size(const inet_addr_type_impl::native_type &v) { return v.get().size(); }
static size_t concrete_serialized_size(const duration_type_impl::native_type &v)
{
const auto &d = v.get();
return signed_vint::serialized_size(d.months) + signed_vint::serialized_size(d.days) + signed_vint::serialized_size(d.nanoseconds);
}
static size_t concrete_serialized_size(const tuple_type_impl::native_type &v)
{
size_t len = 0;
for (auto &&e : v)
{
    len += 4 + e.serialized_size();
}
return len;
}
static size_t serialized_size(const abstract_type &t, const void *value);
namespace
{
struct serialized_size_visitor
{
    size_t operator()(const reversed_type_impl &t, const void *v) { return serialized_size(*t.underlying_type(), v); }
    size_t operator()(const empty_type_impl &, const void *) { return 0; }
    template <typename T>
    size_t operator()(const concrete_type<T> &t, const typename concrete_type<T>::native_type *v)
    {
        if (v->empty())
        {
        return 0;
        }
        return concrete_serialized_size(*v);
    }
    size_t operator()(const counter_type_impl &, const void *) { fail(unimplemented::cause::COUNTERS); }
    size_t operator()(const map_type_impl &t, const map_type_impl::native_type *v) { return map_serialized_size(v); }
    size_t operator()(const concrete_type<std::vector<data_value>, listlike_collection_type_impl> &t, const std::vector<data_value> *v) { return listlike_serialized_size(v); }
};
}
static size_t serialized_size(const abstract_type &t, const void *value) { return visit(t, value, serialized_size_visitor{}); }
;
namespace
{
struct from_string_visitor
{
    sstring_view s;
    ;
    ;
    
    
    bytes operator()(const collection_type_impl &);
};
}
template <typename N, typename A, typename F>
static sstring format_if_not_empty(const concrete_type<N, A> &type, const typename concrete_type<N, A>::native_type *b, F &&f)
{
if (b->empty())
{
    return {};
}
return f(static_cast<const N &>(*b));
}
static sstring to_string_impl(const abstract_type &t, const void *v);
namespace
{
struct to_string_impl_visitor
{
    template <typename T>
    sstring operator()(const concrete_type<T> &t, const typename concrete_type<T>::native_type *v)
    {
        return format_if_not_empty(t, v, [](const T &v)
                                   { return to_sstring(v); });
    }
    sstring operator()(const counter_type_impl &c, const void *) { fail(unimplemented::cause::COUNTERS); }
    sstring operator()(const empty_type_impl &, const void *) { return sstring(); }
    sstring operator()(const list_type_impl &l, const list_type_impl::native_type *v)
    {
        return format_if_not_empty(l, v, [](const list_type_impl::native_type &v)
                                   { return vector_to_string(v, ", "); });
    }
    sstring operator()(const set_type_impl &s, const set_type_impl::native_type *v)
    {
        return format_if_not_empty(s, v, [](const set_type_impl::native_type &v)
                                   { return vector_to_string(v, "; "); });
    }
    sstring operator()(const map_type_impl &m, const map_type_impl::native_type *v)
    {
        return format_if_not_empty(m, v, [&m](const map_type_impl::native_type &v)
                                   { return map_to_string(v, !m.is_multi_cell()); });
    }
    sstring operator()(const reversed_type_impl &r, const void *v) { return to_string_impl(*r.underlying_type(), v); }
};
}
static sstring to_string_impl(const abstract_type &t, const void *v) { return visit(t, v, to_string_impl_visitor{}); }
sstring abstract_type::to_string_impl(const data_value &v) const { return ::to_string_impl(*this, get_value_ptr(v)); }

// Needed to handle ReversedType in value-compatibility checks.
static bool is_value_compatible_with_internal(const abstract_type &t, const abstract_type &other)
;
static std::optional<std::vector<data_type>> update_types(const std::vector<data_type> types, const user_type updated)
{
std::optional<std::vector<data_type>> new_types = std::nullopt;
for (uint32_t i = 0; i < types.size(); ++i)
{
    auto &&ut = types[i]->update_user_type(updated);
    if (ut)
    {
        if (!new_types)
        {
        new_types = types;
        }
        (*new_types)[i] = std::move(*ut);
    }
}
return new_types;
}

namespace
{
struct native_value_clone_visitor
{
    const void *from;
    void *operator()(const reversed_type_impl &t) { return visit(*t.underlying_type(), native_value_clone_visitor{from}); }
    template <typename N, typename A>
    void *operator()(const concrete_type<N, A> &)
    {
        using nt = typename concrete_type<N, A>::native_type;
        return new nt(*reinterpret_cast<const nt *>(from));
    }
    void *operator()(const counter_type_impl &) { fail(unimplemented::cause::COUNTERS); }
    void *operator()(const empty_type_impl &) { return new empty_type_representation(); }
};
}
void *abstract_type::native_value_clone(const void *from) const { return visit(*this, native_value_clone_visitor{from}); }
namespace
{
struct native_value_delete_visitor
{
    void *object;
    template <typename N, typename A>
    void operator()(const concrete_type<N, A> &) { delete reinterpret_cast<typename concrete_type<N, A>::native_type *>(object); }
    void operator()(const reversed_type_impl &t) { return visit(*t.underlying_type(), native_value_delete_visitor{object}); }
    void operator()(const counter_type_impl &) { fail(unimplemented::cause::COUNTERS); }
    void operator()(const empty_type_impl &) { delete reinterpret_cast<empty_type_representation *>(object); }
};
}
static void native_value_delete(const abstract_type &t, void *object) { visit(t, native_value_delete_visitor{object}); }
namespace
{
struct native_typeid_visitor
{
    template <typename N, typename A>
    const std::type_info &operator()(const concrete_type<N, A> &);
    
    
    const std::type_info &operator()(const empty_type_impl &);
};
}
bytes abstract_type::decompose(const data_value &value) const
{
if (!value._value)
{
    return {};
}
bytes b(bytes::initialized_later(), serialized_size(*this, value._value));
auto i = b.begin();
value.serialize(i);
return b;
}
size_t data_value::serialized_size() const
{
if (!_value)
{
    return 0;
}
return ::serialized_size(*_type, _value);
}
void data_value::serialize(bytes::iterator &out) const
{
if (_value)
{
    ::serialize(*_type, _value, out);
}
}
bytes_opt data_value::serialize() const
{
if (!_value)
{
    return std::nullopt;
}
bytes b(bytes::initialized_later(), serialized_size());
auto i = b.begin();
serialize(i);
return b;
}
bytes data_value::serialize_nonnull() const
{
if (!_value)
{
    on_internal_error(tlogger, "serialize_nonnull called on null");
}
return std::move(*serialize());
}
sstring abstract_type::get_string(const bytes &b) const
{
struct visitor
{
    const bytes &b;
    sstring operator()(const abstract_type &t)
    {
        t.validate(b);
        return t.to_string(b);
    }
    sstring operator()(const reversed_type_impl &r) { return r.underlying_type()->get_string(b); }
};
return visit(*this, visitor{b});
}
std::ostream &user_type_impl::describe(std::ostream &os) const
{
os << "CREATE TYPE " << cql3::util::maybe_quote(_keyspace) << "." << get_name_as_cql_string() << " (\n";
for (size_t i = 0; i < _string_field_names.size(); i++)
{
    os << "    " << cql3::util::maybe_quote(_string_field_names[i]) << " " << _types[i]->cql3_type_name();
    if (i < _string_field_names.size() - 1)
    {
        os << ",";
    }
    os << "\n";
}
os << ");";
return os;
}
data_type user_type_impl::freeze() const
{
if (_is_multi_cell)
{
    return get_instance(_keyspace, _name, _field_names, _types, false);
}
else
{
    return shared_from_this();
}
}
static bytes_ostream serialize_for_cql_aux(const map_type_impl &, collection_mutation_view_description mut);


static bytes_ostream serialize_for_cql_aux(const user_type_impl &type, collection_mutation_view_description mut);
bytes serialize_field_index(size_t idx)
{
if (idx >= size_t(std::numeric_limits<int16_t>::max()))
{ // should've been rejected earlier, but just to be sure...
    throw std::runtime_error(format("index for user type field too large: {}", idx));
}
bytes b(bytes::initialized_later(), sizeof(int16_t));
write_be(reinterpret_cast<char *>(b.data()), static_cast<int16_t>(idx));
return b;
}
size_t deserialize_field_index(const bytes_view &b)
{
assert(b.size() == sizeof(int16_t));
return read_be<int16_t>(reinterpret_cast<const char *>(b.data()));
}
size_t deserialize_field_index(managed_bytes_view b);
thread_local const shared_ptr<const abstract_type> byte_type(make_shared<byte_type_impl>());
thread_local const shared_ptr<const abstract_type> short_type(make_shared<short_type_impl>());
thread_local const shared_ptr<const abstract_type> int32_type(make_shared<int32_type_impl>());
thread_local const shared_ptr<const abstract_type> long_type(make_shared<long_type_impl>());
thread_local const shared_ptr<const abstract_type> ascii_type(make_shared<ascii_type_impl>());
thread_local const shared_ptr<const abstract_type> bytes_type(make_shared<bytes_type_impl>());
thread_local const shared_ptr<const abstract_type> utf8_type(make_shared<utf8_type_impl>());
thread_local const shared_ptr<const abstract_type> boolean_type(make_shared<boolean_type_impl>());
thread_local const shared_ptr<const abstract_type> date_type(make_shared<date_type_impl>());
thread_local const shared_ptr<const abstract_type> timeuuid_type(make_shared<timeuuid_type_impl>());
thread_local const shared_ptr<const abstract_type> timestamp_type(make_shared<timestamp_type_impl>());
thread_local const shared_ptr<const abstract_type> simple_date_type(make_shared<simple_date_type_impl>());
thread_local const shared_ptr<const abstract_type> time_type(make_shared<time_type_impl>());
thread_local const shared_ptr<const abstract_type> uuid_type(make_shared<uuid_type_impl>());
thread_local const shared_ptr<const abstract_type> inet_addr_type(make_shared<inet_addr_type_impl>());
thread_local const shared_ptr<const abstract_type> float_type(make_shared<float_type_impl>());
thread_local const shared_ptr<const abstract_type> double_type(make_shared<double_type_impl>());
thread_local const shared_ptr<const abstract_type> counter_type(make_shared<counter_type_impl>());
thread_local const shared_ptr<const abstract_type> duration_type(make_shared<duration_type_impl>());
thread_local const data_type empty_type(make_shared<empty_type_impl>());

data_value::data_value(const data_value &v) : _value(nullptr), _type(v._type)
{
if (v._value)
{
    _value = _type->native_value_clone(v._value);
}
}

data_value::data_value(bytes v) : data_value(make_new(bytes_type, v)) {}
data_value::data_value(sstring &&v) : data_value(make_new(utf8_type, std::move(v))) {}
data_value::data_value(std::string_view v) : data_value(sstring(v)) {}
data_value::data_value(const sstring &v) : data_value(std::string_view(v)) {}
data_value::data_value(ascii_native_type v) : data_value(make_new(ascii_type, v.string)) {}
data_value::data_value(bool v) : data_value(make_new(boolean_type, v)) {}
data_value::data_value(int8_t v) : data_value(make_new(byte_type, v)) {}
data_value::data_value(int16_t v) : data_value(make_new(short_type, v)) {}
data_value::data_value(int32_t v) : data_value(make_new(int32_type, v)) {}
data_value::data_value(int64_t v) : data_value(make_new(long_type, v)) {}
data_value::data_value(utils::UUID v) : data_value(make_new(uuid_type, v)) {}

data_value::data_value(double v) : data_value(make_new(double_type, v)) {}
data_value::data_value(seastar::net::inet_address v) : data_value(make_new(inet_addr_type, v)) {}
data_value::data_value(seastar::net::ipv4_address v) : data_value(seastar::net::inet_address(v)) {}
data_value::data_value(simple_date_native_type v) : data_value(make_new(simple_date_type, v.days)) {}
data_value::data_value(db_clock::time_point v) : data_value(make_new(timestamp_type, v)) {}
data_value::data_value(time_native_type v) : data_value(make_new(time_type, v.nanoseconds)) {}
data_value::data_value(timeuuid_native_type v) : data_value(make_new(timeuuid_type, v.uuid)) {}
data_value::data_value(date_type_native_type v) : data_value(make_new(date_type, v.tp)) {}
data_value::data_value(cql_duration d) : data_value(make_new(duration_type, d)) {}
data_value::data_value(empty_type_representation e) : data_value(make_new(empty_type, e)) {}
data_value make_list_value(data_type type, list_type_impl::native_type value) { return data_value::make_new(std::move(type), std::move(value)); }
data_value make_set_value(data_type type, set_type_impl::native_type value) { return data_value::make_new(std::move(type), std::move(value)); }
data_value make_map_value(data_type type, map_type_impl::native_type value) { return data_value::make_new(std::move(type), std::move(value)); }
data_value make_tuple_value(data_type type, tuple_type_impl::native_type value) { return data_value::make_new(std::move(type), std::move(value)); }
data_value make_user_value(data_type type, user_type_impl::native_type value) { return data_value::make_new(std::move(type), std::move(value)); }
std::ostream &operator<<(std::ostream &out, const data_value &v)
{
if (v.is_null())
{
    return out << "null";
}
return out << v.type()->to_string_impl(v);
}
// compile once the template instance that was externed in marshal_exception.hh
namespace seastar
{
template void throw_with_backtrace<marshal_exception, sstring>(sstring &&);
}
namespace cql3
{
column_identifier::column_identifier(bytes bytes_, data_type type) : bytes_(std::move(bytes_)), _text(type->get_string(this->bytes_)) {}
}
namespace cql3
{
column_specification::column_specification(std::string_view ks_name_, std::string_view cf_name_, ::shared_ptr<column_identifier> name_, data_type type_) : ks_name(ks_name_), cf_name(cf_name_), name(name_), type(type_) {}
}
namespace utils
{
static int64_t make_thread_local_node(int64_t node)
{ // An atomic counter to issue thread identifiers.
    // We should take current core number into consideration
    // because create_time_safe() doesn't synchronize across cores and
    // it's easy to get duplicates. Use an own counter since
    // seastar::this_shard_id() may not yet be available.
    static std::atomic<int64_t> thread_id_counter;
    static thread_local int64_t thread_id = thread_id_counter.fetch_add(1); // Mix in the core number into Organisational Unique
    // Identifier, to leave NIC intact, assuming tampering
    // with NIC is more likely to lead to collision within
    // a single network than tampering with OUI.
    //
    // Make sure the result fits into 6 bytes reserved for MAC
    // (adding the core number may overflow the original
    // value).
    return (node + (thread_id << 32)) & 0xFFFF'FFFF'FFFFL;
}
static int64_t make_random_node()
{
    static int64_t random_node = []
    {         int64_t node = 0;         std::random_device rndgen;         do {             auto i = rndgen();             node = i;             if (sizeof(i) < sizeof(node)) {                 node = (node << 32) + rndgen();             }         } while (node == 0); // 0 may mean "node is uninitialized", so avoid it.
        return node; }();
    return random_node;
}
static int64_t make_node()
{
    static int64_t global_node = 3;
    return make_thread_local_node(global_node);
}
static int64_t make_clock_seq_and_node()
{ // The original Java code did this, shuffling the number of millis
    // since the epoch, and taking 14 bits of it. We don't do exactly
    // the same, but the idea is the same.
    // long clock = new Random(System.currentTimeMillis()).nextLong();
    unsigned int seed = std::chrono::system_clock::now().time_since_epoch().count();
    int clock = rand_r(&seed);
    long lsb = 0;
    lsb |= 0x8000000000000000L;                 // variant (2 bits)
    lsb |= (clock & 0x0000000000003FFFL) << 48; // clock sequence (14 bits)
    lsb |= make_node();                         // 6 bytes
    return lsb;
}
const thread_local int64_t UUID_gen::spoof_node = make_thread_local_node(make_random_node());
const thread_local int64_t UUID_gen::clock_seq_and_node = make_clock_seq_and_node();
thread_local UUID_gen UUID_gen::_instance;
}
// namespace utils
namespace utils
{
static logging::logger filterlog("bloom_filter");
}
namespace utils
{
namespace filter
{
    thread_local bloom_filter::stats bloom_filter::_shard_stats;
    template <typename Func>
    void for_each_index(hashed_key hk, int count, int64_t max, filter_format format, Func &&func);
}
}
namespace utils
{
namespace bloom_calculations
{
    const std::vector<std::vector<double>> probs = {
        {1.0},      // dummy row representing 0 buckets per element
        {1.0, 1.0}, // dummy row representing 1 buckets per element
        {1.0, 0.393, 0.400},
        {1.0, 0.283, 0.237, 0.253},
        {1.0, 0.221, 0.155, 0.147, 0.160},
        {1.0, 0.181, 0.109, 0.092, 0.092, 0.101}, // 5
        {1.0, 0.154, 0.0804, 0.0609, 0.0561, 0.0578, 0.0638},
        {1.0, 0.133, 0.0618, 0.0423, 0.0359, 0.0347, 0.0364},
        {1.0, 0.118, 0.0489, 0.0306, 0.024, 0.0217, 0.0216, 0.0229},
        {1.0, 0.105, 0.0397, 0.0228, 0.0166, 0.0141, 0.0133, 0.0135, 0.0145},
        {1.0, 0.0952, 0.0329, 0.0174, 0.0118, 0.00943, 0.00844, 0.00819, 0.00846}, // 10
        {1.0, 0.0869, 0.0276, 0.0136, 0.00864, 0.0065, 0.00552, 0.00513, 0.00509},
        {1.0, 0.08, 0.0236, 0.0108, 0.00646, 0.00459, 0.00371, 0.00329, 0.00314},
        {1.0, 0.074, 0.0203, 0.00875, 0.00492, 0.00332, 0.00255, 0.00217, 0.00199, 0.00194},
        {1.0, 0.0689, 0.0177, 0.00718, 0.00381, 0.00244, 0.00179, 0.00146, 0.00129, 0.00121, 0.0012},
        {1.0, 0.0645, 0.0156, 0.00596, 0.003, 0.00183, 0.00128, 0.001, 0.000852, 0.000775, 0.000744}, // 15
        {1.0, 0.0606, 0.0138, 0.005, 0.00239, 0.00139, 0.000935, 0.000702, 0.000574, 0.000505, 0.00047, 0.000459},
        {1.0, 0.0571, 0.0123, 0.00423, 0.00193, 0.00107, 0.000692, 0.000499, 0.000394, 0.000335, 0.000302, 0.000287, 0.000284},
        {1.0, 0.054, 0.0111, 0.00362, 0.00158, 0.000839, 0.000519, 0.00036, 0.000275, 0.000226, 0.000198, 0.000183, 0.000176},
        {1.0, 0.0513, 0.00998, 0.00312, 0.0013, 0.000663, 0.000394, 0.000264, 0.000194, 0.000155, 0.000132, 0.000118, 0.000111, 0.000109},
        {1.0, 0.0488, 0.00906, 0.0027, 0.00108, 0.00053, 0.000303, 0.000196, 0.00014, 0.000108, 8.89e-05, 7.77e-05, 7.12e-05, 6.79e-05, 6.71e-05} // 20
    };                                                                                                                                            // the first column is a dummy column representing K=0.
    static std::vector<int> initialize_opt_k()
    {
        std::vector<int> arr;
        arr.resize(probs.size());
        for (auto i = 0; i < int(probs.size()); i++)
        {
        double min = std::numeric_limits<double>::max();
        auto &prob = probs[i];
        for (auto j = 0; j < int(prob.size()); j++)
        {
            if (prob[j] < min)
            {
                min = prob[j];
                arr[i] = std::max(min_k, j);
            }
        }
        }
        return arr;
    }
    const std::vector<int> opt_k_per_buckets = initialize_opt_k();
}
}
class utils::file_lock::impl
{
public:
impl(fs::path path) : _path(std::move(path)), _fd(file_desc::open(_path.native(), O_RDWR | O_CREAT | O_CLOEXEC, S_IRWXU))
{
    if (::lockf(_fd.get(), F_TLOCK, 0) != 0)
    {
        throw std::system_error(errno, std::system_category(), "Could not acquire lock: " + _path.native());
    }
}


fs::path _path;
file_desc _fd;
};
future<utils::file_lock> utils::file_lock::acquire(fs::path path)
{ // meh. not really any future stuff here. but pretend, for the
// day when a future version of lock etc is added.
try
{
    return make_ready_future<file_lock>(file_lock(path));
}
catch (...)
{
    return make_exception_future<utils::file_lock>(std::current_exception());
}
}
namespace utils
{
void dynamic_bitset::set(size_t n) noexcept
{
    for (auto &level : _bits)
    {
        auto idx = n / bits_per_int;
        auto old = level[idx];
        level[idx] |= int_type(1u) << (n % bits_per_int);
        if (old)
        {
        break;
        }
        n = idx; // prepare for next level
    }
}
void dynamic_bitset::clear(size_t n) noexcept
{
    for (auto &level : _bits)
    {
        auto idx = n / bits_per_int;
        auto old = level[idx];
        level[idx] &= ~(int_type(1u) << (n % bits_per_int));
        if (!old || level[idx])
        {
        break;
        }
        n = idx; // prepare for next level
    }
}
size_t dynamic_bitset::find_first_set() const noexcept
{
    size_t pos = 0;
    for (auto &vv : _bits | boost::adaptors::reversed)
    {
        auto v = vv[pos];
        pos *= bits_per_int;
        if (v)
        {
        pos += count_trailing_zeros(v);
        }
        else
        {
        return npos;
        }
    }
    return pos;
}
size_t dynamic_bitset::find_next_set(size_t n) const noexcept
{
    ++n;
    unsigned level = 0;
    unsigned nlevels = _bits.size(); // Climb levels until we find a set bit in the right place
    while (level != nlevels)
    {
        if (n >= _bits_count)
        {
        return npos;
        }
        auto v = _bits[level][level_idx(level, n)];
        v &= ~mask_lower_bits(level_remainder(level, n));
        if (v)
        {
        break;
        }
        ++level;
        n = align_up(n, size_t(1) << (level_shift * level));
    }
    if (level == nlevels)
    {
        return npos;
    } // Descend levels until we reach level 0
    do
    {
        auto v = _bits[level][level_idx(level, n)];
        v &= ~mask_lower_bits(level_remainder(level, n));
        n = align_down(n, size_t(1) << (level_shift * (level + 1)));
        n += count_trailing_zeros(v) << (level_shift * level);
    } while (level-- != 0);
    return n;
}
size_t dynamic_bitset::find_last_set() const noexcept
{
    size_t pos = 0;
    for (auto &vv : _bits | boost::adaptors::reversed)
    {
        auto v = vv[pos];
        pos *= bits_per_int;
        if (v)
        {
        pos += bits_per_int - 1 - count_leading_zeros(v);
        }
        else
        {
        return npos;
        }
    }
    return pos;
}
dynamic_bitset::dynamic_bitset(size_t nr_bits) : _bits_count(nr_bits)
{
    auto div_ceil = [](size_t num, size_t den)
    { return (num + den - 1) / den; }; // 1-64: 1 level
    // 65-4096: 2 levels
    // 4097-262144: 3 levels
    // etc.
    unsigned nr_levels = div_ceil(log2ceil(align_up(nr_bits, size_t(bits_per_int))), level_shift);
    _bits.resize(nr_levels);
    size_t level_bits = nr_bits;
    for (unsigned level = 0; level != nr_levels; ++level)
    {
        auto level_words = align_up(level_bits, bits_per_int) / bits_per_int;
        _bits[level].resize(level_words);
        level_bits = level_words; // for next iteration
    }
}
}
std::unique_ptr<bytes_view::value_type[]> managed_bytes::do_linearize_pure() const
{
auto b = _u.ptr;
auto data = std::unique_ptr<bytes_view::value_type[]>(new bytes_view::value_type[b->size]);
auto e = data.get();
while (b)
{
    e = std::copy_n(b->data, b->frag_size, e);
    b = b->next;
}
return data;
}
bool should_stop_on_system_error(const std::system_error &e)
{
if (e.code().category() == std::system_category())
{ // Whitelist of errors that don't require us to stop the server:
    switch (e.code().value())
    {
    case EEXIST:
    case ENOENT:
        return false;
    default:
        break;
    }
}
return true;
}
void *utils::internal::try_catch_dynamic(std::exception_ptr &eptr, const std::type_info *catch_type) noexcept
{ // In both libstdc++ and libc++, exception_ptr has just one field
// which is a pointer to the exception data
void *raw_ptr = reinterpret_cast<void *&>(eptr);
const std::type_info *ex_type = utils::abi::get_cxa_exception(raw_ptr)->exceptionType; // __do_catch can return true and set raw_ptr to nullptr, but only in the case
// when catch_type is a pointer and a nullptr is thrown. try_catch_dynamic
// doesn't work with catching pointers.
if (catch_type->__do_catch(ex_type, &raw_ptr, 1))
{
    return raw_ptr;
}
return nullptr;
}
namespace bpo = boost::program_options;


thread_local unsigned utils::config_file::s_shard_id = 0;

void utils::config_file::add(std::initializer_list<cfg_ref> cfgs) { _cfgs.insert(_cfgs.end(), cfgs.begin(), cfgs.end()); }
void utils::config_file::add(const std::vector<cfg_ref> &cfgs) { _cfgs.insert(_cfgs.end(), cfgs.begin(), cfgs.end()); }
bpo::options_description utils::config_file::get_options_description()
{
bpo::options_description opts("");
return get_options_description(opts);
}
bpo::options_description utils::config_file::get_options_description(boost::program_options::options_description opts)
{
auto init = opts.add_options();
add_options(init);
return opts;
}
bpo::options_description_easy_init &utils::config_file::add_options(bpo::options_description_easy_init &init)
{
for (config_src &src : _cfgs)
{
    if (src.status() == value_status::Used)
    {
        src.add_command_line_option(init);
    }
}
return init;
}
void utils::config_file::read_from_yaml(const sstring &yaml, error_handler h) { read_from_yaml(yaml.c_str(), std::move(h)); }
void utils::config_file::read_from_yaml(const char *yaml, error_handler h)
{
std::unordered_map<sstring, cfg_ref> values;
if (!h)
{
    h = [](auto &opt, auto &msg, auto)
    { throw std::invalid_argument(msg + " : " + opt); };
}
auto doc = YAML::Load(yaml);
for (auto node : doc)
{
    auto label = node.first.as<sstring>();
    auto i = std::find_if(_cfgs.begin(), _cfgs.end(), [&label](const config_src &cfg)
                          { return cfg.matches(label); });
    if (i == _cfgs.end())
    {
        h(label, "Unknown option", std::nullopt);
        continue;
    }
    config_src &cfg = *i;
    if (cfg.source() > config_source::SettingsFile)
    { // already set
        continue;
    }
    switch (cfg.status())
    {
    case value_status::Invalid:
        h(label, "Option is not applicable", cfg.status());
        continue;
    case value_status::Unused:
    default:
        break;
    }
    if (node.second.IsNull())
    {
        continue;
    } // Still, a syntax error is an error warning, not a fail
    try
    {
        cfg.set_value(node.second);
    }
    catch (std::exception &e)
    {
        h(label, e.what(), cfg.status());
    }
    catch (...)
    {
        h(label, "Could not convert value", cfg.status());
    }
}
}
utils::config_file::configs utils::config_file::set_values() const
{
return boost::copy_range<configs>(_cfgs | boost::adaptors::filtered([](const config_src &cfg)
                                                                    { return cfg.status() > value_status::Used || cfg.source() > config_source::None; }));
}
utils::config_file::configs utils::config_file::unset_values() const
{
configs res;
for (config_src &cfg : _cfgs)
{
    if (cfg.status() > value_status::Used)
    {
        continue;
    }
    if (cfg.source() > config_source::None)
    {
        continue;
    }
    res.emplace_back(cfg);
}
return res;
}
future<> utils::config_file::read_from_file(file f, error_handler h) { return make_ready_future<>(); }
future<> utils::config_file::read_from_file(const sstring &filename, error_handler h)
{
return open_file_dma(filename, open_flags::ro).then([this, h](file f)
                                                    { return read_from_file(std::move(f), h); });
}
future<> utils::config_file::broadcast_to_all_shards()
{
return async([this]
             {         if (_per_shard_values.size() != smp::count) {             _per_shard_values.resize(smp::count);             smp::invoke_on_all([this] {                 auto cpu = this_shard_id();                 if (cpu != 0) {                     s_shard_id = cpu;                     auto& shard_0_values = _per_shard_values[0];                     auto nr_values = shard_0_values.size();                     auto& this_shard_values = _per_shard_values[cpu];                     this_shard_values.resize(nr_values);                     for (size_t i = 0; i != nr_values; ++i) {                         this_shard_values[i] = shard_0_values[i]->clone();                     }                 }             }).get();         } else {             smp::invoke_on_all([this] {                 if (s_shard_id != 0) {                     auto& shard_0_values = _per_shard_values[0];                     auto nr_values = shard_0_values.size();                     auto& this_shard_values = _per_shard_values[s_shard_id];                     for (size_t i = 0; i != nr_values; ++i) {                         this_shard_values[i]->update_from(shard_0_values[i].get());                     }                 }             }).get();         }         // #4713
        // We can have values retained that are not pointing to
        // our storage (extension config). Need to broadcast
        // these configs as well.
        std::set<config_file *> files;         for (config_src& v : _cfgs) {             auto f = v.get_config_file();             if (f != this) {                 files.insert(f);             }         }         for (auto* f : files) {             f->broadcast_to_all_shards().get();         } });
}
sstring utils::config_file::config_src::source_name() const noexcept
{
auto src = source();
switch (src)
{
case utils::config_file::config_source::None:
    return "default";
case utils::config_file::config_source::SettingsFile:
    return "config";
case utils::config_file::config_source::CommandLine:
    return "cli";
case utils::config_file::config_source::Internal:
    return "internal";
case utils::config_file::config_source::CQL:
    return "cql";
case utils::config_file::config_source::API:
    return "api";
}
__builtin_unreachable();
}
using u32 = uint32_t;
using u64 = uint64_t;


static u32 mul_by_x_pow_mul8(u32 p, u64 e)
;
template <int bits>
static constexpr std::array<uint32_t, bits> make_crc32_power_table()
{
std::array<uint32_t, bits> pows;
pows[0] = 0x00800000; // x^8
for (int i = 1; i < bits; ++i)
{ //   x^(2*N)          mod G(x)
    // = (x^N)*(x^N)      mod G(x)
    // = (x^N mod G(x))^2 mod G(x)
    pows[i] = crc32_fold_barrett_u64(clmul(pows[i - 1], pows[i - 1]) << 1);
}
return pows;
}
static constexpr std::array<uint32_t, 256> make_crc32_table(int base, int radix_bits, uint32_t one, std::array<uint32_t, 32> pows)
{
std::array<uint32_t, 256> table;
for (int i = 0; i < (1 << radix_bits); ++i)
{
    uint32_t product = one;
    for (int j = 0; j < radix_bits; ++j)
    {
        if (i & (1 << j))
        {
        product = crc32_fold_barrett_u64(clmul(product, pows[base + j]) << 1);
        }
    }
    table[i] = product;
}
return table;
}
static constexpr int bits = 32;
static constexpr int radix_bits = 8;
static constexpr uint32_t one = 0x80000000;
// x^0
static constexpr auto pows = make_crc32_power_table<bits>();
// pows[i] = x^(2^i*8) mod G(x)
constinit std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_0 = make_crc32_table(0, radix_bits, one, pows);
constinit std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_8 = make_crc32_table(8, radix_bits, one, pows);
constinit std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_16 = make_crc32_table(16, radix_bits, one, pows);
constinit std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_24 = make_crc32_table(24, radix_bits, one, pows);
namespace utils
{
 size_t iovec_len(const std::vector<iovec> &iov)
;
}
namespace s3
{
static logging::logger s3l("s3");

class dns_connection_factory : public http::experimental::connection_factory
{
protected:
    std::string _host;
    int _port;
    struct state
    {
        bool initialized = false;
        socket_address addr;
        ::shared_ptr<tls::certificate_credentials> creds;
    };
    lw_shared_ptr<state> _state;
    shared_future<> _done;
    
public:
}; // TODO: possibly move this to seastar's http subsystem.
class client::upload_sink : public data_sink_impl
{ // "Each part must be at least 5 MB in size, except the last part."
    // https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPart.html
    static constexpr size_t minimum_part_size = 5 << 20;
    static constexpr int flush_concurrency = 3;
    shared_ptr<client> _client;
    http::experimental::client &_http;
    sstring _object_name;
    memory_data_sink_buffers _bufs;
    sstring _upload_id;
    utils::chunked_vector<sstring> _part_etags;
    semaphore _flush_sem{flush_concurrency};
public:
};
static constexpr std::string_view multipart_upload_complete_header = "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n"
                                                                     "<CompleteMultipartUpload xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">";
static constexpr std::string_view multipart_upload_complete_entry = "<Part><ETag>{}</ETag><PartNumber>{}</PartNumber></Part>";
static constexpr std::string_view multipart_upload_complete_trailer = "</CompleteMultipartUpload>";
class client::readable_file : public file_impl
{
    shared_ptr<client> _client;
    http::experimental::client &_http;
    sstring _object_name;
    [[noreturn]] void unsupported() { throw_with_backtrace<std::logic_error>("unsupported operation on s3 readable file"); }
public:
    readable_file(shared_ptr<client> cln, sstring object_name) : _client(std::move(cln)), _http(_client->_http), _object_name(std::move(object_name)) {}
    virtual future<size_t> write_dma(uint64_t pos, const void *buffer, size_t len, const io_priority_class &pc) override { unsupported(); }
    virtual future<size_t> write_dma(uint64_t pos, std::vector<iovec> iov, const io_priority_class &pc) override { unsupported(); }
    virtual future<> truncate(uint64_t length) override { unsupported(); }
    virtual subscription<directory_entry> list_directory(std::function<future<>(directory_entry de)> next) override { unsupported(); }
    virtual future<> flush(void) override { return make_ready_future<>(); }
    virtual future<> allocate(uint64_t position, uint64_t length) override { return make_ready_future<>(); }
    virtual future<> discard(uint64_t offset, uint64_t length) override { return make_ready_future<>(); }
    class readable_file_handle_impl final : public file_handle_impl
    {
        client::handle _h;
        sstring _object_name;
    public:
        readable_file_handle_impl(client::handle h, sstring object_name) : _h(std::move(h)), _object_name(std::move(object_name)) {}
        virtual std::unique_ptr<file_handle_impl> clone() const override { return std::make_unique<readable_file_handle_impl>(_h, _object_name); }
        virtual shared_ptr<file_impl> to_file() && override { return make_shared<readable_file>(std::move(_h).to_client(), std::move(_object_name)); }
    };
    virtual std::unique_ptr<file_handle_impl> dup() override;
    virtual future<uint64_t> size(void) override;
    virtual future<struct stat> stat(void) override;
    virtual future<size_t> read_dma(uint64_t pos, void *buffer, size_t len, const io_priority_class &pc) override;
    virtual future<size_t> read_dma(uint64_t pos, std::vector<iovec> iov, const io_priority_class &pc) override;
    virtual future<temporary_buffer<uint8_t>> dma_read_bulk(uint64_t offset, size_t range_size, const io_priority_class &pc) override;
    virtual future<> close() override;
};
}
// s3 namespace
namespace dht
{
static logging::logger logger("i_partitioner");
sharder::sharder(unsigned shard_count, unsigned sharding_ignore_msb_bits) : _shard_count(shard_count) // if one shard, ignore sharding_ignore_msb_bits as they will just cause needless
                                                                            // range breaks
                                                                            ,
                                                                            _sharding_ignore_msb_bits(shard_count > 1 ? sharding_ignore_msb_bits : 0), _shard_start(init_zero_based_shard_start(_shard_count, _sharding_ignore_msb_bits))
{
}
unsigned sharder::shard_of(const token &t) const { return dht::shard_of(_shard_count, _sharding_ignore_msb_bits, t); }
token sharder::token_for_next_shard(const token &t, shard_id shard, unsigned spans) const { return dht::token_for_next_shard(_shard_start, _shard_count, _sharding_ignore_msb_bits, t, shard, spans); }
std::unique_ptr<dht::i_partitioner> make_partitioner(sstring partitioner_name)
{
    try
    {
        return create_object<i_partitioner>(partitioner_name);
    }
    catch (std::exception &e)
    {
        auto supported_partitioners = fmt::join(class_registry<i_partitioner>::classes() | boost::adaptors::map_keys, ", ");
        throw std::runtime_error(format("Partitioner {} is not supported, supported partitioners = {{ {} }} : {}", partitioner_name, supported_partitioners, e.what()));
    }
}
future<utils::chunked_vector<partition_range>> split_range_to_single_shard(const schema &s, const partition_range &pr, shard_id shard);
}
namespace dht
{
using uint128_t = unsigned __int128;
inline int64_t long_token(const token &t)
{
    if (t.is_minimum() || t.is_maximum())
    {
        return std::numeric_limits<int64_t>::min();
    }
    return t._data;
}
static const token min_token{token::kind::before_all_keys, 0};
static const token max_token{token::kind::after_all_keys, 0};
const token &maximum_token() noexcept { return max_token; }
static float ratio_helper(int64_t a, int64_t b);
uint64_t unbias(const token &t) { return uint64_t(long_token(t)) + uint64_t(std::numeric_limits<int64_t>::min()); }
token bias(uint64_t n) { return token(token::kind::key, n - uint64_t(std::numeric_limits<int64_t>::min())); }
inline unsigned zero_based_shard_of(uint64_t token, unsigned shards, unsigned sharding_ignore_msb_bits)
{                                       // This is the master function, the inverses have to match it wrt. rounding errors.
    token <<= sharding_ignore_msb_bits; // Treat "token" as a fraction in the interval [0, 1); compute:
    //    shard = floor((0.token) * shards)
    return (uint128_t(token) * shards) >> 64;
}
std::vector<uint64_t> init_zero_based_shard_start(unsigned shards, unsigned sharding_ignore_msb_bits)
{ // computes the inverse of zero_based_shard_of(). ret[s] will return the smallest token that belongs to s
    if (shards == 1)
    { // Avoid the while loops below getting confused finding the "edge" between two nonexistent shards
        return std::vector<uint64_t>(1, uint64_t(0));
    }
    auto ret = std::vector<uint64_t>(shards);
    for (auto s : boost::irange<unsigned>(0, shards))
    {
        uint64_t token = (uint128_t(s) << 64) / shards;
        token >>= sharding_ignore_msb_bits; // leftmost bits are ignored by zero_based_shard_of
        // token is the start of the next shard, and can be slightly before due to rounding errors; adjust
        while (zero_based_shard_of(token, shards, sharding_ignore_msb_bits) != s)
        {
        ++token;
        }
        ret[s] = token;
    }
    return ret;
}
unsigned shard_of(unsigned shard_count, unsigned sharding_ignore_msb_bits, const token &t)
{
    switch (t._kind)
    {
    case token::kind::before_all_keys:
        return token::shard_of_minimum_token();
    case token::kind::after_all_keys:
        return shard_count - 1;
    case token::kind::key:
        uint64_t adjusted = unbias(t);
        return zero_based_shard_of(adjusted, shard_count, sharding_ignore_msb_bits);
    }
    abort();
}
token token_for_next_shard(const std::vector<uint64_t> &shard_start, unsigned shard_count, unsigned sharding_ignore_msb_bits, const token &t, shard_id shard, unsigned spans)
{
    uint64_t n = 0;
    switch (t._kind)
    {
    case token::kind::before_all_keys:
        break;
    case token::kind::after_all_keys:
        return maximum_token();
    case token::kind::key:
        n = unbias(t);
        break;
    }
    auto s = zero_based_shard_of(n, shard_count, sharding_ignore_msb_bits);
    if (!sharding_ignore_msb_bits)
    { // This ought to be the same as the else branch, but avoids shifts by 64
        n = shard_start[shard];
        if (spans > 1 || shard <= s)
        {
        return maximum_token();
        }
    }
    else
    {
        auto left_part = n >> (64 - sharding_ignore_msb_bits);
        left_part += spans - unsigned(shard > s);
        if (left_part >= (1u << sharding_ignore_msb_bits))
        {
        return maximum_token();
        }
        left_part <<= (64 - sharding_ignore_msb_bits);
        auto right_part = shard_start[shard];
        n = left_part | right_part;
    }
    return bias(n);
}
static dht::token find_first_token_for_shard_in_not_wrap_around_range(const dht::sharder &sharder, dht::token start, dht::token end, size_t shard_idx);
}
// namespace dht
namespace dht
{ // Note: Cassandra has a special case where for an empty key it returns
// minimum_token() instead of 0 (the naturally-calculated hash function for
// an empty string). Their thinking was that empty partition keys are not
// allowed anyway. However, they *are* allowed in materialized views, so the
// empty-key partition should get a real token, not an invalid token, so
// we dropped this special case. Since we don't support migrating sstables of
// materialized-views from Cassandra, this Cassandra-Scylla incompatiblity
// will not cause problems in practice.
// Note that get_token(const schema& s, partition_key_view key) below must
// use exactly the same algorithm as this function.
token murmur3_partitioner::get_token(bytes_view key) const
{
    std::array<uint64_t, 2> hash;
    utils::murmur_hash::hash3_x64_128(key, 0, hash);
    return get_token(hash[0]);
}
token murmur3_partitioner::get_token(uint64_t value) const { return token(token::kind::key, value); }
token murmur3_partitioner::get_token(const sstables::key_view &key) const
{
    return key.with_linearized([&](bytes_view v)
                               { return get_token(v); });
}
token murmur3_partitioner::get_token(const schema &s, partition_key_view key) const
{
    std::array<uint64_t, 2> hash;
    auto &&legacy = key.legacy_form(s);
    utils::murmur_hash::hash3_x64_128(legacy.begin(), legacy.size(), 0, hash);
    return get_token(hash[0]);
}
using registry = class_registrator<i_partitioner, murmur3_partitioner>;
static registry registrator("org.apache.cassandra.dht.Murmur3Partitioner");
static registry registrator_short_name("Murmur3Partitioner");
}
static logging::logger blogger("boot_strapper");
namespace dht
{
}
// namespace dht
namespace dht
{
using inet_address = gms::inet_address; // Must be called from a seastar thread
std::unordered_map<dht::token_range, std::vector<inet_address>> range_streamer::get_all_ranges_with_sources_for(const sstring &keyspace_name, locator::vnode_effective_replication_map_ptr erm, dht::token_range_vector desired_ranges)
{
    logger.debug("{} ks={}", __func__, keyspace_name);
    auto range_addresses = erm->get_range_addresses().get0();
    logger.debug("keyspace={}, desired_ranges.size={}, range_addresses.size={}", keyspace_name, desired_ranges.size(), range_addresses.size());
    std::unordered_map<dht::token_range, std::vector<inet_address>> range_sources;
    for (auto &desired_range : desired_ranges)
    {
        auto found = false;
        for (auto &x : range_addresses)
        {
        if (need_preempt())
        {
            seastar::thread::yield();
        }
        const range<token> &src_range = x.first;
        if (src_range.contains(desired_range, dht::operator<=>))
        {
            inet_address_vector_replica_set preferred(x.second.begin(), x.second.end());
            get_token_metadata().get_topology().sort_by_proximity(_address, preferred);
            for (inet_address &p : preferred)
            {
                range_sources[desired_range].push_back(p);
            }
            found = true;
        }
        }
        if (!found)
        {
        throw std::runtime_error(format("No sources found for {}", desired_range));
        }
    }
    return range_sources;
} // Must be called from a seastar thread
std::unordered_map<dht::token_range, std::vector<inet_address>> range_streamer::get_all_ranges_with_strict_sources_for(const sstring &keyspace_name, locator::vnode_effective_replication_map_ptr erm, dht::token_range_vector desired_ranges, gms::gossiper &gossiper)
{
    logger.debug("{} ks={}", __func__, keyspace_name);
    assert(_tokens.empty() == false);
    auto &strat = erm->get_replication_strategy(); // Active ranges
    auto metadata_clone = get_token_metadata().clone_only_token_map().get0();
    auto range_addresses = strat.get_range_addresses(metadata_clone).get0(); // Pending ranges
    metadata_clone.update_topology(_address, _dr);
    metadata_clone.update_normal_tokens(_tokens, _address).get();
    auto pending_range_addresses = strat.get_range_addresses(metadata_clone).get0();
    metadata_clone.clear_gently().get(); // Collects the source that will have its range moved to the new node
    std::unordered_map<dht::token_range, std::vector<inet_address>> range_sources;
    logger.debug("keyspace={}, desired_ranges.size={}, range_addresses.size={}", keyspace_name, desired_ranges.size(), range_addresses.size());
    for (auto &desired_range : desired_ranges)
    {
        for (auto &x : range_addresses)
        {
        const range<token> &src_range = x.first;
        if (need_preempt())
        {
            seastar::thread::yield();
        }
        if (src_range.contains(desired_range, dht::operator<=>))
        {
            std::vector<inet_address> old_endpoints(x.second.begin(), x.second.end());
            auto it = pending_range_addresses.find(desired_range);
            if (it == pending_range_addresses.end())
            {
                throw std::runtime_error(format("Can not find desired_range = {} in pending_range_addresses", desired_range));
            }
            std::unordered_set<inet_address> new_endpoints(it->second.begin(), it->second.end()); // Due to CASSANDRA-5953 we can have a higher RF then we have endpoints.
            // So we need to be careful to only be strict when endpoints == RF
            if (old_endpoints.size() == erm->get_replication_factor())
            {
                std::erase_if(old_endpoints, [&new_endpoints](inet_address ep)
                              { return new_endpoints.contains(ep); });
                if (old_endpoints.size() != 1)
                {
                    throw std::runtime_error(format("Expected 1 endpoint but found {:d}", old_endpoints.size()));
                }
            }
            range_sources[desired_range].push_back(old_endpoints.front());
        }
        } // Validate
        auto it = range_sources.find(desired_range);
        if (it == range_sources.end())
        {
        throw std::runtime_error(format("No sources found for {}", desired_range));
        }
        if (it->second.size() != 1)
        {
        throw std::runtime_error(format("Multiple endpoints found for {}", desired_range));
        }
        inet_address source_ip = it->second.front();
        if (gossiper.is_enabled() && !gossiper.is_alive(source_ip))
        {
        throw std::runtime_error(format("A node required to move the data consistently is down ({}).  If you wish to move the data from a potentially inconsistent replica, restart the node with consistent_rangemovement=false", source_ip));
        }
    }
    return range_sources;
} // TODO: This is the legacy range_streamer interface, it is add_rx_ranges which adds rx ranges.
}
// dht
namespace unimplemented
{
static thread_local std::unordered_map<cause, bool> _warnings;
static logging::logger ulogger("unimplemented");
std::ostream &operator<<(std::ostream &out, cause c)
{
    switch (c)
    {
    case cause::INDEXES:
        return out << "INDEXES";
    case cause::LWT:
        return out << "LWT";
    case cause::PAGING:
        return out << "PAGING";
    case cause::AUTH:
        return out << "AUTH";
    case cause::PERMISSIONS:
        return out << "PERMISSIONS";
    case cause::TRIGGERS:
        return out << "TRIGGERS";
    case cause::COUNTERS:
        return out << "COUNTERS";
    case cause::METRICS:
        return out << "METRICS";
    case cause::MIGRATIONS:
        return out << "MIGRATIONS";
    case cause::GOSSIP:
        return out << "GOSSIP";
    case cause::TOKEN_RESTRICTION:
        return out << "TOKEN_RESTRICTION";
    case cause::LEGACY_COMPOSITE_KEYS:
        return out << "LEGACY_COMPOSITE_KEYS";
    case cause::COLLECTION_RANGE_TOMBSTONES:
        return out << "COLLECTION_RANGE_TOMBSTONES";
    case cause::RANGE_DELETES:
        return out << "RANGE_DELETES";
    case cause::THRIFT:
        return out << "THRIFT";
    case cause::VALIDATION:
        return out << "VALIDATION";
    case cause::REVERSED:
        return out << "REVERSED";
    case cause::COMPRESSION:
        return out << "COMPRESSION";
    case cause::NONATOMIC:
        return out << "NONATOMIC";
    case cause::CONSISTENCY:
        return out << "CONSISTENCY";
    case cause::HINT:
        return out << "HINT";
    case cause::SUPER:
        return out << "SUPER";
    case cause::WRAP_AROUND:
        return out << "WRAP_AROUND";
    case cause::STORAGE_SERVICE:
        return out << "STORAGE_SERVICE";
    case cause::API:
        return out << "API";
    case cause::SCHEMA_CHANGE:
        return out << "SCHEMA_CHANGE";
    case cause::MIXED_CF:
        return out << "MIXED_CF";
    case cause::SSTABLE_FORMAT_M:
        return out << "SSTABLE_FORMAT_M";
    }
    abort();
}
void fail(cause c) { throw std::runtime_error(format("Not implemented: {}", c)); }
}
namespace query
{
static logging::logger qlogger("query");
constexpr size_t result_memory_limiter::minimum_result_size;
constexpr size_t result_memory_limiter::maximum_result_size;
constexpr size_t result_memory_limiter::unlimited_result_size;
thread_local semaphore result_memory_tracker::_dummy{0};
const dht::partition_range full_partition_range = dht::partition_range::make_open_ended_both_sides();
const clustering_range full_clustering_range = clustering_range::make_open_ended_both_sides();
partition_slice::partition_slice(clustering_row_ranges row_ranges, query::column_id_vector static_columns, query::column_id_vector regular_columns, option_set options, std::unique_ptr<specific_ranges> specific_ranges, cql_serialization_format cql_format, uint32_t partition_row_limit_low_bits, uint32_t partition_row_limit_high_bits) : _row_ranges(std::move(row_ranges)), static_columns(std::move(static_columns)), regular_columns(std::move(regular_columns)), options(options), _specific_ranges(std::move(specific_ranges)), _partition_row_limit_low_bits(partition_row_limit_low_bits), _partition_row_limit_high_bits(partition_row_limit_high_bits) { cql_format.ensure_supported(); }
partition_slice::partition_slice(clustering_row_ranges row_ranges, query::column_id_vector static_columns, query::column_id_vector regular_columns, option_set options, std::unique_ptr<specific_ranges> specific_ranges, uint64_t partition_row_limit) : partition_slice(std::move(row_ranges), std::move(static_columns), std::move(regular_columns), options, std::move(specific_ranges), cql_serialization_format::latest(), static_cast<uint32_t>(partition_row_limit), static_cast<uint32_t>(partition_row_limit >> 32)) {}
partition_slice::partition_slice(partition_slice &&) = default;
 // Only needed because selection_statement::execute does copies of its read_command
                                                                                         // in the map-reduce op.



}
std::atomic<int64_t> clocks_offset;
partition_slice_builder::partition_slice_builder(const schema &schema) : _schema(schema)
{
_options.set<query::partition_slice::option::send_partition_key>();
_options.set<query::partition_slice::option::send_clustering_key>();
_options.set<query::partition_slice::option::send_timestamp>();
_options.set<query::partition_slice::option::send_expiry>();
}
query::partition_slice partition_slice_builder::build()
{
std::vector<query::clustering_range> ranges;
if (_row_ranges)
{
    ranges = std::move(*_row_ranges);
}
else
{
    ranges.emplace_back(query::clustering_range::make_open_ended_both_sides());
}
query::column_id_vector static_columns;
if (_static_columns)
{
    static_columns = std::move(*_static_columns);
}
else
{
    boost::range::push_back(static_columns, _schema.static_columns() | boost::adaptors::transformed(std::mem_fn(&column_definition::id)));
}
query::column_id_vector regular_columns;
if (_regular_columns)
{
    regular_columns = std::move(*_regular_columns);
}
else
{
    boost::range::push_back(regular_columns, _schema.regular_columns() | boost::adaptors::transformed(std::mem_fn(&column_definition::id)));
}
return {
    std::move(ranges),
    std::move(static_columns),
    std::move(regular_columns),
    std::move(_options),
    std::move(_specific_ranges),
    _partition_row_limit,
};
}
partition_slice_builder &partition_slice_builder::with_ranges(std::vector<query::clustering_range> ranges)
{
if (!_row_ranges)
{
    _row_ranges = std::move(ranges);
}
else
{
    for (auto &&r : ranges)
    {
        with_range(std::move(r));
    }
}
return *this;
}
partition_slice_builder &partition_slice_builder::mutate_ranges(std::function<void(std::vector<query::clustering_range> &)> func)
{
if (_row_ranges)
{
    func(*_row_ranges);
}
return *this;
}
partition_slice_builder &partition_slice_builder::mutate_specific_ranges(std::function<void(query::specific_ranges &)> func)
{
if (_specific_ranges)
{
    func(*_specific_ranges);
}
return *this;
}
partition_slice_builder &partition_slice_builder::with_no_regular_columns()
{
_regular_columns = query::column_id_vector();
return *this;
}
partition_slice_builder &partition_slice_builder::with_regular_column(bytes name)
{
if (!_regular_columns)
{
    _regular_columns = query::column_id_vector();
}
const column_definition *def = _schema.get_column_definition(name);
if (!def)
{
    throw std::runtime_error(format("No such column: {}", _schema.regular_column_name_type()->to_string(name)));
}
if (!def->is_regular())
{
    throw std::runtime_error(format("Column is not regular: {}", _schema.column_name_type(*def)->to_string(name)));
}
_regular_columns->push_back(def->id);
return *this;
}
partition_slice_builder &partition_slice_builder::with_no_static_columns()
{
_static_columns = query::column_id_vector();
return *this;
}
partition_slice_builder &partition_slice_builder::with_static_column(bytes name)
{
if (!_static_columns)
{
    _static_columns = query::column_id_vector();
}
const column_definition *def = _schema.get_column_definition(name);
if (!def)
{
    throw std::runtime_error(format("No such column: {}", utf8_type->to_string(name)));
}
if (!def->is_static())
{
    throw std::runtime_error(format("Column is not static: {}", utf8_type->to_string(name)));
}
_static_columns->push_back(def->id);
return *this;
}
partition_slice_builder &partition_slice_builder::reversed()
{
_options.set<query::partition_slice::option::reversed>();
return *this;
}
partition_slice_builder &partition_slice_builder::without_partition_key_columns()
{
_options.remove<query::partition_slice::option::send_partition_key>();
return *this;
}
partition_slice_builder &partition_slice_builder::without_clustering_key_columns()
{
_options.remove<query::partition_slice::option::send_clustering_key>();
return *this;
}
partition_slice_builder &partition_slice_builder::with_partition_row_limit(uint64_t limit)
{
_partition_row_limit = limit;
return *this;
}
thread_local disk_error_signal_type commit_error;
thread_local disk_error_signal_type general_disk_error;
thread_local disk_error_signal_type sstable_write_error;
thread_local io_error_handler commit_error_handler = default_io_error_handler(commit_error);
thread_local io_error_handler general_disk_error_handler = default_io_error_handler(general_disk_error);
thread_local io_error_handler sstable_write_error_handler = default_io_error_handler(sstable_write_error);
io_error_handler default_io_error_handler(disk_error_signal_type &signal)
{
return [&signal](std::exception_ptr eptr)
{         try {             std::rethrow_exception(eptr);         } catch(std::system_error& e) {             if (should_stop_on_system_error(e)) {                 signal();                 throw storage_io_error(e);             }         } };
}
static_assert(Hasher<hasher>);
static_assert(HasherReturningBytes<md5_hasher>);
static_assert(HasherReturningBytes<sha256_hasher>);
static_assert(HasherReturningBytes<xx_hasher>);
static_assert(SimpleHasher<simple_xx_hasher>);
template <typename T>
struct hasher_traits;
template <>
struct hasher_traits<md5_hasher>
{
using impl_type = CryptoPP::Weak::MD5;
};
template <>
struct hasher_traits<sha256_hasher>
{
using impl_type = CryptoPP::SHA256;
};
template <typename H>
concept HashUpdater = requires(typename hasher_traits<H>::impl_type &h, const CryptoPP::byte *ptr, size_t size) { // We need Update() not to throw, but it isn't marked noexcept
    // in CryptoPP source. We'll just hope it doesn't throw.
    {
        h.Update(ptr, size)
    } -> std::same_as<void>;
};
template <typename T, size_t size>
struct cryptopp_hasher<T, size>::impl
{
static_assert(HashUpdater<T>);
using impl_type = typename hasher_traits<T>::impl_type;
impl_type hash{};
void update(const char *ptr, size_t length) noexcept;
bytes finalize();
std::array<uint8_t, size> finalize_array();
};
template <typename T, size_t size>
cryptopp_hasher<T, size>::~cryptopp_hasher() = default;

template <typename T, size_t size>
cryptopp_hasher<T, size> &cryptopp_hasher<T, size>::operator=(cryptopp_hasher &&o) noexcept = default;
template class cryptopp_hasher<md5_hasher, 16>;
template class cryptopp_hasher<sha256_hasher, 32>;
using namespace std::chrono_literals;
namespace utils
{
namespace aws
{
    static hmac_sha256_digest hmac_sha256(std::string_view key, std::string_view msg);
    static hmac_sha256_digest get_signature_key(std::string_view key, std::string_view date_stamp, std::string_view region_name, std::string_view service_name);
    static std::string apply_sha256(std::string_view msg);
    static std::string apply_sha256(const std::vector<temporary_buffer<char>> &msg);
    void check_expiry(std::string_view signature_date);
    std::string get_signature(std::string_view access_key_id, std::string_view secret_access_key, std::string_view host, std::string_view canonical_uri, std::string_view method, std::optional<std::string_view> orig_datestamp, std::string_view signed_headers_str, const std::map<std::string_view, std::string_view> &signed_headers_map, const std::vector<temporary_buffer<char>> *body_content, std::string_view region, std::string_view service, std::string_view query_string)
    {
        auto amz_date_it = signed_headers_map.find("x-amz-date");
        if (amz_date_it == signed_headers_map.end())
        {
        throw std::runtime_error("X-Amz-Date header is mandatory for signature verification");
        }
        std::string_view amz_date = amz_date_it->second;
        std::string_view datestamp = amz_date.substr(0, 8);
        if (orig_datestamp)
        {
        check_expiry(amz_date);
        if (datestamp != *orig_datestamp)
        {
            throw std::runtime_error(format("X-Amz-Date date does not match the provided datestamp. Expected {}, got {}", *orig_datestamp, datestamp));
        }
        }
        std::stringstream canonical_headers;
        for (const auto &header : signed_headers_map)
        {
        canonical_headers << fmt::format("{}:{}", header.first, header.second) << '\n';
        }
        std::string payload_hash = body_content != nullptr ? apply_sha256(*body_content) : "UNSIGNED-PAYLOAD";
        std::string canonical_request = fmt::format("{}\n{}\n{}\n{}\n{}\n{}", method, canonical_uri, query_string, canonical_headers.str(), signed_headers_str, payload_hash);
        std::string_view algorithm = "AWS4-HMAC-SHA256";
        std::string credential_scope = fmt::format("{}/{}/{}/aws4_request", datestamp, region, service);
        std::string string_to_sign = fmt::format("{}\n{}\n{}\n{}", algorithm, amz_date, credential_scope, apply_sha256(canonical_request));
        hmac_sha256_digest signing_key = get_signature_key(secret_access_key, datestamp, region, service);
        hmac_sha256_digest signature = hmac_sha256(std::string_view(signing_key.data(), signing_key.size()), string_to_sign);
        return to_hex(bytes_view(reinterpret_cast<const int8_t *>(signature.data()), signature.size()));
    }
} // aws namespace
}
// utils namespace
namespace
{ //
// Helper for retrieving the counter based on knowing its type.
//
template <class Counter>
constexpr typename Counter::value_type &counter_ref(cql_duration &) noexcept;
template <>
constexpr months_counter::value_type &counter_ref<months_counter>(cql_duration &d) noexcept { return d.months; }
template <>
constexpr days_counter::value_type &counter_ref<days_counter>(cql_duration &d) noexcept { return d.days; }
template <>
constexpr nanoseconds_counter::value_type &counter_ref<nanoseconds_counter>(cql_duration &d) noexcept { return d.nanoseconds; } // Unit for a component of a duration. For example, years.
class duration_unit
{
public:
    using index_type = uint8_t;
    using common_counter_type = cql_duration::common_counter_type;
    virtual ~duration_unit() = default; // Units with larger indicies are greater. For example, "months" have a greater index than "days".
    virtual index_type index() const noexcept = 0;
    virtual const char *short_name() const noexcept = 0;
    virtual const char *long_name() const noexcept = 0;                                   // Increment the appropriate counter in the duration instance based on a count of this unit.
    virtual void increment_count(cql_duration &, common_counter_type) const noexcept = 0; // The remaining capacity (in terms of this unit) of the appropriate counter in the duration instance.
    virtual common_counter_type available_count(const cql_duration &) const noexcept = 0;
}; // `_index` is the assigned index of this unit.
// `Counter` is the counter type in the `cql_duration` instance that is used to store this unit.
// `_factor` is the conversion factor of one count of this unit to the corresponding count in `Counter`.
template <uint8_t _index, class Counter, cql_duration::common_counter_type _factor>
class duration_unit_impl : public duration_unit
{
public:
    static constexpr auto factor = _factor;
    index_type index() const noexcept override { return _index; }
    void increment_count(cql_duration &d, common_counter_type c) const noexcept override { counter_ref<Counter>(d) += (c * factor); }
    common_counter_type available_count(const cql_duration &d) const noexcept override
    {
        const auto limit = std::numeric_limits<typename Counter::value_type>::max();
        return {(limit - counter_ref<Counter>(const_cast<cql_duration &>(d))) / factor};
    }
};
struct nanosecond_unit final : public duration_unit_impl<0, nanoseconds_counter, 1>
{
    const char *short_name() const noexcept override { return "ns"; }
    const char *long_name() const noexcept override { return "nanoseconds"; }
} const nanosecond{};
struct microsecond_unit final : public duration_unit_impl<1, nanoseconds_counter, 1000>
{
    const char *short_name() const noexcept override { return "us"; }
    const char *long_name() const noexcept override { return "microseconds"; }
} const microsecond{};
struct millisecond_unit final : public duration_unit_impl<2, nanoseconds_counter, microsecond_unit::factor * 1000>
{
    const char *short_name() const noexcept override { return "ms"; }
    const char *long_name() const noexcept override { return "milliseconds"; }
} const millisecond{};
struct second_unit final : public duration_unit_impl<3, nanoseconds_counter, millisecond_unit::factor * 1000>
{
    const char *short_name() const noexcept override { return "s"; }
    const char *long_name() const noexcept override { return "seconds"; }
} const second{};
struct minute_unit final : public duration_unit_impl<4, nanoseconds_counter, second_unit::factor * 60>
{
    const char *short_name() const noexcept override { return "m"; }
    const char *long_name() const noexcept override { return "minutes"; }
} const minute{};
struct hour_unit final : public duration_unit_impl<5, nanoseconds_counter, minute_unit::factor * 60>
{
    const char *short_name() const noexcept override { return "h"; }
    const char *long_name() const noexcept override { return "hours"; }
} const hour{};
struct day_unit final : public duration_unit_impl<6, days_counter, 1>
{
    const char *short_name() const noexcept override { return "d"; }
    const char *long_name() const noexcept override { return "days"; }
} const day{};
struct week_unit final : public duration_unit_impl<7, days_counter, 7>
{
    const char *short_name() const noexcept override { return "w"; }
    const char *long_name() const noexcept override { return "weeks"; }
} const week{};
struct month_unit final : public duration_unit_impl<8, months_counter, 1>
{
    const char *short_name() const noexcept override { return "mo"; }
    const char *long_name() const noexcept override { return "months"; }
} const month{};
struct year_unit final : public duration_unit_impl<9, months_counter, 12>
{
    const char *short_name() const noexcept override { return "y"; }
    const char *long_name() const noexcept override { return "years"; }
} const year{};
const auto unit_table = std::unordered_map<std::string_view, std::reference_wrapper<const duration_unit>>{{year.short_name(), year}, {month.short_name(), month}, {week.short_name(), week}, {day.short_name(), day}, {hour.short_name(), hour}, {minute.short_name(), minute}, {second.short_name(), second}, {millisecond.short_name(), millisecond}, {microsecond.short_name(), microsecond}, {"µs", microsecond}, {nanosecond.short_name(), nanosecond}}; //
// Convenient helper to parse the indexed sub-expression from a match group as a duration counter.
//
// Throws `std::out_of_range` if a counter is out of range.
//
template <class Match, class Index = typename Match::size_type>
cql_duration::common_counter_type parse_count(const Match &m, Index group_index)
; //
// Build up a duration unit-by-unit.
//
// We support overflow detection on construction for convenience and compatibility with Cassandra.
//
// We maintain some additional state over a `cql_duration` in order to track the order in which components are added when
// parsing the standard format.
//
class duration_builder final
{
public:
    ;
private:
    const duration_unit *_current_unit{nullptr};
    cql_duration _duration{}; //
    // Throws `cql_duration_error` if the addition of a quantity of the designated unit would overflow one of the
    // counters.
    //
     //
    // Validate that an addition of a quantity of the designated unit is not out of order. We require that units are
    // added in decreasing size.
    //
    // This function also updates the last-observed unit for the next invocation.
    //
    // Throws `cql_duration_error` for order violations.
    //
}; //
// These functions assume no sign information ('-). That is left to the `cql_duration` constructor.
//
 // Parse a duration string without sign information assuming one of the supported formats.
}
std::ostream &operator<<(std::ostream &os, const cql_duration &d)
{
if ((d.months < 0) || (d.days < 0) || (d.nanoseconds < 0))
{
    os << '-';
} // If a non-zero integral component of the count can be expressed in `unit`, then append it to the stream with its
// unit.
//
// Returns the remaining count.
const auto append = [&os](cql_duration::common_counter_type count, auto &&unit)
{         const auto divider = unit.factor;         if ((count == 0) || (count < divider)) {             return count;         }         os << (count / divider) << unit.short_name();         return count % divider; };
const auto month_remainder = append(std::abs(d.months), year);
append(month_remainder, month);
append(std::abs(d.days), day);
auto nanosecond_remainder = append(std::abs(d.nanoseconds), hour);
nanosecond_remainder = append(nanosecond_remainder, minute);
nanosecond_remainder = append(nanosecond_remainder, second);
nanosecond_remainder = append(nanosecond_remainder, millisecond);
nanosecond_remainder = append(nanosecond_remainder, microsecond);
append(nanosecond_remainder, nanosecond);
return os;
}
static_assert(-1 == ~0, "Not a twos-complement architecture");
// Accounts for the case that all bits are zero.
static vint_size_type count_leading_zero_bits(uint64_t n) noexcept
{
if (n == 0)
{
    return vint_size_type(std::numeric_limits<uint64_t>::digits);
}
return vint_size_type(count_leading_zeros(n));
}
static constexpr uint64_t encode_zigzag(int64_t n) noexcept
{ // The right shift has to be arithmetic and not logical.
return (static_cast<uint64_t>(n) << 1) ^ static_cast<uint64_t>(n >> 63);
}
static constexpr int64_t decode_zigzag(uint64_t n) noexcept { return static_cast<int64_t>((n >> 1) ^ -(n & 1)); }
// Mask for extracting from the first byte the part that is not used for indicating the total number of bytes.
static uint64_t first_byte_value_mask(vint_size_type extra_bytes_size)
{ // Include the sentinel zero bit in the mask.
return uint64_t(0xff) >> extra_bytes_size;
}
vint_size_type signed_vint::serialize(int64_t value, bytes::iterator out) { return unsigned_vint::serialize(encode_zigzag(value), out); }
vint_size_type signed_vint::serialized_size(int64_t value) noexcept { return unsigned_vint::serialized_size(encode_zigzag(value)); }
int64_t signed_vint::deserialize(bytes_view v)
{
const auto un = unsigned_vint::deserialize(v);
return decode_zigzag(un);
}
vint_size_type signed_vint::serialized_size_from_first_byte(bytes::value_type first_byte) { return unsigned_vint::serialized_size_from_first_byte(first_byte); }
// The number of additional bytes that we need to read.
static vint_size_type count_extra_bytes(int8_t first_byte)
{ // Sign extension.
const int64_t v(first_byte);
return count_leading_zero_bits(static_cast<uint64_t>(~v)) - vint_size_type(64 - 8);
}
static void encode(uint64_t value, vint_size_type size, bytes::iterator out)
{
std::array<int8_t, 9> buffer({}); // `size` is always in the range [1, 9].
const auto extra_bytes_size = size - 1;
for (vint_size_type i = 0; i <= extra_bytes_size; ++i)
{
    buffer[extra_bytes_size - i] = static_cast<int8_t>(value & 0xff);
    value >>= 8;
}
buffer[0] |= ~first_byte_value_mask(extra_bytes_size);
std::copy_n(buffer.cbegin(), size, out);
}
vint_size_type unsigned_vint::serialize(uint64_t value, bytes::iterator out)
{
const auto size = serialized_size(value);
if (size == 1)
{
    *out = static_cast<int8_t>(value & 0xff);
    return 1;
}
encode(value, size, out);
return size;
}
vint_size_type unsigned_vint::serialized_size(uint64_t value) noexcept
{ // No need for the overhead of checking that all bits are zero.
//
// A signed quantity, to allow the case of `magnitude == 0` to result in a value of 9 below.
const auto magnitude = static_cast<int64_t>(count_leading_zeros(value | uint64_t(1)));
return vint_size_type(9) - vint_size_type((magnitude - 1) / 7);
}
uint64_t unsigned_vint::deserialize(bytes_view v)
{
auto src = v.data();
auto len = v.size();
const int8_t first_byte = *src; // No additional bytes, since the most significant bit is not set.
if (first_byte >= 0)
{
    return uint64_t(first_byte);
}
const auto extra_bytes_size = count_extra_bytes(first_byte); // Extract the bits not used for counting bytes.
auto result = uint64_t(first_byte) & first_byte_value_mask(extra_bytes_size);
for (vint_size_type index = 0; index < extra_bytes_size; ++index)
{
    result <<= 8;
    result |= (uint64_t(v[index + 1]) & uint64_t(0xff));
}
return result;
}
vint_size_type unsigned_vint::serialized_size_from_first_byte(bytes::value_type first_byte)
{
int8_t first_byte_casted = first_byte;
return 1 + (first_byte_casted >= 0 ? 0 : count_extra_bytes(first_byte_casted));
}
namespace utils
{
namespace utf8
{
    using namespace internal;
    struct codepoint_status
    {
        size_t bytes_validated;
        bool error;
        uint8_t more_bytes_needed;
    };
    static codepoint_status evaluate_codepoint(const uint8_t *data, size_t len); // 3x faster than boost utf_to_utf
    
} // namespace utf8
}
// namespace utils
namespace utils
{
namespace utf8
{ // Map high nibble of "First Byte" to legal character length minus 1
    // 0x00 ~ 0xBF --> 0
    // 0xC0 ~ 0xDF --> 1
    // 0xE0 ~ 0xEF --> 2
    // 0xF0 ~ 0xFF --> 3
    alignas(16) static const int8_t s_first_len_tbl[] = {
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        1,
        2,
        3,
    }; // Map "First Byte" to 8-th item of range table (0xC2 ~ 0xF4)
    alignas(16) static const int8_t s_first_range_tbl[] = {
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        8,
        8,
        8,
        8,
    }; // Range table, map range index to min and max values
    // Index 0    : 00 ~ 7F (First Byte, ascii)
    // Index 1,2,3: 80 ~ BF (Second, Third, Fourth Byte)
    // Index 4    : A0 ~ BF (Second Byte after E0)
    // Index 5    : 80 ~ 9F (Second Byte after ED)
    // Index 6    : 90 ~ BF (Second Byte after F0)
    // Index 7    : 80 ~ 8F (Second Byte after F4)
    // Index 8    : C2 ~ F4 (First Byte, non ascii)
    // Index 9~15 : illegal: i >= 127 && i <= -128
    alignas(16) static const int8_t s_range_min_tbl[] = {
        '\x00',
        '\x80',
        '\x80',
        '\x80',
        '\xA0',
        '\x80',
        '\x90',
        '\x80',
        '\xC2',
        '\x7F',
        '\x7F',
        '\x7F',
        '\x7F',
        '\x7F',
        '\x7F',
        '\x7F',
    };
    alignas(16) static const int8_t s_range_max_tbl[] = {
        '\x7F',
        '\xBF',
        '\xBF',
        '\xBF',
        '\xBF',
        '\x9F',
        '\xBF',
        '\x8F',
        '\xF4',
        '\x80',
        '\x80',
        '\x80',
        '\x80',
        '\x80',
        '\x80',
        '\x80',
    }; // Tables for fast handling of four special First Bytes(E0,ED,F0,F4), after
    // which the Second Byte are not 80~BF. It contains "range index adjustment".
    // +------------+---------------+------------------+----------------+
    // | First Byte | original range| range adjustment | adjusted range |
    // +------------+---------------+------------------+----------------+
    // | E0         | 2             | 2                | 4              |
    // +------------+---------------+------------------+----------------+
    // | ED         | 2             | 3                | 5              |
    // +------------+---------------+------------------+----------------+
    // | F0         | 3             | 3                | 6              |
    // +------------+---------------+------------------+----------------+
    // | F4         | 4             | 4                | 8              |
    // +------------+---------------+------------------+----------------+
    // index1 -> E0, index14 -> ED
    alignas(16) static const int8_t s_df_ee_tbl[] = {
        0,
        2,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        3,
        0,
    }; // index1 -> F0, index5 -> F4
    alignas(16) static const int8_t s_ef_fe_tbl[] = {
        0,
        3,
        0,
        0,
        0,
        4,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
    }; // 5x faster than naive method
} // namespace utf8
}
// namespace utils
namespace utils
{
namespace utf8
{
} // namespace utf8
}
// namespace utils
namespace utils
{
namespace ascii
{
} // namespace ascii
}
// namespace utils
namespace
{
using std::wstring;                                        /// Processes a new pattern character, extending re with the equivalent regex pattern.
 /// Returns a regex string matching the given LIKE pattern.
}
// anonymous namespace
class like_matcher::impl
{
bytes _pattern;
boost::u32regex _re; // Performs pattern matching.
public:

private:

};
like_matcher::~like_matcher() = default;
like_matcher::like_matcher(like_matcher &&that) noexcept = default;
namespace utils
{
logging::logger errinj_logger("debug_error_injection");
thread_local error_injection<false> error_injection<false>::_local;
}
// namespace utils
using namespace seastar;

namespace ser
{
logging::logger serlog("serializer");
}
// namespace ser
namespace utils
{
managed_bytes_view_opt buffer_view_to_managed_bytes_view(std::optional<ser::buffer_view<bytes_ostream::fragment_iterator>> bvo)
{
    if (!bvo)
    {
        return std::nullopt;
    }
    return buffer_view_to_managed_bytes_view(*bvo);
}
}
// namespace utils
namespace std
{
}
// namespace std
seastar::logger testlog("testlog");
namespace tests
{
namespace
{
}
}
// partitions must be sorted by decorated key
namespace
{ // Helper class for testing mutation_reader::fast_forward_to(dht::partition_range).
class partition_range_walker
{
    std::vector<dht::partition_range> _ranges;
    size_t _current_position = 0;
private:
public:
};
}
// Reproduces https://github.com/scylladb/scylla/issues/2733
struct mutation_sets
{
std::vector<std::vector<mutation>> equal;
std::vector<std::vector<mutation>> unequal;
};
static const mutation_sets &get_mutation_sets();
void for_each_mutation_pair(std::function<void(const mutation &, const mutation &, are_equal)> callback)
{
auto &&ms = get_mutation_sets();
for (auto &&mutations : ms.equal)
{
    auto i = mutations.begin();
    assert(i != mutations.end());
    const mutation &first = *i++;
    while (i != mutations.end())
    {
        callback(first, *i, are_equal::yes);
        ++i;
    }
}
for (auto &&mutations : ms.unequal)
{
    auto i = mutations.begin();
    assert(i != mutations.end());
    const mutation &first = *i++;
    while (i != mutations.end())
    {
        callback(first, *i, are_equal::no);
        ++i;
    }
}
}
void for_each_mutation(std::function<void(const mutation &)> callback)
{
auto &&ms = get_mutation_sets();
for (auto &&mutations : ms.equal)
{
    for (auto &&m : mutations)
    {
        callback(m);
    }
}
for (auto &&mutations : ms.unequal)
{
    for (auto &&m : mutations)
    {
        callback(m);
    }
}
}
bytes make_blob(size_t blob_size) { return tests::random::get_bytes(blob_size); };
class random_mutation_generator::impl
{
enum class timestamp_level
{
    partition_tombstone = 0,
    range_tombstone = 1,
    row_shadowable_tombstone = 2,
    row_tombstone = 3,
    row_marker_tombstone = 4,
    collection_tombstone = 5,
    cell_tombstone = 6,
    data = 7,
};
private:                              // Set to true in order to produce mutations which are easier to work with during debugging.
static const bool debuggable = false; // The "333" prefix is so that it's easily distinguishable from other numbers in the printout.
static const api::timestamp_type min_timestamp = debuggable ? 3330000 : ::api::min_timestamp;
friend class random_mutation_generator;
generate_counters _generate_counters;
local_shard_only _local_shard_only;
generate_uncompactable _uncompactable;
const size_t _external_blob_size = debuggable ? 4 : 128; // Should be enough to force use of external bytes storage
const size_t n_blobs = debuggable ? 32 : 1024;
const column_id column_count = debuggable ? 3 : 64;
std::mt19937 _gen;
schema_ptr _schema;
std::vector<bytes> _blobs;
std::uniform_int_distribution<size_t> _ck_index_dist{0, n_blobs - 1};
std::uniform_int_distribution<int> _bool_dist{0, 1};
std::uniform_int_distribution<int> _not_dummy_dist{0, 19};
std::uniform_int_distribution<int> _range_tombstone_dist{0, 29};
std::uniform_int_distribution<api::timestamp_type> _timestamp_dist{min_timestamp, min_timestamp + 2}; // Sequence number for mutation elements.
// Intended to be put as "deletion time".
// The "777" prefix is so that it's easily distinguishable from other numbers in the printout.
// Also makes it easy to grep for a particular element.
uint64_t _seq = 777000000;
;
public:
};
void for_each_schema_change(std::function<void(schema_ptr, const std::vector<mutation> &, schema_ptr, const std::vector<mutation> &)> fn)
{
auto map_of_int_to_int = map_type_impl::get_instance(int32_type, int32_type, true);
auto map_of_int_to_bytes = map_type_impl::get_instance(int32_type, bytes_type, true);
auto frozen_map_of_int_to_int = map_type_impl::get_instance(int32_type, int32_type, false);
auto frozen_map_of_int_to_bytes = map_type_impl::get_instance(int32_type, bytes_type, false);
auto tuple_of_int_long = tuple_type_impl::get_instance({int32_type, long_type});
auto tuple_of_bytes_long = tuple_type_impl::get_instance({bytes_type, long_type});
auto tuple_of_bytes_bytes = tuple_type_impl::get_instance({bytes_type, bytes_type});
auto set_of_text = set_type_impl::get_instance(utf8_type, true);
auto set_of_bytes = set_type_impl::get_instance(bytes_type, true);
auto udt_int_text = user_type_impl::get_instance("ks", "udt", {
                                                                  utf8_type->decompose("f1"),
                                                                  utf8_type->decompose("f2"),
                                                              },
                                                 {int32_type, utf8_type}, true);
auto udt_int_blob_long = user_type_impl::get_instance("ks", "udt", {
                                                                       utf8_type->decompose("v1"),
                                                                       utf8_type->decompose("v2"),
                                                                       utf8_type->decompose("v3"),
                                                                   },
                                                      {int32_type, bytes_type, long_type}, true);
auto frozen_udt_int_text = user_type_impl::get_instance("ks", "udt", {
                                                                         utf8_type->decompose("f1"),
                                                                         utf8_type->decompose("f2"),
                                                                     },
                                                        {int32_type, utf8_type}, false);
auto frozen_udt_int_blob_long = user_type_impl::get_instance("ks", "udt", {
                                                                              utf8_type->decompose("v1"),
                                                                              utf8_type->decompose("v2"),
                                                                              utf8_type->decompose("v3"),
                                                                          },
                                                             {int32_type, bytes_type, long_type}, false);
auto random_int32_value = []
{ return int32_type->decompose(tests::random::get_int<int32_t>()); };
auto random_text_value = []
{ return utf8_type->decompose(tests::random::get_sstring()); };
int32_t key_id = 0;
auto random_partition_key = [&]() -> tests::data_model::mutation_description::key
{ return {
      random_int32_value(),
      random_int32_value(),
      int32_type->decompose(key_id++),
  }; };
auto random_clustering_key = [&]() -> tests::data_model::mutation_description::key
{ return {
      utf8_type->decompose(tests::random::get_sstring()),
      utf8_type->decompose(tests::random::get_sstring()),
      utf8_type->decompose(format("{}", key_id++)),
  }; };
auto random_map = [&]() -> tests::data_model::mutation_description::collection
{ return {
      {int32_type->decompose(1), random_int32_value()},
      {int32_type->decompose(2), random_int32_value()},
      {int32_type->decompose(3), random_int32_value()},
  }; };
auto random_frozen_map = [&]
{ return map_of_int_to_int->decompose(make_map_value(map_of_int_to_int, map_type_impl::native_type({
                                                                            {1, tests::random::get_int<int32_t>()},
                                                                            {2, tests::random::get_int<int32_t>()},
                                                                            {3, tests::random::get_int<int32_t>()},
                                                                        }))); };
auto random_tuple = [&]
{ return tuple_of_int_long->decompose(make_tuple_value(tuple_of_int_long, tuple_type_impl::native_type{
                                                                              tests::random::get_int<int32_t>(),
                                                                              tests::random::get_int<int64_t>(),
                                                                          })); };
auto random_set = [&]() -> tests::data_model::mutation_description::collection
{ return {
      {utf8_type->decompose("a"), bytes()},
      {utf8_type->decompose("b"), bytes()},
      {utf8_type->decompose("c"), bytes()},
  }; };
auto random_udt = [&]() -> tests::data_model::mutation_description::collection
{ return {
      {serialize_field_index(0), random_int32_value()},
      {serialize_field_index(1), random_text_value()},
  }; };
auto random_frozen_udt = [&]
{ return frozen_udt_int_text->decompose(make_user_value(udt_int_text, user_type_impl::native_type{
                                                                          tests::random::get_int<int32_t>(),
                                                                          tests::random::get_sstring(),
                                                                      })); };
struct column_description
{
    int id;
    data_type type;
    std::vector<data_type> alter_to;
    std::vector<std::function<tests::data_model::mutation_description::value()>> data_generators;
    data_type old_type;
};
auto columns = std::vector<column_description>{
    {100, int32_type, {bytes_type, bytes_type}, {[&]
                                                  { return random_int32_value(); },
                                                  [&]
                                                  { return bytes(); }},
     uuid_type},
    {200, map_of_int_to_int, {map_of_int_to_bytes}, {[&]
                                                     { return random_map(); }},
     empty_type},
    {300, int32_type, {bytes_type, bytes_type}, {[&]
                                                  { return random_int32_value(); },
                                                  [&]
                                                  { return bytes(); }},
     empty_type},
    {400, frozen_map_of_int_to_int, {frozen_map_of_int_to_bytes}, {[&]
                                                                   { return random_frozen_map(); }},
     empty_type},
    {500, tuple_of_int_long, {tuple_of_bytes_long, tuple_of_bytes_bytes}, {[&]
                                                                           { return random_tuple(); }},
     empty_type},
    {600, set_of_text, {set_of_bytes}, {[&]
                                        { return random_set(); }},
     empty_type},
    {700, udt_int_text, {udt_int_blob_long}, {[&]
                                              { return random_udt(); }},
     empty_type},
    {800, frozen_udt_int_text, {frozen_udt_int_blob_long}, {[&]
                                                            { return random_frozen_udt(); }},
     empty_type},
};
auto static_columns = columns;
auto regular_columns = columns; // Base schema
auto s = tests::data_model::table_description({
                                                  {"pk1", int32_type},
                                                  {"pk2", int32_type},
                                                  {"pk3", int32_type},
                                              },
                                              {
                                                  {"ck1", utf8_type},
                                                  {"ck2", utf8_type},
                                                  {"ck3", utf8_type},
                                              });
for (auto &sc : static_columns)
{
    auto name = format("s{}", sc.id);
    s.add_static_column(name, sc.type);
    if (sc.old_type != empty_type)
    {
        s.add_old_static_column(name, sc.old_type);
    }
}
for (auto &rc : regular_columns)
{
    auto name = format("r{}", rc.id);
    s.add_regular_column(name, rc.type);
    if (rc.old_type != empty_type)
    {
        s.add_old_regular_column(name, rc.old_type);
    }
}
auto max_generator_count = std::max( // boost::max_elements wants the iterators to be copy-assignable. The ones we get
                                     // from boost::adaptors::transformed aren't.
    boost::accumulate(static_columns | boost::adaptors::transformed([](const column_description &c)
                                                                    { return c.data_generators.size(); }),
                      0u, [](size_t a, size_t b)
                      { return std::max(a, b); }),
    boost::accumulate(regular_columns | boost::adaptors::transformed([](const column_description &c)
                                                                     { return c.data_generators.size(); }),
                      0u, [](size_t a, size_t b)
                      { return std::max(a, b); })); // Base data
// Single column in a static row, nothing else
for (auto &[id, type, alter_to, data_generators, old_type] : static_columns)
{
    auto name = format("s{}", id);
    for (auto &dg : data_generators)
    {
        auto m = tests::data_model::mutation_description(random_partition_key());
        m.add_static_cell(name, dg());
        s.unordered_mutations().emplace_back(std::move(m));
    }
} // Partition with rows each having a single column
auto m = tests::data_model::mutation_description(random_partition_key());
for (auto &[id, type, alter_to, data_generators, old_type] : regular_columns)
{
    auto name = format("r{}", id);
    for (auto &dg : data_generators)
    {
        m.add_clustered_cell(random_clustering_key(), name, dg());
    }
}
s.unordered_mutations().emplace_back(std::move(m)); // Absolutely everything
for (auto i = 0u; i < max_generator_count; i++)
{
    auto m = tests::data_model::mutation_description(random_partition_key());
    for (auto &[id, type, alter_to, data_generators, old_type] : static_columns)
    {
        auto name = format("s{}", id);
        m.add_static_cell(name, data_generators[std::min<size_t>(i, data_generators.size() - 1)]());
    }
    for (auto &[id, type, alter_to, data_generators, old_type] : regular_columns)
    {
        auto name = format("r{}", id);
        m.add_clustered_cell(random_clustering_key(), name, data_generators[std::min<size_t>(i, data_generators.size() - 1)]());
    }
    m.add_range_tombstone(random_clustering_key(), random_clustering_key());
    m.add_range_tombstone(random_clustering_key(), random_clustering_key());
    m.add_range_tombstone(random_clustering_key(), random_clustering_key());
    s.unordered_mutations().emplace_back(std::move(m));
} // Transformations
auto base = s.build();
std::vector<tests::data_model::table_description::table> schemas;
schemas.emplace_back(base);
auto test_mutated_schemas = [&]
{         auto& [ base_change_log, base_schema, base_mutations ] = base;         for (auto&& [ mutated_change_log, mutated_schema, mutated_mutations ] : schemas) {             testlog.info("\nSchema change from:\n\n{}\n\nto:\n\n{}\n", base_change_log, mutated_change_log);             fn(base_schema, base_mutations, mutated_schema, mutated_mutations);         }         for (auto i = 2u; i < schemas.size(); i++) {             auto& [ base_change_log, base_schema, base_mutations ] = schemas[i - 1];             auto& [ mutated_change_log, mutated_schema, mutated_mutations ] = schemas[i];             testlog.info("\nSchema change from:\n\n{}\n\nto:\n\n{}\n", base_change_log, mutated_change_log);             fn(base_schema, base_mutations, mutated_schema, mutated_mutations);         }         schemas.clear();         schemas.emplace_back(base); };
auto original_s = s; // Remove and add back all static columns
for (auto &sc : static_columns)
{
    s.remove_static_column(format("s{}", sc.id));
    schemas.emplace_back(s.build());
}
for (auto &sc : static_columns)
{
    s.add_static_column(format("s{}", sc.id), uuid_type);
    auto mutated = s.build();
    schemas.emplace_back(s.build());
}
test_mutated_schemas();
s = original_s; // Remove and add back all regular columns
for (auto &rc : regular_columns)
{
    s.remove_regular_column(format("r{}", rc.id));
    schemas.emplace_back(s.build());
}
auto temp_s = s;
auto temp_schemas = schemas;
for (auto &rc : regular_columns)
{
    s.add_regular_column(format("r{}", rc.id), uuid_type);
    schemas.emplace_back(s.build());
}
test_mutated_schemas();
s = temp_s;
schemas = temp_schemas; // Add back all regular columns as collections
for (auto &rc : regular_columns)
{
    s.add_regular_column(format("r{}", rc.id), map_of_int_to_bytes);
    schemas.emplace_back(s.build());
}
test_mutated_schemas();
s = temp_s;
schemas = temp_schemas; // Add back all regular columns as frozen collections
for (auto &rc : regular_columns)
{
    s.add_regular_column(format("r{}", rc.id), frozen_map_of_int_to_int);
    schemas.emplace_back(s.build());
}
test_mutated_schemas();
s = original_s; // Add more static columns
for (auto &sc : static_columns)
{
    s.add_static_column(format("s{}", sc.id + 1), uuid_type);
    schemas.emplace_back(s.build());
}
test_mutated_schemas();
s = original_s; // Add more regular columns
for (auto &rc : regular_columns)
{
    s.add_regular_column(format("r{}", rc.id + 1), uuid_type);
    schemas.emplace_back(s.build());
}
test_mutated_schemas();
s = original_s; // Alter column types
for (auto &sc : static_columns)
{
    for (auto &target : sc.alter_to)
    {
        s.alter_static_column_type(format("s{}", sc.id), target);
        schemas.emplace_back(s.build());
    }
}
for (auto &rc : regular_columns)
{
    for (auto &target : rc.alter_to)
    {
        s.alter_regular_column_type(format("r{}", rc.id), target);
        schemas.emplace_back(s.build());
    }
}
for (auto i = 1; i <= 3; i++)
{
    s.alter_clustering_column_type(format("ck{}", i), bytes_type);
    schemas.emplace_back(s.build());
}
for (auto i = 1; i <= 3; i++)
{
    s.alter_partition_column_type(format("pk{}", i), bytes_type);
    schemas.emplace_back(s.build());
}
test_mutated_schemas();
s = original_s; // Rename clustering key
for (auto i = 1; i <= 3; i++)
{
    s.rename_clustering_column(format("ck{}", i), format("ck{}", 100 - i));
    schemas.emplace_back(s.build());
}
test_mutated_schemas();
s = original_s; // Rename partition key
for (auto i = 1; i <= 3; i++)
{
    s.rename_partition_column(format("pk{}", i), format("pk{}", 100 - i));
    schemas.emplace_back(s.build());
}
test_mutated_schemas();
}
static bool compare_readers(const schema &s, flat_mutation_reader_v2 &authority, flat_reader_assertions_v2 &tested)
{
bool empty = true;
while (auto expected = authority().get())
{
    tested.produces(s, *expected);
    empty = false;
}
tested.produces_end_of_stream();
return !empty;
}
void compare_readers(const schema &s, flat_mutation_reader_v2 authority, flat_mutation_reader_v2 tested)
{
auto close_authority = deferred_close(authority);
auto assertions = assert_that(std::move(tested));
compare_readers(s, authority, assertions);
}
// Assumes that the readers return fragments from (at most) a single (and the same) partition.
void compare_readers(const schema &s, flat_mutation_reader_v2 authority, flat_mutation_reader_v2 tested, const std::vector<position_range> &fwd_ranges)
{
auto close_authority = deferred_close(authority);
auto assertions = assert_that(std::move(tested));
if (compare_readers(s, authority, assertions))
{
    for (auto &r : fwd_ranges)
    {
        authority.fast_forward_to(r).get();
        assertions.fast_forward_to(r);
        compare_readers(s, authority, assertions);
    }
}
}
mutation forwardable_reader_to_mutation(flat_mutation_reader_v2 r, const std::vector<position_range> &fwd_ranges)
{
auto close_reader = deferred_close(r);
struct consumer
{
    schema_ptr _s;
    std::optional<mutation_rebuilder_v2> &_builder;
    consumer(schema_ptr s, std::optional<mutation_rebuilder_v2> &builder) : _s(std::move(s)), _builder(builder) {}
    void consume_new_partition(const dht::decorated_key &dk)
    {
        assert(!_builder);
        _builder = mutation_rebuilder_v2(std::move(_s));
        _builder->consume_new_partition(dk);
    }
    stop_iteration consume(tombstone t)
    {
        assert(_builder);
        return _builder->consume(t);
    }
    stop_iteration consume(range_tombstone_change &&rt)
    {
        assert(_builder);
        return _builder->consume(std::move(rt));
    }
    stop_iteration consume(static_row &&sr)
    {
        assert(_builder);
        return _builder->consume(std::move(sr));
    }
    stop_iteration consume(clustering_row &&cr)
    {
        assert(_builder);
        return _builder->consume(std::move(cr));
    }
    stop_iteration consume_end_of_partition()
    {
        assert(_builder);
        return stop_iteration::yes;
    }
    void consume_end_of_stream() {}
};
std::optional<mutation_rebuilder_v2> builder{};
r.consume(consumer(r.schema(), builder)).get();
BOOST_REQUIRE(builder);
for (auto &range : fwd_ranges)
{
    testlog.trace("forwardable_reader_to_mutation: forwarding to {}", range);
    r.fast_forward_to(range).get();
    r.consume(consumer(r.schema(), builder)).get();
}
auto m = builder->consume_end_of_stream();
BOOST_REQUIRE(m);
return std::move(*m);
}
std::vector<mutation> squash_mutations(std::vector<mutation> mutations)
{
if (mutations.empty())
{
    return {};
}
std::map<dht::decorated_key, mutation, dht::ring_position_less_comparator> merged_muts{dht::ring_position_less_comparator{*mutations.front().schema()}};
for (const auto &mut : mutations)
{
    auto [it, inserted] = merged_muts.try_emplace(mut.decorated_key(), mut);
    if (!inserted)
    {
        it->second.apply(mut);
    }
}
return boost::copy_range<std::vector<mutation>>(merged_muts | boost::adaptors::map_values);
}
namespace tests::data_model
{
mutation_description::atomic_value::atomic_value(bytes value, api::timestamp_type timestamp) : value(std::move(value)), timestamp(timestamp) {}
mutation_description::atomic_value::atomic_value(bytes value, api::timestamp_type timestamp, gc_clock::duration ttl, gc_clock::time_point expiry_point) : value(std::move(value)), timestamp(timestamp), expiring(expiry_info{ttl, expiry_point}) {}
mutation_description::collection::collection(std::initializer_list<collection_element> elements) : elements(elements) {}
mutation_description::collection::collection(std::vector<collection_element> elements) : elements(std::move(elements)) {}
mutation_description::row_marker::row_marker(api::timestamp_type timestamp) : timestamp(timestamp) {}
mutation_description::row_marker::row_marker(api::timestamp_type timestamp, gc_clock::duration ttl, gc_clock::time_point expiry_point) : timestamp(timestamp), expiring(expiry_info{ttl, expiry_point}) {}
void mutation_description::remove_column(row &r, const sstring &name)
{
    auto it = boost::range::find_if(r, [&](const cell &c)
                                    { return c.column_name == name; });
    if (it != r.end())
    {
        r.erase(it);
    }
}
mutation_description::mutation_description(key partition_key) : _partition_key(std::move(partition_key)) {}
void mutation_description::set_partition_tombstone(tombstone partition_tombstone) { _partition_tombstone = partition_tombstone; }
void mutation_description::add_static_cell(const sstring &column, value v) { _static_row.emplace_back(cell{column, std::move(v)}); }
void mutation_description::add_clustered_cell(const key &ck, const sstring &column, value v) { _clustered_rows[ck].cells.emplace_back(cell{column, std::move(v)}); }
void mutation_description::add_clustered_row_marker(const key &ck, row_marker marker) { _clustered_rows[ck].marker = marker; }
void mutation_description::add_clustered_row_tombstone(const key &ck, row_tombstone tomb) { _clustered_rows[ck].tomb = tomb; }
void mutation_description::remove_static_column(const sstring &name) { remove_column(_static_row, name); }
void mutation_description::remove_regular_column(const sstring &name)
{
    for (auto &[ckey, cr] : _clustered_rows)
    {
        (void)ckey;
        remove_column(cr.cells, name);
    }
}
void mutation_description::add_range_tombstone(const key &start, const key &end, tombstone tomb) { add_range_tombstone(nonwrapping_range<key>::make(start, end), tomb); }
void mutation_description::add_range_tombstone(nonwrapping_range<key> range, tombstone tomb) { _range_tombstones.emplace_back(range_tombstone{std::move(range), tomb}); }
mutation mutation_description::build(schema_ptr s) const
{
    auto m = mutation(s, partition_key::from_exploded(*s, _partition_key));
    m.partition().apply(_partition_tombstone);
    for (auto &[column, value_or_collection] : _static_row)
    {
        auto cdef = s->get_column_definition(utf8_type->decompose(column));
        assert(cdef);
        std::visit(make_visitor([&](const atomic_value &v)
                                {                 assert(cdef->is_atomic());                 if (!v.expiring) {                     m.set_static_cell(*cdef, atomic_cell::make_live(*cdef->type, v.timestamp, v.value));                 } else {                     m.set_static_cell(*cdef, atomic_cell::make_live(*cdef->type, v.timestamp, v.value,                                                                     v.expiring->expiry_point, v.expiring->ttl));                 } },
                                [&](const collection &c)
                                {                 assert(!cdef->is_atomic());                 auto get_value_type = visit(*cdef->type, make_visitor(                     [] (const collection_type_impl& ctype) -> std::function<const abstract_type&(bytes_view)> {                         return [&] (bytes_view) -> const abstract_type& { return *ctype.value_comparator(); };                     },                     [] (const user_type_impl& utype) -> std::function<const abstract_type&(bytes_view)> {                         return [&] (bytes_view key) -> const abstract_type& { return *utype.type(deserialize_field_index(key)); };                     },                     [] (const abstract_type& o) -> std::function<const abstract_type&(bytes_view)> {                         assert(false);                     }                 ));                 collection_mutation_description mut;                 mut.tomb = c.tomb;                 for (auto& [ key, value ] : c.elements) {                     if (!value.expiring) {                         mut.cells.emplace_back(key, atomic_cell::make_live(get_value_type(key), value.timestamp,                                                                             value.value, atomic_cell::collection_member::yes));                     } else {                         mut.cells.emplace_back(key, atomic_cell::make_live(get_value_type(key),                                                                            value.timestamp,                                                                            value.value,                                                                            value.expiring->expiry_point,                                                                            value.expiring->ttl,                                                                            atomic_cell::collection_member::yes));                     }                 }                 m.set_static_cell(*cdef, mut.serialize(*cdef->type)); }),
                   value_or_collection);
    }
    for (auto &[ckey, cr] : _clustered_rows)
    {
        auto &[marker, tomb, cells] = cr;
        auto ck = clustering_key::from_exploded(*s, ckey);
        for (auto &[column, value_or_collection] : cells)
        {
        auto cdef = s->get_column_definition(utf8_type->decompose(column));
        assert(cdef);
        std::visit(make_visitor([&](const atomic_value &v)
                                {                     assert(cdef->is_atomic());                     if (!v.expiring) {                         m.set_clustered_cell(ck, *cdef, atomic_cell::make_live(*cdef->type, v.timestamp, v.value));                     } else {                         m.set_clustered_cell(ck, *cdef, atomic_cell::make_live(*cdef->type, v.timestamp, v.value,                                                                                v.expiring->expiry_point, v.expiring->ttl));                     } },
                                [&](const collection &c)
                                {                     assert(!cdef->is_atomic());                     auto get_value_type = visit(*cdef->type, make_visitor(                         [] (const collection_type_impl& ctype) -> std::function<const abstract_type&(bytes_view)> {                             return [&] (bytes_view) -> const abstract_type& { return *ctype.value_comparator(); };                         },                         [] (const user_type_impl& utype) -> std::function<const abstract_type&(bytes_view)> {                             return [&] (bytes_view key) -> const abstract_type& { return *utype.type(deserialize_field_index(key)); };                         },                         [] (const abstract_type& o) -> std::function<const abstract_type&(bytes_view)> {                             assert(false);                         }                     ));                     collection_mutation_description mut;                     mut.tomb = c.tomb;                     for (auto& [ key, value ] : c.elements) {                         if (!value.expiring) {                             mut.cells.emplace_back(key, atomic_cell::make_live(get_value_type(key), value.timestamp,                                                                             value.value, atomic_cell::collection_member::yes));                         } else {                             mut.cells.emplace_back(key, atomic_cell::make_live(get_value_type(key),                                                                                value.timestamp,                                                                                value.value,                                                                                value.expiring->expiry_point,                                                                                value.expiring->ttl,                                                                                atomic_cell::collection_member::yes));                         }                     }                     m.set_clustered_cell(ck, *cdef, mut.serialize(*cdef->type)); }),
                   value_or_collection);
        }
        if (marker.timestamp != api::missing_timestamp)
        {
        if (marker.expiring)
        {
            m.partition().clustered_row(*s, ckey).apply(::row_marker(marker.timestamp, marker.expiring->ttl, marker.expiring->expiry_point));
        }
        else
        {
            m.partition().clustered_row(*s, ckey).apply(::row_marker(marker.timestamp));
        }
        }
        if (tomb)
        {
        m.partition().clustered_row(*s, ckey).apply(tomb);
        }
    }
    clustering_key::less_compare cmp(*s);
    for (auto &[range, tomb] : _range_tombstones)
    {
        auto clustering_range = range.transform([&s = *s](const key &k)
                                                { return clustering_key::from_exploded(s, k); });
        if (!clustering_range.is_singular())
        {
        auto start = clustering_range.start();
        auto end = clustering_range.end();
        if (start && end && cmp(end->value(), start->value()))
        {
            clustering_range = nonwrapping_range<clustering_key>(std::move(end), std::move(start));
        }
        }
        auto rt = ::range_tombstone(bound_view::from_range_start(clustering_range), bound_view::from_range_end(clustering_range), tomb);
        m.partition().apply_delete(*s, std::move(rt));
    }
    return m;
}
std::vector<table_description::column>::iterator table_description::find_column(std::vector<column> &columns, const sstring &name)
{
    return boost::range::find_if(columns, [&](const column &c)
                                 { return std::get<sstring>(c) == name; });
}
void table_description::add_column(std::vector<column> &columns, const sstring &name, data_type type)
{
    assert(find_column(columns, name) == columns.end());
    columns.emplace_back(name, type);
}
void table_description::add_old_column(const sstring &name, data_type type) { _removed_columns.emplace_back(removed_column{name, type, previously_removed_column_timestamp}); }
void table_description::remove_column(std::vector<column> &columns, const sstring &name)
{
    auto it = find_column(columns, name);
    assert(it != columns.end());
    _removed_columns.emplace_back(removed_column{name, std::get<data_type>(*it), column_removal_timestamp});
    columns.erase(it);
}
void table_description::alter_column_type(std::vector<column> &columns, const sstring &name, data_type new_type)
{
    auto it = find_column(columns, name);
    assert(it != columns.end());
    std::get<data_type>(*it) = new_type;
}
schema_ptr table_description::build_schema() const
{
    auto sb = schema_builder("ks", "cf");
    for (auto &&[name, type] : _partition_key)
    {
        sb.with_column(utf8_type->decompose(name), type, column_kind::partition_key);
    }
    for (auto &&[name, type] : _clustering_key)
    {
        sb.with_column(utf8_type->decompose(name), type, column_kind::clustering_key);
    }
    for (auto &&[name, type] : _static_columns)
    {
        sb.with_column(utf8_type->decompose(name), type, column_kind::static_column);
    }
    for (auto &&[name, type] : _regular_columns)
    {
        sb.with_column(utf8_type->decompose(name), type);
    }
    for (auto &&[name, type, timestamp] : _removed_columns)
    {
        sb.without_column(name, type, timestamp);
    }
    return sb.build();
}
std::vector<mutation> table_description::build_mutations(schema_ptr s) const
{
    auto ms = boost::copy_range<std::vector<mutation>>(_mutations | boost::adaptors::transformed([&](const mutation_description &md)
                                                                                                 { return md.build(s); }));
    boost::sort(ms, mutation_decorated_key_less_comparator());
    return ms;
}
table_description::table_description(std::vector<column> partition_key, std::vector<column> clustering_key) : _partition_key(std::move(partition_key)), _clustering_key(std::move(clustering_key)) {}
void table_description::add_static_column(const sstring &name, data_type type)
{
    _change_log.emplace_back(format("added static column \'{}\' of type \'{}\'", name, type->as_cql3_type().to_string()));
    add_column(_static_columns, name, type);
}
void table_description::add_regular_column(const sstring &name, data_type type)
{
    _change_log.emplace_back(format("added regular column \'{}\' of type \'{}\'", name, type->as_cql3_type().to_string()));
    add_column(_regular_columns, name, type);
}
void table_description::add_old_static_column(const sstring &name, data_type type) { add_old_column(name, type); }
void table_description::add_old_regular_column(const sstring &name, data_type type) { add_old_column(name, type); }
void table_description::remove_static_column(const sstring &name)
{
    _change_log.emplace_back(format("removed static column \'{}\'", name));
    remove_column(_static_columns, name);
    for (auto &m : _mutations)
    {
        m.remove_static_column(name);
    }
}
void table_description::remove_regular_column(const sstring &name)
{
    _change_log.emplace_back(format("removed regular column \'{}\'", name));
    remove_column(_regular_columns, name);
    for (auto &m : _mutations)
    {
        m.remove_regular_column(name);
    }
}
void table_description::alter_partition_column_type(const sstring &name, data_type new_type)
{
    _change_log.emplace_back(format("altered partition column \'{}\' type to \'{}\'", name, new_type->as_cql3_type().to_string()));
    alter_column_type(_partition_key, name, new_type);
}
void table_description::alter_clustering_column_type(const sstring &name, data_type new_type)
{
    _change_log.emplace_back(format("altered clustering column \'{}\' type to \'{}\'", name, new_type->as_cql3_type().to_string()));
    alter_column_type(_clustering_key, name, new_type);
}
void table_description::alter_static_column_type(const sstring &name, data_type new_type)
{
    _change_log.emplace_back(format("altered static column \'{}\' type to \'{}\'", name, new_type->as_cql3_type().to_string()));
    alter_column_type(_static_columns, name, new_type);
}
void table_description::alter_regular_column_type(const sstring &name, data_type new_type)
{
    _change_log.emplace_back(format("altered regular column \'{}\' type to \'{}\'", name, new_type->as_cql3_type().to_string()));
    alter_column_type(_regular_columns, name, new_type);
}
void table_description::rename_partition_column(const sstring &from, const sstring &to)
{
    _change_log.emplace_back(format("renamed partition column \'{}\' to \'{}\'", from, to));
    auto it = find_column(_partition_key, from);
    assert(it != _partition_key.end());
    std::get<sstring>(*it) = to;
}
void table_description::rename_clustering_column(const sstring &from, const sstring &to)
{
    _change_log.emplace_back(format("renamed clustering column \'{}\' to \'{}\'", from, to));
    auto it = find_column(_clustering_key, from);
    assert(it != _clustering_key.end());
    std::get<sstring>(*it) = to;
}
table_description::table table_description::build() const
{
    auto s = build_schema();
    return {boost::algorithm::join(_change_log, "\n"), s, build_mutations(s)};
}
}
std::function<bool(const std::exception &)> exception_predicate::make(std::function<bool(const std::exception &)> check, std::function<sstring(const std::exception &)> err)
{
return [check = std::move(check), err = std::move(err)](const std::exception &e)
{                const bool status = check(e);                BOOST_CHECK_MESSAGE(status, err(e));                return status; };
}
std::function<bool(const std::exception &)> exception_predicate::message_contains(const sstring &fragment, const std::source_location &loc)
{
return make([=](const std::exception &e)
            { return sstring(e.what()).find(fragment) != sstring::npos; },
            [=](const std::exception &e)
            { return fmt::format("Message '{}' doesn't contain '{}'\n{}:{}: invoked here", e.what(), fragment, loc.file_name(), loc.line()); });
}
std::function<bool(const std::exception &)> exception_predicate::message_equals(const sstring &text, const std::source_location &loc)
{
return make([=](const std::exception &e)
            { return text == e.what(); },
            [=](const std::exception &e)
            { return fmt::format("Message '{}' doesn't equal '{}'\n{}:{}: invoked here", e.what(), text, loc.file_name(), loc.line()); });
}
std::function<bool(const std::exception &)> exception_predicate::message_matches(const std::string &regex, const std::source_location &loc)
{ // Use boost::regex since std::regex (with libstdc++ 12) uses too much stack
return make([=](const std::exception &e)
            { return boost::regex_search(e.what(), boost::regex(regex)); },
            [=](const std::exception &e)
            { return fmt::format("Message '{}' doesn't match '{}'\n{}:{}: invoked here", e.what(), regex, loc.file_name(), loc.line()); });
}
namespace tests
{
type_generator::type_generator(random_schema_specification &spec) : _spec(spec)
{
    struct simple_type_generator
    {
        data_type type;
        data_type operator()(std::mt19937 &, is_multi_cell) { return type; }
    };
    _generators = {simple_type_generator{bytes_type}}; // tuple
}
data_type type_generator::operator()(std::mt19937 &engine, is_multi_cell multi_cell)
{
    auto dist = std::uniform_int_distribution<size_t>(0, _generators.size() - 1);
    auto type = _generators.at(dist(engine))(engine, multi_cell); // duration type is not allowed in:
    // * primary key components
    // * as member types of collections
    //
    // To cover all this, we simply disallow it altogether when multi_cell is
    // no, which will be the case in all the above cases.
    while (!multi_cell && type == duration_type)
    {
        type = (*this)(engine, multi_cell);
    }
    return type;
}
namespace
{
    class default_random_schema_specification : public random_schema_specification
    {
        std::unordered_set<unsigned> _used_table_ids;
        std::unordered_set<unsigned> _used_udt_ids;
        std::uniform_int_distribution<size_t> _partition_column_count_dist;
        std::uniform_int_distribution<size_t> _clustering_column_count_dist;
        std::uniform_int_distribution<size_t> _regular_column_count_dist;
        std::uniform_int_distribution<size_t> _static_column_count_dist;
        type_generator _type_generator;
    private:
        static unsigned generate_unique_id(std::mt19937 &engine, std::unordered_set<unsigned> &used_ids)
        {
        std::uniform_int_distribution<unsigned> id_dist(0, 1024);
        unsigned id;
        do
        {
            id = id_dist(engine);
        } while (used_ids.contains(id));
        used_ids.insert(id);
        return id;
        }
        std::vector<data_type> generate_types(std::mt19937 &engine, std::uniform_int_distribution<size_t> &count_dist, type_generator::is_multi_cell multi_cell, bool allow_reversed = false)
        {
        std::vector<data_type> types;
        const auto count = count_dist(engine);
        for (size_t c = 0; c < count; ++c)
        {
            auto type = _type_generator(engine, type_generator::is_multi_cell(false));
            {
                types.emplace_back(std::move(type));
            }
        }
        return types;
        }
    public:
        default_random_schema_specification(sstring keyspace_name, std::uniform_int_distribution<size_t> partition_column_count_dist, std::uniform_int_distribution<size_t> clustering_column_count_dist, std::uniform_int_distribution<size_t> regular_column_count_dist, std::uniform_int_distribution<size_t> static_column_count_dist) : random_schema_specification(std::move(keyspace_name)), _partition_column_count_dist(partition_column_count_dist), _clustering_column_count_dist(clustering_column_count_dist), _regular_column_count_dist(regular_column_count_dist), _static_column_count_dist(static_column_count_dist), _type_generator(*this)
        {
        assert(_partition_column_count_dist.a() > 0);
        assert(_regular_column_count_dist.a() > 0);
        }
        virtual sstring table_name(std::mt19937 &engine) override { return format("table{}", generate_unique_id(engine, _used_table_ids)); }
        virtual sstring udt_name(std::mt19937 &engine) override { return format("udt{}", generate_unique_id(engine, _used_udt_ids)); }
        virtual std::vector<data_type> partition_key_columns(std::mt19937 &engine) override { return generate_types(engine, _partition_column_count_dist, type_generator::is_multi_cell::no, false); }
        virtual std::vector<data_type> clustering_key_columns(std::mt19937 &engine) override { return generate_types(engine, _clustering_column_count_dist, type_generator::is_multi_cell::no, true); }
        virtual std::vector<data_type> regular_columns(std::mt19937 &engine) override { return generate_types(engine, _regular_column_count_dist, type_generator::is_multi_cell::yes, false); }
        virtual std::vector<data_type> static_columns(std::mt19937 &engine) override { return generate_types(engine, _static_column_count_dist, type_generator::is_multi_cell::yes, false); }
    };
} // anonymous namespace
std::unique_ptr<random_schema_specification> make_random_schema_specification(sstring keyspace_name, std::uniform_int_distribution<size_t> partition_column_count_dist, std::uniform_int_distribution<size_t> clustering_column_count_dist, std::uniform_int_distribution<size_t> regular_column_count_dist, std::uniform_int_distribution<size_t> static_column_count_dist) { return std::make_unique<default_random_schema_specification>(std::move(keyspace_name), partition_column_count_dist, clustering_column_count_dist, regular_column_count_dist, static_column_count_dist); }
namespace
{
    template <typename String>
    String generate_string_value(std::mt19937 &engine, typename String::value_type min, typename String::value_type max, size_t min_size_in_bytes, size_t max_size_in_bytes)
    {
        auto size_dist = random::stepped_int_distribution<size_t>{{{95.0, {0, 31}}, {4.5, {32, 99}}, {0.4, {100, 999}}, {0.1, {1000, 9999}}}};
        auto char_dist = std::uniform_int_distribution<typename String::value_type>(min, max);
        const auto size = std::clamp(size_dist(engine), min_size_in_bytes / sizeof(typename String::value_type), max_size_in_bytes / sizeof(typename String::value_type));
        String str(size, '\0');
        for (size_t i = 0; i < size; ++i)
        {
        str[i] = char_dist(engine);
        }
        return str;
    }
    std::vector<data_value> generate_frozen_tuple_values(std::mt19937 &engine, value_generator &val_gen, const std::vector<data_type> &member_types, size_t min_size_in_bytes, size_t max_size_in_bytes)
    {
        std::vector<data_value> values;
        values.reserve(member_types.size());
        const auto member_min_size_in_bytes = min_size_in_bytes / member_types.size();
        const auto member_max_size_in_bytes = max_size_in_bytes / member_types.size();
        for (auto member_type : member_types)
        {
        values.push_back(val_gen.generate_atomic_value(engine, *member_type, member_min_size_in_bytes, member_max_size_in_bytes));
        }
        return values;
    }
    data_model::mutation_description::collection generate_user_value(std::mt19937 &engine, const user_type_impl &type, value_generator &val_gen)
    {
        using md = data_model::mutation_description; // Non-null fields.
        auto fields_num = std::uniform_int_distribution<size_t>(1, type.size())(engine);
        auto field_idxs = random::random_subset<unsigned>(type.size(), fields_num, engine);
        std::sort(field_idxs.begin(), field_idxs.end());
        md::collection collection;
        for (auto i : field_idxs)
        {
        collection.elements.push_back({serialize_field_index(i), val_gen.generate_atomic_value(engine, *type.type(i), value_generator::no_size_in_bytes_limit).serialize_nonnull()});
        }
        return collection;
    }
    data_model::mutation_description::collection generate_collection(std::mt19937 &engine, const abstract_type &key_type, const abstract_type &value_type, value_generator &val_gen)
    {
        using md = data_model::mutation_description;
        auto key_generator = val_gen.get_atomic_value_generator(key_type);
        auto value_generator = val_gen.get_atomic_value_generator(value_type);
        auto size_dist = std::uniform_int_distribution<size_t>(0, 16);
        const auto size = size_dist(engine);
        std::map<bytes, md::atomic_value, serialized_compare> collection{key_type.as_less_comparator()};
        for (size_t i = 0; i < size; ++i)
        {
        collection.emplace(key_generator(engine, 0, value_generator::no_size_in_bytes_limit).serialize_nonnull(), value_generator(engine, 0, value_generator::no_size_in_bytes_limit).serialize().value_or(""));
        }
        md::collection flat_collection;
        flat_collection.elements.reserve(collection.size());
        for (auto &&[key, value] : collection)
        {
        flat_collection.elements.emplace_back(md::collection_element{key, value});
        }
        return flat_collection;
    }
    std::vector<data_value> generate_frozen_list(std::mt19937 &engine, const abstract_type &value_type, value_generator &val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes)
    {
        auto value_generator = val_gen.get_atomic_value_generator(value_type);
        auto size_dist = std::uniform_int_distribution<size_t>(0, 4);
        const auto size = std::min(size_dist(engine), max_size_in_bytes / std::max(val_gen.min_size(value_type), size_t(1)));
        std::vector<data_value> collection;
        if (!size)
        {
        return collection;
        }
        const auto value_min_size_in_bytes = min_size_in_bytes / size;
        const auto value_max_size_in_bytes = max_size_in_bytes / size;
        for (size_t i = 0; i < size; ++i)
        {
        collection.emplace_back(value_generator(engine, value_min_size_in_bytes, value_max_size_in_bytes));
        }
        return collection;
    }
    std::vector<data_value> generate_frozen_set(std::mt19937 &engine, const abstract_type &key_type, value_generator &val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes)
    {
        auto key_generator = val_gen.get_atomic_value_generator(key_type);
        auto size_dist = std::uniform_int_distribution<size_t>(0, 4);
        const auto size = std::min(size_dist(engine), max_size_in_bytes / std::max(val_gen.min_size(key_type), size_t(1)));
        std::map<bytes, data_value, serialized_compare> collection{key_type.as_less_comparator()};
        std::vector<data_value> flat_collection;
        if (!size)
        {
        return flat_collection;
        }
        const auto value_max_size_in_bytes = max_size_in_bytes / size;
        const auto value_min_size_in_bytes = min_size_in_bytes / size;
        for (size_t i = 0; i < size; ++i)
        {
        auto val = key_generator(engine, value_min_size_in_bytes, value_max_size_in_bytes);
        auto serialized_key = val.serialize_nonnull();
        collection.emplace(std::move(serialized_key), std::move(val));
        }
        flat_collection.reserve(collection.size());
        for (auto &&element : collection)
        {
        flat_collection.emplace_back(std::move(element.second));
        }
        return flat_collection;
    }
    std::vector<std::pair<data_value, data_value>> generate_frozen_map(std::mt19937 &engine, const abstract_type &key_type, const abstract_type &value_type, value_generator &val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes)
    {
        auto key_generator = val_gen.get_atomic_value_generator(key_type);
        auto value_generator = val_gen.get_atomic_value_generator(value_type);
        auto size_dist = std::uniform_int_distribution<size_t>(0, 4);
        const auto min_item_size_in_bytes = val_gen.min_size(key_type) + val_gen.min_size(value_type);
        const auto size = std::min(size_dist(engine), max_size_in_bytes / std::max(min_item_size_in_bytes, size_t(1)));
        std::map<bytes, std::pair<data_value, data_value>, serialized_compare> collection(key_type.as_less_comparator());
        std::vector<std::pair<data_value, data_value>> flat_collection;
        if (!size)
        {
        return flat_collection;
        }
        const auto item_max_size_in_bytes = max_size_in_bytes / size;
        const auto key_max_size_in_bytes = item_max_size_in_bytes / 2;
        const auto value_max_size_in_bytes = item_max_size_in_bytes / 2;
        const auto item_min_size_in_bytes = min_size_in_bytes / size;
        const auto key_min_size_in_bytes = item_min_size_in_bytes / 2;
        const auto value_min_size_in_bytes = item_min_size_in_bytes / 2;
        for (size_t i = 0; i < size; ++i)
        {
        auto key = key_generator(engine, key_min_size_in_bytes, key_max_size_in_bytes);
        auto serialized_key = key.serialize_nonnull();
        auto value = value_generator(engine, value_min_size_in_bytes, value_max_size_in_bytes);
        collection.emplace(std::move(serialized_key), std::pair(std::move(key), std::move(value)));
        }
        flat_collection.reserve(collection.size());
        for (auto &&element : collection)
        {
        flat_collection.emplace_back(std::move(element.second));
        }
        return flat_collection;
    }
    data_value generate_empty_value(std::mt19937 &, size_t, size_t) { return data_value::make_null(empty_type); }
    data_value generate_byte_value(std::mt19937 &engine, size_t, size_t) { return data_value(random::get_int<int8_t>(engine)); }
    data_value generate_short_value(std::mt19937 &engine, size_t, size_t) { return data_value(random::get_int<int16_t>(engine)); }
    data_value generate_int32_value(std::mt19937 &engine, size_t, size_t) { return data_value(random::get_int<int32_t>(engine)); }
    data_value generate_long_value(std::mt19937 &engine, size_t, size_t) { return data_value(random::get_int<int64_t>(engine)); }
    data_value generate_ascii_value(std::mt19937 &engine, size_t min_size_in_bytes, size_t max_size_in_bytes) { return data_value(ascii_native_type{generate_string_value<sstring>(engine, 0, 127, min_size_in_bytes, max_size_in_bytes)}); }
    data_value generate_bytes_value(std::mt19937 &engine, size_t min_size_in_bytes, size_t max_size_in_bytes) { return data_value(generate_string_value<bytes>(engine, std::numeric_limits<bytes::value_type>::min(), std::numeric_limits<bytes::value_type>::max(), min_size_in_bytes, max_size_in_bytes)); }
    data_value generate_utf8_value(std::mt19937 &engine, size_t min_size_in_bytes, size_t max_size_in_bytes)
    {
        auto wstr = generate_string_value<std::wstring>(engine, 0, 0x0FFF, min_size_in_bytes, max_size_in_bytes);
        std::locale locale("en_US.utf8");
        using codec = std::codecvt<wchar_t, char, std::mbstate_t>;
        auto &f = std::use_facet<codec>(locale);
        sstring utf8_str(wstr.size() * f.max_length(), '\0');
        const wchar_t *from_next;
        char *to_next;
        std::mbstate_t mb{};
        auto res = f.out(mb, &wstr[0], &wstr[wstr.size()], from_next, &utf8_str[0], &utf8_str[utf8_str.size()], to_next);
        assert(res == codec::ok);
        utf8_str.resize(to_next - &utf8_str[0]);
        return data_value(std::move(utf8_str));
    }
    data_value generate_boolean_value(std::mt19937 &engine, size_t, size_t)
    {
        auto dist = std::uniform_int_distribution<int8_t>(0, 1);
        return data_value(bool(dist(engine)));
    }
    data_value generate_date_value(std::mt19937 &engine, size_t, size_t) { return data_value(date_type_native_type{db_clock::time_point(db_clock::duration(random::get_int<std::make_unsigned_t<db_clock::rep>>(engine)))}); }
    data_value generate_timeuuid_value(std::mt19937 &, size_t, size_t) { return data_value(timeuuid_native_type{utils::UUID_gen::get_time_UUID()}); }
    data_value generate_timestamp_value(std::mt19937 &engine, size_t, size_t)
    {
        using pt = db_clock::time_point;
        return data_value(pt(pt::duration(random::get_int<pt::rep>(engine))));
    }
    data_value generate_simple_date_value(std::mt19937 &engine, size_t, size_t) { return data_value(simple_date_native_type{random::get_int<simple_date_native_type::primary_type>(engine)}); }
    data_value generate_time_value(std::mt19937 &engine, size_t, size_t) { return data_value(time_native_type{random::get_int<time_native_type::primary_type>(engine)}); }
    data_value generate_uuid_value(std::mt19937 &engine, size_t, size_t) { return data_value(utils::make_random_uuid()); }
    data_value generate_inet_addr_value(std::mt19937 &engine, size_t, size_t) { return data_value(net::ipv4_address(random::get_int<int32_t>(engine))); }
    data_value generate_float_value(std::mt19937 &engine, size_t, size_t) { return data_value(random::get_real<float>(engine)); }
    data_value generate_double_value(std::mt19937 &engine, size_t, size_t) { return data_value(random::get_real<double>(engine)); }
    data_value generate_duration_value(std::mt19937 &engine, size_t, size_t)
    {
        auto months = months_counter(random::get_int<months_counter::value_type>(engine));
        auto days = days_counter(random::get_int<days_counter::value_type>(0, 31, engine));
        auto nanoseconds = nanoseconds_counter(random::get_int<nanoseconds_counter::value_type>(86400000000000, engine));
        return data_value(cql_duration{months, days, nanoseconds});
    }
    data_value generate_frozen_tuple_value(std::mt19937 &engine, const tuple_type_impl &type, value_generator &val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes)
    {
        assert(!type.is_multi_cell());
        return make_tuple_value(type.shared_from_this(), generate_frozen_tuple_values(engine, val_gen, type.all_types(), min_size_in_bytes, max_size_in_bytes));
    }
    data_value generate_frozen_user_value(std::mt19937 &engine, const user_type_impl &type, value_generator &val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes)
    {
        assert(!type.is_multi_cell());
        return make_user_value(type.shared_from_this(), generate_frozen_tuple_values(engine, val_gen, type.all_types(), min_size_in_bytes, max_size_in_bytes));
    }
    data_model::mutation_description::collection generate_list_value(std::mt19937 &engine, const list_type_impl &type, value_generator &val_gen)
    {
        assert(type.is_multi_cell());
        return generate_collection(engine, *type.name_comparator(), *type.value_comparator(), val_gen);
    }
    data_value generate_frozen_list_value(std::mt19937 &engine, const list_type_impl &type, value_generator &val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes)
    {
        assert(!type.is_multi_cell());
        return make_list_value(type.shared_from_this(), generate_frozen_list(engine, *type.get_elements_type(), val_gen, min_size_in_bytes, max_size_in_bytes));
    }
    data_model::mutation_description::collection generate_set_value(std::mt19937 &engine, const set_type_impl &type, value_generator &val_gen)
    {
        assert(type.is_multi_cell());
        return generate_collection(engine, *type.name_comparator(), *type.value_comparator(), val_gen);
    }
    data_value generate_frozen_set_value(std::mt19937 &engine, const set_type_impl &type, value_generator &val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes)
    {
        assert(!type.is_multi_cell());
        return make_set_value(type.shared_from_this(), generate_frozen_set(engine, *type.get_elements_type(), val_gen, min_size_in_bytes, max_size_in_bytes));
    }
    data_model::mutation_description::collection generate_map_value(std::mt19937 &engine, const map_type_impl &type, value_generator &val_gen)
    {
        assert(type.is_multi_cell());
        return generate_collection(engine, *type.name_comparator(), *type.value_comparator(), val_gen);
    }
    data_value generate_frozen_map_value(std::mt19937 &engine, const map_type_impl &type, value_generator &val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes)
    {
        assert(!type.is_multi_cell());
        return make_map_value(type.shared_from_this(), generate_frozen_map(engine, *type.get_keys_type(), *type.get_values_type(), val_gen, min_size_in_bytes, max_size_in_bytes));
    }
} // anonymous namespace
data_value value_generator::generate_atomic_value(std::mt19937 &engine, const abstract_type &type, size_t max_size_in_bytes) { return generate_atomic_value(engine, type, 0, max_size_in_bytes); }
data_value value_generator::generate_atomic_value(std::mt19937 &engine, const abstract_type &type, size_t min_size_in_bytes, size_t max_size_in_bytes)
{
    assert(!type.is_multi_cell());
    return get_atomic_value_generator(type)(engine, min_size_in_bytes, max_size_in_bytes);
}
value_generator::value_generator() : _regular_value_generators{{empty_type.get(), &generate_empty_value}, {byte_type.get(), &generate_byte_value}, {short_type.get(), &generate_short_value}, {int32_type.get(), &generate_int32_value}, {long_type.get(), &generate_long_value}, {ascii_type.get(), &generate_ascii_value}, {bytes_type.get(), &generate_bytes_value}, {utf8_type.get(), &generate_utf8_value}, {boolean_type.get(), &generate_boolean_value}, {date_type.get(), &generate_date_value}, {timeuuid_type.get(), &generate_timeuuid_value}, {timestamp_type.get(), &generate_timestamp_value}, {simple_date_type.get(), &generate_simple_date_value}, {time_type.get(), &generate_time_value}, {uuid_type.get(), &generate_uuid_value}, {inet_addr_type.get(), &generate_inet_addr_value}, {float_type.get(), &generate_float_value}, {double_type.get(), &generate_double_value}, {duration_type.get(), &generate_duration_value}}
{
    std::mt19937 engine;
    for (const auto &[regular_type, regular_value_gen] : _regular_value_generators)
    {
        _regular_value_min_sizes.emplace(regular_type, regular_value_gen(engine, size_t{}, size_t{}).serialized_size());
    }
}
size_t value_generator::min_size(const abstract_type &type)
{
    assert(!type.is_multi_cell());
    auto it = _regular_value_min_sizes.find(&type);
    if (it != _regular_value_min_sizes.end())
    {
        return it->second;
    }
    std::mt19937 engine;
    if (auto maybe_user_type = dynamic_cast<const user_type_impl *>(&type))
    {
        return generate_frozen_user_value(engine, *maybe_user_type, *this, size_t{}, size_t{}).serialized_size();
    }
    if (auto maybe_tuple_type = dynamic_cast<const tuple_type_impl *>(&type))
    {
        return generate_frozen_tuple_value(engine, *maybe_tuple_type, *this, size_t{}, size_t{}).serialized_size();
    }
    if (auto maybe_list_type = dynamic_cast<const list_type_impl *>(&type))
    {
        return generate_frozen_list_value(engine, *maybe_list_type, *this, size_t{}, size_t{}).serialized_size();
    }
    if (auto maybe_set_type = dynamic_cast<const set_type_impl *>(&type))
    {
        return generate_frozen_set_value(engine, *maybe_set_type, *this, size_t{}, size_t{}).serialized_size();
    }
    if (auto maybe_map_type = dynamic_cast<const map_type_impl *>(&type))
    {
        return generate_frozen_map_value(engine, *maybe_map_type, *this, size_t{}, size_t{}).serialized_size();
    }
    if (auto maybe_reversed_type = dynamic_cast<const reversed_type_impl *>(&type))
    {
        return min_size(*maybe_reversed_type->underlying_type());
    }
    throw std::runtime_error(fmt::format("Don't know how to calculate min size for unknown type {}", type.name()));
}
value_generator::atomic_value_generator value_generator::get_atomic_value_generator(const abstract_type &type)
{
    assert(!type.is_multi_cell());
    auto it = _regular_value_generators.find(&type);
    if (it != _regular_value_generators.end())
    {
        return it->second;
    }
    if (auto maybe_user_type = dynamic_cast<const user_type_impl *>(&type))
    {
        return [this, maybe_user_type](std::mt19937 &engine, size_t min_size_in_bytes, size_t max_size_in_bytes)
        { return generate_frozen_user_value(engine, *maybe_user_type, *this, min_size_in_bytes, max_size_in_bytes); };
    }
    if (auto maybe_tuple_type = dynamic_cast<const tuple_type_impl *>(&type))
    {
        return [this, maybe_tuple_type](std::mt19937 &engine, size_t min_size_in_bytes, size_t max_size_in_bytes)
        { return generate_frozen_tuple_value(engine, *maybe_tuple_type, *this, min_size_in_bytes, max_size_in_bytes); };
    }
    if (auto maybe_list_type = dynamic_cast<const list_type_impl *>(&type))
    {
        return [this, maybe_list_type](std::mt19937 &engine, size_t min_size_in_bytes, size_t max_size_in_bytes)
        { return generate_frozen_list_value(engine, *maybe_list_type, *this, min_size_in_bytes, max_size_in_bytes); };
    }
    if (auto maybe_set_type = dynamic_cast<const set_type_impl *>(&type))
    {
        return [this, maybe_set_type](std::mt19937 &engine, size_t min_size_in_bytes, size_t max_size_in_bytes)
        { return generate_frozen_set_value(engine, *maybe_set_type, *this, min_size_in_bytes, max_size_in_bytes); };
    }
    if (auto maybe_map_type = dynamic_cast<const map_type_impl *>(&type))
    {
        return [this, maybe_map_type](std::mt19937 &engine, size_t min_size_in_bytes, size_t max_size_in_bytes)
        { return generate_frozen_map_value(engine, *maybe_map_type, *this, min_size_in_bytes, max_size_in_bytes); };
    }
    if (auto maybe_reversed_type = dynamic_cast<const reversed_type_impl *>(&type))
    {
        return get_atomic_value_generator(*maybe_reversed_type->underlying_type());
    }
    throw std::runtime_error(fmt::format("Don't know how to generate value for unknown type {}", type.name()));
}
value_generator::generator value_generator::get_generator(const abstract_type &type)
{
    auto it = _regular_value_generators.find(&type);
    if (it != _regular_value_generators.end())
    {
        return [gen = it->second](std::mt19937 &engine) -> data_model::mutation_description::value
        { return gen(engine, 0, no_size_in_bytes_limit).serialize_nonnull(); };
    }
    if (auto maybe_user_type = dynamic_cast<const user_type_impl *>(&type))
    {
        if (maybe_user_type->is_multi_cell())
        {
        return [this, maybe_user_type](std::mt19937 &engine) -> data_model::mutation_description::value
        { return generate_user_value(engine, *maybe_user_type, *this); };
        }
        else
        {
        return [this, maybe_user_type](std::mt19937 &engine) -> data_model::mutation_description::value
        { return generate_frozen_user_value(engine, *maybe_user_type, *this, 0, no_size_in_bytes_limit).serialize_nonnull(); };
        }
    }
    if (auto maybe_tuple_type = dynamic_cast<const tuple_type_impl *>(&type))
    {
        return [this, maybe_tuple_type](std::mt19937 &engine) -> data_model::mutation_description::value
        { return generate_frozen_tuple_value(engine, *maybe_tuple_type, *this, 0, no_size_in_bytes_limit).serialize_nonnull(); };
    }
    if (auto maybe_list_type = dynamic_cast<const list_type_impl *>(&type))
    {
        if (maybe_list_type->is_multi_cell())
        {
        return [this, maybe_list_type](std::mt19937 &engine) -> data_model::mutation_description::value
        { return generate_list_value(engine, *maybe_list_type, *this); };
        }
        else
        {
        return [this, maybe_list_type](std::mt19937 &engine) -> data_model::mutation_description::value
        { return generate_frozen_list_value(engine, *maybe_list_type, *this, 0, no_size_in_bytes_limit).serialize_nonnull(); };
        }
    }
    if (auto maybe_set_type = dynamic_cast<const set_type_impl *>(&type))
    {
        if (maybe_set_type->is_multi_cell())
        {
        return [this, maybe_set_type](std::mt19937 &engine) -> data_model::mutation_description::value
        { return generate_set_value(engine, *maybe_set_type, *this); };
        }
        else
        {
        return [this, maybe_set_type](std::mt19937 &engine) -> data_model::mutation_description::value
        { return generate_frozen_set_value(engine, *maybe_set_type, *this, 0, no_size_in_bytes_limit).serialize_nonnull(); };
        }
    }
    if (auto maybe_map_type = dynamic_cast<const map_type_impl *>(&type))
    {
        if (maybe_map_type->is_multi_cell())
        {
        return [this, maybe_map_type](std::mt19937 &engine) -> data_model::mutation_description::value
        { return generate_map_value(engine, *maybe_map_type, *this); };
        }
        else
        {
        return [this, maybe_map_type](std::mt19937 &engine) -> data_model::mutation_description::value
        { return generate_frozen_map_value(engine, *maybe_map_type, *this, 0, no_size_in_bytes_limit).serialize_nonnull(); };
        }
    }
    if (auto maybe_reversed_type = dynamic_cast<const reversed_type_impl *>(&type))
    {
        return get_generator(*maybe_reversed_type->underlying_type());
    }
    throw std::runtime_error(fmt::format("Don't know how to generate value for unknown type {}", type.name()));
}
data_model::mutation_description::value value_generator::generate_value(std::mt19937 &engine, const abstract_type &type) { return get_generator(type)(engine); }
timestamp_generator default_timestamp_generator()
{
    return [](std::mt19937 &engine, timestamp_destination, api::timestamp_type min_timestamp)
    {         auto ts_dist = std::uniform_int_distribution<api::timestamp_type>(min_timestamp, api::max_timestamp);         return ts_dist(engine); };
}
expiry_generator no_expiry_expiry_generator()
{
    return [](std::mt19937 &engine, timestamp_destination destination) -> std::optional<expiry_info>
    { return std::nullopt; };
}
namespace
{
    schema_ptr build_random_schema(uint32_t seed, random_schema_specification &spec)
    {
        auto engine = std::mt19937{seed};
        auto builder = schema_builder(spec.keyspace_name(), spec.table_name(engine));
        auto pk_columns = spec.partition_key_columns(engine);
        assert(!pk_columns.empty()); // Let's not pull in boost::test here
        for (size_t pk = 0; pk < pk_columns.size(); ++pk)
        {
        builder.with_column(to_bytes(format("pk{}", pk)), std::move(pk_columns[pk]), column_kind::partition_key);
        }
        const auto ck_columns = spec.clustering_key_columns(engine);
        for (size_t ck = 0; ck < ck_columns.size(); ++ck)
        {
        builder.with_column(to_bytes(format("ck{}", ck)), std::move(ck_columns[ck]), column_kind::clustering_key);
        }
        if (!ck_columns.empty())
        {
        const auto static_columns = spec.static_columns(engine);
        for (size_t s = 0; s < static_columns.size(); ++s)
        {
            builder.with_column(to_bytes(format("s{}", s)), std::move(static_columns[s]), column_kind::static_column);
        }
        }
        const auto regular_columns = spec.regular_columns(engine);
        assert(!regular_columns.empty()); // Let's not pull in boost::test here
        for (size_t r = 0; r < regular_columns.size(); ++r)
        {
        builder.with_column(to_bytes(format("v{}", r)), std::move(regular_columns[r]), column_kind::regular_column);
        }
        return builder.build();
    }
    sstring udt_to_str(const user_type_impl &udt)
    {
        std::stringstream ss;
        udt.describe(ss);
        return ss.str();
    }
    struct udt_list
    {
        std::vector<const user_type_impl *> vector;
        void insert(const user_type_impl *udt)
        {
        auto it = std::find(vector.begin(), vector.end(), udt);
        if (it == vector.end())
        {
            vector.push_back(udt);
        }
        }
        void merge(udt_list other)
        {
        for (auto &udt : other.vector)
        {
            insert(udt);
        }
        }
    };
    udt_list dump_udts(const std::vector<data_type> &types)
    {
        udt_list udts;
        for (const auto &dt : types)
        {
        const auto *const type = dt.get();
        if (auto maybe_user_type = dynamic_cast<const user_type_impl *>(type))
        {
            udts.merge(dump_udts(maybe_user_type->field_types()));
            udts.insert(maybe_user_type);
        }
        else if (auto maybe_tuple_type = dynamic_cast<const tuple_type_impl *>(type))
        {
            udts.merge(dump_udts(maybe_tuple_type->all_types()));
        }
        else if (auto maybe_list_type = dynamic_cast<const list_type_impl *>(type))
        {
            udts.merge(dump_udts({maybe_list_type->get_elements_type()}));
        }
        else if (auto maybe_set_type = dynamic_cast<const set_type_impl *>(type))
        {
            udts.merge(dump_udts({maybe_set_type->get_elements_type()}));
        }
        else if (auto maybe_map_type = dynamic_cast<const map_type_impl *>(type))
        {
            udts.merge(dump_udts({maybe_map_type->get_keys_type(), maybe_map_type->get_values_type()}));
        }
        else if (auto maybe_reversed_type = dynamic_cast<const reversed_type_impl *>(type))
        {
            udts.merge(dump_udts({maybe_reversed_type->underlying_type()}));
        }
        }
        return udts;
    }
    std::vector<const user_type_impl *> dump_udts(const schema &schema)
    {
        udt_list udts;
        const auto cdefs_to_types = [](const schema::const_iterator_range_type &cdefs) -> std::vector<data_type>
        { return boost::copy_range<std::vector<data_type>>(cdefs | boost::adaptors::transformed([](const column_definition &cdef)
                                                                                                { return cdef.type; })); };
        udts.merge(dump_udts(cdefs_to_types(schema.partition_key_columns())));
        udts.merge(dump_udts(cdefs_to_types(schema.clustering_key_columns())));
        udts.merge(dump_udts(cdefs_to_types(schema.regular_columns())));
        udts.merge(dump_udts(cdefs_to_types(schema.static_columns())));
        return udts.vector;
    }
    std::vector<sstring> columns_specs(schema_ptr schema, column_kind kind)
    {
        const auto count = schema->columns_count(kind);
        if (!count)
        {
        return {};
        }
        std::vector<sstring> col_specs;
        for (column_count_type c = 0; c < count; ++c)
        {
        const auto &cdef = schema->column_at(kind, c);
        col_specs.emplace_back(format("{} {}{}", cdef.name_as_cql_string(), cdef.type->as_cql3_type().to_string(), kind == column_kind::static_column ? " static" : ""));
        }
        return col_specs;
    }
    std::vector<sstring> column_names(schema_ptr schema, column_kind kind)
    {
        const auto count = schema->columns_count(kind);
        if (!count)
        {
        return {};
        }
        std::vector<sstring> col_names;
        for (column_count_type c = 0; c < count; ++c)
        {
        const auto &cdef = schema->column_at(kind, c);
        col_names.emplace_back(cdef.name_as_cql_string());
        }
        return col_names;
    }
}
void decorate_with_timestamps(const schema &schema, std::mt19937 &engine, timestamp_generator &ts_gen, expiry_generator exp_gen, data_model::mutation_description::value &value)
{
    std::visit(make_visitor([&](data_model::mutation_description::atomic_value &v)
                            {                         v.timestamp = ts_gen(engine, timestamp_destination::cell_timestamp, api::min_timestamp);                         if (auto expiry_opt = exp_gen(engine, timestamp_destination::cell_timestamp)) {                             v.expiring = data_model::mutation_description::expiry_info{expiry_opt->ttl, expiry_opt->expiry_point};                         } },
                            [&](data_model::mutation_description::collection &c)
                            {                         if (auto ts = ts_gen(engine, timestamp_destination::collection_tombstone, api::min_timestamp);                                 ts != api::missing_timestamp) {                             if (ts == api::max_timestamp) {                                 // Caveat: leave some headroom for the cells
                                // having a timestamp larger than the
                                // tombstone's.
                                ts--;                             }                             auto expiry_opt = exp_gen(engine, timestamp_destination::collection_tombstone);                             const auto deletion_time = expiry_opt ? expiry_opt->expiry_point : gc_clock::now();                             c.tomb = tombstone(ts, deletion_time);                         }                         for (auto& [ key, value ] : c.elements) {                             value.timestamp = ts_gen(engine, timestamp_destination::collection_cell_timestamp, c.tomb.timestamp);                             assert(!c.tomb || value.timestamp > c.tomb.timestamp);                             if (auto expiry_opt = exp_gen(engine, timestamp_destination::collection_cell_timestamp)) {                                 value.expiring = data_model::mutation_description::expiry_info{expiry_opt->ttl, expiry_opt->expiry_point};                             }                         } }),
               value);
}
data_model::mutation_description::key random_schema::make_key(uint32_t n, value_generator &gen, schema::const_iterator_range_type columns, size_t max_size_in_bytes)
{
    std::mt19937 engine(n);
    const size_t max_component_size = max_size_in_bytes / std::distance(columns.begin(), columns.end());
    std::vector<bytes> key;
    for (const auto &cdef : columns)
    {
        key.emplace_back(gen.generate_atomic_value(engine, *cdef.type, max_component_size).serialize_nonnull());
    }
    return key;
}
data_model::mutation_description::key random_schema::make_partition_key(uint32_t n, value_generator &gen) const { return make_key(n, gen, _schema->partition_key_columns(), std::numeric_limits<partition_key::compound::element_type::size_type>::max()); }
data_model::mutation_description::key random_schema::make_clustering_key(uint32_t n, value_generator &gen) const
{
    assert(_schema->clustering_key_size() > 0);
    return make_key(n, gen, _schema->clustering_key_columns(), std::numeric_limits<clustering_key::compound::element_type::size_type>::max());
}
random_schema::random_schema(uint32_t seed, random_schema_specification &spec) : _schema(build_random_schema(seed, spec)) {}
sstring random_schema::cql() const
{
    auto udts = dump_udts(*_schema);
    sstring udts_str;
    if (!udts.empty())
    {
        udts_str = boost::algorithm::join(udts | boost::adaptors::transformed([](const user_type_impl *const udt)
                                                                              { return udt_to_str(*udt); }),
                                          "\n");
    }
    std::vector<sstring> col_specs;
    for (auto kind : {column_kind::partition_key, column_kind::clustering_key, column_kind::regular_column, column_kind::static_column})
    {
        auto cols = columns_specs(_schema, kind);
        std::move(cols.begin(), cols.end(), std::back_inserter(col_specs));
    }
    sstring primary_key;
    auto partition_column_names = column_names(_schema, column_kind::partition_key);
    auto clustering_key_names = column_names(_schema, column_kind::clustering_key);
    if (!clustering_key_names.empty())
    {
        primary_key = format("({}), {}", boost::algorithm::join(partition_column_names, ", "), boost::algorithm::join(clustering_key_names, ", "));
    }
    else
    {
        primary_key = format("{}", boost::algorithm::join(partition_column_names, ", "));
    } // FIXME include the clustering column orderings
    return format("{}\nCREATE TABLE {}.{} (\n\t{}\n\tPRIMARY KEY ({}))", udts_str, _schema->ks_name(), _schema->cf_name(), boost::algorithm::join(col_specs, ",\n\t"), primary_key);
}
data_model::mutation_description::key random_schema::make_pkey(uint32_t n)
{
    value_generator g;
    return make_partition_key(n, g);
}
std::vector<data_model::mutation_description::key> random_schema::make_pkeys(size_t n)
{
    std::set<dht::decorated_key, dht::ring_position_less_comparator> keys{dht::ring_position_less_comparator{*_schema}};
    value_generator val_gen;
    uint32_t i{0};
    while (keys.size() < n)
    {
        keys.emplace(dht::decorate_key(*_schema, partition_key::from_exploded(make_partition_key(i, val_gen))));
        ++i;
    }
    return boost::copy_range<std::vector<data_model::mutation_description::key>>(keys | boost::adaptors::transformed([](const dht::decorated_key &dkey)
                                                                                                                     { return dkey.key().explode(); }));
}
data_model::mutation_description::key random_schema::make_ckey(uint32_t n)
{
    value_generator g;
    return make_clustering_key(n, g);
}
std::vector<data_model::mutation_description::key> random_schema::make_ckeys(size_t n)
{
    std::set<clustering_key, clustering_key::less_compare> keys{clustering_key::less_compare{*_schema}};
    value_generator val_gen;
    for (uint32_t i = 0; i < n; i++)
    {
        keys.emplace(clustering_key::from_exploded(make_clustering_key(i, val_gen)));
    }
    return boost::copy_range<std::vector<data_model::mutation_description::key>>(keys | boost::adaptors::transformed([](const clustering_key &ckey)
                                                                                                                     { return ckey.explode(); }));
}
data_model::mutation_description random_schema::new_mutation(data_model::mutation_description::key pkey) { return data_model::mutation_description(std::move(pkey)); }
data_model::mutation_description random_schema::new_mutation(uint32_t n) { return new_mutation(make_pkey(n)); }
void random_schema::set_partition_tombstone(std::mt19937 &engine, data_model::mutation_description &md, timestamp_generator ts_gen, expiry_generator exp_gen)
{
    if (const auto ts = ts_gen(engine, timestamp_destination::partition_tombstone, api::min_timestamp); ts != api::missing_timestamp)
    {
        auto expiry_opt = exp_gen(engine, timestamp_destination::partition_tombstone);
        const auto deletion_time = expiry_opt ? expiry_opt->expiry_point : gc_clock::now();
        md.set_partition_tombstone(tombstone(ts, deletion_time));
    }
}
void random_schema::add_row(std::mt19937 &engine, data_model::mutation_description &md, uint32_t n, timestamp_generator ts_gen, expiry_generator exp_gen) { add_row(engine, md, make_ckey(n), std::move(ts_gen), std::move(exp_gen)); }
void random_schema::add_static_row(std::mt19937 &engine, data_model::mutation_description &md, timestamp_generator ts_gen, expiry_generator exp_gen)
{
    value_generator gen;
    for (const auto &cdef : _schema->static_columns())
    {
        auto value = gen.generate_value(engine, *cdef.type);
        decorate_with_timestamps(*_schema, engine, ts_gen, exp_gen, value);
        md.add_static_cell(cdef.name_as_text(), std::move(value));
    }
}
void random_schema::delete_range(std::mt19937 &engine, data_model::mutation_description &md, nonwrapping_range<data_model::mutation_description::key> range, timestamp_generator ts_gen, expiry_generator exp_gen)
{
    auto expiry_opt = exp_gen(engine, timestamp_destination::range_tombstone);
    const auto deletion_time = expiry_opt ? expiry_opt->expiry_point : gc_clock::now();
    md.add_range_tombstone(std::move(range), tombstone{ts_gen(engine, timestamp_destination::range_tombstone, api::min_timestamp), deletion_time});
}
future<> random_schema::create_with_cql(cql_test_env &env)
{
    return async([this, &env]
                 {         const auto ks_name = _schema->ks_name();         const auto tbl_name = _schema->cf_name();         for (const auto& udt : dump_udts(*_schema)) {             env.execute_cql(udt_to_str(*udt)).get();             eventually_true([&] () mutable {                 return env.db().map_reduce0([&] (replica::database& db) {                     return db.user_types().get(ks_name).has_type(udt->get_name());                 }, true, std::logical_and<bool>{}).get();             });         }         auto& db = env.local_db();         std::stringstream ss;         _schema->describe(db, ss, false);         env.execute_cql(ss.str()).get();         env.require_table_exists(ks_name, tbl_name).get();         auto& tbl = db.find_column_family(ks_name, tbl_name);         _schema = tbl.schema(); });
}
future<std::vector<mutation>> generate_random_mutations(uint32_t seed, tests::random_schema &random_schema, timestamp_generator ts_gen, expiry_generator exp_gen, std::uniform_int_distribution<size_t> partition_count_dist, std::uniform_int_distribution<size_t> clustering_row_count_dist, std::uniform_int_distribution<size_t> range_tombstone_count_dist)
{
    auto engine = std::mt19937(seed);
    const auto schema_has_clustering_columns = random_schema.schema()->clustering_key_size() > 0;
    const auto partition_count = partition_count_dist(engine);
    std::vector<mutation> muts;
    muts.reserve(partition_count);
    for (size_t pk = 0; pk != partition_count; ++pk)
    {
        auto mut = random_schema.new_mutation(pk);
        random_schema.set_partition_tombstone(engine, mut, ts_gen, exp_gen);
        random_schema.add_static_row(engine, mut, ts_gen, exp_gen);
        if (!schema_has_clustering_columns)
        {
        muts.emplace_back(mut.build(random_schema.schema()));
        continue;
        }
        const auto clustering_row_count = clustering_row_count_dist(engine);
        const auto range_tombstone_count = range_tombstone_count_dist(engine);
        auto ckeys = random_schema.make_ckeys(std::max(clustering_row_count, range_tombstone_count));
        for (uint32_t ck = 0; ck < ckeys.size(); ++ck)
        {
        random_schema.add_row(engine, mut, ckeys[ck], ts_gen, exp_gen);
        co_await coroutine::maybe_yield();
        }
        for (size_t i = 0; i < range_tombstone_count; ++i)
        {
        const auto a = tests::random::get_int<size_t>(0, ckeys.size() - 1, engine);
        const auto b = tests::random::get_int<size_t>(0, ckeys.size() - 1, engine);
        random_schema.delete_range(engine, mut, nonwrapping_range<tests::data_model::mutation_description::key>::make(ckeys.at(std::min(a, b)), ckeys.at(std::max(a, b))), ts_gen, exp_gen);
        co_await coroutine::maybe_yield();
        }
        muts.emplace_back(mut.build(random_schema.schema()));
    }
    boost::sort(muts, [s = random_schema.schema()](const mutation &a, const mutation &b)
                { return a.decorated_key().less_compare(*s, b.decorated_key()); });
    auto range = boost::unique(muts, [s = random_schema.schema()](const mutation &a, const mutation &b)
                               { return a.decorated_key().equal(*s, b.decorated_key()); });
    muts.erase(range.end(), muts.end());
    co_return std::move(muts);
}
future<std::vector<mutation>> generate_random_mutations(tests::random_schema &random_schema, timestamp_generator ts_gen, expiry_generator exp_gen, std::uniform_int_distribution<size_t> partition_count_dist, std::uniform_int_distribution<size_t> clustering_row_count_dist, std::uniform_int_distribution<size_t> range_tombstone_count_dist) { return generate_random_mutations(tests::random::get_int<uint32_t>(), random_schema, std::move(ts_gen), std::move(exp_gen), partition_count_dist, clustering_row_count_dist, range_tombstone_count_dist); }
future<std::vector<mutation>> generate_random_mutations(tests::random_schema &random_schema, size_t partition_count) { return generate_random_mutations(random_schema, default_timestamp_generator(), no_expiry_expiry_generator(), std::uniform_int_distribution<size_t>(partition_count, partition_count)); }
}
// namespace tests
namespace tests
{
namespace
{
    template <typename RawKey, typename DecoratedKey, typename Comparator>
    std::vector<DecoratedKey> generate_keys(size_t n, schema_ptr s, Comparator cmp, const std::vector<data_type> &types, std::function<std::optional<DecoratedKey>(const RawKey &)> decorate_fun, bool allow_prefixes, std::optional<key_size> size)
    {
        auto keys = std::set<DecoratedKey, Comparator>(cmp);
        const auto effective_size = size.value_or(tests::key_size{1, 128});
        std::mt19937 engine(tests::random::get_int<uint32_t>());
        std::uniform_int_distribution<size_t> component_count_dist(1, types.size());
        tests::value_generator value_gen;
        std::vector<data_value> components;
        components.reserve(types.size());
        while (keys.size() != n)
        {
        components.clear();
        auto component_count = allow_prefixes ? component_count_dist(engine) : types.size();
        for (size_t i = 0; i < component_count; ++i)
        {
            components.emplace_back(value_gen.generate_atomic_value(engine, *types.at(i), effective_size.min, effective_size.max));
        }
        auto raw_key = RawKey::from_deeply_exploded(*s, components); // discard empty keys on the off chance that we generate one
        if (raw_key.is_empty() || (types.size() == 1 && raw_key.begin(*s)->empty()))
        {
            continue;
        }
        if constexpr (std::is_same_v<RawKey, DecoratedKey>)
        {
            keys.emplace(std::move(raw_key));
        }
        else if (auto decorated_key_opt = decorate_fun(raw_key); decorated_key_opt)
        {
            keys.emplace(std::move(*decorated_key_opt));
        }
        }
        return std::vector<DecoratedKey>(keys.begin(), keys.end());
    }
}
std::vector<dht::decorated_key> generate_partition_keys(size_t n, schema_ptr s, std::optional<shard_id> shard, std::optional<key_size> size)
{
    return generate_keys<partition_key, dht::decorated_key, dht::decorated_key::less_comparator>(
        n, s, dht::decorated_key::less_comparator(s), s->partition_key_type()->types(), [s, shard, tokens = std::set<dht::token>()](const partition_key &pkey) mutable -> std::optional<dht::decorated_key>
        {                 auto dkey = dht::decorate_key(*s, pkey);                 if (shard && *shard != dht::shard_of(*s, dkey.token())) {                     return {};                 }                 if (!tokens.insert(dkey.token()).second) {                     return {};                 }                 return dkey; },
        false, size);
}
std::vector<dht::decorated_key> generate_partition_keys(size_t n, schema_ptr s, local_shard_only lso, std::optional<key_size> size) { return generate_partition_keys(n, std::move(s), lso == local_shard_only::yes ? std::optional(this_shard_id()) : std::nullopt, size); }
dht::decorated_key generate_partition_key(schema_ptr s, std::optional<shard_id> shard, std::optional<key_size> size)
{
    auto &&keys = generate_partition_keys(1, std::move(s), shard, size);
    return std::move(keys.front());
}
dht::decorated_key generate_partition_key(schema_ptr s, local_shard_only lso, std::optional<key_size> size) { return generate_partition_key(std::move(s), lso == local_shard_only::yes ? std::optional(this_shard_id()) : std::nullopt, size); }
std::vector<clustering_key> generate_clustering_keys(size_t n, schema_ptr s, bool allow_prefixes, std::optional<key_size> size) { return generate_keys<clustering_key, clustering_key, clustering_key::less_compare>(n, s, clustering_key::less_compare(*s), s->clustering_key_type()->types(), {}, allow_prefixes, size); }
clustering_key generate_clustering_key(schema_ptr s, bool allow_prefix, std::optional<key_size> size)
{
    auto &&keys = generate_clustering_keys(1, std::move(s), allow_prefix, size);
    return std::move(keys.front());
}
}
// namespace tests
