#define CRYPTOPP_ENABLE_NAMESPACE_WEAK 1


// rapidjson configuration macros
#define RAPIDJSON_HAS_STDSTRING 1
// Default rjson policy is to use assert() - which is dangerous for two reasons:
// 1. assert() can be turned off with -DNDEBUG
// 2. assert() crashes a program
// Fortunately, the default policy can be overridden, and so rapidjson errors will
// throw an rjson::error exception instead.
#define RAPIDJSON_ASSERT(x) (void)(x)
// This macro is used for functions which are called for every json char making it
// quite costly if not inlined, by default rapidjson only enables it if NDEBUG
// is defined which isn't the case for us.
#define RAPIDJSON_FORCEINLINE __attribute__((always_inline))



#include <chrono>
#include <cstdint>
#include <ratio>
#include <type_traits>
#include <fmt/chrono.h>
#include <list>
#include <deque>
#include <set>
#include <unordered_set>
#include <boost/variant.hpp>

// the database clock follows Java - 1ms granularity, 64-bit counter, 1970 epoch

#include <algorithm>
#include <atomic>
#include <chrono>
#include <cstdint>

extern std::atomic<int64_t> clocks_offset;

template<typename Duration>
static inline void forward_jump_clocks(Duration delta)
{
    auto d = std::chrono::duration_cast<std::chrono::seconds>(delta).count();
    clocks_offset.fetch_add(d, std::memory_order_relaxed);
}

static inline std::chrono::seconds get_clocks_offset()
{
    auto off = clocks_offset.load(std::memory_order_relaxed);
    return std::chrono::seconds(off);
}

// Returns a time point which is earlier from t by d, or minimum time point if it cannot be represented.
template<typename Clock, typename Duration, typename Rep, typename Period>
inline
auto saturating_subtract(std::chrono::time_point<Clock, Duration> t, std::chrono::duration<Rep, Period> d) -> decltype(t) {
    return std::max(t, decltype(t)::min() + d) - d;
}

#include <boost/asio/ip/address_v4.hpp>  // avoid conflict between ::socket and seastar::socket

namespace seastar {

template <typename T>
class shared_ptr;

template <typename T, typename... A>
shared_ptr<T> make_shared(A&&... a);

}


using namespace seastar;
using seastar::shared_ptr;
using seastar::make_shared;

#include <chrono>
#include <map>
#include <optional>
#include <concepts>
#include <seastar/core/byteorder.hh>
#include <seastar/core/sstring.hh>

//
// This hashing differs from std::hash<> in that it decouples knowledge about
// type structure from the way the hash value is calculated:
//  * appending_hash<T> instantiation knows about what data should be included in the hash for type T.
//  * Hasher object knows how to combine the data into the final hash.
//
// The appending_hash<T> should always feed some data into the hasher, regardless of the state the object is in,
// in order for the hash to be highly sensitive for value changes. For example, vector<optional<T>> should
// ideally feed different values for empty vector and a vector with a single empty optional.
//
// appending_hash<T> is machine-independent.
//

template<typename H>
concept Hasher =
    requires(H& h, const char* ptr, size_t size) {
        { h.update(ptr, size) } noexcept -> std::same_as<void>;
    };

template<typename H, typename ValueType>
concept HasherReturning = Hasher<H> &&
    requires (H& h) {
        { h.finalize() } -> std::convertible_to<ValueType>;
    };

class hasher {
public:
    virtual ~hasher() = default;
    virtual void update(const char* ptr, size_t size) noexcept = 0;
};

template<typename T>
struct appending_hash;

template<typename H, typename T, typename... Args>
requires Hasher<H>
inline
void feed_hash(H& h, const T& value, Args&&... args) noexcept(noexcept(std::declval<appending_hash<T>>()(h, value, args...))) {
    appending_hash<T>()(h, value, std::forward<Args>(args)...);
};

template<typename T>
requires std::is_arithmetic_v<T>
struct appending_hash<T> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, T value) const noexcept {
        auto value_le = cpu_to_le(value);
        h.update(reinterpret_cast<const char*>(&value_le), sizeof(T));
    }
};

template<>
struct appending_hash<bool> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, bool value) const noexcept {
        feed_hash(h, static_cast<uint8_t>(value));
    }
};

template<typename T>
requires std::is_enum_v<T>
struct appending_hash<T> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const T& value) const noexcept {
        feed_hash(h, static_cast<std::underlying_type_t<T>>(value));
    }
};

template<typename T>
struct appending_hash<std::optional<T>>  {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const std::optional<T>& value) const noexcept {
        if (value) {
            feed_hash(h, true);
            feed_hash(h, *value);
        } else {
            feed_hash(h, false);
        }
    }
};

template<size_t N>
struct appending_hash<char[N]>  {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const char (&value) [N]) const noexcept {
        feed_hash(h, N);
        h.update(value, N);
    }
};

template<typename T>
struct appending_hash<std::vector<T>> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const std::vector<T>& value) const noexcept {
        feed_hash(h, value.size());
        for (auto&& v : value) {
            appending_hash<T>()(h, v);
        }
    }
};

template<typename K, typename V>
struct appending_hash<std::map<K, V>> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const std::map<K, V>& value) const noexcept {
        feed_hash(h, value.size());
        for (auto&& e : value) {
            appending_hash<K>()(h, e.first);
            appending_hash<V>()(h, e.second);
        }
    }
};

template<>
struct appending_hash<sstring> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const sstring& v) const noexcept {
        feed_hash(h, v.size());
        h.update(reinterpret_cast<const char*>(v.cbegin()), v.size() * sizeof(sstring::value_type));
    }
};

template<>
struct appending_hash<std::string> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, const std::string& v) const noexcept {
        feed_hash(h, v.size());
        h.update(reinterpret_cast<const char*>(v.data()), v.size() * sizeof(std::string::value_type));
    }
};

template<typename T, typename R>
struct appending_hash<std::chrono::duration<T, R>> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, std::chrono::duration<T, R> v) const noexcept {
        feed_hash(h, v.count());
    }
};

template<typename Clock, typename Duration>
struct appending_hash<std::chrono::time_point<Clock, Duration>> {
    template<typename H>
    requires Hasher<H>
    void operator()(H& h, std::chrono::time_point<Clock, Duration> v) const noexcept {
        feed_hash(h, v.time_since_epoch().count());
    }
};

#include <seastar/core/lowres_clock.hh>

#include <chrono>
#include <optional>

class gc_clock final {
public:
    using base = seastar::lowres_system_clock;
    using rep = int64_t;
    using period = std::ratio<1, 1>; // seconds
    using duration = std::chrono::duration<rep, period>;
    using time_point = std::chrono::time_point<gc_clock, duration>;

    static constexpr auto is_steady = base::is_steady;

    static constexpr std::time_t to_time_t(time_point t) {
        return std::chrono::duration_cast<std::chrono::seconds>(t.time_since_epoch()).count();
    }

    static constexpr time_point from_time_t(std::time_t t) {
        return time_point(std::chrono::duration_cast<duration>(std::chrono::seconds(t)));
    }

    static time_point now() noexcept {
        return time_point(std::chrono::duration_cast<duration>(base::now().time_since_epoch())) + get_clocks_offset();
    }

    static int32_t as_int32(duration d) {
        auto count = d.count();
        int32_t count_32 = static_cast<int32_t>(count);
        if (count_32 != count) {
            throw std::runtime_error("Duration too big");
        }
        return count_32;
    }

    static int32_t as_int32(time_point tp) {
        return as_int32(tp.time_since_epoch());
    }
};

using expiry_opt = std::optional<gc_clock::time_point>;
using ttl_opt = std::optional<gc_clock::duration>;

// 20 years in seconds
static constexpr gc_clock::duration max_ttl = gc_clock::duration{20 * 365 * 24 * 60 * 60};

std::ostream& operator<<(std::ostream& os, gc_clock::time_point tp);

template<>
struct appending_hash<gc_clock::time_point> {
    template<typename Hasher>
    void operator()(Hasher& h, gc_clock::time_point t) const noexcept {
        // Remain backwards-compatible with the 32-bit duration::rep (refs #4460).
        uint64_t d64 = t.time_since_epoch().count();
        feed_hash(h, uint32_t(d64 & 0xffff'ffff));
        uint32_t msb = d64 >> 32;
        if (msb) {
            feed_hash(h, msb);
        }
    }
};


namespace ser {

// Forward-declaration - defined in serializer.hh, to avoid including it here.

template <typename Output>
void serialize_gc_clock_duration_value(Output& out, int64_t value);

template <typename Input>
int64_t deserialize_gc_clock_duration_value(Input& in);

template <typename T>
struct serializer;

template <>
struct serializer<gc_clock::duration> {
    template <typename Input>
    static gc_clock::duration read(Input& in) {
        return gc_clock::duration(deserialize_gc_clock_duration_value(in));
    }

    template <typename Output>
    static void write(Output& out, gc_clock::duration d) {
        serialize_gc_clock_duration_value(out, d.count());
    }

    template <typename Input>
    static void skip(Input& in) {
        read(in);
    }
};

}


class db_clock final {
public:
    using base = std::chrono::system_clock;
    using rep = int64_t;
    using period = std::ratio<1, 1000>; // milliseconds
    using duration = std::chrono::duration<rep, period>;
    using time_point = std::chrono::time_point<db_clock, duration>;

    static constexpr bool is_steady = base::is_steady;
    static constexpr std::time_t to_time_t(time_point t) {
        return std::chrono::duration_cast<std::chrono::seconds>(t.time_since_epoch()).count();
    }
    static constexpr time_point from_time_t(std::time_t t) {
        return time_point(std::chrono::duration_cast<duration>(std::chrono::seconds(t)));
    }
    static time_point now() noexcept {
        return time_point(std::chrono::duration_cast<duration>(base::now().time_since_epoch())) + get_clocks_offset();
    }
};

static inline
gc_clock::time_point to_gc_clock(db_clock::time_point tp) noexcept {
    // Converting time points through `std::time_t` means that we don't have to make any assumptions about the epochs
    // of `gc_clock` and `db_clock`, though we require that that the period of `gc_clock` is also 1 s like
    // `std::time_t` to avoid loss of information.
    {
        using second = std::ratio<1, 1>;
        static_assert(
                std::is_same<gc_clock::period, second>::value,
                "Conversion via std::time_t would lose information.");
    }

    return gc_clock::from_time_t(db_clock::to_time_t(tp));
}

#include <cstdint>

#include <cstdint>
#include <type_traits>

inline
constexpr uint64_t clmul_u32_constexpr(uint32_t p1, uint32_t p2) {
    uint64_t result = 0;
    for (unsigned i = 0; i < 32; ++i) {
        result ^= (((p1 >> i) & 1) * uint64_t(p2)) << i;
    }
    return result;
}

// returns the low half of the result
inline
constexpr uint64_t clmul_u64_low_constexpr(uint64_t p1, uint64_t p2) {
    uint64_t result = 0;
    for (unsigned i = 0; i < 64; ++i) {
        result ^= (((p1 >> i) & 1) * p2) << i;
    }
    return result;
}

#if defined(__x86_64__) || defined(__i386__)

#include <wmmintrin.h>
#include <smmintrin.h>

// Performs a carry-less multiplication of two integers.
inline
uint64_t clmul_u32(uint32_t p1, uint32_t p2) {
    __m128i p = _mm_set_epi64x(p1, p2);
    p = _mm_clmulepi64_si128(p, p, 0x01);
    return _mm_extract_epi64(p, 0);
}

constexpr
inline
uint64_t clmul(uint32_t p1, uint32_t p2) {
    return std::is_constant_evaluated() ? clmul_u32_constexpr(p1, p2) : clmul_u32(p1, p2);
}

#elif defined(__aarch64__)

#include <arm_neon.h>

// Performs a carry-less multiplication of two integers.
inline
uint64_t clmul_u32(uint32_t p1, uint32_t p2) {
    return vmull_p64(p1, p2);
}

constexpr
inline
uint64_t clmul(uint32_t p1, uint32_t p2) {
    return std::is_constant_evaluated() ? clmul_u32_constexpr(p1, p2) : clmul_u32(p1, p2);
}

#endif


inline
constexpr uint64_t barrett_reduction_constants[2] = { 0x00000001F7011641, 0x00000001DB710641 };

/*
 * Calculates representation of p(x) mod G(x) using Barrett reduction.
 *
 * p(x) is a polynomial of degree 64.
 *
 * The parameter p is a bit-reversed representation of the polynomial,
 * the least significant bit corresponds to the coefficient of x^63.
 */
inline constexpr uint32_t crc32_fold_barrett_u64_constexpr(uint64_t p) {
    auto x0 = p;
    auto x1 = x0;
    uint64_t mask32 = 0xffff'ffff;
    x0 = clmul_u64_low_constexpr(x0 & mask32, barrett_reduction_constants[0]);
    x0 = clmul_u64_low_constexpr(x0 & mask32, barrett_reduction_constants[1]);
    return (x0 ^ x1) >> 32;
}

#include <wmmintrin.h>

inline
uint32_t crc32_fold_barrett_u64_in_m128(__m128i x0) {
    __m128i x1;
    const __m128i mask32 = (__m128i)(__v4si){ int32_t(0xFFFFFFFF) };
    const __v2di brc =
        (__v2di){ barrett_reduction_constants[0], barrett_reduction_constants[1] };

    /*
     * Reduce 64 => 32 bits using Barrett reduction.
     *
     * Let M(x) = A(x)*x^32 + B(x) be the remaining message.  The goal is to
     * compute R(x) = M(x) mod G(x).  Since degree(B(x)) < degree(G(x)):
     *
     *	R(x) = (A(x)*x^32 + B(x)) mod G(x)
     *	     = (A(x)*x^32) mod G(x) + B(x)
     *
     * Then, by the Division Algorithm there exists a unique q(x) such that:
     *
     *	A(x)*x^32 mod G(x) = A(x)*x^32 - q(x)*G(x)
     *
     * Since the left-hand side is of maximum degree 31, the right-hand side
     * must be too.  This implies that we can apply 'mod x^32' to the
     * right-hand side without changing its value:
     *
     *	(A(x)*x^32 - q(x)*G(x)) mod x^32 = q(x)*G(x) mod x^32
     *
     * Note that '+' is equivalent to '-' in polynomials over GF(2).
     *
     * We also know that:
     *
     *	              / A(x)*x^32 \
     *	q(x) = floor (  ---------  )
     *	              \    G(x)   /
     *
     * To compute this efficiently, we can multiply the top and bottom by
     * x^32 and move the division by G(x) to the top:
     *
     *	              / A(x) * floor(x^64 / G(x)) \
     *	q(x) = floor (  -------------------------  )
     *	              \           x^32            /
     *
     * Note that floor(x^64 / G(x)) is a constant.
     *
     * So finally we have:
     *
     *	                          / A(x) * floor(x^64 / G(x)) \
     *	R(x) = B(x) + G(x)*floor (  -------------------------  )
     *	                          \           x^32            /
     *
     */
    x1 = x0;
    x0 = _mm_clmulepi64_si128(x0 & mask32, brc, 0x00);
    x0 = _mm_clmulepi64_si128(x0 & mask32, brc, 0x10);
    return _mm_cvtsi128_si32(_mm_srli_si128(x0 ^ x1, 4));
}

inline
uint32_t crc32_fold_barrett_u64_native(uint64_t p) {
    return crc32_fold_barrett_u64_in_m128(_mm_set_epi64x(0, p));
}


inline
constexpr
uint32_t crc32_fold_barrett_u64(uint64_t p) {
    return std::is_constant_evaluated() ? crc32_fold_barrett_u64_constexpr(p) : crc32_fold_barrett_u64_native(p);
}

#include <string_view>
#include <seastar/core/sstring.hh>

template<typename CharT>
class basic_mutable_view {
    CharT* _begin = nullptr;
    CharT* _end = nullptr;
public:
    using value_type = CharT;
    using pointer = CharT*;
    using iterator = CharT*;
    using const_iterator = CharT*;

    basic_mutable_view() = default;

    template<typename U, U N>
    basic_mutable_view(basic_sstring<CharT, U, N>& str)
        : _begin(str.begin())
        , _end(str.end())
    { }

    basic_mutable_view(CharT* ptr, size_t length)
        : _begin(ptr)
        , _end(ptr + length)
    { }

    operator std::basic_string_view<CharT>() const noexcept {
        return std::basic_string_view<CharT>(begin(), size());
    }

    CharT& operator[](size_t idx) const { return _begin[idx]; }

    iterator begin() const { return _begin; }
    iterator end() const { return _end; }

    CharT* data() const { return _begin; }
    size_t size() const { return _end - _begin; }
    bool empty() const { return _begin == _end; }
    CharT& front() { return *_begin; }
    const CharT& front() const { return *_begin; }

    void remove_prefix(size_t n) {
        _begin += n;
    }
    void remove_suffix(size_t n) {
        _end -= n;
    }

    basic_mutable_view substr(size_t pos, size_t count) {
        size_t n = std::min(count, (_end - _begin) - pos);
        return basic_mutable_view{_begin + pos, n};
    }
};

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Warray-bounds"
#include <xxhash.h>
#pragma GCC diagnostic pop



template<typename H>
concept SimpleHasher = HasherReturning<H, size_t>;

struct simple_xx_hasher : public hasher {
    XXH64_state_t _state;
    simple_xx_hasher(uint64_t seed = 0) noexcept {
        XXH64_reset(&_state, seed);
    }
    void update(const char* ptr, size_t length) noexcept override {
        XXH64_update(&_state, ptr, length);
    }
    size_t finalize() {
        return static_cast<size_t>(XXH64_digest(&_state));
    }
};

#include <fmt/format.h>
#include <seastar/core/sstring.hh>
#include <optional>
#include <iosfwd>
#include <functional>
#include <compare>

using bytes = basic_sstring<int8_t, uint32_t, 31, false>;
using bytes_view = std::basic_string_view<int8_t>;
using bytes_mutable_view = basic_mutable_view<bytes_view::value_type>;
using bytes_opt = std::optional<bytes>;
using sstring_view = std::string_view;

inline bytes to_bytes(bytes&& b) {
    return std::move(b);
}

inline sstring_view to_sstring_view(bytes_view view) {
    return {reinterpret_cast<const char*>(view.data()), view.size()};
}

inline bytes_view to_bytes_view(sstring_view view) {
    return {reinterpret_cast<const int8_t*>(view.data()), view.size()};
}

struct fmt_hex {
    const bytes_view& v;
    fmt_hex(const bytes_view& v) noexcept : v(v) {}
};

std::ostream& operator<<(std::ostream& os, const fmt_hex& hex);

bytes from_hex(sstring_view s);
sstring to_hex(bytes_view b);
sstring to_hex(const bytes& b);
sstring to_hex(const bytes_opt& b);

std::ostream& operator<<(std::ostream& os, const bytes& b);
std::ostream& operator<<(std::ostream& os, const bytes_opt& b);

template <>
struct fmt::formatter<fmt_hex> {
    size_t _group_size_in_bytes = 0;
    char _delimiter = ' ';
public:
    // format_spec := [group_size[delimeter]]
    // group_size := a char from '0' to '9'
    // delimeter := a char other than '{'  or '}'
    //
    // by default, the given bytes are printed without delimeter, just
    // like a string. so a string view of {0x20, 0x01, 0x0d, 0xb8} is
    // printed like:
    // "20010db8".
    //
    // but the format specifier can be used to customize how the bytes
    // are printed. for instance, to print an bytes_view like IPv6. so
    // the format specfier would be "{:2:}", where
    // - "2": bytes are printed in groups of 2 bytes
    // - ":": each group is delimeted by ":"
    // and the formatted output will look like:
    // "2001:0db8:0000"
    //
    // or we can mimic how the default format of used by hexdump using
    // "{:2 }", where
    // - "2": bytes are printed in group of 2 bytes
    // - " ": each group is delimeted by " "
    // and the formatted output will look like:
    // "2001 0db8 0000"
    //
    // or we can just print each bytes and separate them by a dash using
    // "{:1-}"
    // and the formatted output will look like:
    // "20-01-0b-b8-00-00"
    constexpr auto parse(fmt::format_parse_context& ctx) {
        // get the delimeter if any
        auto it = ctx.begin();
        auto end = ctx.end();
        if (it != end) {
            int group_size = *it++ - '0';
            if (group_size < 0 ||
                static_cast<size_t>(group_size) > sizeof(uint64_t)) {
                throw format_error("invalid group_size");
            }
            _group_size_in_bytes = group_size;
            if (it != end) {
                // optional delimiter
                _delimiter = *it++;
            }
        }
        if (it != end && *it != '}') {
            throw format_error("invalid format");
        }
        return it;
    }
    template <typename FormatContext>
    auto format(const ::fmt_hex& s, FormatContext& ctx) const {
        auto out = ctx.out();
        const auto& v = s.v;
        if (_group_size_in_bytes > 0) {
            for (size_t i = 0, size = v.size(); i < size; i++) {
                if (i != 0 && i % _group_size_in_bytes == 0) {
                    fmt::format_to(out, "{}{:02x}", _delimiter, std::byte(v[i]));
                } else {
                    fmt::format_to(out, "{:02x}", std::byte(v[i]));
                }
            }
        } else {
            for (auto b : v) {
                fmt::format_to(out, "{:02x}", std::byte(b));
            }
        }
        return out;
    }
};

template <>
struct fmt::formatter<bytes> : fmt::formatter<fmt_hex> {
    template <typename FormatContext>
    auto format(const ::bytes& s, FormatContext& ctx) const {
        return fmt::formatter<::fmt_hex>::format(::fmt_hex(bytes_view(s)), ctx);
    }
};

namespace std {

// Must be in std:: namespace, or ADL fails
std::ostream& operator<<(std::ostream& os, const bytes_view& b);

}

template<>
struct appending_hash<bytes> {
    template<typename Hasher>
    void operator()(Hasher& h, const bytes& v) const {
        feed_hash(h, v.size());
        h.update(reinterpret_cast<const char*>(v.cbegin()), v.size() * sizeof(bytes::value_type));
    }
};

template<>
struct appending_hash<bytes_view> {
    template<typename Hasher>
    void operator()(Hasher& h, bytes_view v) const {
        feed_hash(h, v.size());
        h.update(reinterpret_cast<const char*>(v.begin()), v.size() * sizeof(bytes_view::value_type));
    }
};

using bytes_view_hasher = simple_xx_hasher;

namespace std {
template <>
struct hash<bytes_view> {
    size_t operator()(bytes_view v) const {
        bytes_view_hasher h;
        appending_hash<bytes_view>{}(h, v);
        return h.finalize();
    }
};
} // namespace std

inline std::strong_ordering compare_unsigned(bytes_view v1, bytes_view v2) {
  auto size = std::min(v1.size(), v2.size());
  if (size) {
    auto n = memcmp(v1.begin(), v2.begin(), size);
    if (n) {
        return n <=> 0;
    }
  }
    return v1.size() <=> v2.size();
}

#include <iosfwd>

namespace db {

/// CQL consistency levels.
///
/// Values are guaranteed to be dense and in the tight range [MIN_VALUE, MAX_VALUE].
enum class consistency_level {
    ANY, MIN_VALUE = ANY,
    ONE,
    TWO,
    THREE,
    QUORUM,
    ALL,
    LOCAL_QUORUM,
    EACH_QUORUM,
    SERIAL,
    LOCAL_SERIAL,
    LOCAL_ONE, MAX_VALUE = LOCAL_ONE
};

std::ostream& operator<<(std::ostream& os, consistency_level cl);

}


#include <assert.h>
#include <cstdint>
#include <iosfwd>

namespace db {

enum class write_type : uint8_t {
    SIMPLE,
    BATCH,
    UNLOGGED_BATCH,
    COUNTER,
    BATCH_LOG,
    CAS,
    VIEW,
};

std::ostream& operator<<(std::ostream& os, const write_type& t);

}



#include <cstdint>
#include <iosfwd>

namespace db {

enum class operation_type : uint8_t {
    read = 0,
    write = 1
};

std::ostream& operator<<(std::ostream& os, operation_type op_type);

}

#include <seastar/core/print.hh>

namespace utils {

/**
 * The following calculations are taken from:
 * http://www.cs.wisc.edu/~cao/papers/summary-cache/node8.html
 * "Bloom Filters - the math"
 *
 * This class's static methods are meant to facilitate the use of the Bloom
 * Filter class by helping to choose correct values of 'bits per element' and
 * 'number of hash functions, k'.
 */
namespace bloom_calculations {

    /**
     * A wrapper class that holds two key parameters for a Bloom Filter: the
     * number of hash functions used, and the number of buckets per element used.
     */
    struct bloom_specification final {
        int K; // number of hash functions.
        int buckets_per_element;

        bloom_specification(int k, int buckets_per_element) : K(k), buckets_per_element(buckets_per_element) { }

        operator sstring() {
            return format("bloom_specification(K={:d}, buckets_per_element={:d})", K, buckets_per_element);
        }
    };

    int constexpr min_buckets = 2;
    int constexpr min_k = 1;
    int constexpr EXCESS = 20;

    extern const std::vector<std::vector<double>> probs;
    extern const std::vector<int> opt_k_per_buckets;

    /**
     * Given the number of buckets that can be used per element, return a
     * specification that minimizes the false positive rate.
     *
     * @param buckets_per_element The number of buckets per element for the filter.
     * @return A spec that minimizes the false positive rate.
     */
    inline bloom_specification compute_bloom_spec(int buckets_per_element) {
        assert(buckets_per_element >= 1);
        assert(buckets_per_element <= int(probs.size()) - 1);
        return bloom_specification(opt_k_per_buckets[buckets_per_element], buckets_per_element);
    }

    /**
     * Given a maximum tolerable false positive probability, compute a Bloom
     * specification which will give less than the specified false positive rate,
     * but minimize the number of buckets per element and the number of hash
     * functions used.  Because bandwidth (and therefore total bitvector size)
     * is considered more expensive than computing power, preference is given
     * to minimizing buckets per element rather than number of hash functions.
     *
     * @param max_buckets_per_element The maximum number of buckets available for the filter.
     * @param max_false_pos_prob The maximum tolerable false positive rate.
     * @return A Bloom Specification which would result in a false positive rate
     * less than specified by the function call
     * @throws unsupported_operation_exception if a filter satisfying the parameters cannot be met
     */
    inline bloom_specification compute_bloom_spec(int max_buckets_per_element, double max_false_pos_prob) {
        assert(max_buckets_per_element >= 1);
        assert(max_buckets_per_element <= int(probs.size()) - 1);

        auto max_k = int(probs[max_buckets_per_element].size()) - 1;

        // Handle the trivial cases
        if(max_false_pos_prob >= probs[min_buckets][min_k]) {
            return bloom_specification(2, opt_k_per_buckets[2]);
        }

        if (max_false_pos_prob < probs[max_buckets_per_element][max_k]) {
        }

        // First find the minimal required number of buckets:
        int buckets_per_element = 2;
        int K = opt_k_per_buckets[2];

        while(probs[buckets_per_element][K] > max_false_pos_prob){
            buckets_per_element++;
            K = opt_k_per_buckets[buckets_per_element];
        }
        // Now that the number of buckets is sufficient, see if we can relax K
        // without losing too much precision.
        while(probs[buckets_per_element][K - 1] <= max_false_pos_prob){
            K--;
        }

        return bloom_specification(K, buckets_per_element);
    }

    /**
     * Calculates the maximum number of buckets per element that this implementation
     * can support.  Crucially, it will lower the bucket count if necessary to meet
     * BitSet's size restrictions.
     */
    inline int max_buckets_per_element(long num_elements) {
        num_elements = std::max(1l, num_elements);

        auto v = std::numeric_limits<long>::max() - EXCESS;
        v = v / num_elements;

        if (v < 1) {
        }
        return std::min(probs.size() - 1, size_t(v));
    }

    /**
     * Retrieves the minimum supported bloom_filter_fp_chance value
     * if compute_bloom_spec() above is attempted with bloom_filter_fp_chance
     * lower than this, it will throw an unsupported_operation_exception.
     */
    inline double min_supported_bloom_filter_fp_chance() {
        return probs.back().back();
    }

}

}

#if 0
package org.apache.cassandra.utils;

/**
 * The following calculations are taken from:
 * http://www.cs.wisc.edu/~cao/papers/summary-cache/node8.html
 * "Bloom Filters - the math"
 *
 * This class's static methods are meant to facilitate the use of the Bloom
 * Filter class by helping to choose correct values of 'bits per element' and
 * 'number of hash functions, k'.
 */
class BloomCalculations {

    private static final int minBuckets = 2;
    private static final int minK = 1;

    private static final int EXCESS = 20;

    /**
     * In the following keyspaceName, the row 'i' shows false positive rates if i buckets
     * per element are used.  Cell 'j' shows false positive rates if j hash
     * functions are used.  The first row is 'i=0', the first column is 'j=0'.
     * Each cell (i,j) the false positive rate determined by using i buckets per
     * element and j hash functions.
     */
    static final double[][] probs = new double[][]{
        {1.0}, // dummy row representing 0 buckets per element
        {1.0, 1.0}, // dummy row representing 1 buckets per element
        {1.0, 0.393,  0.400},
        {1.0, 0.283,  0.237,   0.253},
        {1.0, 0.221,  0.155,   0.147,   0.160},
        {1.0, 0.181,  0.109,   0.092,   0.092,   0.101}, // 5
        {1.0, 0.154,  0.0804,  0.0609,  0.0561,  0.0578,   0.0638},
        {1.0, 0.133,  0.0618,  0.0423,  0.0359,  0.0347,   0.0364},
        {1.0, 0.118,  0.0489,  0.0306,  0.024,   0.0217,   0.0216,   0.0229},
        {1.0, 0.105,  0.0397,  0.0228,  0.0166,  0.0141,   0.0133,   0.0135,   0.0145},
        {1.0, 0.0952, 0.0329,  0.0174,  0.0118,  0.00943,  0.00844,  0.00819,  0.00846}, // 10
        {1.0, 0.0869, 0.0276,  0.0136,  0.00864, 0.0065,   0.00552,  0.00513,  0.00509},
        {1.0, 0.08,   0.0236,  0.0108,  0.00646, 0.00459,  0.00371,  0.00329,  0.00314},
        {1.0, 0.074,  0.0203,  0.00875, 0.00492, 0.00332,  0.00255,  0.00217,  0.00199,  0.00194},
        {1.0, 0.0689, 0.0177,  0.00718, 0.00381, 0.00244,  0.00179,  0.00146,  0.00129,  0.00121,  0.0012},
        {1.0, 0.0645, 0.0156,  0.00596, 0.003,   0.00183,  0.00128,  0.001,    0.000852, 0.000775, 0.000744}, // 15
        {1.0, 0.0606, 0.0138,  0.005,   0.00239, 0.00139,  0.000935, 0.000702, 0.000574, 0.000505, 0.00047,  0.000459},
        {1.0, 0.0571, 0.0123,  0.00423, 0.00193, 0.00107,  0.000692, 0.000499, 0.000394, 0.000335, 0.000302, 0.000287, 0.000284},
        {1.0, 0.054,  0.0111,  0.00362, 0.00158, 0.000839, 0.000519, 0.00036,  0.000275, 0.000226, 0.000198, 0.000183, 0.000176},
        {1.0, 0.0513, 0.00998, 0.00312, 0.0013,  0.000663, 0.000394, 0.000264, 0.000194, 0.000155, 0.000132, 0.000118, 0.000111, 0.000109},
        {1.0, 0.0488, 0.00906, 0.0027,  0.00108, 0.00053,  0.000303, 0.000196, 0.00014,  0.000108, 8.89e-05, 7.77e-05, 7.12e-05, 6.79e-05, 6.71e-05} // 20
    };  // the first column is a dummy column representing K=0.

    /**
     * The optimal number of hashes for a given number of bits per element.
     * These values are automatically calculated from the data above.
     */
    private static final int[] optKPerBuckets = new int[probs.length];

    static
    {
        for (int i = 0; i < probs.length; i++)
        {
            double min = Double.MAX_VALUE;
            double[] prob = probs[i];
            for (int j = 0; j < prob.length; j++)
            {
                if (prob[j] < min)
                {
                    min = prob[j];
                    optKPerBuckets[i] = Math.max(minK, j);
                }
            }
        }
    }

    /**
     * Given the number of buckets that can be used per element, return a
     * specification that minimizes the false positive rate.
     *
     * @param bucketsPerElement The number of buckets per element for the filter.
     * @return A spec that minimizes the false positive rate.
     */
    public static BloomSpecification computeBloomSpec(int bucketsPerElement)
    {
        assert bucketsPerElement >= 1;
        assert bucketsPerElement <= probs.length - 1;
        return new BloomSpecification(optKPerBuckets[bucketsPerElement], bucketsPerElement);
    }

    /**
     * A wrapper class that holds two key parameters for a Bloom Filter: the
     * number of hash functions used, and the number of buckets per element used.
     */
    public static class BloomSpecification
    {
        final int K; // number of hash functions.
        final int bucketsPerElement;

        public BloomSpecification(int k, int bucketsPerElement)
        {
            K = k;
            this.bucketsPerElement = bucketsPerElement;
        }

        public String toString()
        {
            return String.format("BloomSpecification(K=%d, bucketsPerElement=%d)", K, bucketsPerElement);
        }
    }

    /**
     * Given a maximum tolerable false positive probability, compute a Bloom
     * specification which will give less than the specified false positive rate,
     * but minimize the number of buckets per element and the number of hash
     * functions used.  Because bandwidth (and therefore total bitvector size)
     * is considered more expensive than computing power, preference is given
     * to minimizing buckets per element rather than number of hash functions.
     *
     * @param maxBucketsPerElement The maximum number of buckets available for the filter.
     * @param maxFalsePosProb The maximum tolerable false positive rate.
     * @return A Bloom Specification which would result in a false positive rate
     * less than specified by the function call
     * @throws UnsupportedOperationException if a filter satisfying the parameters cannot be met
     */
    public static BloomSpecification computeBloomSpec(int maxBucketsPerElement, double maxFalsePosProb)
    {
        assert maxBucketsPerElement >= 1;
        assert maxBucketsPerElement <= probs.length - 1;
        int maxK = probs[maxBucketsPerElement].length - 1;

        // Handle the trivial cases
        if(maxFalsePosProb >= probs[minBuckets][minK]) {
            return new BloomSpecification(2, optKPerBuckets[2]);
        }
        if (maxFalsePosProb < probs[maxBucketsPerElement][maxK]) {
            throw new UnsupportedOperationException(String.format("Unable to satisfy %s with %s buckets per element",
                                                                  maxFalsePosProb, maxBucketsPerElement));
        }

        // First find the minimal required number of buckets:
        int bucketsPerElement = 2;
        int K = optKPerBuckets[2];
        while(probs[bucketsPerElement][K] > maxFalsePosProb){
            bucketsPerElement++;
            K = optKPerBuckets[bucketsPerElement];
        }
        // Now that the number of buckets is sufficient, see if we can relax K
        // without losing too much precision.
        while(probs[bucketsPerElement][K - 1] <= maxFalsePosProb){
            K--;
        }

        return new BloomSpecification(K, bucketsPerElement);
    }

    /**
     * Calculates the maximum number of buckets per element that this implementation
     * can support.  Crucially, it will lower the bucket count if necessary to meet
     * BitSet's size restrictions.
     */
    public static int maxBucketsPerElement(long numElements)
    {
        numElements = Math.max(1, numElements);
        double v = (Long.MAX_VALUE - EXCESS) / (double)numElements;
        if (v < 1.0)
        {
            throw new UnsupportedOperationException("Cannot compute probabilities for " + numElements + " elements.");
        }
        return Math.min(BloomCalculations.probs.length - 1, (int)v);
    }
}
#endif



namespace utils {

struct i_filter;
using filter_ptr = std::unique_ptr<i_filter>;

enum class filter_format {
    k_l_format,
    m_format,
};

class hashed_key {
private:
    std::array<uint64_t, 2> _hash;
public:
    hashed_key(std::array<uint64_t, 2> h) : _hash(h) {}
    std::array<uint64_t, 2> hash() const { return _hash; };
};

hashed_key make_hashed_key(bytes_view key);

// FIXME: serialize() and serialized_size() not implemented. We should only be serializing to
// disk, not in the wire.
struct i_filter {
    virtual ~i_filter() {}

    virtual void add(const bytes_view& key) = 0;
    virtual bool is_present(const bytes_view& key) = 0;
    virtual bool is_present(hashed_key) = 0;
    virtual void clear() = 0;
    virtual void close() = 0;

    virtual size_t memory_size() = 0;

    /**
     * @return The smallest bloom_filter that can provide the given false
     *         positive probability rate for the given number of elements.
     *
     *         Asserts that the given probability can be satisfied using this
     *         filter.
     */
    static filter_ptr get_filter(int64_t num_elements, double max_false_pos_prob, filter_format format);
};
}

/**
 * This is a very fast, non-cryptographic hash suitable for general hash-based
 * lookup. See http://murmurhash.googlepages.com/ (Murmur Hash 2) and
 * https://code.google.com/p/smhasher/wiki/MurmurHash3.
 *
 * This code is not based on the original Murmur Hash C code, but rather
 * a translation of Cassandra's Java version back to C.
 **/

namespace utils {

namespace murmur_hash {

uint32_t hash32(bytes_view data, int32_t seed);
uint64_t hash2_64(bytes_view key, uint64_t seed);

template<typename InputIterator>
inline
uint64_t read_block(InputIterator& in) {
    typename std::iterator_traits<InputIterator>::value_type tmp[8];
    for (int i = 0; i < 8; ++i) {
        tmp[i] = *in;
        ++in;
    }
    return ((uint64_t) tmp[0] & 0xff) + (((uint64_t) tmp[1] & 0xff) << 8) +
           (((uint64_t) tmp[2] & 0xff) << 16) + (((uint64_t) tmp[3] & 0xff) << 24) +
           (((uint64_t) tmp[4] & 0xff) << 32) + (((uint64_t) tmp[5] & 0xff) << 40) +
           (((uint64_t) tmp[6] & 0xff) << 48) + (((uint64_t) tmp[7] & 0xff) << 56);
}

static inline
uint64_t rotl64(uint64_t v, uint32_t n) {
    return ((v << n) | ((uint64_t)v >> (64 - n)));
}

static inline
uint64_t fmix(uint64_t k) {
    k ^= (uint64_t)k >> 33;
    k *= 0xff51afd7ed558ccdL;
    k ^= (uint64_t)k >> 33;
    k *= 0xc4ceb9fe1a85ec53L;
    k ^= (uint64_t)k >> 33;

    return k;
}

template <typename InputIterator>
void hash3_x64_128(InputIterator in, uint32_t length, uint64_t seed, std::array<uint64_t, 2>& result) {
    const uint32_t nblocks = length >> 4; // Process as 128-bit blocks.

    uint64_t h1 = seed;
    uint64_t h2 = seed;

    uint64_t c1 = 0x87c37b91114253d5L;
    uint64_t c2 = 0x4cf5ad432745937fL;

    //----------
    // body

    for(uint32_t i = 0; i < nblocks; i++)
    {
        uint64_t k1 = read_block(in);
        uint64_t k2 = read_block(in);

        k1 *= c1; k1 = rotl64(k1,31); k1 *= c2; h1 ^= k1;

        h1 = rotl64(h1,27); h1 += h2; h1 = h1*5+0x52dce729;

        k2 *= c2; k2  = rotl64(k2,33); k2 *= c1; h2 ^= k2;

        h2 = rotl64(h2,31); h2 += h1; h2 = h2*5+0x38495ab5;
    }

    //----------
    // tail

    uint64_t k1 = 0;
    uint64_t k2 = 0;

    typename std::iterator_traits<InputIterator>::value_type tmp[15];
    std::copy_n(in, length & 15, tmp);

    switch (length & 15)
    {
        case 15: k2 ^= ((uint64_t) tmp[14]) << 48;
        case 14: k2 ^= ((uint64_t) tmp[13]) << 40;
        case 13: k2 ^= ((uint64_t) tmp[12]) << 32;
        case 12: k2 ^= ((uint64_t) tmp[11]) << 24;
        case 11: k2 ^= ((uint64_t) tmp[10]) << 16;
        case 10: k2 ^= ((uint64_t) tmp[9]) << 8;
        case  9: k2 ^= ((uint64_t) tmp[8]) << 0;
            k2 *= c2; k2  = rotl64(k2,33); k2 *= c1; h2 ^= k2;
        case  8: k1 ^= ((uint64_t) tmp[7]) << 56;
        case  7: k1 ^= ((uint64_t) tmp[6]) << 48;
        case  6: k1 ^= ((uint64_t) tmp[5]) << 40;
        case  5: k1 ^= ((uint64_t) tmp[4]) << 32;
        case  4: k1 ^= ((uint64_t) tmp[3]) << 24;
        case  3: k1 ^= ((uint64_t) tmp[2]) << 16;
        case  2: k1 ^= ((uint64_t) tmp[1]) << 8;
        case  1: k1 ^= ((uint64_t) tmp[0]);
            k1 *= c1; k1  = rotl64(k1,31); k1 *= c2; h1 ^= k1;
    };

    //----------
    // finalization

    h1 ^= length;
    h2 ^= length;

    h1 += h2;
    h2 += h1;

    h1 = fmix(h1);
    h2 = fmix(h2);

    h1 += h2;
    h2 += h1;

    result[0] = h1;
    result[1] = h2;
}

void hash3_x64_128(bytes_view key, uint64_t seed, std::array<uint64_t, 2>& result);

} // namespace murmur_hash

} // namespace utils


#include <boost/range/adaptor/transformed.hpp>

namespace utils {

template <std::ranges::range Range>
std::ostream& format_range(std::ostream& os, const Range& items, std::string_view paren = "{}") {
    fmt::print(os, "{}{}{}", paren.front(), fmt::join(items, ", "), paren.back());
    return os;
}

namespace internal {

template<bool NeedsComma, typename Printable>
struct print_with_comma {
    const Printable& v;
};

template<bool NeedsComma, typename Printable>
std::ostream& operator<<(std::ostream& os, const print_with_comma<NeedsComma, Printable>& x) {
    os << x.v;
    if (NeedsComma) {
        os << ", ";
    }
    return os;
}

} // namespace internal
} // namespace utils

namespace std {

template <typename K, typename V>
std::ostream& operator<<(std::ostream& os, const std::pair<K, V>& p) {
    os << "{" << p.first << ", " << p.second << "}";
    return os;
}

template<typename... T, size_t... I>
std::ostream& print_tuple(std::ostream& os, const std::tuple<T...>& p, std::index_sequence<I...>) {
    return ((os << "{" ) << ... << utils::internal::print_with_comma<I < sizeof...(I) - 1, T>{std::get<I>(p)}) << "}";
}

template <typename... T>
std::ostream& operator<<(std::ostream& os, const std::tuple<T...>& p) {
    return print_tuple(os, p, std::make_index_sequence<sizeof...(T)>());
}

// Vector-like ranges
template <std::ranges::range Range>
requires (
       std::same_as<Range, std::vector<std::ranges::range_value_t<Range>>>
    || std::same_as<Range, std::list<std::ranges::range_value_t<Range>>>
    || std::same_as<Range, std::initializer_list<std::ranges::range_value_t<Range>>>
    || std::same_as<Range, std::deque<std::ranges::range_value_t<Range>>>
)
std::ostream& operator<<(std::ostream& os, const Range& items) {
    return utils::format_range(os, items);
}

template <typename T, typename... Args>
std::ostream& operator<<(std::ostream& os, const std::set<T, Args...>& items) {
    return utils::format_range(os, items);
}

template <typename T, typename... Args>
std::ostream& operator<<(std::ostream& os, const std::unordered_set<T, Args...>& items) {
    return utils::format_range(os, items);
}

template <typename K, typename V, typename... Args>
std::ostream& operator<<(std::ostream& os, const std::map<K, V, Args...>& items) {
    return utils::format_range(os, items);
}

template <typename... Args>
std::ostream& operator<<(std::ostream& os, const boost::transformed_range<Args...>& items) {
    return utils::format_range(os, items);
}

template <typename T, std::size_t N>
std::ostream& operator<<(std::ostream& os, const std::array<T, N>& items) {
    return utils::format_range(os, items, "[]");
}

template <typename T>
std::ostream& operator<<(std::ostream& os, const std::optional<T>& opt) {
    if (opt) {
        os << "{" << *opt << "}";
    } else {
        os << "{}";
    }
    return os;
}

std::ostream& operator<<(std::ostream& os, const std::strong_ordering& order);
std::ostream& operator<<(std::ostream& os, const std::weak_ordering& order);
std::ostream& operator<<(std::ostream& os, const std::partial_ordering& order);

} // namespace std

template<typename T>
struct fmt::formatter<std::optional<T>> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const std::optional<T>& opt, FormatContext& ctx) const {
        if (opt) {
            return fmt::format_to(ctx.out(), "{}", *opt);
        } else {
            return fmt::format_to(ctx.out(), "{{}}");
        }
     }
};



#include <compare>
#include <cstddef>
#include <cstdlib>
#include <cstring>
#include <new>
#include <utility>
#include <algorithm>
#include <initializer_list>
#include <memory>
#include <stdexcept>
#include <malloc.h>
#include <iostream>


namespace utils {

/// A vector with small buffer optimisation
///
/// small_vector is a variation of std::vector<> that reserves a configurable
/// amount of storage internally, without the need for memory allocation.
/// This can bring measurable gains if the expected number of elements is
/// small. The drawback is that moving such small_vector is more expensive
/// and invalidates iterators as well as references which disqualifies it in
/// some cases.
///
/// All member functions of small_vector provide strong exception guarantees.
///
/// It is unspecified when small_vector is going to use internal storage, except
/// for the obvious case when size() > N. In other situations user must not
/// attempt to guess if data is stored internally or externally. The same applies
/// to capacity(). Apart from the obvious fact that capacity() >= size() the user
/// must not assume anything else. In particular it may not always hold that
/// capacity() >= N.
///
/// Unless otherwise specified (e.g. move ctor and assignment) small_vector
/// provides guarantees at least as strong as those of std::vector<>.
template<typename T, size_t N>
requires std::is_nothrow_move_constructible_v<T> && std::is_nothrow_move_assignable_v<T> && std::is_nothrow_destructible_v<T> && (N > 0)
class small_vector {
private:
    T* _begin;
    T* _end;
    T* _capacity_end;

    // Use union instead of std::aligned_storage so that debuggers can see
    // the contained objects without needing any pretty printers.
    union internal {
        internal() { }
        ~internal() { }
        T storage[N];
    };
    internal _internal;

private:
    bool uses_internal_storage() const noexcept {
        return _begin == _internal.storage;
    }

    [[gnu::cold]] [[gnu::noinline]]
    void expand(size_t new_capacity) {
        auto ptr = static_cast<T*>(::aligned_alloc(alignof(T), new_capacity * sizeof(T)));
        if (!ptr) {
            throw std::bad_alloc();
        }
        auto n_end = std::uninitialized_move(begin(), end(), ptr);
        std::destroy(begin(), end());
        if (!uses_internal_storage()) {
            std::free(_begin);
        }
        _begin = ptr;
        _end = n_end;
        _capacity_end = ptr + new_capacity;
    }

    [[gnu::cold]] [[gnu::noinline]]
    void slow_copy_assignment(const small_vector& other) {
        auto ptr = static_cast<T*>(::aligned_alloc(alignof(T), other.size() * sizeof(T)));
        if (!ptr) {
            throw std::bad_alloc();
        }
        auto n_end = ptr;
        try {
            n_end = std::uninitialized_copy(other.begin(), other.end(), n_end);
        } catch (...) {
            std::free(ptr);
            throw;
        }
        std::destroy(begin(), end());
        if (!uses_internal_storage()) {
            std::free(_begin);
        }
        _begin = ptr;
        _end = n_end;
        _capacity_end = n_end;
    }

    void reserve_at_least(size_t n) {
        if (__builtin_expect(_begin + n > _capacity_end, false)) {
            expand(std::max(n, capacity() * 2));
        }
    }

    [[noreturn]] [[gnu::cold]] [[gnu::noinline]]
    void throw_out_of_range() const {
        throw std::out_of_range("out of range small vector access");
    }

public:
    using value_type = T;
    using pointer = T*;
    using const_pointer = const T*;
    using reference = T&;
    using const_reference = const T&;

    using iterator = T*;
    using const_iterator = const T*;

    using reverse_iterator = std::reverse_iterator<iterator>;
    using const_reverse_iterator = std::reverse_iterator<const_iterator>;

    small_vector() noexcept
        : _begin(_internal.storage)
        , _end(_begin)
        , _capacity_end(_begin + N)
    { }

    template<typename InputIterator>
    small_vector(InputIterator first, InputIterator last) : small_vector() {
        if constexpr (std::is_base_of_v<std::forward_iterator_tag, typename std::iterator_traits<InputIterator>::iterator_category>) {
            reserve(std::distance(first, last));
            _end = std::uninitialized_copy(first, last, _end);
        } else {
            std::copy(first, last, std::back_inserter(*this));
        }
    }

    small_vector(std::initializer_list<T> list) : small_vector(list.begin(), list.end()) { }

    // May invalidate iterators and references.
    small_vector(small_vector&& other) noexcept {
        if (other.uses_internal_storage()) {
            _begin = _internal.storage;
            _capacity_end = _begin + N;
            if constexpr (std::is_trivially_copyable_v<T>) {
                // Compilers really like loops with the number of iterations known at
                // the compile time, the usually emit less code which can be more aggressively
                // optimised. Since we can assume that N is small it is most likely better
                // to just copy everything, regardless of how many elements are actually in
                // the vector.
                std::memcpy(_internal.storage, other._internal.storage, N * sizeof(T));
                _end = _begin + other.size();
            } else {
                _end = _begin;

                // What we would really like here is std::uninintialized_move_and_destroy.
                // It is beneficial to do move and destruction in a single pass since the compiler
                // may be able to merge those operations (e.g. the destruction of a move-from
                // std::unique_ptr is a no-op).
                for (auto& e : other) {
                    new (_end++) T(std::move(e));
                    e.~T();
                }
            }
            other._end = other._internal.storage;
        } else {
            _begin = std::exchange(other._begin, other._internal.storage);
            _end = std::exchange(other._end, other._internal.storage);
            _capacity_end = std::exchange(other._capacity_end, other._internal.storage + N);
        }
    }

    small_vector(const small_vector& other) : small_vector() {
        reserve(other.size());
        _end = std::uninitialized_copy(other.begin(), other.end(), _end);
    }

    // May invalidate iterators and references.
    small_vector& operator=(small_vector&& other) noexcept {
        clear();
        if (other.uses_internal_storage()) {
            if (__builtin_expect(!uses_internal_storage(), false)) {
                std::free(_begin);
                _begin = _internal.storage;
            }
            _capacity_end = _begin + N;
            if constexpr (std::is_trivially_copyable_v<T>) {
                std::memcpy(_internal.storage, other._internal.storage, N * sizeof(T));
                _end = _begin + other.size();
            } else {
                _end = _begin;

                // Better to use single pass than std::uninitialize_move + std::destroy.
                // See comment in move ctor for details.
                for (auto& e : other) {
                    new (_end++) T(std::move(e));
                    e.~T();
                }
            }
            other._end = other._internal.storage;
        } else {
            if (__builtin_expect(!uses_internal_storage(), false)) {
                std::free(_begin);
            }
            _begin = std::exchange(other._begin, other._internal.storage);
            _end = std::exchange(other._end, other._internal.storage);
            _capacity_end = std::exchange(other._capacity_end, other._internal.storage + N);
        }
        return *this;
    }

    small_vector& operator=(const small_vector& other) {
        if constexpr (std::is_nothrow_copy_constructible_v<T>) {
            if (capacity() >= other.size()) {
                clear();
                _end = std::uninitialized_copy(other.begin(), other.end(), _end);
                return *this;
            }
        }
        slow_copy_assignment(other);
        return *this;
    }

    ~small_vector() {
        clear();
        if (__builtin_expect(!uses_internal_storage(), false)) {
            std::free(_begin);
        }
    }

    size_t external_memory_usage() const {
        if (uses_internal_storage()) {
            return 0;
        }
        return ::malloc_usable_size(_begin);
    }

    void reserve(size_t n) {
        if (__builtin_expect(_begin + n > _capacity_end, false)) {
            expand(n);
        }
    }

    void clear() noexcept {
        std::destroy(_begin, _end);
        _end = _begin;
    }

    iterator begin() noexcept { return _begin; }
    const_iterator begin() const noexcept { return _begin; }
    const_iterator cbegin() const noexcept { return _begin; }

    iterator end() noexcept { return _end; }
    const_iterator end() const noexcept { return _end; }
    const_iterator cend() const noexcept { return _end; }

    reverse_iterator rbegin() noexcept { return reverse_iterator(end()); }
    const_reverse_iterator rbegin() const noexcept { return const_reverse_iterator(end()); }
    const_reverse_iterator crbegin() const noexcept { return const_reverse_iterator(end()); }

    reverse_iterator rend() noexcept { return reverse_iterator(begin()); }
    const_reverse_iterator rend() const noexcept { return const_reverse_iterator(begin()); }
    const_reverse_iterator crend() const noexcept { return const_reverse_iterator(begin()); }

    T* data() noexcept { return _begin; }
    const T* data() const noexcept { return _begin; }

    T& front() noexcept { return *begin(); }
    const T& front() const noexcept { return *begin(); }

    T& back() noexcept { return end()[-1]; }
    const T& back() const noexcept { return end()[-1]; }

    T& operator[](size_t idx) noexcept { return data()[idx]; }
    const T& operator[](size_t idx) const noexcept { return data()[idx]; }

    T& at(size_t idx) {
        if (__builtin_expect(idx >= size(), false)) {
            throw_out_of_range();
        }
        return operator[](idx);
    }
    const T& at(size_t idx) const {
        if (__builtin_expect(idx >= size(), false)) {
            throw_out_of_range();
        }
        return operator[](idx);
    }

    bool empty() const noexcept { return _begin == _end; }
    size_t size() const noexcept { return _end - _begin; }
    size_t capacity() const noexcept { return _capacity_end - _begin; }

    template<typename... Args>
    T& emplace_back(Args&&... args) {
        if (__builtin_expect(_end == _capacity_end, false)) {
            expand(std::max<size_t>(capacity() * 2, 1));
        }
        auto& ref = *new (_end) T(std::forward<Args>(args)...);
        ++_end;
        return ref;
    }

    T& push_back(const T& value) {
        return emplace_back(value);
    }

    T& push_back(T&& value) {
        return emplace_back(std::move(value));
    }

    template<typename InputIterator>
    iterator insert(const_iterator cpos, InputIterator first, InputIterator last) {
        if constexpr (std::is_base_of_v<std::forward_iterator_tag, typename std::iterator_traits<InputIterator>::iterator_category>) {
            if (first == last) {
                return const_cast<iterator>(cpos);
            }
            auto idx = cpos - _begin;
            auto new_count = std::distance(first, last);
            reserve_at_least(size() + new_count);
            auto pos = _begin + idx;
            auto after = std::distance(pos, end());
            if (__builtin_expect(pos == end(), true)) {
                _end = std::uninitialized_copy(first, last, end());
                return pos;
            } else if (after > new_count) {
                std::uninitialized_move(end() - new_count, end(), end());
                std::move_backward(pos, end() - new_count, end());
                try {
                    std::copy(first, last, pos);
                } catch (...) {
                    std::move(pos + new_count, end() + new_count, pos);
                    std::destroy(end(), end() + new_count);
                    throw;
                }
            } else {
                std::uninitialized_move(pos, end(), pos + new_count);
                auto mid = std::next(first, after);
                try {
                    std::uninitialized_copy(mid, last, end());
                    try {
                        std::copy(first, mid, pos);
                    } catch (...) {
                        std::destroy(end(), pos + new_count);
                        throw;
                    }
                } catch (...) {
                    std::move(pos + new_count, end() + new_count, pos);
                    std::destroy(pos + new_count, end() + new_count);
                    throw;
                }

            }
            _end += new_count;
            return pos;
        } else {
            auto start = cpos - _begin;
            auto idx = start;
            while (first != last) {
                try {
                    insert(begin() + idx, *first);
                    ++first;
                    ++idx;
                } catch (...) {
                    erase(begin() + start, begin() + idx);
                    throw;
                }
            }
            return begin() + idx;
        }
    }

    template<typename... Args>
    iterator emplace(const_iterator cpos, Args&&... args) {
        auto idx = cpos - _begin;
        reserve_at_least(size() + 1);
        auto pos = _begin + idx;
        if (pos != _end) {
            new (_end) T(std::move(_end[-1]));
            std::move_backward(pos, _end - 1, _end);
            pos->~T();
        }
        try {
            new (pos) T(std::forward<Args>(args)...);
        } catch (...) {
            if (pos != _end) {
                new (pos) T(std::move(pos[1]));
                std::move(pos + 2, _end + 1, pos + 1);
                _end->~T();
            }
            throw;
        }
        _end++;
        return pos;
    }

    iterator insert(const_iterator cpos, const T& obj) {
        return emplace(cpos, obj);
    }

    iterator insert(const_iterator cpos, T&& obj) {
        return emplace(cpos, std::move(obj));
    }

    void resize(size_t n) {
        if (n < size()) {
            erase(end() - (size() - n), end());
        } else if (n > size()) {
            reserve_at_least(n);
            _end = std::uninitialized_value_construct_n(_end, n - size());
        }
    }

    void resize(size_t n, const T& value) {
        if (n < size()) {
            erase(end() - (size() - n), end());
        } else if (n > size()) {
            reserve_at_least(n);
            auto nend = _begin + n;
            std::uninitialized_fill(_end, nend, value);
            _end = nend;
        }
    }

    void pop_back() noexcept {
        (--_end)->~T();
    }

    iterator erase(const_iterator cit) noexcept {
        return erase(cit, cit + 1);
    }

    iterator erase(const_iterator cfirst, const_iterator clast) noexcept {
        auto first = const_cast<iterator>(cfirst);
        auto last = const_cast<iterator>(clast);
        std::move(last, end(), first);
        auto nend = _end - (clast - cfirst);
        std::destroy(nend, _end);
        _end = nend;
        return first;
    }

    void swap(small_vector& other) noexcept {
        std::swap(*this, other);
    }

    auto operator<=>(const small_vector& other) const noexcept requires std::three_way_comparable<T> {
        return std::lexicographical_compare_three_way(this->begin(), this->end(),
                                                      other.begin(), other.end());
    }

    bool operator==(const small_vector& other) const noexcept {
        return size() == other.size() && std::equal(_begin, _end, other.begin());
    }
};

template <typename T, size_t N>
std::ostream& operator<<(std::ostream& os, const utils::small_vector<T, N>& v) {
    return utils::format_range(os, v);
}

}


#include <seastar/core/sstring.hh>
#include <ranges>
#include <vector>
#include <sstream>
#include <unordered_set>
#include <set>
#include <optional>
#include <list>
#include <map>
#include <array>
#include <deque>

#include <fmt/format.h>


// chunked_vector is a vector-like container that uses discontiguous storage.
// It provides fast random access, the ability to append at the end, and aims
// to avoid large contiguous allocations - unlike std::vector which allocates
// all the data in one contiguous allocation.
//
// std::deque aims to achieve the same goals, but its implementation in
// libstdc++ still results in large contiguous allocations: std::deque
// keeps the items in small (512-byte) chunks, and then keeps a contiguous
// vector listing these chunks. This chunk vector can grow pretty big if the
// std::deque grows big: When an std::deque contains just 8 MB of data, it
// needs 16384 chunks, and the vector listing those needs 128 KB.
//
// Therefore, in chunked_vector we use much larger 128 KB chunks (this is
// configurable, with the max_contiguous_allocation template parameter).
// With 128 KB chunks, the contiguous vector listing them is 256 times
// smaller than it would be in std::dequeue with its 512-byte chunks.
//
// In particular, when a chunked_vector stores up to 2 GB of data, the
// largest contiguous allocation is guaranteed to be 128 KB: 2 GB of data
// fits in 16384 chunks of 128 KB each, and the vector of 16384 8-byte
// pointers requires another 128 KB allocation.
//
// Remember, however, that when the chunked_vector grows beyond 2 GB, its
// largest contiguous allocation (used to store the chunk list) continues to
// grow as O(N). This is not a problem for current real-world uses of
// chunked_vector which never reach 2 GB.
//
// Always allocating large 128 KB chunks can be wasteful for small vectors;
// This is why std::deque chose small 512-byte chunks. chunked_vector solves
// this problem differently: It makes the last chunk variable in size,
// possibly smaller than a full 128 KB.

#include <boost/range/algorithm/equal.hpp>
#include <boost/algorithm/clamp.hpp>
#include <boost/version.hpp>
#include <memory>
#include <type_traits>
#include <iterator>
#include <utility>
#include <algorithm>
#include <stdexcept>
#include <malloc.h>


namespace utils {

struct chunked_vector_free_deleter {
    void operator()(void* x) const { ::free(x); }
};

template <typename T, size_t max_contiguous_allocation = 128*1024>
class chunked_vector {
    static_assert(std::is_nothrow_move_constructible<T>::value, "T must be nothrow move constructible");
    using chunk_ptr = std::unique_ptr<T[], chunked_vector_free_deleter>;
    // Each chunk holds max_chunk_capacity() items, except possibly the last
    utils::small_vector<chunk_ptr, 1> _chunks;
    size_t _size = 0;
    size_t _capacity = 0;
public:
    // Maximum number of T elements fitting in a single chunk.
    static size_t max_chunk_capacity() {
        return std::max(max_contiguous_allocation / sizeof(T), size_t(1));
    }
private:
    void reserve_for_push_back() {
        if (_size == _capacity) {
            do_reserve_for_push_back();
        }
    }
    void do_reserve_for_push_back();
    size_t make_room(size_t n, bool stop_after_one);
    chunk_ptr new_chunk(size_t n);
    T* addr(size_t i) const {
        return &_chunks[i / max_chunk_capacity()][i % max_chunk_capacity()];
    }
    void check_bounds(size_t i) const {
        if (i >= _size) {
            throw std::out_of_range("chunked_vector out of range access");
        }
    }
    static void migrate(T* begin, T* end, T* result);
public:
    using value_type = T;
    using size_type = size_t;
    using difference_type = ssize_t;
    using reference = T&;
    using const_reference = const T&;
    using pointer = T*;
    using const_pointer = const T*;
public:
    chunked_vector() = default;
    chunked_vector(const chunked_vector& x);
    chunked_vector(chunked_vector&& x) noexcept;
    template <typename Iterator>
    chunked_vector(Iterator begin, Iterator end);
    template <std::ranges::range Range>
    chunked_vector(const Range& r) : chunked_vector(r.begin(), r.end()) {}
    explicit chunked_vector(size_t n, const T& value = T());
    ~chunked_vector();
    chunked_vector& operator=(const chunked_vector& x);
    chunked_vector& operator=(chunked_vector&& x) noexcept;

    bool empty() const {
        return !_size;
    }
    size_t size() const {
        return _size;
    }
    size_t capacity() const {
        return _capacity;
    }
    T& operator[](size_t i) {
        return *addr(i);
    }
    const T& operator[](size_t i) const {
        return *addr(i);
    }
    T& at(size_t i) {
        check_bounds(i);
        return *addr(i);
    }
    const T& at(size_t i) const {
        check_bounds(i);
        return *addr(i);
    }

    void push_back(const T& x) {
        reserve_for_push_back();
        new (addr(_size)) T(x);
        ++_size;
    }
    void push_back(T&& x) {
        reserve_for_push_back();
        new (addr(_size)) T(std::move(x));
        ++_size;
    }
    template <typename... Args>
    T& emplace_back(Args&&... args) {
        reserve_for_push_back();
        auto& ret = *new (addr(_size)) T(std::forward<Args>(args)...);
        ++_size;
        return ret;
    }
    void pop_back() {
        --_size;
        addr(_size)->~T();
    }
    const T& back() const {
        return *addr(_size - 1);
    }
    T& back() {
        return *addr(_size - 1);
    }

    void clear();
    void shrink_to_fit();
    void resize(size_t n);
    void reserve(size_t n) {
        if (n > _capacity) {
            make_room(n, false);
        }
    }
    /// Reserve some of the memory.
    ///
    /// Allows reserving the memory chunk-by-chunk, avoiding stalls when a lot of
    /// chunks are needed. To drive the reservation to completion, call this
    /// repeatedly with the value returned from the previous call until it
    /// returns 0, yielding between calls when necessary. Example usage:
    ///
    ///     return do_until([&size] { return !size; }, [&my_vector, &size] () mutable {
    ///         size = my_vector.reserve_partial(size);
    ///     });
    ///
    /// Here, `do_until()` takes care of yielding between iterations when
    /// necessary.
    ///
    /// \returns the memory that remains to be reserved
    size_t reserve_partial(size_t n) {
        if (n > _capacity) {
            return make_room(n, true);
        }
        return 0;
    }

    size_t memory_size() const {
        return _capacity * sizeof(T);
    }

    size_t external_memory_usage() const;
public:
    template <class ValueType>
    class iterator_type {
        const chunk_ptr* _chunks;
        size_t _i;
    public:
        using iterator_category = std::random_access_iterator_tag;
        using value_type = ValueType;
        using difference_type = ssize_t;
        using pointer = ValueType*;
        using reference = ValueType&;
    private:
        pointer addr() const {
            return &_chunks[_i / max_chunk_capacity()][_i % max_chunk_capacity()];
        }
        iterator_type(const chunk_ptr* chunks, size_t i) : _chunks(chunks), _i(i) {}
    public:
        iterator_type() = default;
        iterator_type(const iterator_type<std::remove_const_t<ValueType>>& x) : _chunks(x._chunks), _i(x._i) {} // needed for iterator->const_iterator conversion
        reference operator*() const {
            return *addr();
        }
        pointer operator->() const {
            return addr();
        }
        reference operator[](ssize_t n) const {
            return *(*this + n);
        }
        iterator_type& operator++() {
            ++_i;
            return *this;
        }
        iterator_type operator++(int) {
            auto x = *this;
            ++_i;
            return x;
        }
        iterator_type& operator--() {
            --_i;
            return *this;
        }
        iterator_type operator--(int) {
            auto x = *this;
            --_i;
            return x;
        }
        iterator_type& operator+=(ssize_t n) {
            _i += n;
            return *this;
        }
        iterator_type& operator-=(ssize_t n) {
            _i -= n;
            return *this;
        }
        iterator_type operator+(ssize_t n) const {
            auto x = *this;
            return x += n;
        }
        iterator_type operator-(ssize_t n) const {
            auto x = *this;
            return x -= n;
        }
        friend iterator_type operator+(ssize_t n, iterator_type a) {
            return a + n;
        }
        friend ssize_t operator-(iterator_type a, iterator_type b) {
            return a._i - b._i;
        }
        bool operator==(iterator_type x) const {
            return _i == x._i;
        }
        bool operator<(iterator_type x) const {
            return _i < x._i;
        }
        bool operator<=(iterator_type x) const {
            return _i <= x._i;
        }
        bool operator>(iterator_type x) const {
            return _i > x._i;
        }
        bool operator>=(iterator_type x) const {
            return _i >= x._i;
        }
        friend class chunked_vector;
    };
    using iterator = iterator_type<T>;
    using const_iterator = iterator_type<const T>;
public:
    const T& front() const { return *cbegin(); }
    T& front() { return *begin(); }
    iterator begin() { return iterator(_chunks.data(), 0); }
    iterator end() { return iterator(_chunks.data(), _size); }
    const_iterator begin() const { return const_iterator(_chunks.data(), 0); }
    const_iterator end() const { return const_iterator(_chunks.data(), _size); }
    const_iterator cbegin() const { return const_iterator(_chunks.data(), 0); }
    const_iterator cend() const { return const_iterator(_chunks.data(), _size); }
    std::reverse_iterator<iterator> rbegin() { return std::reverse_iterator(end()); }
    std::reverse_iterator<iterator> rend() { return std::reverse_iterator(begin()); }
    std::reverse_iterator<const_iterator> rbegin() const { return std::reverse_iterator(end()); }
    std::reverse_iterator<const_iterator> rend() const { return std::reverse_iterator(begin()); }
    std::reverse_iterator<const_iterator> crbegin() const { return std::reverse_iterator(cend()); }
    std::reverse_iterator<const_iterator> crend() const { return std::reverse_iterator(cbegin()); }
public:
    bool operator==(const chunked_vector& x) const {
        return boost::equal(*this, x);
    }
};

template<typename T, size_t max_contiguous_allocation>
size_t chunked_vector<T, max_contiguous_allocation>::external_memory_usage() const {
    size_t result = 0;
    for (auto&& chunk : _chunks) {
        result += ::malloc_usable_size(chunk.get());
    }
    return result;
}

template <typename T, size_t max_contiguous_allocation>
chunked_vector<T, max_contiguous_allocation>::chunked_vector(const chunked_vector& x)
        : chunked_vector() {
    reserve(x.size());
    std::copy(x.begin(), x.end(), std::back_inserter(*this));
}

template <typename T, size_t max_contiguous_allocation>
chunked_vector<T, max_contiguous_allocation>::chunked_vector(chunked_vector&& x) noexcept
        : _chunks(std::exchange(x._chunks, {}))
        , _size(std::exchange(x._size, 0))
        , _capacity(std::exchange(x._capacity, 0)) {
}

template <typename T, size_t max_contiguous_allocation>
template <typename Iterator>
chunked_vector<T, max_contiguous_allocation>::chunked_vector(Iterator begin, Iterator end)
        : chunked_vector() {
    auto is_random_access = std::is_base_of<std::random_access_iterator_tag, typename std::iterator_traits<Iterator>::iterator_category>::value;
    if (is_random_access) {
        reserve(std::distance(begin, end));
    }
    std::copy(begin, end, std::back_inserter(*this));
    if (!is_random_access) {
        shrink_to_fit();
    }
}

template <typename T, size_t max_contiguous_allocation>
chunked_vector<T, max_contiguous_allocation>::chunked_vector(size_t n, const T& value) {
    reserve(n);
    std::fill_n(std::back_inserter(*this), n, value);
}


template <typename T, size_t max_contiguous_allocation>
chunked_vector<T, max_contiguous_allocation>&
chunked_vector<T, max_contiguous_allocation>::operator=(const chunked_vector& x) {
    auto tmp = chunked_vector(x);
    return *this = std::move(tmp);
}

template <typename T, size_t max_contiguous_allocation>
inline
chunked_vector<T, max_contiguous_allocation>&
chunked_vector<T, max_contiguous_allocation>::operator=(chunked_vector&& x) noexcept {
    if (this != &x) {
        this->~chunked_vector();
        new (this) chunked_vector(std::move(x));
    }
    return *this;
}

template <typename T, size_t max_contiguous_allocation>
chunked_vector<T, max_contiguous_allocation>::~chunked_vector() {
    if constexpr (!std::is_trivially_destructible_v<T>) {
        for (auto i = size_t(0); i != _size; ++i) {
            addr(i)->~T();
        }
    }
}

template <typename T, size_t max_contiguous_allocation>
typename chunked_vector<T, max_contiguous_allocation>::chunk_ptr
chunked_vector<T, max_contiguous_allocation>::new_chunk(size_t n) {
    auto p = malloc(n * sizeof(T));
    if (!p) {
        throw std::bad_alloc();
    }
    return chunk_ptr(reinterpret_cast<T*>(p));
}

template <typename T, size_t max_contiguous_allocation>
void
chunked_vector<T, max_contiguous_allocation>::migrate(T* begin, T* end, T* result) {
    while (begin != end) {
        new (result) T(std::move(*begin));
        begin->~T();
        ++begin;
        ++result;
    }
}

template <typename T, size_t max_contiguous_allocation>
size_t
chunked_vector<T, max_contiguous_allocation>::make_room(size_t n, bool stop_after_one) {
    // First, if the last chunk is below max_chunk_capacity(), enlarge it

    auto last_chunk_capacity_deficit = _chunks.size() * max_chunk_capacity() - _capacity;
    if (last_chunk_capacity_deficit) {
        auto last_chunk_capacity = max_chunk_capacity() - last_chunk_capacity_deficit;
        auto capacity_increase = std::min(last_chunk_capacity_deficit, n - _capacity);
        auto new_last_chunk_capacity = last_chunk_capacity + capacity_increase;
        // FIXME: realloc? maybe not worth the complication; only works for PODs
        auto new_last_chunk = new_chunk(new_last_chunk_capacity);
        if (_size > _capacity - last_chunk_capacity) {
            migrate(addr(_capacity - last_chunk_capacity), addr(_size), new_last_chunk.get());
        }
        _chunks.back() = std::move(new_last_chunk);
        _capacity += capacity_increase;
    }

    // Reduce reallocations in the _chunks vector

    auto nr_chunks = (n + max_chunk_capacity() - 1) / max_chunk_capacity();
    _chunks.reserve(nr_chunks);

    // Add more chunks as needed

    bool stop = false;
    while (_capacity < n && !stop) {
        auto now = std::min(n - _capacity, max_chunk_capacity());
        _chunks.push_back(new_chunk(now));
        _capacity += now;
        stop = stop_after_one;
    }
    return (n - _capacity);
}

template <typename T, size_t max_contiguous_allocation>
void
chunked_vector<T, max_contiguous_allocation>::do_reserve_for_push_back() {
    if (_capacity == 0) {
        // allocate a bit of room in case utilization will be low
        reserve(boost::algorithm::clamp(512 / sizeof(T), 1, max_chunk_capacity()));
    } else if (_capacity < max_chunk_capacity() / 2) {
        // exponential increase when only one chunk to reduce copying
        reserve(_capacity * 2);
    } else {
        // add a chunk at a time later, since no copying will take place
        reserve((_capacity / max_chunk_capacity() + 1) * max_chunk_capacity());
    }
}

template <typename T, size_t max_contiguous_allocation>
void
chunked_vector<T, max_contiguous_allocation>::resize(size_t n) {
    reserve(n);
    // FIXME: construct whole chunks at once
    while (_size > n) {
        pop_back();
    }
    while (_size < n) {
        push_back(T{});
    }
    shrink_to_fit();
}

template <typename T, size_t max_contiguous_allocation>
void
chunked_vector<T, max_contiguous_allocation>::shrink_to_fit() {
    if (_chunks.empty()) {
        return;
    }
    while (!_chunks.empty() && _size <= (_chunks.size() - 1) * max_chunk_capacity()) {
        _chunks.pop_back();
        _capacity = _chunks.size() * max_chunk_capacity();
    }

    auto overcapacity = _size - _capacity;
    if (overcapacity) {
        auto new_last_chunk_capacity = _size - (_chunks.size() - 1) * max_chunk_capacity();
        // FIXME: realloc? maybe not worth the complication; only works for PODs
        auto new_last_chunk = new_chunk(new_last_chunk_capacity);
        migrate(addr((_chunks.size() - 1) * max_chunk_capacity()), addr(_size), new_last_chunk.get());
        _chunks.back() = std::move(new_last_chunk);
        _capacity = _size;
    }
}

template <typename T, size_t max_contiguous_allocation>
void
chunked_vector<T, max_contiguous_allocation>::clear() {
    while (_size > 0) {
        pop_back();
    }
    shrink_to_fit();
}

template <typename T, size_t max_contiguous_allocation>
std::ostream& operator<<(std::ostream& os, const chunked_vector<T, max_contiguous_allocation>& v) {
    return utils::format_range(os, v);
}

}



#include <limits>
#include <seastar/core/preempt.hh>

using namespace seastar;

class large_bitset {
    using int_type = uint64_t;
    static constexpr size_t bits_per_int() {
        return std::numeric_limits<int_type>::digits;
    }
    size_t _nr_bits = 0;
    utils::chunked_vector<int_type> _storage;
public:
    explicit large_bitset(size_t nr_bits);
    explicit large_bitset(size_t nr_bits, utils::chunked_vector<int_type> storage) : _nr_bits(nr_bits), _storage(std::move(storage)) {}
    large_bitset(large_bitset&&) = default;
    large_bitset(const large_bitset&) = delete;
    large_bitset& operator=(const large_bitset&) = delete;
    size_t size() const {
        return _nr_bits;
    }

    size_t memory_size() const {
        return _storage.memory_size();
    }

    bool test(size_t idx) const {
        auto idx1 = idx / bits_per_int();
        idx %= bits_per_int();
        auto idx2 = idx;
        return (_storage[idx1] >> idx2) & 1;
    }
    void set(size_t idx) {
        auto idx1 = idx / bits_per_int();
        idx %= bits_per_int();
        auto idx2 = idx;
        _storage[idx1] |= int_type(1) << idx2;
    }
    void clear(size_t idx) {
        auto idx1 = idx / bits_per_int();
        idx %= bits_per_int();
        auto idx2 = idx;
        _storage[idx1] &= ~(int_type(1) << idx2);
    }
    void clear();

    const utils::chunked_vector<int_type>& get_storage() const {
        return _storage;
    }
};

#include <vector>

namespace utils {
namespace filter {

class bloom_filter: public i_filter {
public:
    using bitmap = large_bitset;

private:
    bitmap _bitset;
    int _hash_count;
    filter_format _format;

    static thread_local struct stats {
        uint64_t memory_size = 0;
    } _shard_stats;
    stats& _stats = _shard_stats;

public:
    int num_hashes() { return _hash_count; }
    bitmap& bits() { return _bitset; }

    bloom_filter(int hashes, bitmap&& bs, filter_format format) noexcept;
    ~bloom_filter() noexcept;

    virtual void add(const bytes_view& key) override;

    virtual bool is_present(const bytes_view& key) override;

    virtual bool is_present(hashed_key key) override;

    virtual void clear() override {
        _bitset.clear();
    }

    virtual void close() override { }

    virtual size_t memory_size() override {
        return sizeof(_hash_count) + _bitset.memory_size();
    }

    static const stats& get_shard_stats() noexcept {
        return _shard_stats;
    }
};

struct murmur3_bloom_filter: public bloom_filter {

    murmur3_bloom_filter(int hashes, bitmap&& bs, filter_format format)
        : bloom_filter(hashes, std::move(bs), format)
    {}
};

struct always_present_filter: public i_filter {

    virtual bool is_present(const bytes_view& key) override {
        return true;
    }

    virtual bool is_present(hashed_key key) override {
        return true;
    }

    virtual void add(const bytes_view& key) override { }

    virtual void clear() override { }

    virtual void close() override { }

    virtual size_t memory_size() override {
        return 0;
    }
};

filter_ptr create_filter(int hash, large_bitset&& bitset, filter_format format);
filter_ptr create_filter(int hash, int64_t num_elements, int buckets_per, filter_format format);
}
}


#include <boost/algorithm/string.hpp>
#include <boost/date_time/c_local_time_adjustor.hpp>
#include <boost/date_time/posix_time/posix_time.hpp>
#include <boost/locale/encoding_utf.hpp>
#include <boost/range/algorithm_ext/push_back.hpp>
#include <boost/range/algorithm.hpp>
#include <boost/range/combine.hpp>
#include <boost/regex/icu.hpp>


#include <map>

#include <seastar/core/sstring.hh>

#include <boost/iterator/transform_iterator.hpp>
#include <seastar/core/bitset-iter.hh>

#include <algorithm>
#include <cstddef>
#include <optional>
#include <stdexcept>
#include <type_traits>
#include <limits>

/**
 *
 * Allows to take full advantage of compile-time information when operating
 * on a set of enum values.
 *
 * Examples:
 *
 *   enum class x { A, B, C };
 *   using my_enum = super_enum<x, x::A, x::B, x::C>;
 *   using my_enumset = enum_set<my_enum>;
 *
 *   static_assert(my_enumset::frozen<x::A, x::B>::contains<x::A>(), "it should...");
 *
 *   assert(my_enumset::frozen<x::A, x::B>::contains(my_enumset::prepare<x::A>()));
 *
 *   assert(my_enumset::frozen<x::A, x::B>::contains(x::A));
 *
 */


template<typename EnumType, EnumType... Items>
struct super_enum {
    using enum_type = EnumType;

    template<enum_type... values>
    struct max {
        static constexpr enum_type max_of(enum_type a, enum_type b) {
            return a > b ? a : b;
        }

        template<enum_type first, enum_type second, enum_type... rest>
        static constexpr enum_type get() {
            return max_of(first, get<second, rest...>());
        }

        template<enum_type first>
        static constexpr enum_type get() { return first; }

        static constexpr enum_type value = get<values...>();
    };

    template<enum_type... values>
    struct min {
        static constexpr enum_type min_of(enum_type a, enum_type b) {
            return a < b ? a : b;
        }

        template<enum_type first, enum_type second, enum_type... rest>
        static constexpr enum_type get() {
            return min_of(first, get<second, rest...>());
        }

        template<enum_type first>
        static constexpr enum_type get() { return first; }

        static constexpr enum_type value = get<values...>();
    };

    using sequence_type = typename std::underlying_type<enum_type>::type;

    template <enum_type first, enum_type... rest>
    struct valid_sequence {
        static constexpr bool apply(sequence_type v) noexcept {
            return (v == static_cast<sequence_type>(first)) || valid_sequence<rest...>::apply(v);
        }
    };

    template <enum_type first>
    struct valid_sequence<first> {
        static constexpr bool apply(sequence_type v) noexcept {
            return v == static_cast<sequence_type>(first);
        }
    };

    static constexpr bool is_valid_sequence(sequence_type v) noexcept {
        return valid_sequence<Items...>::apply(v);
    }

    template<enum_type Elem>
    static constexpr sequence_type sequence_for() {
        return static_cast<sequence_type>(Elem);
    }

    static sequence_type sequence_for(enum_type elem) {
        return static_cast<sequence_type>(elem);
    }

    static constexpr sequence_type max_sequence = sequence_for<max<Items...>::value>();
    static constexpr sequence_type min_sequence = sequence_for<min<Items...>::value>();

    static_assert(min_sequence >= 0, "negative enum values unsupported");
};

class bad_enum_set_mask : public std::invalid_argument {
public:
    bad_enum_set_mask() : std::invalid_argument("Bit mask contains invalid enumeration indices.") {
    }
};

template<typename Enum>
class enum_set {
public:
    using mask_type = size_t; // TODO: use the smallest sufficient type
    using enum_type = typename Enum::enum_type;

private:
    static constexpr int mask_digits = std::numeric_limits<mask_type>::digits;
    using mask_iterator = seastar::bitsets::set_iterator<mask_digits>;

    mask_type _mask;
    constexpr enum_set(mask_type mask) : _mask(mask) {}

    template<enum_type Elem>
    static constexpr unsigned shift_for() {
        return Enum::template sequence_for<Elem>();
    }

    static auto make_iterator(mask_iterator iter) {
        return boost::make_transform_iterator(std::move(iter), [](typename Enum::sequence_type s) {
            return enum_type(s);
        });
    }

public:
    using iterator = std::invoke_result_t<decltype(&enum_set::make_iterator), mask_iterator>;

    constexpr enum_set() : _mask(0) {}

    /**
     * \throws \ref bad_enum_set_mask
     */
    static constexpr enum_set from_mask(mask_type mask) {
        const auto bit_range = seastar::bitsets::for_each_set(std::bitset<mask_digits>(mask));

        if (!std::all_of(bit_range.begin(), bit_range.end(), &Enum::is_valid_sequence)) {
            throw bad_enum_set_mask();
        }

        return enum_set(mask);
    }

    static constexpr mask_type full_mask() {
        return ~(std::numeric_limits<mask_type>::max() << (Enum::max_sequence + 1));
    }

    static constexpr enum_set full() {
        return enum_set(full_mask());
    }

    static inline mask_type mask_for(enum_type e) {
        return mask_type(1) << Enum::sequence_for(e);
    }

    template<enum_type Elem>
    static constexpr mask_type mask_for() {
        return mask_type(1) << shift_for<Elem>();
    }

    struct prepared {
        mask_type mask;
        bool operator==(const prepared& o) const {
            return mask == o.mask;
        }
    };

    static prepared prepare(enum_type e) {
        return {mask_for(e)};
    }

    template<enum_type e>
    static constexpr prepared prepare() {
        return {mask_for<e>()};
    }

    static_assert(std::numeric_limits<mask_type>::max() >= ((size_t)1 << Enum::max_sequence), "mask type too small");

    template<enum_type e>
    bool contains() const {
        return bool(_mask & mask_for<e>());
    }

    bool contains(enum_type e) const {
        return bool(_mask & mask_for(e));
    }

    template<enum_type e>
    void remove() {
        _mask &= ~mask_for<e>();
    }

    void remove(enum_type e) {
        _mask &= ~mask_for(e);
    }

    template<enum_type e>
    void set() {
        _mask |= mask_for<e>();
    }

    template<enum_type e>
    void set_if(bool condition) {
        _mask |= mask_type(condition) << shift_for<e>();
    }

    void set(enum_type e) {
        _mask |= mask_for(e);
    }

    template<enum_type e>
    void toggle() {
        _mask ^= mask_for<e>();
    }

    void toggle(enum_type e) {
        _mask ^= mask_for(e);
    }

    void add(const enum_set& other) {
        _mask |= other._mask;
    }

    explicit operator bool() const {
        return bool(_mask);
    }

    mask_type mask() const {
        return _mask;
    }

    iterator begin() const {
        return make_iterator(mask_iterator(_mask));
    }

    iterator end() const {
        return make_iterator(mask_iterator(0));
    }

    template<enum_type... items>
    struct frozen {
        template<enum_type first>
        static constexpr mask_type make_mask() {
            return mask_for<first>();
        }

        static constexpr mask_type make_mask() {
            return 0;
        }

        template<enum_type first, enum_type second, enum_type... rest>
        static constexpr mask_type make_mask() {
            return mask_for<first>() | make_mask<second, rest...>();
        }

        static constexpr mask_type mask = make_mask<items...>();

        template<enum_type Elem>
        static constexpr bool contains() {
            return mask & mask_for<Elem>();
        }

        static bool contains(enum_type e) {
            return mask & mask_for(e);
        }

        static bool contains(prepared e) {
            return mask & e.mask;
        }

        static constexpr enum_set<Enum> unfreeze() {
            return enum_set<Enum>(mask);
        }
    };

    template<enum_type... items>
    static constexpr enum_set<Enum> of() {
        return frozen<items...>::unfreeze();
    }
};


#include <any>
#include <cstdlib>
#include <seastar/core/memory.hh>
#include <seastar/util/alloc_failure_injector.hh>
#include <malloc.h>

// A function used by compacting collectors to migrate objects during
// compaction. The function should reconstruct the object located at src
// in the location pointed by dst. The object at old location should be
// destroyed. See standard_migrator() above for example. Both src and dst
// are aligned as requested during alloc()/construct().
class migrate_fn_type {
    // Migrators may be registered by thread-local objects. The table of all
    // registered migrators is also thread-local which may cause problems with
    // the order of object destruction and lead to use-after-free.
    // This can be worked around by making migrators keep a shared pointer
    // to the table of migrators. std::any is used so that its type doesn't
    // have to be made public.
    std::any _migrators;
    uint32_t _align = 0;
    uint32_t _index;
private:
    static uint32_t register_migrator(migrate_fn_type* m);
    static void unregister_migrator(uint32_t index);
protected:
    explicit migrate_fn_type(size_t align) : _align(align), _index(register_migrator(this)) {}
public:
    virtual ~migrate_fn_type() { unregister_migrator(_index); }
    virtual void migrate(void* src, void* dsts, size_t size) const noexcept = 0;
    virtual size_t size(const void* obj) const = 0;
    size_t align() const { return _align; }
    uint32_t index() const { return _index; }
};

// Non-constant-size classes (ending with `char data[0]`) must provide
// the method telling the underlying storage size
template <typename T>
concept DynamicObject = requires (const T& obj) {
    { obj.storage_size() } noexcept -> std::same_as<size_t>;
};

template <typename T>
inline
size_t
size_for_allocation_strategy(const T& obj) noexcept {
    if constexpr (DynamicObject<T>) {
        return obj.storage_size();
    } else {
        return sizeof(T);
    }
}

template <typename T>
requires std::is_nothrow_move_constructible_v<T> && std::is_nothrow_destructible_v<T>
class standard_migrator final : public migrate_fn_type {
    friend class allocation_strategy;
    standard_migrator() : migrate_fn_type(alignof(T)) {}

public:
    virtual void migrate(void* src, void* dst, size_t size) const noexcept override {
        T* src_t = static_cast<T*>(src);
        new (static_cast<T*>(dst)) T(std::move(*src_t));
        src_t->~T();
    }
    virtual size_t size(const void* obj) const override {
        return size_for_allocation_strategy(*static_cast<const T*>(obj));
    }
};

//
// Abstracts allocation strategy for managed objects.
//
// Managed objects may be moved by the allocator during compaction, which
// invalidates any references to those objects. Compaction may be started
// synchronously with allocations. To ensure that references remain valid, use
// logalloc::compaction_lock.
//
// Because references may get invalidated, managing allocators can't be used
// with standard containers, because they assume the reference is valid until freed.
//
// For example containers compatible with compacting allocators see:
//   - managed_ref - managed version of std::unique_ptr<>
//   - managed_bytes - managed version of "bytes"
//
// Note: When object is used as an element inside intrusive containers,
// typically no extra measures need to be taken for reference tracking, if the
// link member is movable. When object is moved, the member hook will be moved
// too and it should take care of updating any back-references. The user must
// be aware though that any iterators into such container may be invalidated
// across deferring points.
//
class allocation_strategy {
    template <typename T>
    standard_migrator<T>& get_standard_migrator()
    {
        seastar::memory::scoped_critical_alloc_section dfg;
        static thread_local standard_migrator<T> instance;
        return instance;
    }

protected:
    size_t _preferred_max_contiguous_allocation = std::numeric_limits<size_t>::max();
    uint64_t _invalidate_counter = 1;
public:
    using migrate_fn = const migrate_fn_type*;

    virtual ~allocation_strategy() {}

    virtual void* alloc(migrate_fn, size_t size, size_t alignment) = 0;
    //
    // Allocates space for a new ManagedObject. The caller must construct the
    // object before compaction runs. "size" is the amount of space to reserve
    // in bytes. It can be larger than MangedObjects's size.
    //
    // Throws std::bad_alloc on allocation failure.
    //
    // Doesn't invalidate references to objects allocated with this strategy.
    //
    template <typename T>
    requires DynamicObject<T>
    void* alloc(size_t size) {
        return alloc(&get_standard_migrator<T>(), size, alignof(T));
    }

    // Releases storage for the object. Doesn't invoke object's destructor.
    // Doesn't invalidate references to objects allocated with this strategy.
    virtual void free(void* object, size_t size) = 0;
    virtual void free(void* object) = 0;

    // Returns the total immutable memory size used by the allocator to host
    // this object.  This will be at least the size of the object itself, plus
    // any immutable overhead needed to represent the object (if any).
    //
    // The immutable overhead is the overhead that cannot change over the
    // lifetime of the object (such as padding, etc).
    virtual size_t object_memory_size_in_allocator(const void* obj) const noexcept = 0;

    // Like alloc() but also constructs the object with a migrator using
    // standard move semantics. Allocates respecting object's alignment
    // requirement.
    template<typename T, typename... Args>
    T* construct(Args&&... args) {
        void* storage = alloc(&get_standard_migrator<T>(), sizeof(T), alignof(T));
        try {
            return new (storage) T(std::forward<Args>(args)...);
        } catch (...) {
            free(storage, sizeof(T));
            throw;
        }
    }

    // Destroys T and releases its storage.
    // Doesn't invalidate references to allocated objects.
    template<typename T>
    void destroy(T* obj) {
        size_t size = size_for_allocation_strategy(*obj);
        obj->~T();
        free(obj, size);
    }

    size_t preferred_max_contiguous_allocation() const noexcept {
        return _preferred_max_contiguous_allocation;
    }

    // Returns a number which is increased when references to objects managed by this allocator
    // are invalidated, e.g. due to internal events like compaction or eviction.
    // When the value returned by this method doesn't change, references obtained
    // between invocations remain valid.
    uint64_t invalidate_counter() const noexcept {
        return _invalidate_counter;
    }

    void invalidate_references() noexcept {
        ++_invalidate_counter;
    }
};

class standard_allocation_strategy : public allocation_strategy {
public:
    constexpr standard_allocation_strategy() {
        _preferred_max_contiguous_allocation = 128 * 1024;
    }
    virtual void* alloc(migrate_fn, size_t size, size_t alignment) override {
        seastar::memory::on_alloc_point();
        // ASAN doesn't intercept aligned_alloc() and complains on free().
        void* ret;
        // The system posix_memalign will return EINVAL if alignment is not
        // a multiple of pointer size.
        if (alignment < sizeof(void*)) {
            alignment = sizeof(void*);
        }
        if (posix_memalign(&ret, alignment, size) != 0) {
            throw std::bad_alloc();
        }
        return ret;
    }

    virtual void free(void* obj, size_t size) override {
        ::free(obj);
    }

    virtual void free(void* obj) override {
        ::free(obj);
    }

    virtual size_t object_memory_size_in_allocator(const void* obj) const noexcept override {
        return ::malloc_usable_size(const_cast<void *>(obj));
    }
};

extern standard_allocation_strategy standard_allocation_strategy_instance;

inline
standard_allocation_strategy& standard_allocator() {
    return standard_allocation_strategy_instance;
}

inline
allocation_strategy*& current_allocation_strategy_ptr() {
    static thread_local allocation_strategy* current = &standard_allocation_strategy_instance;
    return current;
}

inline
allocation_strategy& current_allocator() {
    return *current_allocation_strategy_ptr();
}

template<typename T>
inline
auto current_deleter() {
    auto& alloc = current_allocator();
    return [&alloc] (T* obj) noexcept {
        alloc.destroy(obj);
    };
}

template<typename T>
struct alloc_strategy_deleter {
    void operator()(T* ptr) const noexcept {
        current_allocator().destroy(ptr);
    }
};

// std::unique_ptr which can be used for owning an object allocated using allocation_strategy.
// Must be destroyed before the pointer is invalidated. For compacting allocators, that
// means it must not escape outside allocating_section or reclaim lock.
// Must be destroyed in the same allocating context in which T was allocated.
template<typename T>
using alloc_strategy_unique_ptr = std::unique_ptr<T, alloc_strategy_deleter<T>>;

//
// Passing allocators to objects.
//
// The same object type can be allocated using different allocators, for
// example standard allocator (for temporary data), or log-structured
// allocator for long-lived data. In case of LSA, objects may be allocated
// inside different LSA regions. Objects should be freed only from the region
// which owns it.
//
// There's a problem of how to ensure correct usage of allocators. Storing the
// reference to the allocator used for construction of some object inside that
// object is a possible solution. This has a disadvantage of extra space
// overhead per-object though. We could avoid that if the code which decides
// about which allocator to use is also the code which controls object's life
// time. That seems to be the case in current uses, so a simplified scheme of
// passing allocators will do. Allocation strategy is set in a thread-local
// context, as shown below. From there, aware objects pick up the allocation
// strategy. The code controling the objects must ensure that object allocated
// in one regime is also freed in the same regime.
//
// with_allocator() provides a way to set the current allocation strategy used
// within given block of code. with_allocator() can be nested, which will
// temporarily shadow enclosing strategy. Use current_allocator() to obtain
// currently active allocation strategy. Use current_deleter() to obtain a
// Deleter object using current allocation strategy to destroy objects.
//
// Example:
//
//   logalloc::region r;
//   with_allocator(r.allocator(), [] {
//       auto obj = make_managed<int>();
//   });
//

class allocator_lock {
    allocation_strategy* _prev;
public:
    allocator_lock(allocation_strategy& alloc) {
        _prev = current_allocation_strategy_ptr();
        current_allocation_strategy_ptr() = &alloc;
    }

    ~allocator_lock() {
        current_allocation_strategy_ptr() = _prev;
    }
};

template<typename Func>
inline
decltype(auto) with_allocator(allocation_strategy& alloc, Func&& func) {
    allocator_lock l(alloc);
    return func();
}


#include <stdexcept>
#include <seastar/core/sstring.hh>



class marshal_exception : public std::exception {
    sstring _why;
public:
    marshal_exception() = delete;
    marshal_exception(sstring why) : _why(sstring("marshaling error: ") + why) {}
    virtual const char* what() const noexcept override { return _why.c_str(); }
};

// Speed up compilation of code using throw_with_backtrace<marshal_exception,
// sstring> by compiling it only once (in types.cc), and elsewhere say that
// it is extern and not compile it again.
namespace seastar {
template <class Exc, typename... Args> [[noreturn]] void throw_with_backtrace(Args&&... args);
extern template void throw_with_backtrace<marshal_exception, sstring>(sstring&&);
}


#include <bit>
#include <cstring>
#include <cstddef>
#include <type_traits>

template <class T> concept Trivial = std::is_trivial_v<T>;
template <class T> concept TriviallyCopyable = std::is_trivially_copyable_v<T>;

template <TriviallyCopyable To>
To read_unaligned(const void* src) {
    To dst;
    std::memcpy(&dst, src, sizeof(To));
    return dst;
}

template <TriviallyCopyable From>
void write_unaligned(void* dst, const From& src) {
    std::memcpy(dst, &src, sizeof(From));
}


#include <concepts>
#include <compare>
#include <boost/range/algorithm/copy.hpp>
#include <seastar/net/byteorder.hh>
#include <seastar/core/print.hh>
#include <seastar/util/backtrace.hh>


enum class mutable_view { no, yes, };

/// Fragmented buffer
///
/// Concept `FragmentedBuffer` is satisfied by any class that is a range of
/// fragments and provides a method `size_bytes()` which returns the total
/// size of the buffer. The interfaces accepting `FragmentedBuffer` will attempt
/// to avoid unnecessary linearisation.
template<typename T>
concept FragmentRange = requires (T range) {
    typename T::fragment_type;
    requires std::is_same_v<typename T::fragment_type, bytes_view>
        || std::is_same_v<typename T::fragment_type, bytes_mutable_view>;
    { *range.begin() } -> std::convertible_to<const typename T::fragment_type&>;
    { *range.end() } -> std::convertible_to<const typename T::fragment_type&>;
    { range.size_bytes() } -> std::convertible_to<size_t>;
    { range.empty() } -> std::same_as<bool>; // returns true iff size_bytes() == 0.
};

template<typename T, typename = void>
struct is_fragment_range : std::false_type { };

template<typename T>
struct is_fragment_range<T, std::void_t<typename T::fragment_type>> : std::true_type { };

template<typename T>
static constexpr bool is_fragment_range_v = is_fragment_range<T>::value;

/// A non-mutable view of a FragmentRange
///
/// Provide a trivially copyable and movable, non-mutable view on a
/// fragment range. This allows uniform ownership semantics across
/// multi-fragment ranges and the single fragment and empty fragment
/// adaptors below, i.e. it allows treating all fragment ranges
/// uniformly as views.
template <typename T>
requires FragmentRange<T>
class fragment_range_view {
    const T* _range;
public:
    using fragment_type = typename T::fragment_type;
    using iterator = typename T::const_iterator;
    using const_iterator = typename T::const_iterator;

public:
    explicit fragment_range_view(const T& range) : _range(&range) { }

    const_iterator begin() const { return _range->begin(); }
    const_iterator end() const { return _range->end(); }

    size_t size_bytes() const { return _range->size_bytes(); }
    bool empty() const { return _range->empty(); }
};

/// Single-element fragment range
///
/// This is a helper that allows converting a bytes_view into a FragmentRange.
template<mutable_view is_mutable>
class single_fragment_range {
public:
    using fragment_type = std::conditional_t<is_mutable == mutable_view::no,
                                             bytes_view, bytes_mutable_view>;
private:
    fragment_type _view;
public:
    using iterator = const fragment_type*;
    using const_iterator = const fragment_type*;

    explicit single_fragment_range(fragment_type f) : _view { f } { }

    const_iterator begin() const { return &_view; }
    const_iterator end() const { return &_view + 1; }

    size_t size_bytes() const { return _view.size(); }
    bool empty() const { return _view.empty(); }
};

single_fragment_range(bytes_view) -> single_fragment_range<mutable_view::no>;
single_fragment_range(bytes_mutable_view) -> single_fragment_range<mutable_view::yes>;

/// Empty fragment range.
struct empty_fragment_range {
    using fragment_type = bytes_view;
    using iterator = bytes_view*;
    using const_iterator = bytes_view*;

    iterator begin() const { return nullptr; }
    iterator end() const { return nullptr; }

    size_t size_bytes() const { return 0; }
    bool empty() const { return true; }
};

static_assert(FragmentRange<empty_fragment_range>);
static_assert(FragmentRange<single_fragment_range<mutable_view::no>>);
static_assert(FragmentRange<single_fragment_range<mutable_view::yes>>);

template<typename FragmentedBuffer>
requires FragmentRange<FragmentedBuffer>
bytes linearized(const FragmentedBuffer& buffer)
{
    bytes b(bytes::initialized_later(), buffer.size_bytes());
    auto dst = b.begin();
    for (bytes_view fragment : buffer) {
        dst = boost::copy(fragment, dst);
    }
    return b;
}

template<typename FragmentedBuffer, typename Function>
requires FragmentRange<FragmentedBuffer> && requires (Function fn, bytes_view bv) {
    fn(bv);
}
decltype(auto) with_linearized(const FragmentedBuffer& buffer, Function&& fn)
{
    bytes b;
    bytes_view bv;
    if (__builtin_expect(!buffer.empty() && std::next(buffer.begin()) == buffer.end(), true)) {
        bv = *buffer.begin();
    } else if (!buffer.empty()) {
        b = linearized(buffer);
        bv = b;
    }
    return fn(bv);
}

template<typename T>
concept FragmentedView = requires (T view, size_t n) {
    typename T::fragment_type;
    requires std::is_same_v<typename T::fragment_type, bytes_view>
            || std::is_same_v<typename T::fragment_type, bytes_mutable_view>;
    // No preconditions.
    { view.current_fragment() } -> std::convertible_to<const typename T::fragment_type&>;
    // No preconditions.
    { view.empty() } -> std::same_as<bool>;
    // No preconditions.
    { view.size_bytes() } -> std::convertible_to<size_t>;
    // Precondition: n <= size_bytes()
    { view.prefix(n) } -> std::same_as<T>;
    // Precondition: n <= size_bytes()
    view.remove_prefix(n);
    // Precondition: size_bytes() > 0
    view.remove_current();
};

template<typename T>
concept FragmentedMutableView = requires (T view) {
    requires FragmentedView<T>;
    requires std::is_same_v<typename T::fragment_type, bytes_mutable_view>;
};

template<FragmentedView View>
struct fragment_range {
    using fragment_type = typename View::fragment_type;
    View view;
    class fragment_iterator {
        using iterator_category = std::input_iterator_tag;
        using value_type = typename View::fragment_type;
        using difference_type = std::ptrdiff_t;
        using pointer = const value_type*;
        using reference = const value_type&;
        View _view;
        value_type _current;
    public:
        fragment_iterator() : _view(value_type()) {}
        fragment_iterator(const View& v) : _view(v) {
            _current = _view.current_fragment();
        }
        fragment_iterator& operator++() {
            _view.remove_current();
            _current = _view.current_fragment(); 
            return *this;
        }
        fragment_iterator operator++(int) {
            fragment_iterator i(*this);
            ++(*this);
            return i;
        }
        reference operator*() const { return _current; }
        pointer operator->() const { return &_current; }
        bool operator==(const fragment_iterator& i) const { return _view.size_bytes() == i._view.size_bytes(); }
    };
    using iterator = fragment_iterator;
    fragment_range(const View& v) : view(v) {}
    fragment_iterator begin() const { return fragment_iterator(view); }
    fragment_iterator end() const { return fragment_iterator(); }
    size_t size_bytes() const { return view.size_bytes(); }
    bool empty() const { return view.empty(); }
};

template<FragmentedView View>
requires (!FragmentRange<View>)
bytes linearized(View v)
{
    bytes b(bytes::initialized_later(), v.size_bytes());
    auto out = b.begin();
    while (v.size_bytes()) {
        out = std::copy(v.current_fragment().begin(), v.current_fragment().end(), out);
        v.remove_current();
    }
    return b;
}

template<FragmentedView View, typename Function>
requires (!FragmentRange<View>) && std::invocable<Function, bytes_view>
decltype(auto) with_linearized(const View& v, Function&& fn)
{
    if (v.size_bytes() == v.current_fragment().size()) [[likely]] {
        return fn(v.current_fragment());
    } else {
        return fn(linearized(v));
    }
}

template <mutable_view is_mutable>
class basic_single_fragmented_view {
public:
    using fragment_type = std::conditional_t<is_mutable == mutable_view::yes, bytes_mutable_view, bytes_view>;
private:
    fragment_type _view;
public:
    explicit basic_single_fragmented_view(fragment_type bv) : _view(bv) {}
    size_t size_bytes() const { return _view.size(); }
    bool empty() const { return _view.empty(); }
    void remove_prefix(size_t n) { _view.remove_prefix(n); }
    void remove_current() { _view = fragment_type(); }
    fragment_type current_fragment() const { return _view; }
    basic_single_fragmented_view prefix(size_t n) { return basic_single_fragmented_view(_view.substr(0, n)); }
};
using single_fragmented_view = basic_single_fragmented_view<mutable_view::no>;
using single_fragmented_mutable_view = basic_single_fragmented_view<mutable_view::yes>;
static_assert(FragmentedView<single_fragmented_view>);
static_assert(FragmentedMutableView<single_fragmented_mutable_view>);
static_assert(FragmentRange<fragment_range<single_fragmented_view>>);
static_assert(FragmentRange<fragment_range<single_fragmented_mutable_view>>);

template<FragmentedView View, typename Function>
requires std::invocable<Function, View> && std::invocable<Function, single_fragmented_view>
decltype(auto) with_simplified(const View& v, Function&& fn)
{
    if (v.size_bytes() == v.current_fragment().size()) [[likely]] {
        return fn(single_fragmented_view(v.current_fragment()));
    } else {
        return fn(v);
    }
}

template<FragmentedView View>
void skip_empty_fragments(View& v) {
    while (!v.empty() && v.current_fragment().empty()) {
        v.remove_current();
    }
}

template<FragmentedView V1, FragmentedView V2>
std::strong_ordering compare_unsigned(V1 v1, V2 v2) {
    while (!v1.empty() && !v2.empty()) {
        size_t n = std::min(v1.current_fragment().size(), v2.current_fragment().size());
        if (int d = memcmp(v1.current_fragment().data(), v2.current_fragment().data(), n)) {
            return d <=> 0;
        }
        v1.remove_prefix(n);
        v2.remove_prefix(n);
        skip_empty_fragments(v1);
        skip_empty_fragments(v2);
    }
    return v1.size_bytes() <=> v2.size_bytes();
}

template<FragmentedView V1, FragmentedView V2>
int equal_unsigned(V1 v1, V2 v2) {
    return v1.size_bytes() == v2.size_bytes() && compare_unsigned(v1, v2) == 0;
}

template<FragmentedMutableView Dest, FragmentedView Src>
void write_fragmented(Dest& dest, Src src) {
    if (dest.size_bytes() < src.size_bytes()) [[unlikely]] {
        throw std::out_of_range(format("tried to copy a buffer of size {} to a buffer of smaller size {}", src.size_bytes(), dest.size_bytes()));
    }
    while (!src.empty()) {
        size_t n = std::min(dest.current_fragment().size(), src.current_fragment().size());
        memcpy(dest.current_fragment().data(), src.current_fragment().data(), n);
        dest.remove_prefix(n);
        src.remove_prefix(n);
        skip_empty_fragments(dest);
        skip_empty_fragments(src);
    }
}

template<FragmentedMutableView Dest, FragmentedView Src>
void copy_fragmented_view(Dest dest, Src src) {
    if (dest.size_bytes() < src.size_bytes()) [[unlikely]] {
        throw std::out_of_range(format("tried to copy a buffer of size {} to a buffer of smaller size {}", src.size_bytes(), dest.size_bytes()));
    }
    while (!src.empty()) {
        size_t n = std::min(dest.current_fragment().size(), src.current_fragment().size());
        memcpy(dest.current_fragment().data(), src.current_fragment().data(), n);
        dest.remove_prefix(n);
        src.remove_prefix(n);
        skip_empty_fragments(dest);
        skip_empty_fragments(src);
    }
}

// Does not check bounds. Must be called only after size is already checked.
template<FragmentedView View>
void read_fragmented(View& v, size_t n, bytes::value_type* out) {
    while (n) {
        if (n <= v.current_fragment().size()) {
            std::copy_n(v.current_fragment().data(), n, out);
            v.remove_prefix(n);
            n = 0;
        } else {
            out = std::copy_n(v.current_fragment().data(), v.current_fragment().size(), out);
            n -= v.current_fragment().size();
            v.remove_current();
        }
    }
}
template<> void inline read_fragmented(single_fragmented_view& v, size_t n, bytes::value_type* out) {
    std::copy_n(v.current_fragment().data(), n, out);
    v.remove_prefix(n);
}

template<typename T, FragmentedView View>
T read_simple_native(View& v) {
    if (v.current_fragment().size() >= sizeof(T)) [[likely]] {
        auto p = v.current_fragment().data();
        v.remove_prefix(sizeof(T));
        return read_unaligned<T>(p);
    } else if (v.size_bytes() >= sizeof(T)) {
        T buf;
        read_fragmented(v, sizeof(T), reinterpret_cast<bytes::value_type*>(&buf));
        return buf;
    } else {
        throw_with_backtrace<marshal_exception>(format("read_simple - not enough bytes (expected {:d}, got {:d})", sizeof(T), v.size_bytes()));
    }
}

template<typename T, FragmentedView View>
T read_simple(View& v) {
    if (v.current_fragment().size() >= sizeof(T)) [[likely]] {
        auto p = v.current_fragment().data();
        v.remove_prefix(sizeof(T));
        return net::ntoh(read_unaligned<T>(p));
    } else if (v.size_bytes() >= sizeof(T)) {
        T buf;
        read_fragmented(v, sizeof(T), reinterpret_cast<bytes::value_type*>(&buf));
        return net::ntoh(buf);
    } else {
        throw_with_backtrace<marshal_exception>(format("read_simple - not enough bytes (expected {:d}, got {:d})", sizeof(T), v.size_bytes()));
    }
}

template<typename T, FragmentedView View>
T read_simple_exactly(View v) {
    if (v.current_fragment().size() == sizeof(T)) [[likely]] {
        auto p = v.current_fragment().data();
        return net::ntoh(read_unaligned<T>(p));
    } else if (v.size_bytes() == sizeof(T)) {
        T buf;
        read_fragmented(v, sizeof(T), reinterpret_cast<bytes::value_type*>(&buf));
        return net::ntoh(buf);
    } else {
        throw_with_backtrace<marshal_exception>(format("read_simple_exactly - size mismatch (expected {:d}, got {:d})", sizeof(T), v.size_bytes()));
    }
}

template<typename T, FragmentedMutableView Out>
static inline
void write(Out& out, std::type_identity_t<T> val) {
    auto v = net::ntoh(val);
    auto p = reinterpret_cast<const bytes_view::value_type*>(&v);
    if (out.current_fragment().size() >= sizeof(v)) [[likely]] {
        std::copy_n(p, sizeof(v), out.current_fragment().data());
        out.remove_prefix(sizeof(v));
    } else {
        write_fragmented(out, single_fragmented_view(bytes_view(p, sizeof(v))));
    }
}

template<typename T, FragmentedMutableView Out>
static inline
void write_native(Out& out, std::type_identity_t<T> v) {
    auto p = reinterpret_cast<const bytes_view::value_type*>(&v);
    if (out.current_fragment().size() >= sizeof(v)) [[likely]] {
        std::copy_n(p, sizeof(v), out.current_fragment().data());
        out.remove_prefix(sizeof(v));
    } else {
        write_fragmented(out, single_fragmented_view(bytes_view(p, sizeof(v))));
    }
}

template <FragmentedView View>
struct fmt::formatter<View> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const View& b, FormatContext& ctx) const {
        auto out = ctx.out();
        for (auto frag : fragment_range(b)) {
            fmt::format_to(out, "{}", fmt_hex(frag));
        }
        return out;
    }
};

#include <stdint.h>
#include <memory>
#include <seastar/util/alloc_failure_injector.hh>
#include <unordered_map>
#include <type_traits>

class bytes_ostream;

template <mutable_view is_mutable_view>
class managed_bytes_basic_view;
using managed_bytes_view = managed_bytes_basic_view<mutable_view::no>;
using managed_bytes_mutable_view = managed_bytes_basic_view<mutable_view::yes>;

struct blob_storage {
    struct [[gnu::packed]] ref_type {
        blob_storage* ptr = nullptr;

        ref_type() {}
        ref_type(blob_storage* ptr) : ptr(ptr) {}
        operator blob_storage*() const { return ptr; }
        blob_storage* operator->() const { return ptr; }
        blob_storage& operator*() const { return *ptr; }
    };
    using size_type = uint32_t;
    using char_type = bytes_view::value_type;

    ref_type* backref;
    size_type size;
    size_type frag_size;
    ref_type next;
    char_type data[];

    blob_storage(ref_type* backref, size_type size, size_type frag_size) noexcept
        : backref(backref)
        , size(size)
        , frag_size(frag_size)
        , next(nullptr)
    {
        *backref = this;
    }

    blob_storage(blob_storage&& o) noexcept
        : backref(o.backref)
        , size(o.size)
        , frag_size(o.frag_size)
        , next(o.next)
    {
        *backref = this;
        o.next = nullptr;
        if (next) {
            next->backref = &next;
        }
        memcpy(data, o.data, frag_size);
    }

    size_t storage_size() const noexcept {
        return sizeof(*this) + frag_size;
    }
} __attribute__((packed));

// A managed version of "bytes" (can be used with LSA).
class managed_bytes {
    friend class bytes_ostream;
    static constexpr size_t max_inline_size = 15;
    struct small_blob {
        bytes_view::value_type data[max_inline_size];
        int8_t size; // -1 -> use blob_storage
    };
    union u {
        u() {}
        ~u() {}
        blob_storage::ref_type ptr;
        small_blob small;
    } _u;
    static_assert(sizeof(small_blob) > sizeof(blob_storage*), "inline size too small");
private:
    bool external() const noexcept {
        return _u.small.size < 0;
    }
    size_t max_seg(allocation_strategy& alctr) {
        return alctr.preferred_max_contiguous_allocation() - sizeof(blob_storage);
    }
    void free_chain(blob_storage* p) noexcept {
        auto& alctr = current_allocator();
        while (p) {
            auto n = p->next;
            alctr.destroy(p);
            p = n;
        }
    }
    bytes_view::value_type& value_at_index(blob_storage::size_type index) {
        if (!external()) {
            return _u.small.data[index];
        }
        blob_storage* a = _u.ptr;
        while (index >= a->frag_size) {
            index -= a->frag_size;
            a = a->next;
        }
        return a->data[index];
    }
    std::unique_ptr<bytes_view::value_type[]> do_linearize_pure() const;

    explicit managed_bytes(blob_storage* data) {
        _u.small.size = -1;
        _u.ptr.ptr = data;
        data->backref = &_u.ptr;
    }
public:
    using size_type = blob_storage::size_type;
    struct initialized_later {};

    managed_bytes() {
        _u.small.size = 0;
    }

    managed_bytes(const blob_storage::char_type* ptr, size_type size)
        : managed_bytes(bytes_view(ptr, size)) {}

    explicit managed_bytes(const bytes& b) : managed_bytes(static_cast<bytes_view>(b)) {}

    template <FragmentedView View>
    explicit managed_bytes(View v);

    managed_bytes(initialized_later, size_type size) {
        memory::on_alloc_point();
        if (size <= max_inline_size) {
            _u.small.size = size;
        } else {
            _u.small.size = -1;
            auto& alctr = current_allocator();
            auto maxseg = max_seg(alctr);
            auto now = std::min(size_t(size), maxseg);
            void* p = alctr.alloc<blob_storage>(sizeof(blob_storage) + now);
            auto first = new (p) blob_storage(&_u.ptr, size, now);
            auto last = first;
            size -= now;
            try {
                while (size) {
                    auto now = std::min(size_t(size), maxseg);
                    void* p = alctr.alloc<blob_storage>(sizeof(blob_storage) + now);
                    last = new (p) blob_storage(&last->next, 0, now);
                    size -= now;
                }
            } catch (...) {
                free_chain(first);
                throw;
            }
        }
    }

    explicit managed_bytes(bytes_view v) : managed_bytes(initialized_later(), v.size()) {
        if (!external()) {
            // Workaround for https://github.com/scylladb/scylla/issues/4086
            #pragma GCC diagnostic push
            #pragma GCC diagnostic ignored "-Warray-bounds"
            std::copy(v.begin(), v.end(), _u.small.data);
            #pragma GCC diagnostic pop
            return;
        }
        auto p = v.data();
        auto s = v.size();
        auto b = _u.ptr;
        while (s) {
            memcpy(b->data, p, b->frag_size);
            p += b->frag_size;
            s -= b->frag_size;
            b = b->next;
        }
        assert(!b);
    }

    managed_bytes(std::initializer_list<bytes::value_type> b) : managed_bytes(b.begin(), b.size()) {}

    ~managed_bytes() noexcept {
        if (external()) {
            free_chain(_u.ptr);
        }
    }

    managed_bytes(const managed_bytes& o) : managed_bytes(initialized_later(), o.size()) {
        if (!o.external()) {
            _u.small = o._u.small;
            return;
        }
        auto s = size();
        const blob_storage::ref_type* next_src = &o._u.ptr;
        blob_storage* blob_src = nullptr;
        size_type size_src = 0;
        size_type offs_src = 0;
        blob_storage::ref_type* next_dst = &_u.ptr;
        blob_storage* blob_dst = nullptr;
        size_type size_dst = 0;
        size_type offs_dst = 0;
        while (s) {
            if (!size_src) {
                blob_src = *next_src;
                next_src = &blob_src->next;
                size_src = blob_src->frag_size;
                offs_src = 0;
            }
            if (!size_dst) {
                blob_dst = *next_dst;
                next_dst = &blob_dst->next;
                size_dst = blob_dst->frag_size;
                offs_dst = 0;
            }
            auto now = std::min(size_src, size_dst);
            memcpy(blob_dst->data + offs_dst, blob_src->data + offs_src, now);
            s -= now;
            offs_src += now; size_src -= now;
            offs_dst += now; size_dst -= now;
        }
        assert(size_src == 0 && size_dst == 0);
    }

    managed_bytes(managed_bytes&& o) noexcept
        : _u(o._u)
    {
        if (external()) {
            // _u.ptr cannot be null
            _u.ptr->backref = &_u.ptr;
        }
        o._u.small.size = 0;
    }

    managed_bytes& operator=(managed_bytes&& o) noexcept {
        if (this != &o) {
            this->~managed_bytes();
            new (this) managed_bytes(std::move(o));
        }
        return *this;
    }

    managed_bytes& operator=(const managed_bytes& o) {
        if (this != &o) {
            managed_bytes tmp(o);
            this->~managed_bytes();
            new (this) managed_bytes(std::move(tmp));
        }
        return *this;
    }

    bool operator==(const managed_bytes& o) const {
        if (size() != o.size()) {
            return false;
        }
        if (!external()) {
            return std::equal(_u.small.data, _u.small.data + _u.small.size, o._u.small.data);
        } else {
            auto a = _u.ptr;
            auto a_data = a->data;
            auto a_remain = a->frag_size;
            a = a->next;
            auto b = o._u.ptr;
            auto b_data = b->data;
            auto b_remain = b->frag_size;
            b = b->next;
            while (a_remain || b_remain) {
                auto now = std::min(a_remain, b_remain);
                if (bytes_view(a_data, now) != bytes_view(b_data, now)) {
                    return false;
                }
                a_data += now;
                a_remain -= now;
                if (!a_remain && a) {
                    a_data = a->data;
                    a_remain = a->frag_size;
                    a = a->next;
                }
                b_data += now;
                b_remain -= now;
                if (!b_remain && b) {
                    b_data = b->data;
                    b_remain = b->frag_size;
                    b = b->next;
                }
            }
            return true;
        }
    }

    bytes_view::value_type& operator[](size_type index) {
        return value_at_index(index);
    }

    const bytes_view::value_type& operator[](size_type index) const {
        return const_cast<const bytes_view::value_type&>(
                const_cast<managed_bytes*>(this)->value_at_index(index));
    }

    size_type size() const {
        if (external()) {
            return _u.ptr->size;
        } else {
            return _u.small.size;
        }
    }

    bool empty() const {
        return _u.small.size == 0;
    }

    // Returns the amount of external memory used.
    size_t external_memory_usage() const noexcept {
        if (external()) {
            size_t mem = 0;
            blob_storage* blob = _u.ptr;
            while (blob) {
                mem += blob->frag_size + sizeof(blob_storage);
                blob = blob->next;
            }
            return mem;
        }
        return 0;
    }

    // Returns the minimum possible amount of external memory used by a managed_bytes
    // of the same size as us.
    // In other words, it returns the amount of external memory that would used by this
    // managed_bytes if all data was allocated in one big fragment.
    size_t minimal_external_memory_usage() const noexcept {
        if (external()) {
            return sizeof(blob_storage) + _u.ptr->size;
        } else {
            return 0;
        }
    }

    template <std::invocable<bytes_view> Func>
    std::invoke_result_t<Func, bytes_view> with_linearized(Func&& func) const {
        const bytes_view::value_type* start = nullptr;
        size_t size = 0;
        if (!external()) {
            start = _u.small.data;
            size = _u.small.size;
        } else if (!_u.ptr->next) {
            start = _u.ptr->data;
            size = _u.ptr->size;
        }
        if (start) {
            return func(bytes_view(start, size));
        } else {
            auto data = do_linearize_pure();
            return func(bytes_view(data.get(), _u.ptr->size));
        }
    }

    template <mutable_view is_mutable_view>
    friend class managed_bytes_basic_view;
};

template <mutable_view is_mutable>
class managed_bytes_basic_view {
public:
    using fragment_type = std::conditional_t<is_mutable == mutable_view::yes, bytes_mutable_view, bytes_view>;
    using owning_type = std::conditional_t<is_mutable == mutable_view::yes, managed_bytes, const managed_bytes>;
    using value_type = typename fragment_type::value_type;
private:
    fragment_type _current_fragment = {};
    blob_storage* _next_fragments = nullptr;
    size_t _size = 0;
private:
    managed_bytes_basic_view(fragment_type current_fragment, blob_storage* next_fragments, size_t size)
        : _current_fragment(current_fragment)
        , _next_fragments(next_fragments)
        , _size(size) {
    }
public:
    managed_bytes_basic_view() = default;
    managed_bytes_basic_view(const managed_bytes_basic_view&) = default;
    managed_bytes_basic_view(owning_type& mb) {
        if (mb._u.small.size != -1) {
            _current_fragment = fragment_type(mb._u.small.data, mb._u.small.size);
            _size = mb._u.small.size;
        } else {
            auto p = mb._u.ptr;
            _current_fragment = fragment_type(p->data, p->frag_size);
            _next_fragments = p->next;
            _size = p->size;
        }
    }
    managed_bytes_basic_view(fragment_type bv)
        : _current_fragment(bv)
        , _size(bv.size()) {
    }
    size_t size() const { return _size; }
    size_t size_bytes() const { return _size; }
    bool empty() const { return _size == 0; }
    fragment_type current_fragment() const { return _current_fragment; }
    void remove_prefix(size_t n) {
        while (n >= _current_fragment.size() && n > 0) {
            n -= _current_fragment.size();
            remove_current();
        }
        _size -= n;
        _current_fragment.remove_prefix(n);
    }
    void remove_current() {
        _size -= _current_fragment.size();
        if (_size) {
            _current_fragment = fragment_type(_next_fragments->data, _next_fragments->frag_size);
            _next_fragments = _next_fragments->next;
            _current_fragment = _current_fragment.substr(0, _size);
        } else {
            _current_fragment = fragment_type();
        }
    }
    managed_bytes_basic_view prefix(size_t len) const {
        managed_bytes_basic_view v = *this;
        v._size = len;
        v._current_fragment = v._current_fragment.substr(0, len);
        return v;
    }
    managed_bytes_basic_view substr(size_t offset, size_t len) const {
        size_t end = std::min(offset + len, _size);
        managed_bytes_basic_view v = prefix(end);
        v.remove_prefix(offset);
        return v;
    }
    const auto& front() const { return _current_fragment.front(); }
    auto& front() { return _current_fragment.front(); }
    const value_type& operator[](size_t index) const {
        auto v = *this;
        v.remove_prefix(index);
        return v.current_fragment().front();
    }
    bytes linearize() const {
        return linearized(*this);
    }
    bool is_linearized() const {
        return _current_fragment.size() == _size;
    }

    // Allow casting mutable views to immutable views.
    template <mutable_view Other>
    friend class managed_bytes_basic_view;

    template <mutable_view Other>
    managed_bytes_basic_view(const managed_bytes_basic_view<Other>& other)
    requires (is_mutable == mutable_view::no) && (Other == mutable_view::yes)
        : _current_fragment(other._current_fragment.data(), other._current_fragment.size())
        , _next_fragments(other._next_fragments)
        , _size(other._size)
    {}

    template <std::invocable<bytes_view> Func>
    std::invoke_result_t<Func, bytes_view> with_linearized(Func&& func) const {
        bytes b;
        auto bv = std::invoke([&] () -> bytes_view {
            if (is_linearized()) {
                return _current_fragment;
            } else {
                b = linearize();
                return b;
            }
        });
        return func(bv);
    }

    friend managed_bytes_basic_view<mutable_view::no> build_managed_bytes_view_from_internals(bytes_view current_fragment, blob_storage* next_fragment, size_t size);
};
static_assert(FragmentedView<managed_bytes_view>);
static_assert(FragmentedMutableView<managed_bytes_mutable_view>);

using managed_bytes_opt = std::optional<managed_bytes>;
using managed_bytes_view_opt = std::optional<managed_bytes_view>;

inline bytes to_bytes(const managed_bytes& v) {
    return linearized(managed_bytes_view(v));
}
inline bytes to_bytes(managed_bytes_view v) {
    return linearized(v);
}

/// Converts a possibly fragmented managed_bytes_opt to a
/// linear bytes_opt.
///
/// \note copies data
bytes_opt to_bytes_opt(const managed_bytes_opt&);

/// Converts a linear bytes_opt to a possibly fragmented
/// managed_bytes_opt.
///
/// \note copies data
managed_bytes_opt to_managed_bytes_opt(const bytes_opt&);

template<FragmentedView View>
inline managed_bytes::managed_bytes(View v) : managed_bytes(initialized_later(), v.size_bytes()) {
    managed_bytes_mutable_view self(*this);
    write_fragmented(self, v);
}

inline
managed_bytes_view
build_managed_bytes_view_from_internals(bytes_view current_fragment, blob_storage* next_fragment, size_t size) {
    return managed_bytes_view(current_fragment, next_fragment, size);
}

template<>
struct appending_hash<managed_bytes_view> {
    template<Hasher Hasher>
    void operator()(Hasher& h, managed_bytes_view v) const {
        feed_hash(h, v.size_bytes());
        for (bytes_view frag : fragment_range(v)) {
            h.update(reinterpret_cast<const char*>(frag.data()), frag.size());
        }
    }
};

namespace std {
template <>
struct hash<managed_bytes_view> {
    size_t operator()(managed_bytes_view v) const {
        bytes_view_hasher h;
        appending_hash<managed_bytes_view>{}(h, v);
        return h.finalize();
    }
};
template <>
struct hash<managed_bytes> {
    size_t operator()(const managed_bytes& v) const {
        return hash<managed_bytes_view>{}(v);
    }
};
} // namespace std

sstring to_hex(const managed_bytes& b);
sstring to_hex(const managed_bytes_opt& b);

// The operators below are used only by tests.

inline bool operator==(const managed_bytes_view& a, const managed_bytes_view& b) {
    return a.size_bytes() == b.size_bytes() && compare_unsigned(a, b) == 0;
}

inline std::ostream& operator<<(std::ostream& os, const managed_bytes_view& v) {
    for (bytes_view frag : fragment_range(v)) {
        os << to_hex(frag);
    }
    return os;
}
inline std::ostream& operator<<(std::ostream& os, const managed_bytes& b) {
    return (os << managed_bytes_view(b));
}
std::ostream& operator<<(std::ostream& os, const managed_bytes_opt& b);



#include <boost/range/iterator_range.hpp>

#include <seastar/core/simple-stream.hh>
#include <seastar/core/loop.hh>
#include <bit>
#include <concepts>

/**
 * Utility for writing data into a buffer when its final size is not known up front.
 *
 * Internally the data is written into a chain of chunks allocated on-demand.
 * No resizing of previously written data happens.
 *
 */
class bytes_ostream {
public:
    using size_type = bytes::size_type;
    using value_type = bytes::value_type;
    using fragment_type = bytes_view;
    static constexpr size_type max_chunk_size() { return max_alloc_size() - sizeof(chunk); }
private:
    static_assert(sizeof(value_type) == 1, "value_type is assumed to be one byte long");
    // Note: while appending data, chunk::size refers to the allocated space in the chunk,
    //       and chunk::frag_size refers to the currently occupied space in the chunk.
    //       After building, the first chunk::size is the whole object size, and chunk::frag_size
    //       doesn't change. This fits with managed_bytes interpretation.
    using chunk = blob_storage;
    static constexpr size_type default_chunk_size{512};
    static constexpr size_type max_alloc_size() { return 128 * 1024; }
private:
    blob_storage::ref_type _begin;
    chunk* _current;
    size_type _size;
    size_type _initial_chunk_size = default_chunk_size;
public:
    class fragment_iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = bytes_view;
        using difference_type = std::ptrdiff_t;
        using pointer = bytes_view*;
        using reference = bytes_view&;

        struct implementation {
            blob_storage* current_chunk;
        };
    private:
        chunk* _current = nullptr;
    public:
        fragment_iterator() = default;
        fragment_iterator(chunk* current) : _current(current) {}
        fragment_iterator(const fragment_iterator&) = default;
        fragment_iterator& operator=(const fragment_iterator&) = default;
        bytes_view operator*() const {
            return { _current->data, _current->frag_size };
        }
        bytes_view operator->() const {
            return *(*this);
        }
        fragment_iterator& operator++() {
            _current = _current->next;
            return *this;
        }
        fragment_iterator operator++(int) {
            fragment_iterator tmp(*this);
            ++(*this);
            return tmp;
        }
        bool operator==(const fragment_iterator&) const = default;
        implementation extract_implementation() const {
            return implementation {
                .current_chunk = _current,
            };
        }
    };
    using const_iterator = fragment_iterator;

    class output_iterator {
    public:
        using iterator_category = std::output_iterator_tag;
        using difference_type = std::ptrdiff_t;
        using value_type = bytes_ostream::value_type;
        using pointer = bytes_ostream::value_type*;
        using reference = bytes_ostream::value_type&;

        friend class bytes_ostream;

    private:
        bytes_ostream* _ostream = nullptr;

    private:
        explicit output_iterator(bytes_ostream& os) : _ostream(&os) { }

    public:
        reference operator*() const { return *_ostream->write_place_holder(1); }
        output_iterator& operator++() { return *this; }
        output_iterator operator++(int) { return *this; }
    };
private:
    inline size_type current_space_left() const {
        if (!_current) {
            return 0;
        }
        return _current->size - _current->frag_size;
    }
    // Figure out next chunk size.
    //   - must be enough for data_size + sizeof(chunk)
    //   - must be at least _initial_chunk_size
    //   - try to double each time to prevent too many allocations
    //   - should not exceed max_alloc_size, unless data_size requires so
    //   - will be power-of-two so the allocated memory can be fully utilized.
    size_type next_alloc_size(size_t data_size) const {
        auto next_size = _current
                ? _current->size * 2
                : _initial_chunk_size;
        next_size = std::min(next_size, max_alloc_size());
        auto r = std::max<size_type>(next_size, data_size + sizeof(chunk));
        return std::bit_ceil(r);
    }
    // Makes room for a contiguous region of given size.
    // The region is accounted for as already written.
    // size must not be zero.
    [[gnu::always_inline]]
    value_type* alloc(size_type size) {
        if (__builtin_expect(size <= current_space_left(), true)) {
            auto ret = _current->data + _current->frag_size;
            _current->frag_size += size;
            _size += size;
            return ret;
        } else {
            return alloc_new(size);
        }
    }
    [[gnu::noinline]]
    value_type* alloc_new(size_type size) {
            auto alloc_size = next_alloc_size(size);
            auto space = malloc(alloc_size);
            if (!space) {
                throw std::bad_alloc();
            }
            auto backref = _current ? &_current->next : &_begin;
            auto new_chunk = new (space) chunk(backref, alloc_size - sizeof(chunk), size);
            _current = new_chunk;
            _size += size;
            return _current->data;
    }
    [[gnu::noinline]]
    void free_chain(chunk* c) noexcept {
        while (c) {
            auto n = c->next;
            c->~chunk();
            ::free(c);
            c = n;
        }
    }
public:
    explicit bytes_ostream(size_t initial_chunk_size) noexcept
        : _begin()
        , _current(nullptr)
        , _size(0)
        , _initial_chunk_size(initial_chunk_size)
    { }

    bytes_ostream() noexcept : bytes_ostream(default_chunk_size) {}

    bytes_ostream(bytes_ostream&& o) noexcept
        : _begin(std::exchange(o._begin, {}))
        , _current(o._current)
        , _size(o._size)
        , _initial_chunk_size(o._initial_chunk_size)
    {
        o._current = nullptr;
        o._size = 0;
    }

    bytes_ostream(const bytes_ostream& o)
        : _begin()
        , _current(nullptr)
        , _size(0)
        , _initial_chunk_size(o._initial_chunk_size)
    {
        append(o);
    }

    ~bytes_ostream() {
        free_chain(_begin.ptr);
    }

    bytes_ostream& operator=(const bytes_ostream& o) {
        if (this != &o) {
            auto x = bytes_ostream(o);
            *this = std::move(x);
        }
        return *this;
    }

    bytes_ostream& operator=(bytes_ostream&& o) noexcept {
        if (this != &o) {
            this->~bytes_ostream();
            new (this) bytes_ostream(std::move(o));
        }
        return *this;
    }

    template <typename T>
    struct place_holder {
        value_type* ptr;
        // makes the place_holder looks like a stream
        seastar::simple_output_stream get_stream() {
            return seastar::simple_output_stream(reinterpret_cast<char*>(ptr), sizeof(T));
        }
    };

    // Returns a place holder for a value to be written later.
    template <std::integral T>
    inline
    place_holder<T>
    write_place_holder() {
        return place_holder<T>{alloc(sizeof(T))};
    }

    [[gnu::always_inline]]
    value_type* write_place_holder(size_type size) {
        return alloc(size);
    }

    // Writes given sequence of bytes
    [[gnu::always_inline]]
    inline void write(bytes_view v) {
        if (v.empty()) {
            return;
        }

        auto this_size = std::min(v.size(), size_t(current_space_left()));
        if (__builtin_expect(this_size, true)) {
            memcpy(_current->data + _current->frag_size, v.begin(), this_size);
            _current->frag_size += this_size;
            _size += this_size;
            v.remove_prefix(this_size);
        }

        while (!v.empty()) {
            auto this_size = std::min(v.size(), size_t(max_chunk_size()));
            std::copy_n(v.begin(), this_size, alloc_new(this_size));
            v.remove_prefix(this_size);
        }
    }

    [[gnu::always_inline]]
    void write(const char* ptr, size_t size) {
        write(bytes_view(reinterpret_cast<const signed char*>(ptr), size));
    }

    bool is_linearized() const {
        return !_begin || !_begin->next;
    }

    // Call only when is_linearized()
    bytes_view view() const {
        assert(is_linearized());
        if (!_current) {
            return bytes_view();
        }

        return bytes_view(_current->data, _size);
    }

    // Makes the underlying storage contiguous and returns a view to it.
    // Invalidates all previously created placeholders.
    bytes_view linearize() {
        if (is_linearized()) {
            return view();
        }

        auto space = malloc(_size + sizeof(chunk));
        if (!space) {
            throw std::bad_alloc();
        }

        auto old_begin = _begin;
        auto new_chunk = new (space) chunk(&_begin, _size, _size);

        auto dst = new_chunk->data;
        auto r = old_begin.ptr;
        while (r) {
            auto next = r->next;
            dst = std::copy_n(r->data, r->frag_size, dst);
            r->~chunk();
            ::free(r);
            r = next;
        }

        _current = new_chunk;
        _begin = std::move(new_chunk);
        return bytes_view(_current->data, _size);
    }

    // Returns the amount of bytes written so far
    size_type size() const {
        return _size;
    }

    // For the FragmentRange concept
    size_type size_bytes() const {
        return _size;
    }

    bool empty() const {
        return _size == 0;
    }

    void reserve(size_t size) {
        // FIXME: implement
    }

    void append(const bytes_ostream& o) {
        for (auto&& bv : o.fragments()) {
            write(bv);
        }
    }

    // Removes n bytes from the end of the bytes_ostream.
    // Beware of O(n) algorithm.
    void remove_suffix(size_t n) {
        _size -= n;
        auto left = _size;
        auto current = _begin.ptr;
        while (current) {
            if (current->frag_size >= left) {
                current->frag_size = left;
                _current = current;
                free_chain(current->next);
                current->next = nullptr;
                return;
            }
            left -= current->frag_size;
            current = current->next;
        }
    }

    // begin() and end() form an input range to bytes_view representing fragments.
    // Any modification of this instance invalidates iterators.
    fragment_iterator begin() const { return { _begin.ptr }; }
    fragment_iterator end() const { return { nullptr }; }

    output_iterator write_begin() { return output_iterator(*this); }

    boost::iterator_range<fragment_iterator> fragments() const {
        return { begin(), end() };
    }

    struct position {
        chunk* _chunk;
        size_type _offset;
    };

    position pos() const {
        return { _current, _current ? _current->frag_size : 0 };
    }

    // Returns the amount of bytes written since given position.
    // "pos" must be valid.
    size_type written_since(position pos) {
        chunk* c = pos._chunk;
        if (!c) {
            return _size;
        }
        size_type total = c->frag_size - pos._offset;
        c = c->next;
        while (c) {
            total += c->frag_size;
            c = c->next;
        }
        return total;
    }

    // Rollbacks all data written after "pos".
    // Invalidates all placeholders and positions created after "pos".
    void retract(position pos) {
        if (!pos._chunk) {
            *this = {};
            return;
        }
        _size -= written_since(pos);
        _current = pos._chunk;
        free_chain(_current->next);
        _current->next = nullptr;
        _current->frag_size = pos._offset;
    }

    void reduce_chunk_count() {
        // FIXME: This is a simplified version. It linearizes the whole buffer
        // if its size is below max_chunk_size. We probably could also gain
        // some read performance by doing "real" reduction, i.e. merging
        // all chunks until all but the last one is max_chunk_size.
        if (size() < max_chunk_size()) {
            linearize();
        }
    }

    bool operator==(const bytes_ostream& other) const {
        auto as = fragments().begin();
        auto as_end = fragments().end();
        auto bs = other.fragments().begin();
        auto bs_end = other.fragments().end();

        auto a = *as++;
        auto b = *bs++;
        while (!a.empty() || !b.empty()) {
            auto now = std::min(a.size(), b.size());
            if (!std::equal(a.begin(), a.begin() + now, b.begin(), b.begin() + now)) {
                return false;
            }
            a.remove_prefix(now);
            if (a.empty() && as != as_end) {
                a = *as++;
            }
            b.remove_prefix(now);
            if (b.empty() && bs != bs_end) {
                b = *bs++;
            }
        }
        return true;
    }

    // Makes this instance empty.
    //
    // The first buffer is not deallocated, so callers may rely on the
    // fact that if they write less than the initial chunk size between
    // the clear() calls then writes will not involve any memory allocations,
    // except for the first write made on this instance.
    void clear() {
        if (_begin.ptr) {
            _begin.ptr->frag_size = 0;
            _size = 0;
            free_chain(_begin.ptr->next);
            _begin.ptr->next = nullptr;
            _current = _begin.ptr;
        }
    }

    managed_bytes to_managed_bytes() && {
        if (_size) {
            _begin.ptr->size = _size;
            _current = nullptr;
            _size = 0;
            auto begin_ptr = _begin.ptr;
            _begin.ptr = nullptr;
            return managed_bytes(begin_ptr);
        } else {
            return managed_bytes();
        }
    }

    // Makes this instance empty using async continuations, while allowing yielding.
    //
    // The first buffer is not deallocated, so callers may rely on the
    // fact that if they write less than the initial chunk size between
    // the clear() calls then writes will not involve any memory allocations,
    // except for the first write made on this instance.
    future<> clear_gently() noexcept {
        if (!_begin.ptr) {
            return make_ready_future<>();
        }
        _begin->frag_size = 0;
        _current = _begin.ptr;
        _size = 0;
        return do_until([this] { return !_begin.ptr->next; }, [this] {
            auto second_chunk = _begin.ptr->next;
            auto next = second_chunk->next;
            second_chunk->~chunk();
            ::free(second_chunk);
            _begin->next = std::move(next);
            return make_ready_future<>();
        });
    }
};

#include <seastar/core/simple-stream.hh>

namespace utils {

using input_stream = seastar::memory_input_stream<bytes_ostream::fragment_iterator>;

}

#include <vector>
#include <unordered_set>
#include <list>
#include <array>
#include <seastar/core/sstring.hh>
#include <unordered_map>
#include <optional>
#include <seastar/core/simple-stream.hh>
#include <variant>

#include <boost/range/algorithm/for_each.hpp>
#include <boost/type.hpp>

namespace ser {

/// A fragmented view of an opaque buffer in a stream of serialised data
///
/// This class allows reading large, fragmented blobs serialised by the IDL
/// infrastructure without linearising or copying them. The view remains valid
/// as long as the underlying IDL-serialised buffer is alive.
///
/// Satisfies FragmentRange concept.
template<typename FragmentIterator>
class buffer_view {
    bytes_view _first;
    size_t _total_size;
    FragmentIterator _next;
public:
    using fragment_type = bytes_view;

    struct implementation {
        bytes_view current;
        FragmentIterator next;
        size_t size;
    };

    class iterator {
        bytes_view _current;
        size_t _left = 0;
        FragmentIterator _next;
    public:
        using iterator_category	= std::input_iterator_tag;
        using value_type = bytes_view;
        using pointer = const bytes_view*;
        using reference = const bytes_view&;
        using difference_type = std::ptrdiff_t;

        iterator() = default;
        iterator(bytes_view current, size_t left, FragmentIterator next)
            : _current(current), _left(left), _next(next) { }

        bytes_view operator*() const {
            return _current;
        }
        const bytes_view* operator->() const {
            return &_current;
        }

        iterator& operator++() {
            _left -= _current.size();
            if (_left) {
                auto next_view = bytes_view(reinterpret_cast<const bytes::value_type*>((*_next).begin()),
                                            (*_next).size());
                auto next_size = std::min(_left, next_view.size());
                _current = bytes_view(next_view.data(), next_size);
                ++_next;
            }
            return *this;
        }
        iterator operator++(int) {
            iterator it(*this);
            operator++();
            return it;
        }

        bool operator==(const iterator& other) const {
            return _left == other._left;
        }
    };
    using const_iterator = iterator;

    explicit buffer_view(bytes_view current)
        : _first(current), _total_size(current.size()) { }

    buffer_view(bytes_view current, size_t size, FragmentIterator it)
        : _first(current), _total_size(size), _next(it)
    {
        if (_first.size() > _total_size) {
            _first.remove_suffix(_first.size() - _total_size);
        }
    }

    explicit buffer_view(typename seastar::memory_input_stream<FragmentIterator>::simple stream)
        : buffer_view(bytes_view(reinterpret_cast<const int8_t*>(stream.begin()), stream.size()))
    { }

    explicit buffer_view(typename seastar::memory_input_stream<FragmentIterator>::fragmented stream)
        : buffer_view(bytes_view(reinterpret_cast<const int8_t*>(stream.first_fragment_data()), stream.first_fragment_size()),
                      stream.size(), stream.fragment_iterator())
    { }

    iterator begin() const {
        return iterator(_first, _total_size, _next);
    }
    iterator end() const {
        return iterator();
    }

    size_t size_bytes() const {
        return _total_size;
    }
    bool empty() const {
        return !_total_size;
    }

    // FragmentedView implementation
    void remove_prefix(size_t n) {
        while (n >= _first.size() && n > 0) {
            n -= _first.size();
            remove_current();
        }
        _total_size -= n;
        _first.remove_prefix(n);
    }
    void remove_current() {
        _total_size -= _first.size();
        if (_total_size) {
            auto next_data = reinterpret_cast<const bytes::value_type*>((*_next).begin());
            size_t next_size = std::min(_total_size, (*_next).size());
            _first = bytes_view(next_data, next_size);
            ++_next;
        } else {
            _first = bytes_view();
        }
    }
    buffer_view prefix(size_t n) const {
        auto tmp = *this;
        tmp._total_size = std::min(tmp._total_size, n);
        tmp._first = tmp._first.substr(0, n);
        return tmp;
    }
    bytes_view current_fragment() {
        return _first;
    }

    bytes linearize() const {
        bytes b(bytes::initialized_later(), size_bytes());
        using boost::range::for_each;
        auto dst = b.begin();
        for_each(*this, [&] (bytes_view fragment) {
            dst = std::copy(fragment.begin(), fragment.end(), dst);
        });
        return b;
    }

    template<typename Function>
    decltype(auto) with_linearized(Function&& fn) const
    {
        bytes b;
        bytes_view bv;
        if (_first.size() != _total_size) {
            b = linearize();
            bv = b;
        } else {
            bv = _first;
        }
        return fn(bv);
    }

    implementation extract_implementation() const {
        return implementation {
            .current = _first,
            .next = _next,
            .size = _total_size,
        };
    }
};
static_assert(FragmentedView<buffer_view<bytes_ostream::fragment_iterator>>);

using size_type = uint32_t;

template<typename T, typename Input>
requires std::is_integral_v<T>
inline T deserialize_integral(Input& input) {
    T data;
    input.read(reinterpret_cast<char*>(&data), sizeof(T));
    return le_to_cpu(data);
}

template<typename T, typename Output>
requires std::is_integral_v<T>
inline void serialize_integral(Output& output, T data) {
    data = cpu_to_le(data);
    output.write(reinterpret_cast<const char*>(&data), sizeof(T));
}

template<typename T>
struct serializer;

template<typename T>
struct integral_serializer {
    template<typename Input>
    static T read(Input& v) {
        return deserialize_integral<T>(v);
    }
    template<typename Output>
    static void write(Output& out, T v) {
        serialize_integral(out, v);
    }
    template<typename Input>
    static void skip(Input& v) {
        read(v);
    }
};

template<> struct serializer<bool> {
    template <typename Input>
    static bool read(Input& i) {
        return deserialize_integral<uint8_t>(i);
    }
    template< typename Output>
    static void write(Output& out, bool v) {
        serialize_integral(out, uint8_t(v));
    }
    template <typename Input>
    static void skip(Input& i) {
        read(i);
    }

};
template<> struct serializer<int8_t> : public integral_serializer<int8_t> {};
template<> struct serializer<uint8_t> : public integral_serializer<uint8_t> {};
template<> struct serializer<int16_t> : public integral_serializer<int16_t> {};
template<> struct serializer<uint16_t> : public integral_serializer<uint16_t> {};
template<> struct serializer<int32_t> : public integral_serializer<int32_t> {};
template<> struct serializer<uint32_t> : public integral_serializer<uint32_t> {};
template<> struct serializer<int64_t> : public integral_serializer<int64_t> {};
template<> struct serializer<uint64_t> : public integral_serializer<uint64_t> {};

template<typename Output>
void safe_serialize_as_uint32(Output& output, uint64_t data);

template<typename T, typename Output>
inline void serialize(Output& out, const T& v) {
    serializer<T>::write(out, v);
};

template<typename T, typename Output>
inline void serialize(Output& out, const std::reference_wrapper<T> v) {
    serializer<T>::write(out, v.get());
}

template<typename T, typename Input>
inline auto deserialize(Input& in, boost::type<T> t) {
    return serializer<T>::read(in);
}

template<typename T, typename Input>
inline void skip(Input& v, boost::type<T>) {
    return serializer<T>::skip(v);
}

template<typename T>
size_type get_sizeof(const T& obj);

template<typename T>
void set_size(seastar::measuring_output_stream& os, const T& obj);

template<typename Stream, typename T>
void set_size(Stream& os, const T& obj);

template<typename Buffer, typename T>
Buffer serialize_to_buffer(const T& v, size_t head_space = 0);

template<typename T, typename Buffer>
T deserialize_from_buffer(const Buffer&, boost::type<T>, size_t head_space = 0);

template<typename Output, typename ...T>
void serialize(Output& out, const boost::variant<T...>& v);

template<typename Input, typename ...T>
boost::variant<T...> deserialize(Input& in, boost::type<boost::variant<T...>>);

template<typename Output, typename ...T>
void serialize(Output& out, const std::variant<T...>& v);

template<typename Input, typename ...T>
std::variant<T...> deserialize(Input& in, boost::type<std::variant<T...>>);

struct unknown_variant_type {
    size_type index;
    sstring data;
};

template<typename Output>
void serialize(Output& out, const unknown_variant_type& v);

template<typename Input>
unknown_variant_type deserialize(Input& in, boost::type<unknown_variant_type>);

template <typename T>
struct normalize {
    using type = T;
};

template <>
struct normalize<bytes_view> {
     using type = bytes;
};

template <>
struct normalize<managed_bytes> {
     using type = bytes;
};

template <>
struct normalize<bytes_ostream> {
    using type = bytes;
};

template <typename T, typename U>
struct is_equivalent : std::is_same<typename normalize<std::remove_const_t<std::remove_reference_t<T>>>::type, typename normalize<std::remove_const_t <std::remove_reference_t<U>>>::type> {
};

template <typename T, typename U>
struct is_equivalent<std::reference_wrapper<T>, U> : is_equivalent<T, U> {
};

template <typename T, typename U>
struct is_equivalent<T, std::reference_wrapper<U>> : is_equivalent<T, U> {
};

template <typename T, typename U>
struct is_equivalent<std::optional<T>, std::optional<U>> : is_equivalent<T, U> {
};

template <typename T, typename U, bool>
struct is_equivalent_arity;

template <typename ...T, typename ...U>
struct is_equivalent_arity<std::tuple<T...>, std::tuple<U...>, false> : std::false_type {
};

template <typename ...T, typename ...U>
struct is_equivalent_arity<std::tuple<T...>, std::tuple<U...>, true> {
    static constexpr bool value = (is_equivalent<T, U>::value && ...);
};

template <typename ...T, typename ...U>
struct is_equivalent<std::tuple<T...>, std::tuple<U...>> : is_equivalent_arity<std::tuple<T...>, std::tuple<U...>, sizeof...(T) == sizeof...(U)> {
};

template <typename ...T, typename ...U>
struct is_equivalent<std::variant<T...>, std::variant<U...>> : is_equivalent<std::tuple<T...>, std::tuple<U...>> {
};

// gc_clock duration values were serialized as 32-bit prior to 3.1, and
// are serialized as 64-bit in 3.1.0.
//
// TTL values are capped to 20 years, which fits into 32 bits, so
// truncation is not a concern.

inline bool gc_clock_using_3_1_0_serialization = false;

template <typename Output>
void
serialize_gc_clock_duration_value(Output& out, int64_t v) {
    if (!gc_clock_using_3_1_0_serialization) {
        // This should have been caught by the CQL layer, so this is just
        // for extra safety.
        assert(int32_t(v) == v);
        serializer<int32_t>::write(out, v);
    } else {
        serializer<int64_t>::write(out, v);
    }
}

template <typename Input>
int64_t
deserialize_gc_clock_duration_value(Input& in) {
    if (!gc_clock_using_3_1_0_serialization) {
        return serializer<int32_t>::read(in);
    } else {
        return serializer<int64_t>::read(in);
    }
}

}

/*
 * Import the auto generated forward decleration code
 */



/*
 * Copyright (C) 2014-present ScyllaDB
 */

/*
 * SPDX-License-Identifier: AGPL-3.0-or-later
 */

// The following is a redesigned subset of Java's DataOutput,
// DataOutputStream, DataInput, DataInputStream, etc. It allows serializing
// several primitive types (e.g., integer, string, etc.) to an object which
// is only capable of write()ing a single byte (write(char)) or an array of
// bytes (write(char *, int)), and deserializing the same data from an object
// with a char read() interface.
//
// The format of this serialization is identical to the format used by
// Java's DataOutputStream class. This is important to allow us communicate
// with nodes running Java version of the code.
//
// We only support the subset actually used in Cassandra, and the subset
// that is reversible, i.e., can be read back by data_input. For example,
// we only support DataOutput.writeUTF(string) and not
// DataOutput.writeChars(string) - because the latter does not include
// the length, which is necessary for reading the string back.

#include <stdint.h>

#include <seastar/core/sstring.hh>
#include <seastar/net/byteorder.hh>
#include <iosfwd>
#include <iterator>


class UTFDataFormatException { };
class EOFException { };

static constexpr size_t serialize_int8_size = 1;
static constexpr size_t serialize_bool_size = 1;
static constexpr size_t serialize_int16_size = 2;
static constexpr size_t serialize_int32_size = 4;
static constexpr size_t serialize_int64_size = 8;

namespace internal_impl {

template <typename ExplicitIntegerType, typename CharOutputIterator, typename IntegerType>
requires std::is_integral<ExplicitIntegerType>::value && std::is_integral<IntegerType>::value && requires (CharOutputIterator it) {
    *it++ = 'a';
}
inline
void serialize_int(CharOutputIterator& out, IntegerType val) {
    ExplicitIntegerType nval = net::hton(ExplicitIntegerType(val));
    out = std::copy_n(reinterpret_cast<const char*>(&nval), sizeof(nval), out);
}

}

template <typename CharOutputIterator>
inline
void serialize_int8(CharOutputIterator& out, uint8_t val) {
    internal_impl::serialize_int<uint8_t>(out, val);
}

template <typename CharOutputIterator>
inline
void serialize_int16(CharOutputIterator& out, uint16_t val) {
    internal_impl::serialize_int<uint16_t>(out, val);
}

template <typename CharOutputIterator>
inline
void serialize_int32(CharOutputIterator& out, uint32_t val) {
    internal_impl::serialize_int<uint32_t>(out, val);
}

template <typename CharOutputIterator>
inline
void serialize_int64(CharOutputIterator& out, uint64_t val) {
    internal_impl::serialize_int<uint64_t>(out, val);
}

template <typename CharOutputIterator>
inline
void serialize_bool(CharOutputIterator& out, bool val) {
    serialize_int8(out, val ? 1 : 0);
}

// The following serializer is compatible with Java's writeUTF().
// In our C++ implementation, we assume the string is already UTF-8
// encoded. Unfortunately, Java's implementation is a bit different from
// UTF-8 for encoding characters above 16 bits in unicode (see
// http://docs.oracle.com/javase/7/docs/api/java/io/DataInput.html#modified-utf-8)
// For now we'll just assume those aren't in the string...
// TODO: fix the compatibility with Java even in this case.
template <typename CharOutputIterator>
requires requires (CharOutputIterator it) {
    *it++ = 'a';
}
inline
void serialize_string(CharOutputIterator& out, const sstring& s) {
    // Java specifies that nulls in the string need to be replaced by the
    // two bytes 0xC0, 0x80. Let's not bother with such transformation
    // now, but just verify wasn't needed.
    for (char c : s) {
        if (c == '\0') {
            throw UTFDataFormatException();
        }
    }
    if (s.size() > std::numeric_limits<uint16_t>::max()) {
        // Java specifies the string length is written as uint16_t, so we
        // can't serialize longer strings.
        throw UTFDataFormatException();
    }
    serialize_int16(out, s.size());
    out = std::copy(s.begin(), s.end(), out);
}

template <typename CharOutputIterator>
requires requires (CharOutputIterator it) {
    *it++ = 'a';
}
inline
void serialize_string(CharOutputIterator& out, const char* s) {
    // TODO: like above, need to change UTF-8 when above 16-bit.
    auto len = strlen(s);
    if (len > std::numeric_limits<uint16_t>::max()) {
        // Java specifies the string length is written as uint16_t, so we
        // can't serialize longer strings.
        throw UTFDataFormatException();
    }
    serialize_int16(out, len);
    out = std::copy_n(s, len, out);
}

inline
size_t serialize_string_size(const sstring& s) {;
    // As above, this code is missing the case of modified utf-8
    return serialize_int16_size + s.size();
}

template<typename T, typename CharOutputIterator>
static inline
void write(CharOutputIterator& out, const T& val) {
    auto v = net::ntoh(val);
    out = std::copy_n(reinterpret_cast<char*>(&v), sizeof(v), out);
}


/*
 * Copyright (C) 2015-present ScyllaDB
 */

/*
 * SPDX-License-Identifier: AGPL-3.0-or-later
 */

// This class is the parts of java.util.UUID that we need

#include <stdint.h>
#include <cassert>
#include <array>
#include <iosfwd>
#include <compare>

#include <seastar/core/sstring.hh>
#include <seastar/core/print.hh>
#include <seastar/net/byteorder.hh>

namespace utils {

class UUID {
private:
    int64_t most_sig_bits;
    int64_t least_sig_bits;
public:
    constexpr UUID() noexcept : most_sig_bits(0), least_sig_bits(0) {}
    constexpr UUID(int64_t most_sig_bits, int64_t least_sig_bits) noexcept
        : most_sig_bits(most_sig_bits), least_sig_bits(least_sig_bits) {}

    // May throw marshal_exception is failed to parse uuid string.
    explicit UUID(const sstring& uuid_string) : UUID(sstring_view(uuid_string)) { }
    explicit UUID(const char * s) : UUID(sstring_view(s)) {}
    explicit UUID(sstring_view uuid_string);

    int64_t get_most_significant_bits() const noexcept {
        return most_sig_bits;
    }
    int64_t get_least_significant_bits() const noexcept {
        return least_sig_bits;
    }
    int version() const noexcept {
        return (most_sig_bits >> 12) & 0xf;
    }

    bool is_timestamp() const noexcept {
        return version() == 1;
    }

    int64_t timestamp() const noexcept {
        //if (version() != 1) {
        //     throw new UnsupportedOperationException("Not a time-based UUID");
        //}
        assert(is_timestamp());

        return ((most_sig_bits & 0xFFF) << 48) |
               (((most_sig_bits >> 16) & 0xFFFF) << 32) |
               (((uint64_t)most_sig_bits) >> 32);

    }

    friend ::fmt::formatter<UUID>;

    sstring to_sstring() const {
        return fmt::to_string(*this);
    }

    friend std::ostream& operator<<(std::ostream& out, const UUID& uuid);

    bool operator==(const UUID& v) const noexcept = default;

    // Please note that this comparator does not preserve timeuuid
    // monotonicity. For this reason you should avoid using it for
    // UUIDs that could store timeuuids, otherwise bugs like
    // https://github.com/scylladb/scylla/issues/7729 may happen.
    std::strong_ordering operator<=>(const UUID& v) const noexcept {
        auto cmp = uint64_t(most_sig_bits) <=> uint64_t(v.most_sig_bits);
        if (cmp != 0) {
            return cmp;
        }
        return uint64_t(least_sig_bits) <=> uint64_t(v.least_sig_bits);
    }

    // nibble set to a non-zero value
    bool is_null() const noexcept {
        return !most_sig_bits && !least_sig_bits;
    }

    explicit operator bool() const noexcept {
        return !is_null();
    }

    bytes serialize() const {
        bytes b(bytes::initialized_later(), serialized_size());
        auto i = b.begin();
        serialize(i);
        return b;
    }

    static size_t serialized_size() noexcept {
        return 16;
    }

    template <typename CharOutputIterator>
    void serialize(CharOutputIterator& out) const {
        serialize_int64(out, most_sig_bits);
        serialize_int64(out, least_sig_bits);
    }
};

inline UUID null_uuid() noexcept {
    return UUID();
}

UUID make_random_uuid() noexcept;

// Read 8 most significant bytes of timeuuid from serialized bytes
inline uint64_t timeuuid_read_msb(const int8_t *b) noexcept {
    // cast to unsigned to avoid sign-compliment during shift.
    auto u64 = [](uint8_t i) -> uint64_t { return i; };
    // Scylla and Cassandra use a standard UUID memory layout for MSB:
    // 4 bytes    2 bytes    2 bytes
    // time_low - time_mid - time_hi_and_version
    //
    // The storage format uses network byte order.
    // Reorder bytes to allow for an integer compare.
    return u64(b[6] & 0xf) << 56 | u64(b[7]) << 48 |
           u64(b[4]) << 40 | u64(b[5]) << 32 |
           u64(b[0]) << 24 | u64(b[1]) << 16 |
           u64(b[2]) << 8  | u64(b[3]);
}

inline uint64_t uuid_read_lsb(const int8_t *b) noexcept {
    auto u64 = [](uint8_t i) -> uint64_t { return i; };
    return u64(b[8]) << 56 | u64(b[9]) << 48 |
           u64(b[10]) << 40 | u64(b[11]) << 32 |
           u64(b[12]) << 24 | u64(b[13]) << 16 |
           u64(b[14]) << 8  | u64(b[15]);
}

// Compare two values of timeuuid type.
// Cassandra legacy requires:
// - using signed compare for least significant bits.
// - masking off UUID version during compare, to
// treat possible non-version-1 UUID the same way as UUID.
//
// To avoid breaking ordering in existing sstables, Scylla preserves
// Cassandra compare order.
//
inline std::strong_ordering timeuuid_tri_compare(bytes_view o1, bytes_view o2) noexcept {
    auto timeuuid_read_lsb = [](bytes_view o) -> uint64_t {
        return uuid_read_lsb(o.begin()) ^ 0x8080808080808080;
    };
    auto res = timeuuid_read_msb(o1.begin()) <=> timeuuid_read_msb(o2.begin());
    if (res == 0) {
        res = timeuuid_read_lsb(o1) <=> timeuuid_read_lsb(o2);
    }
    return res;
}

// Compare two values of UUID type, if they happen to be
// both of Version 1 (timeuuids).
//
// This function uses memory order for least significant bits,
// which is both faster and monotonic, so should be preferred
// to @timeuuid_tri_compare() used for all new features.
//
inline std::strong_ordering uuid_tri_compare_timeuuid(bytes_view o1, bytes_view o2) noexcept {
    auto res = timeuuid_read_msb(o1.begin()) <=> timeuuid_read_msb(o2.begin());
    if (res == 0) {
        res = uuid_read_lsb(o1.begin()) <=> uuid_read_lsb(o2.begin());
    }
    return res;
}

template<typename Tag>
struct tagged_uuid {
    utils::UUID id;
    std::strong_ordering operator<=>(const tagged_uuid&) const noexcept = default;
    explicit operator bool() const noexcept {
        // The default constructor sets the id to nil, which is
        // guaranteed to not match any valid id.
        return bool(id);
    }
    static tagged_uuid create_random_id() noexcept { return tagged_uuid{utils::make_random_uuid()}; }
    static tagged_uuid create_null_id() noexcept { return tagged_uuid{utils::null_uuid()}; }
    explicit tagged_uuid(const utils::UUID& uuid) noexcept : id(uuid) {}
    tagged_uuid() = default;

    const utils::UUID& uuid() const noexcept {
        return id;
    }

    sstring to_sstring() const {
        return id.to_sstring();
    }
};
} // namespace utils

template<>
struct appending_hash<utils::UUID> {
    template<typename Hasher>
    void operator()(Hasher& h, const utils::UUID& id) const noexcept {
        feed_hash(h, id.get_most_significant_bits());
        feed_hash(h, id.get_least_significant_bits());
    }
};

template<typename Tag>
struct appending_hash<utils::tagged_uuid<Tag>> {
    template<typename Hasher>
    void operator()(Hasher& h, const utils::tagged_uuid<Tag>& id) const noexcept {
        appending_hash<utils::UUID>{}(h, id.uuid());
    }
};

namespace std {
template<>
struct hash<utils::UUID> {
    size_t operator()(const utils::UUID& id) const noexcept {
        auto hilo = id.get_most_significant_bits()
                ^ id.get_least_significant_bits();
        return size_t((hilo >> 32) ^ hilo);
    }
};

template<typename Tag>
struct hash<utils::tagged_uuid<Tag>> {
    size_t operator()(const utils::tagged_uuid<Tag>& id) const noexcept {
        return hash<utils::UUID>()(id.id);
    }
};

template<typename Tag>
std::ostream& operator<<(std::ostream& os, const utils::tagged_uuid<Tag>& id) {
    return os << id.id;
}
} // namespace std

template <>
struct fmt::formatter<utils::UUID> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const utils::UUID& id, FormatContext& ctx) const {
        // This matches Java's UUID.toString() actual implementation. Note that
        // that method's documentation suggest something completely different!
        return fmt::format_to(ctx.out(),
                "{:08x}-{:04x}-{:04x}-{:04x}-{:012x}",
                ((uint64_t)id.most_sig_bits >> 32),
                ((uint64_t)id.most_sig_bits >> 16 & 0xffff),
                ((uint64_t)id.most_sig_bits & 0xffff),
                ((uint64_t)id.least_sig_bits >> 48 & 0xffff),
                ((uint64_t)id.least_sig_bits & 0xffffffffffffLL));
    }
};

template <typename Tag>
struct fmt::formatter<utils::tagged_uuid<Tag>> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const utils::tagged_uuid<Tag>& id, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", id.id);
    }
};

/*
 *
 * Modified by ScyllaDB
 * Copyright (C) 2015-present ScyllaDB
 */

/*
 * SPDX-License-Identifier: (AGPL-3.0-or-later and Apache-2.0)
 */

#include <stdint.h>
#include <assert.h>

#include <memory>
#include <chrono>
#include <random>
#include <limits>

namespace utils {

// Scylla uses specialized timeuuids for list keys. They use
// limited space of timeuuid clockseq component to store
// sub-microsecond time. This exception is thrown when an attempt
// is made to construct such a UUID with a sub-microsecond argument
// which is outside the available bit range.
struct timeuuid_submicro_out_of_range: public std::out_of_range {
    using out_of_range::out_of_range;
};

/**
 * The goods are here: www.ietf.org/rfc/rfc4122.txt.
 */
class UUID_gen
{
public:
    // UUID timestamp time component is represented in intervals
    // of 1/10 of a microsecond since the beginning of GMT epoch.
    using decimicroseconds = std::chrono::duration<int64_t, std::ratio<1, 10'000'000>>;
    using milliseconds = std::chrono::milliseconds;
private:
    // A grand day! millis at 00:00:00.000 15 Oct 1582.
    static constexpr decimicroseconds START_EPOCH = decimicroseconds{-122192928000000000L};
    // UUID time must fit in 60 bits
    static constexpr milliseconds UUID_UNIXTIME_MAX = duration_cast<milliseconds>(
        decimicroseconds{0x0fffffffffffffffL} + START_EPOCH);

    // A random mac address for use in timeuuids
    // where we can not use clockseq to randomize the physical
    // node, and prefer using a random address to a physical one
    // to avoid duplicate timeuuids when system time goes back
    // while scylla is restarting. Using a spoof node also helps
    // avoid timeuuid duplicates when multiple nodes run on the
    // same host and share the physical MAC address.
    static thread_local const int64_t spoof_node;
    static thread_local const int64_t clock_seq_and_node;

    /*
     * The min and max possible lsb for a UUID.
     * Note that his is not 0 and all 1's because Cassandra TimeUUIDType
     * compares the lsb parts as a signed byte array comparison. So the min
     * value is 8 times -128 and the max is 8 times +127.
     *
     * Note that we ignore the uuid variant (namely, MIN_CLOCK_SEQ_AND_NODE
     * have variant 2 as it should, but MAX_CLOCK_SEQ_AND_NODE have variant 0).
     * I don't think that has any practical consequence and is more robust in
     * case someone provides a UUID with a broken variant.
     */
    static constexpr int64_t MIN_CLOCK_SEQ_AND_NODE = 0x8080808080808080L;
    static constexpr int64_t MAX_CLOCK_SEQ_AND_NODE = 0x7f7f7f7f7f7f7f7fL;

    // An instance of UUID_gen uses clock_seq_and_node so should
    // be constructed after it.
    static thread_local UUID_gen _instance;

    decimicroseconds _last_used_time = decimicroseconds{0};

    UUID_gen()
    {
        // make sure someone didn't whack the clockSeqAndNode by changing the order of instantiation.
        assert(clock_seq_and_node != 0);
    }

    // Return decimicrosecond time based on the system time,
    // in milliseconds. If the current millisecond hasn't change
    // from the previous call, increment the previously used
    // value by one decimicrosecond.
    // NOTE: In the original Java code this function was
    // "synchronized". This isn't needed since in Scylla we do not
    // need monotonicity between time UUIDs created at different
    // shards and UUID code uses thread local state on each shard.
    int64_t create_time_safe() {
        using std::chrono::system_clock;
        auto millis = duration_cast<milliseconds>(system_clock::now().time_since_epoch());
        decimicroseconds when = from_unix_timestamp(millis);
        if (when > _last_used_time) {
            _last_used_time = when;
        } else {
            when = ++_last_used_time;
        }
        return create_time(when);
    }

public:
    // We have only 17 timeuuid bits available to store this
    // value.
    static constexpr int SUBMICRO_LIMIT = (1<<17);
    /**
     * Creates a type 1 UUID (time-based UUID).
     *
     * @return a UUID instance
     */
    static UUID get_time_UUID()
    {
        auto uuid = UUID(_instance.create_time_safe(), clock_seq_and_node);
        assert(uuid.is_timestamp());
        return uuid;
    }

    /**
     * Creates a type 1 UUID (time-based UUID) with the wall clock time point @param tp.
     *
     * @return a UUID instance
     */
    static UUID get_time_UUID(std::chrono::system_clock::time_point tp)
    {
        auto uuid = UUID(create_time(from_unix_timestamp(tp.time_since_epoch())), clock_seq_and_node);
        assert(uuid.is_timestamp());
        return uuid;
    }

    /**
     * Creates a type 1 UUID (time-based UUID) with the timestamp of @param when, in milliseconds.
     *
     * @return a UUID instance
     */
    static UUID get_time_UUID(milliseconds when, int64_t clock_seq_and_node = UUID_gen::clock_seq_and_node)
    {
        auto uuid = UUID(create_time(from_unix_timestamp(when)), clock_seq_and_node);
        assert(uuid.is_timestamp());
        return uuid;
    }

    static UUID get_time_UUID_raw(decimicroseconds when, int64_t clock_seq_and_node)
    {
        auto uuid = UUID(create_time(when), clock_seq_and_node);
        assert(uuid.is_timestamp());
        return uuid;
    }

    /**
     * Similar to get_time_UUID, but randomize the clock and sequence.
     * If you can guarantee that the when_in_micros() argument is unique for
     * every call, then you should prefer get_time_UUID_from_micros() which is faster. If you can't
     * guarantee this however, this method will ensure the returned UUID are still unique (across calls)
     * through randomization.
     *
     * @param when_in_micros a unix time in microseconds.
     * @return a new UUID 'id' such that micros_timestamp(id) == when_in_micros. The UUID returned
     * by different calls will be unique even if when_in_micros is not.
     */
    static UUID get_random_time_UUID_from_micros(std::chrono::microseconds when_in_micros) {
        static thread_local std::mt19937_64 rand_gen(std::random_device().operator()());
        static thread_local std::uniform_int_distribution<int64_t> rand_dist(std::numeric_limits<int64_t>::min());

        auto uuid = UUID(create_time(from_unix_timestamp(when_in_micros)), rand_dist(rand_gen));
        assert(uuid.is_timestamp());
        return uuid;
    }
    // Generate a time-based (Version 1) UUID using
    // a microsecond-precision Unix time and a unique number in
    // range [0, 131072).
    // Used to generate many unique, monotonic UUIDs
    // sharing the same microsecond part. In lightweight
    // transactions we must ensure monotonicity between all UUIDs
    // which belong to one lightweight transaction and UUIDs of
    // another transaction, but still need multiple distinct and
    // monotonic UUIDs within the same transaction.
    // \throws timeuuid_submicro_out_of_range
    //
    static std::array<int8_t, 16>
    get_time_UUID_bytes_from_micros_and_submicros(std::chrono::microseconds when_in_micros, int submicros) {
        std::array<int8_t, 16> uuid_bytes;

        if (submicros < 0 || submicros >= SUBMICRO_LIMIT) {
            throw timeuuid_submicro_out_of_range("timeuuid submicro component does not fit into available bits");
        }

        auto dmc = from_unix_timestamp(when_in_micros);
        // We have roughly 3 extra bits we will use to increase
        // sub-microsecond component range from clockseq's 2^14 to 2^17.
        int64_t msb = create_time(dmc + decimicroseconds((submicros >> 14) & 0b111));
        // See RFC 4122 for details.
        msb = net::hton(msb);

        std::copy_n(reinterpret_cast<char*>(&msb), sizeof(msb), uuid_bytes.data());

        // Use 14-bit clockseq to store the rest of sub-microsecond component.
        int64_t clockseq = submicros & 0b11'1111'1111'1111;
        // Scylla, like Cassandra, uses signed int8 compare to
        // compare lower bits of timeuuid. It means 0xA0 > 0xFF.
        // Bit-xor the sign bit to "fix" the order. See also
        // https://issues.apache.org/jira/browse/CASSANDRA-8730
        // and Cassandra commit 6d266253a5bdaf3a25eef14e54deb56aba9b2944
        //
        // Turn 0 into -127, 1 into -126, ... and 128 into 0, ...
        clockseq ^=  0b0000'0000'1000'0000;
        // Least significant bits: UUID variant (1), clockseq and node.
        // To protect against the system clock back-adjustment,
        // use a random (spoof) node identifier. Normally this
        // protection is provided by clockseq component, but we've
        // just stored sub-microsecond time in it.
        int64_t lsb = ((clockseq | 0b1000'0000'0000'0000) << 48) | UUID_gen::spoof_node;
        lsb = net::hton(lsb);

        std::copy_n(reinterpret_cast<char*>(&lsb), sizeof(lsb), uuid_bytes.data() + sizeof(msb));

        return uuid_bytes;
    }

    /** validates uuid from raw bytes. */
    static bool is_valid_UUID(bytes raw) {
        return raw.size() == 16;
    }

    /** creates uuid from raw bytes. */
    static UUID get_UUID(bytes raw) {
        assert(raw.size() == 16);
        return get_UUID(raw.begin());
    }

    /** creates uuid from raw bytes. src must point to a region of 16 bytes*/
    static UUID get_UUID(int8_t* src) {
        struct tmp { uint64_t msb, lsb; } t;
        std::copy(src, src + 16, reinterpret_cast<char*>(&t));
        return UUID(net::ntoh(t.msb), net::ntoh(t.lsb));
    }

    /**
     * Creates a type 3 (name based) UUID based on the specified byte array.
     */
    static UUID get_name_UUID(bytes_view b);
    static UUID get_name_UUID(sstring_view str);
    static UUID get_name_UUID(const unsigned char* s, size_t len);

    /** decomposes a uuid into raw bytes. */
    static std::array<int8_t, 16> decompose(const UUID& uuid)
    {
        uint64_t most = uuid.get_most_significant_bits();
        uint64_t least = uuid.get_least_significant_bits();
        std::array<int8_t, 16> b;
        for (int i = 0; i < 8; i++)
        {
            b[i] = (char)(most >> ((7-i) * 8));
            b[8+i] = (char)(least >> ((7-i) * 8));
        }
        return b;
    }

    /**
     * Returns a 16 byte representation of a type 1 UUID (a time-based UUID),
     * based on the current system time.
     *
     * @return a type 1 UUID represented as a byte[]
     */
    static std::array<int8_t, 16> get_time_UUID_bytes() {

        uint64_t msb = _instance.create_time_safe();
        uint64_t lsb = clock_seq_and_node;
        std::array<int8_t, 16> uuid_bytes;

        for (int i = 0; i < 8; i++) {
            uuid_bytes[i] = (int8_t) (msb >> 8 * (7 - i));
        }

        for (int i = 8; i < 16; i++) {
            uuid_bytes[i] = (int8_t) (lsb >> 8 * (7 - (i - 8)));
        }

        return uuid_bytes;
    }

    /**
     * Returns the smaller possible type 1 UUID having the provided timestamp.
     *
     * <b>Warning:</b> this method should only be used for querying as this
     * doesn't at all guarantee the uniqueness of the resulting UUID.
     */
    static UUID min_time_UUID(decimicroseconds timestamp = decimicroseconds{0})
    {
        auto uuid = UUID(create_time(from_unix_timestamp(timestamp)), MIN_CLOCK_SEQ_AND_NODE);
        assert(uuid.is_timestamp());
        return uuid;
    }

    /**
     * Returns the biggest possible type 1 UUID having the provided timestamp.
     *
     * <b>Warning:</b> this method should only be used for querying as this
     * doesn't at all guarantee the uniqueness of the resulting UUID.
     */
    static UUID max_time_UUID(milliseconds timestamp)
    {
        // unix timestamp are milliseconds precision, uuid timestamp are 100's
        // nanoseconds precision. If we ask for the biggest uuid have unix
        // timestamp 1ms, then we should not extend 100's nanoseconds
        // precision by taking 10000, but rather 19999.
        decimicroseconds uuid_tstamp = from_unix_timestamp(timestamp + milliseconds(1)) - decimicroseconds(1);
        auto uuid = UUID(create_time(uuid_tstamp), MAX_CLOCK_SEQ_AND_NODE);
        assert(uuid.is_timestamp());
        return uuid;
    }

    /**
     * @param uuid
     * @return milliseconds since Unix epoch
     */
    static milliseconds unix_timestamp(UUID uuid)
    {
        return duration_cast<milliseconds>(decimicroseconds(uuid.timestamp()) + START_EPOCH);
    }

    /**
     * @param uuid
     * @return seconds since Unix epoch
     */
    static std::chrono::seconds unix_timestamp_in_sec(UUID uuid)
    {
        using namespace std::chrono;
        return duration_cast<seconds>(static_cast<milliseconds>(unix_timestamp(uuid)));
    }

    /**
     * @param uuid
     * @return microseconds since Unix epoch
     */
    static int64_t micros_timestamp(UUID uuid)
    {
        return (uuid.timestamp() + START_EPOCH.count())/10;
    }

    template <std::intmax_t N, std::intmax_t D>
    static bool is_valid_unix_timestamp(std::chrono::duration<int64_t, std::ratio<N, D>> d) {
        return duration_cast<milliseconds>(d) < UUID_UNIXTIME_MAX;
    }

    template <std::intmax_t N, std::intmax_t D>
    static decimicroseconds from_unix_timestamp(std::chrono::duration<int64_t, std::ratio<N, D>> d) {
        // Avoid 64-bit representation overflow when adding
        // timeuuid epoch to nanosecond resolution time.
        auto dmc = duration_cast<decimicroseconds>(d);
        return dmc - START_EPOCH;
    }

    // std::chrono typeaware wrapper around create_time().
    // Creates a timeuuid compatible time (decimicroseconds since
    // the start of GMT epoch).
    template <std::intmax_t N, std::intmax_t D>
    static int64_t create_time(std::chrono::duration<int64_t, std::ratio<N, D>> d) {
        auto dmc = duration_cast<decimicroseconds>(d);
        uint64_t msb = dmc.count();
        // timeuuid time must fit in 60 bits
        assert(!(0xf000000000000000UL & msb));
        return ((0x00000000ffffffffL & msb) << 32 |
               (0x0000ffff00000000UL & msb) >> 16 |
               (0x0fff000000000000UL & msb) >> 48 |
                0x0000000000001000L); // sets the version to 1.
    }

    // Produce an UUID which is derived from this UUID in a reversible manner
    //
    // Such that:
    //
    //      auto original_uuid = UUID_gen::get_time_UUID();
    //      auto negated_uuid = UUID_gen::negate(original_uuid);
    //      assert(original_uuid != negated_uuid);
    //      assert(original_uuid == UUID_gen::negate(negated_uuid));
    static UUID negate(UUID);
};

// for the curious, here is how I generated START_EPOCH
//        Calendar c = Calendar.getInstance(TimeZone.getTimeZone("GMT-0"));
//        c.set(Calendar.YEAR, 1582);
//        c.set(Calendar.MONTH, Calendar.OCTOBER);
//        c.set(Calendar.DAY_OF_MONTH, 15);
//        c.set(Calendar.HOUR_OF_DAY, 0);
//        c.set(Calendar.MINUTE, 0);
//        c.set(Calendar.SECOND, 0);
//        c.set(Calendar.MILLISECOND, 0);
//        long START_EPOCH = c.getTimeInMillis();

} // namespace utils


#include <seastar/core/shared_ptr.hh>


using column_count_type = uint32_t;

// Column ID, unique within column_kind
using column_id = column_count_type;

class schema;
class schema_extension;

using schema_ptr = seastar::lw_shared_ptr<const schema>;

using table_id = utils::tagged_uuid<struct table_id_tag>;

// Cluster-wide identifier of schema version of particular table.
//
// The version changes the value not only on structural changes but also
// temporal. For example, schemas with the same set of columns but created at
// different times should have different versions. This allows nodes to detect
// if the version they see was already synchronized with or not even if it has
// the same structure as the past versions.
//
// Schema changes merged in any order should result in the same final version.
//
// When table_schema_version changes, schema_tables::calculate_schema_digest() should
// also change when schema mutations are applied.
using table_schema_version = utils::tagged_uuid<struct table_schema_version_tag>;

inline table_schema_version reversed(table_schema_version v) noexcept {
    return table_schema_version(utils::UUID_gen::negate(v.uuid()));
}

#include <set>
#include <stdexcept>
#include <functional>
#include <map>
#include <variant>
#include <vector>
#include <unordered_set>

#include <seastar/core/sstring.hh>


namespace sstables {
class file_io_extension;
}

namespace db {
class commitlog_file_extension;

class extensions {
public:
    extensions();
    ~extensions();

    using map_type = std::map<sstring, sstring>;
    using schema_ext_config = std::variant<sstring, map_type, bytes>;
    using schema_ext_create_func = std::function<seastar::shared_ptr<schema_extension>(schema_ext_config)>;
    using sstable_file_io_extension = std::unique_ptr<sstables::file_io_extension>;
    using commitlog_file_extension_ptr = std::unique_ptr<db::commitlog_file_extension>;

    /**
     * Registered extensions
     */
    const std::map<sstring, schema_ext_create_func>& schema_extensions() const {
        return _schema_extensions;
    }
    /**
     * Returns iterable range of registered sstable IO extensions (see sstable.hh#sstable_file_io_extension)
     * For any sstables wanting to call these on file open...
     */
    std::vector<sstables::file_io_extension*> sstable_file_io_extensions() const;

    /**
     * Returns iterable range of registered commitlog IO extensions (see commitlog_extensions.hh#commitlog_file_extension)
     * For any commitlogs wanting to call these on file open or descriptor scan...
     */
    std::vector<db::commitlog_file_extension*> commitlog_file_extensions() const;

    /**
     * Registered extensions keywords, i.e. custom properties/propery sets
     * for schema extensions
     */
    std::set<sstring> schema_extension_keywords() const;

    /**
     * Init time method to add schema extension.
     */
    void add_schema_extension(sstring w, schema_ext_create_func f) {
        _schema_extensions.emplace(std::move(w), std::move(f));
    }
    /**
     * A shorthand for the add_schema_extension. Adds a function that adds
     * the extension to a schema with appropriate constructor overload.
     */
    template<typename Extension>
    void add_schema_extension(sstring w) {
        add_schema_extension(std::move(w), [] (db::extensions::schema_ext_config cfg) {
            return std::visit([] (auto v) {
                return ::make_shared<Extension>(v);
            }, cfg);
        });
    }
    /**
     * Init time method to add sstable extension
     */
    void add_sstable_file_io_extension(sstring n, sstable_file_io_extension);
    /**
     * Init time method to add sstable extension
     */
    void add_commitlog_file_extension(sstring n, commitlog_file_extension_ptr);

    /**
     * Allows forcible modification of schema extensions of a schema. This should
     * not be done lightly however. In fact, it should only be done on startup
     * at most, and thus this method is non-const, i.e. you can only use it on
     * config apply.
     */
    void add_extension_to_schema(schema_ptr, const sstring&, shared_ptr<schema_extension>);

    /**
     * Adds a keyspace to "extension internal" set.
     *
     * Such a keyspace must be loaded before/destroyed after any "normal" user keyspace.
     * Thus a psuedo-system/internal keyspce.
     * This has little to no use in open source version, and is temporarily bridged with
     * the static version of same functionality in distributed loader. It is however (or will
     * be), essential to enterprise code. Do not remove.
     */
    void add_extension_internal_keyspace(std::string);

    /**
     * Checks if a keyspace is a registered load priority one.
     */
    bool is_extension_internal_keyspace(const std::string&) const;

private:
    std::map<sstring, schema_ext_create_func> _schema_extensions;
    std::map<sstring, sstable_file_io_extension> _sstable_file_io_extensions;
    std::map<sstring, commitlog_file_extension_ptr> _commitlog_file_extensions;
    std::unordered_set<std::string> _extension_internal_keyspaces;
};
}


#include <map>
#include <optional>
#include <seastar/core/sstring.hh>

namespace cdc {

enum class delta_mode : uint8_t {
    keys,
    full,
};

/**
 * (for now only pre-) image collection mode.
 * Stating how much info to record.
 * off == none
 * on == changed columns
 * full == all (changed and unmodified columns)
 */
enum class image_mode : uint8_t {
    off, 
    on,
    full,
};

std::ostream& operator<<(std::ostream& os, delta_mode);
std::ostream& operator<<(std::ostream& os, image_mode);

class options final {
    std::optional<bool> _enabled;
    image_mode _preimage = image_mode::off;
    bool _postimage = false;
    delta_mode _delta_mode = delta_mode::full;
    int _ttl = 86400; // 24h in seconds
public:
    options() = default;
    options(const std::map<sstring, sstring>& map);

    std::map<sstring, sstring> to_map() const;
    sstring to_sstring() const;

    bool enabled() const { return _enabled.value_or(false); }
    bool is_enabled_set() const { return _enabled.has_value(); }
    bool preimage() const { return _preimage != image_mode::off; }
    bool full_preimage() const { return _preimage == image_mode::full; }
    bool postimage() const { return _postimage; }
    delta_mode get_delta_mode() const { return _delta_mode; }
    void set_delta_mode(delta_mode m) { _delta_mode = m; }
    int ttl() const { return _ttl; }

    void enabled(bool b) { _enabled = b; }
    void preimage(bool b) { preimage(b ? image_mode::on : image_mode::off); }
    void preimage(image_mode m) { _preimage = m; }
    void postimage(bool b) { _postimage = b; }
    void ttl(int v) { _ttl = v; }

    bool operator==(const options& o) const;
};

} // namespace cdc

#include <seastar/core/sstring.hh>

#include <cstdint>
#include <string_view>
#include <ostream>
#include <stdexcept>

// Wrapper for a value with a type-tag for differentiating instances.
template <class Value, class Tag>
class cql_duration_counter final {
public:
    using value_type = Value;

    explicit constexpr cql_duration_counter(value_type count) noexcept : _count(count) {}

    constexpr operator value_type() const noexcept { return _count; }
private:
    value_type _count;
};

using months_counter = cql_duration_counter<int32_t, struct month_tag>;
using days_counter = cql_duration_counter<int32_t, struct day_tag>;
using nanoseconds_counter = cql_duration_counter<int64_t, struct nanosecond_tag>;

class cql_duration_error : public std::invalid_argument {
public:
    explicit cql_duration_error(std::string_view what) : std::invalid_argument(what.data()) {}

    virtual ~cql_duration_error() = default;
};

//
// A duration of time.
//
// Three counters represent the time: the number of months, of days, and of nanoseconds. This is necessary because
// the number hours in a day can vary during daylight savings and because the number of days in a month vary.
//
// As a consequence of this representation, there can exist no total ordering relation on durations. To see why,
// consider a duration `1mo5s` (1 month and 5 seconds). In a month with 30 days, this represents a smaller duration of
// time than in a month with 31 days.
//
// The primary use of this type is to manipulate absolute time-stamps with relative offsets. For example,
// `"Jan. 31 2005 at 23:15" + 3mo5d`.
//
class cql_duration final {
public:
    using common_counter_type = int64_t;

    static_assert(
            (sizeof(common_counter_type) >= sizeof(months_counter::value_type)) &&
            (sizeof(common_counter_type) >= sizeof(days_counter::value_type)) &&
            (sizeof(common_counter_type) >= sizeof(nanoseconds_counter::value_type)),
            "The common counter type is smaller than one of the component counter types.");

    // A zero-valued duration.
    constexpr cql_duration() noexcept = default;

    // Construct a duration with explicit values for its three counters.
    constexpr cql_duration(months_counter m, days_counter d, nanoseconds_counter n) noexcept :
            months(m),
            days(d),
            nanoseconds(n) {}

    //
    // Parse a duration string.
    //
    // Three formats for durations are supported:
    //
    // 1. "Standard" format. This consists of one or more pairs of a count and a unit specifier. Examples are "23d1mo"
    //    and "5h23m10s". Components of the total duration must be written in decreasing order. That is, "5h2y" is
    //    an invalid duration string.
    //
    //    The allowed units are:
    //      - "y": years
    //      - "mo": months
    //      - "w": weeks
    //      - "d": days
    //      - "h": hours
    //      - "m": minutes
    //      - "s": seconds
    //      - "ms": milliseconds
    //      - "us" or "µs": microseconds
    //      - "ns": nanoseconds
    //
    //    Units are case-insensitive.
    //
    // 2. ISO-8601 format. "P[n]Y[n]M[n]DT[n]H[n]M[n]S" or "P[n]W". All specifiers are optional. Examples are
    //    "P23Y1M" or "P10W".
    //
    // 3. ISO-8601 alternate format. "P[YYYY]-[MM]-[DD]T[hh]:[mm]:[ss]". All specifiers are mandatory. An example is
    //    "P2000-10-14T07:22:30".
    //
    // For all formats, a negative duration is indicated by beginning the string with the '-' symbol. For example,
    // "-2y10ns".
    //
    // Throws `cql_duration_error` in the event of a parsing error.
    //
    explicit cql_duration(std::string_view s);

    months_counter::value_type months{0};
    days_counter::value_type days{0};
    nanoseconds_counter::value_type nanoseconds{0};

    //
    // Note that equality comparison is based on exact counter matches. It is not valid to expect equivalency across
    // counters like months and days. See the documentation for `duration` for more.
    //
    friend bool operator==(const cql_duration&, const cql_duration&) noexcept = default;
};

//
// Pretty-print a duration using the standard format.
//
// Durations are simplified during printing so that `duration(24, 0, 0)` is printed as "2y".
//
std::ostream& operator<<(std::ostream& os, const cql_duration& d);

// See above.
seastar::sstring to_string(const cql_duration&);



#include <vector>

#include <seastar/core/iostream.hh>
#include <seastar/core/print.hh>
#include <seastar/core/temporary_buffer.hh>
#include <seastar/core/simple-stream.hh>


/// Fragmented buffer consisting of multiple temporary_buffer<char>
class fragmented_temporary_buffer {
    using vector_type = std::vector<seastar::temporary_buffer<char>>;
    vector_type _fragments;
    size_t _size_bytes = 0;
public:
    static constexpr size_t default_fragment_size = 128 * 1024;

    class view;
    class istream;
    class reader;
    using ostream = seastar::memory_output_stream<vector_type::iterator>;

    fragmented_temporary_buffer() = default;

    fragmented_temporary_buffer(std::vector<seastar::temporary_buffer<char>> fragments, size_t size_bytes) noexcept
        : _fragments(std::move(fragments)), _size_bytes(size_bytes)
    { }

    fragmented_temporary_buffer(const char* str, size_t size)
    {
        *this = allocate_to_fit(size);
        size_t pos = 0;
        for (auto& frag : _fragments) {
            std::memcpy(frag.get_write(), str + pos, frag.size());
            pos += frag.size();
        }
    }

    explicit operator view() const noexcept;

    istream get_istream() const noexcept;

    ostream get_ostream() noexcept {
        if (_fragments.size() != 1) {
            return ostream::fragmented(_fragments.begin(), _size_bytes);
        }
        auto& current = *_fragments.begin();
        return ostream::simple(reinterpret_cast<char*>(current.get_write()), current.size());
    }

    size_t size_bytes() const { return _size_bytes; }
    bool empty() const { return !_size_bytes; }

    // Linear complexity, invalidates views and istreams
    void remove_prefix(size_t n) noexcept {
        _size_bytes -= n;
        auto it = _fragments.begin();
        while (it->size() < n) {
            n -= it->size();
            ++it;
        }
        if (n) {
            it->trim_front(n);
        }
        _fragments.erase(_fragments.begin(), it);
    }

    // Linear complexity, invalidates views and istreams
    void remove_suffix(size_t n) noexcept {
        _size_bytes -= n;
        auto it = _fragments.rbegin();
        while (it->size() < n) {
            n -= it->size();
            ++it;
        }
        if (n) {
            it->trim(it->size() - n);
        }
        _fragments.erase(it.base(), _fragments.end());
    }

    // Creates a fragmented temporary buffer of a specified size, supplied as a parameter.
    // Max chunk size is limited to 128kb (the same limit as `bytes_stream` has).
    static fragmented_temporary_buffer allocate_to_fit(size_t data_size) {
        constexpr size_t max_fragment_size = default_fragment_size; // 128KB

        const size_t full_fragment_count = data_size / max_fragment_size; // number of max-sized fragments
        const size_t last_fragment_size = data_size % max_fragment_size;

        std::vector<seastar::temporary_buffer<char>> fragments;
        fragments.reserve(full_fragment_count + !!last_fragment_size);
        for (size_t i = 0; i < full_fragment_count; ++i) {
            fragments.emplace_back(seastar::temporary_buffer<char>(max_fragment_size));
        }
        if (last_fragment_size) {
            fragments.emplace_back(seastar::temporary_buffer<char>(last_fragment_size));
        }
        return fragmented_temporary_buffer(std::move(fragments), data_size);
    }

    vector_type release() && noexcept {
        return std::move(_fragments);
    }
};

class fragmented_temporary_buffer::view {
    vector_type::const_iterator _current;
    const char* _current_position = nullptr;
    size_t _current_size = 0;
    size_t _total_size = 0;
public:
    view() = default;
    view(vector_type::const_iterator it, size_t position, size_t total_size)
        : _current(it)
        , _current_position(it->get() + position)
        , _current_size(std::min(it->size() - position, total_size))
        , _total_size(total_size)
    { }

    explicit view(bytes_view bv) noexcept
        : _current_position(reinterpret_cast<const char*>(bv.data()))
        , _current_size(bv.size())
        , _total_size(bv.size())
    { }

    using fragment_type = bytes_view;

    class iterator {
        vector_type::const_iterator _it;
        size_t _left = 0;
        bytes_view _current;
    public:
        using iterator_category = std::forward_iterator_tag;
        using value_type = bytes_view;
        using difference_type = ptrdiff_t;
        using pointer = const bytes_view*;
        using reference = const bytes_view&;

        iterator() = default;
        iterator(vector_type::const_iterator it, bytes_view current, size_t left) noexcept
            : _it(it)
            , _left(left)
            , _current(current)
        { }

        reference operator*() const noexcept { return _current; }
        pointer operator->() const noexcept { return &_current; }

        iterator& operator++() noexcept {
            _left -= _current.size();
            if (_left) {
                ++_it;
                _current = bytes_view(reinterpret_cast<const bytes::value_type*>(_it->get()),
                                      std::min(_left, _it->size()));
            }
            return *this;
        }

        iterator operator++(int) noexcept {
            auto it = *this;
            operator++();
            return it;
        }

        bool operator==(const iterator& other) const noexcept {
            return _left == other._left;
        }
    };

    using const_iterator = iterator;

    iterator begin() const noexcept {
        return iterator(_current,
                        bytes_view(reinterpret_cast<const bytes::value_type*>(_current_position), _current_size),
                        _total_size);
    }
    iterator end() const noexcept {
        return iterator();
    }

    bool empty() const noexcept { return !size_bytes(); }
    size_t size_bytes() const noexcept { return _total_size; }

    void remove_prefix(size_t n) noexcept {
        if (!_total_size) {
            return;
        }
        while (n > _current_size) {
            _total_size -= _current_size;
            n -= _current_size;
            ++_current;
            _current_size = std::min(_current->size(), _total_size);
            _current_position = _current->get();
        }
        _total_size -= n;
        _current_size -= n;
        _current_position += n;
        if (!_current_size && _total_size) {
            ++_current;
            _current_size = std::min(_current->size(), _total_size);
            _current_position = _current->get();
        }
    }

    void remove_current() noexcept {
        _total_size -= _current_size;
        if (_total_size) {
            ++_current;
            _current_size = std::min(_current->size(), _total_size);
            _current_position = _current->get();
        } else {
            _current_size = 0;
            _current_position = nullptr;
        }
    }

    view prefix(size_t n) const {
        auto tmp = *this;
        tmp._total_size = std::min(tmp._total_size, n);
        tmp._current_size = std::min(tmp._current_size, n);
        return tmp;
    }

    bytes_view current_fragment() const noexcept {
        return bytes_view(reinterpret_cast<const bytes_view::value_type*>(_current_position), _current_size);
    }

    // Invalidates iterators
    void remove_suffix(size_t n) noexcept {
        _total_size -= n;
        _current_size = std::min(_current_size, _total_size);
    }

    bool operator==(const fragmented_temporary_buffer::view& other) const noexcept {
        auto this_it = begin();
        auto other_it = other.begin();

        if (empty() || other.empty()) {
            return empty() && other.empty();
        }

        auto this_fragment = *this_it;
        auto other_fragment = *other_it;
        while (this_it != end() && other_it != other.end()) {
            if (this_fragment.empty()) {
                ++this_it;
                if (this_it != end()) {
                    this_fragment = *this_it;
                }
            }
            if (other_fragment.empty()) {
                ++other_it;
                if (other_it != other.end()) {
                    other_fragment = *other_it;
                }
            }
            auto length = std::min(this_fragment.size(), other_fragment.size());
            if (!std::equal(this_fragment.data(), this_fragment.data() + length, other_fragment.data())) {
                return false;
            }
            this_fragment.remove_prefix(length);
            other_fragment.remove_prefix(length);
        }
        return this_it == end() && other_it == other.end();
    }
};
static_assert(FragmentRange<fragmented_temporary_buffer::view>);
static_assert(FragmentedView<fragmented_temporary_buffer::view>);

inline fragmented_temporary_buffer::operator view() const noexcept
{
    if (!_size_bytes) {
        return view();
    }
    return view(_fragments.begin(), 0, _size_bytes);
}

namespace fragmented_temporary_buffer_concepts {

template<typename T>
concept ExceptionThrower = requires(T obj, size_t n) {
    obj.throw_out_of_range(n, n);
};

}

class fragmented_temporary_buffer::istream {
    vector_type::const_iterator _current;
    const char* _current_position;
    const char* _current_end;
    size_t _bytes_left = 0;
private:
    size_t contig_remain() const {
        return _current_end - _current_position;
    }
    void next_fragment() {
        _bytes_left -= _current->size();
        if (_bytes_left) {
            _current++;
            _current_position = _current->get();
            _current_end = _current->get() + _current->size();
        } else {
            _current_position = nullptr;
            _current_end = nullptr;
        }
    }

    template<typename ExceptionThrower>
    requires fragmented_temporary_buffer_concepts::ExceptionThrower<ExceptionThrower>
    void check_out_of_range(ExceptionThrower& exceptions, size_t n) {
        if (__builtin_expect(bytes_left() < n, false)) {
            exceptions.throw_out_of_range(n, bytes_left());
            // Let's allow skipping this check if the user trusts its input
            // data.
        }
    }

    template<typename T, typename ExceptionThrower>
    [[gnu::noinline]] [[gnu::cold]]
    T read_slow(ExceptionThrower&& exceptions) {
        check_out_of_range(exceptions, sizeof(T));

        T obj;
        size_t left = sizeof(T);
        while (left) {
            auto this_length = std::min<size_t>(left, _current_end - _current_position);
            std::copy_n(_current_position, this_length, reinterpret_cast<char*>(&obj) + sizeof(T) - left);
            left -= this_length;
            if (left) {
                next_fragment();
            } else {
                _current_position += this_length;
            }
        }
        return obj;
    }

    [[gnu::noinline]] [[gnu::cold]]
    void skip_slow(size_t n) noexcept {
        auto left = std::min<size_t>(n, bytes_left());
        while (left) {
            auto this_length = std::min<size_t>(left, _current_end - _current_position);
            left -= this_length;
            if (left) {
                next_fragment();
            } else {
                _current_position += this_length;
            }
        }
    }
public:
    struct default_exception_thrower {
        [[noreturn]] [[gnu::cold]]
        static void throw_out_of_range(size_t attempted_read, size_t actual_left) {
            throw std::out_of_range(format("attempted to read {:d} bytes from a {:d} byte buffer", attempted_read, actual_left));
        }
    };
    static_assert(fragmented_temporary_buffer_concepts::ExceptionThrower<default_exception_thrower>);

    istream(const vector_type& fragments, size_t total_size) noexcept
        : _current(fragments.begin())
        , _current_position(total_size ? _current->get() : nullptr)
        , _current_end(total_size ? _current->get() + _current->size() : nullptr)
        , _bytes_left(total_size)
    { }

    size_t bytes_left() const noexcept {
        return _bytes_left ? _bytes_left - (_current_position - _current->get()) : 0;
    }

    void skip(size_t n) noexcept {
        if (__builtin_expect(contig_remain() < n, false)) {
            return skip_slow(n);
        }
        _current_position += n;
    }

    template<typename T, typename ExceptionThrower = default_exception_thrower>
    requires fragmented_temporary_buffer_concepts::ExceptionThrower<ExceptionThrower>
    T read(ExceptionThrower&& exceptions = default_exception_thrower()) {
        if (__builtin_expect(contig_remain() < sizeof(T), false)) {
            return read_slow<T>(std::forward<ExceptionThrower>(exceptions));
        }
        T obj;
        std::copy_n(_current_position, sizeof(T), reinterpret_cast<char*>(&obj));
        _current_position += sizeof(T);
        return obj;
    }

    template<typename Output, typename ExceptionThrower = default_exception_thrower>
    requires fragmented_temporary_buffer_concepts::ExceptionThrower<ExceptionThrower>
    Output read_to(size_t n, Output out, ExceptionThrower&& exceptions = default_exception_thrower()) {
        if (__builtin_expect(contig_remain() >= n, true)) {
            out = std::copy_n(_current_position, n, out);
            _current_position += n;
            return out;
        }
        check_out_of_range(exceptions, n);
        out = std::copy(_current_position, _current_end, out);
        n -= _current_end - _current_position;
        next_fragment();
        while (n > _current->size()) {
            out = std::copy(_current_position, _current_end, out);
            n -= _current->size();
            next_fragment();
        }
        out = std::copy_n(_current_position, n, out);
        _current_position += n;
        return out;
    }

    template<typename ExceptionThrower = default_exception_thrower>
    requires fragmented_temporary_buffer_concepts::ExceptionThrower<ExceptionThrower>
    view read_view(size_t n, ExceptionThrower&& exceptions = default_exception_thrower()) {
        if (__builtin_expect(contig_remain() >= n, true)) {
            auto v = view(_current, _current_position - _current->get(), n);
            _current_position += n;
            return v;
        }
        check_out_of_range(exceptions, n);
        auto v = view(_current, _current_position - _current->get(), n);
        n -= _current_end - _current_position;
        next_fragment();
        while (n > _current->size()) {
            n -= _current->size();
            next_fragment();
        }
        _current_position += n;
        return v;
    }

    template<typename ExceptionThrower = default_exception_thrower>
    requires fragmented_temporary_buffer_concepts::ExceptionThrower<ExceptionThrower>
    bytes_view read_bytes_view(size_t n, bytes_ostream& linearization_buffer, ExceptionThrower&& exceptions = default_exception_thrower()) {
        if (__builtin_expect(contig_remain() >= n, true)) {
            auto v = bytes_view(reinterpret_cast<const bytes::value_type*>(_current_position), n);
            _current_position += n;
            return v;
        }
        check_out_of_range(exceptions, n);
        auto ptr = linearization_buffer.write_place_holder(n);
        read_to(n, ptr, std::forward<ExceptionThrower>(exceptions));
        return bytes_view(reinterpret_cast<const bytes::value_type*>(ptr), n);
    }
};

inline fragmented_temporary_buffer::istream fragmented_temporary_buffer::get_istream() const noexcept // allow empty (ut for that)
{
    return istream(_fragments, _size_bytes);
}

class fragmented_temporary_buffer::reader {
    std::vector<temporary_buffer<char>> _fragments;
    size_t _left = 0;
public:
    future<fragmented_temporary_buffer> read_exactly(input_stream<char>& in, size_t length) {
        _fragments = std::vector<temporary_buffer<char>>();
        _left = length;
        return repeat_until_value([this, length, &in] {
            if (!_left) {
                return make_ready_future<std::optional<fragmented_temporary_buffer>>(fragmented_temporary_buffer(std::move(_fragments), length));
            }
            return in.read_up_to(_left).then([this] (temporary_buffer<char> buf) {
                if (buf.empty()) {
                    return std::make_optional(fragmented_temporary_buffer());
                }
                _left -= buf.size();
                _fragments.emplace_back(std::move(buf));
                return std::optional<fragmented_temporary_buffer>();
            });
        });
    }
};

// The operator below is used only for logging

inline std::ostream& operator<<(std::ostream& out, const fragmented_temporary_buffer::view& v) {
    for (bytes_view frag : fragment_range(v)) {
        out << to_hex(frag);
    }
    return out;
}


#include <stdexcept>
#include <seastar/core/sstring.hh>

namespace exceptions {

enum class exception_code : int32_t {
    SERVER_ERROR    = 0x0000,
    PROTOCOL_ERROR  = 0x000A,

    BAD_CREDENTIALS = 0x0100,

    // 1xx: problem during request execution
    UNAVAILABLE     = 0x1000,
    OVERLOADED      = 0x1001,
    IS_BOOTSTRAPPING= 0x1002,
    TRUNCATE_ERROR  = 0x1003,
    WRITE_TIMEOUT   = 0x1100,
    READ_TIMEOUT    = 0x1200,
    READ_FAILURE    = 0x1300,
    FUNCTION_FAILURE= 0x1400,
    WRITE_FAILURE   = 0x1500,
    CDC_WRITE_FAILURE = 0x1600,

    // 2xx: problem validating the request
    SYNTAX_ERROR    = 0x2000,
    UNAUTHORIZED    = 0x2100,
    INVALID         = 0x2200,
    CONFIG_ERROR    = 0x2300,
    ALREADY_EXISTS  = 0x2400,
    UNPREPARED      = 0x2500,

    // Scylla-specific error codes
    // The error codes below are advertised to the drivers during connection
    // handshake using the protocol extension negotiation, and are only
    // enabled if the drivers explicitly enable them. Therefore it's perfectly
    // fine to change them in case some new error codes are introduced
    // in Cassandra.
    // NOTE TO DRIVER DEVELOPERS: These constants must not be relied upon,
    // they must be learned from protocol extensions instead.
    RATE_LIMIT_ERROR = 0xF000
};

std::ostream& operator<<(std::ostream& os, exception_code ec);

const std::unordered_map<exception_code, sstring>& exception_map();

class cassandra_exception : public std::exception {
private:
    exception_code _code;
    sstring _msg;
public:
    cassandra_exception(exception_code code, sstring msg) noexcept
        : _code(code)
        , _msg(std::move(msg))
    { }
    virtual const char* what() const noexcept override { return _msg.c_str(); }
    exception_code code() const { return _code; }
    sstring get_message() const { return what(); }
};

class server_exception : public cassandra_exception {
public:
    server_exception(sstring msg) noexcept
        : exceptions::cassandra_exception{exceptions::exception_code::SERVER_ERROR, std::move(msg)}
    { }
};

class protocol_exception : public cassandra_exception {
public:
    protocol_exception(sstring msg) noexcept
        : exceptions::cassandra_exception{exceptions::exception_code::PROTOCOL_ERROR, std::move(msg)}
    { }
};

struct unavailable_exception : cassandra_exception {
    db::consistency_level consistency;
    int32_t required;
    int32_t alive;


    unavailable_exception(sstring msg, db::consistency_level cl, int32_t required, int32_t alive) noexcept
        : exceptions::cassandra_exception(exceptions::exception_code::UNAVAILABLE, std::move(msg))
        , consistency(cl)
        , required(required)
        , alive(alive)
    {}

    unavailable_exception(db::consistency_level cl, int32_t required, int32_t alive) noexcept;
};

class request_execution_exception : public cassandra_exception {
public:
    request_execution_exception(exception_code code, sstring msg) noexcept
        : cassandra_exception(code, std::move(msg))
    { }
};

class truncate_exception : public request_execution_exception
{
public:
    truncate_exception(std::exception_ptr ep);
};

class request_timeout_exception : public cassandra_exception {
public:
    db::consistency_level consistency;
    int32_t received;
    int32_t block_for;

    request_timeout_exception(exception_code code, const sstring& ks, const sstring& cf, db::consistency_level consistency, int32_t received, int32_t block_for) noexcept;
};

class read_timeout_exception : public request_timeout_exception {
public:
    bool data_present;

    read_timeout_exception(const sstring& ks, const sstring& cf, db::consistency_level consistency, int32_t received, int32_t block_for, bool data_present) noexcept
        : request_timeout_exception{exception_code::READ_TIMEOUT, ks, cf, consistency, received, block_for}
        , data_present{data_present}
    { }
};

struct mutation_write_timeout_exception : public request_timeout_exception {
    db::write_type type;
    mutation_write_timeout_exception(const sstring& ks, const sstring& cf, db::consistency_level consistency, int32_t received, int32_t block_for, db::write_type type) noexcept :
        request_timeout_exception(exception_code::WRITE_TIMEOUT, ks, cf, consistency, received, block_for)
        , type{std::move(type)}
    { }
};

class request_failure_exception : public cassandra_exception {
public:
    db::consistency_level consistency;
    int32_t received;
    int32_t failures;
    int32_t block_for;

protected:
    request_failure_exception(exception_code code, const sstring& ks, const sstring& cf, db::consistency_level consistency_, int32_t received_, int32_t failures_, int32_t block_for_) noexcept;

    request_failure_exception(exception_code code, const sstring& msg, db::consistency_level consistency_, int32_t received_, int32_t failures_, int32_t block_for_) noexcept
        : cassandra_exception{code, msg}
        , consistency{consistency_}
        , received{received_}
        , failures{failures_}
        , block_for{block_for_}
    {}
};

struct mutation_write_failure_exception : public request_failure_exception {
    db::write_type type;
    mutation_write_failure_exception(const sstring& ks, const sstring& cf, db::consistency_level consistency_, int32_t received_, int32_t failures_, int32_t block_for_, db::write_type type_) noexcept :
        request_failure_exception(exception_code::WRITE_FAILURE, ks, cf, consistency_, received_, failures_, block_for_)
        , type{std::move(type_)}
    { }

    mutation_write_failure_exception(const sstring& msg, db::consistency_level consistency_, int32_t received_, int32_t failures_, int32_t block_for_, db::write_type type_) noexcept :
        request_failure_exception(exception_code::WRITE_FAILURE, msg, consistency_, received_, failures_, block_for_)
        , type{std::move(type_)}
    { }
};

struct read_failure_exception : public request_failure_exception {
    bool data_present;

    read_failure_exception(const sstring& ks, const sstring& cf, db::consistency_level consistency_, int32_t received_, int32_t failures_, int32_t block_for_, bool data_present_) noexcept
        : request_failure_exception{exception_code::READ_FAILURE, ks, cf, consistency_, received_, failures_, block_for_}
        , data_present{data_present_}
    { }

    read_failure_exception(const sstring& msg, db::consistency_level consistency_, int32_t received_, int32_t failures_, int32_t block_for_, bool data_present_) noexcept
        : request_failure_exception{exception_code::READ_FAILURE, msg, consistency_, received_, failures_, block_for_}
        , data_present{data_present_}
    { }
};

struct overloaded_exception : public cassandra_exception {
    explicit overloaded_exception(size_t c) noexcept;
    explicit overloaded_exception(sstring msg) noexcept :
        cassandra_exception(exception_code::OVERLOADED, std::move(msg)) {}
};

struct rate_limit_exception : public cassandra_exception {
    db::operation_type op_type;
    bool rejected_by_coordinator;

    rate_limit_exception(const sstring& ks, const sstring& cf, db::operation_type op_type_, bool rejected_by_coordinator_) noexcept;
};

class request_validation_exception : public cassandra_exception {
public:
    using cassandra_exception::cassandra_exception;
};

class invalidated_prepared_usage_attempt_exception : public exceptions::request_validation_exception {
public:
    invalidated_prepared_usage_attempt_exception() : request_validation_exception{exception_code::UNPREPARED, "Attempt to execute the invalidated prepared statement."} {}
};

class unauthorized_exception: public request_validation_exception {
public:
    unauthorized_exception(sstring msg) noexcept
                    : request_validation_exception(exception_code::UNAUTHORIZED,
                                    std::move(msg)) {
    }
};

class authentication_exception: public request_validation_exception {
public:
    authentication_exception(sstring msg) noexcept
                    : request_validation_exception(exception_code::BAD_CREDENTIALS,
                                    std::move(msg)) {
    }
};

class invalid_request_exception : public request_validation_exception {
public:
    invalid_request_exception(sstring cause) noexcept
        : request_validation_exception(exception_code::INVALID, std::move(cause))
    { }
};

class keyspace_not_defined_exception : public invalid_request_exception {
public:
    keyspace_not_defined_exception(std::string cause) noexcept
        : invalid_request_exception(std::move(cause))
    { }
};

class overflow_error_exception : public invalid_request_exception {
public:
    overflow_error_exception(std::string msg) noexcept
        : invalid_request_exception(std::move(msg))
    { }
};

class prepared_query_not_found_exception : public request_validation_exception {
public:
    bytes id;

    prepared_query_not_found_exception(bytes id) noexcept;
};

class syntax_exception : public request_validation_exception {
public:
    syntax_exception(sstring msg) noexcept
        : request_validation_exception(exception_code::SYNTAX_ERROR, std::move(msg))
    { }
};

class configuration_exception : public request_validation_exception {
public:
    configuration_exception(sstring msg) noexcept
        : request_validation_exception{exception_code::CONFIG_ERROR, std::move(msg)}
    { }

    configuration_exception(exception_code code, sstring msg) noexcept
        : request_validation_exception{code, std::move(msg)}
    { }
};

class already_exists_exception : public configuration_exception {
public:
    const sstring ks_name;
    const sstring cf_name;
private:
    already_exists_exception(sstring ks_name_, sstring cf_name_, sstring msg)
        : configuration_exception{exception_code::ALREADY_EXISTS, msg}
        , ks_name{ks_name_}
        , cf_name{cf_name_}
    { }
public:
    already_exists_exception(sstring ks_name_, sstring cf_name_);
    already_exists_exception(sstring ks_name_);
};

class recognition_exception : public std::runtime_error {
public:
    recognition_exception(const std::string& msg) : std::runtime_error(msg) {};
};

class unsupported_operation_exception : public std::runtime_error {
public:
    unsupported_operation_exception() : std::runtime_error("unsupported operation") {}
    unsupported_operation_exception(const sstring& msg) : std::runtime_error("unsupported operation: " + msg) {}
};

class function_execution_exception : public cassandra_exception {
public:
    const sstring ks_name;
    const sstring func_name;
    const std::vector<sstring> args;
    function_execution_exception(sstring func_name_, sstring detail, sstring ks_name_, std::vector<sstring> args_) noexcept;
};

}

#include <compare>
#include <cstdint>
#include <iterator>

// Specifies position in a lexicographically ordered sequence
// relative to some value.
//
// For example, if used with a value "bc" with lexicographical ordering on strings,
// each enum value represents the following positions in an example sequence:
//
//   aa
//   aaa
//   b
//   ba
// --> before_all_prefixed
//   bc
// --> before_all_strictly_prefixed
//   bca
//   bcd
// --> after_all_prefixed
//   bd
//   bda
//   c
//   ca
//
enum class lexicographical_relation : int8_t {
    before_all_prefixed,
    before_all_strictly_prefixed,
    after_all_prefixed
};

// Like std::lexicographical_compare but injects values from shared sequence (types) to the comparator
// Compare is an abstract_type-aware less comparator, which takes the type as first argument.
template <typename TypesIterator, typename InputIt1, typename InputIt2, typename Compare>
bool lexicographical_compare(TypesIterator types, InputIt1 first1, InputIt1 last1,
        InputIt2 first2, InputIt2 last2, Compare comp) {
    while (first1 != last1 && first2 != last2) {
        if (comp(*types, *first1, *first2)) {
            return true;
        }
        if (comp(*types, *first2, *first1)) {
            return false;
        }
        ++first1;
        ++first2;
        ++types;
    }
    return (first1 == last1) && (first2 != last2);
}

// Like std::lexicographical_compare but injects values from shared sequence
// (types) to the comparator. Compare is an abstract_type-aware trichotomic
// comparator, which takes the type as first argument.
template <std::input_iterator TypesIterator, std::input_iterator InputIt1, std::input_iterator InputIt2, typename Compare>
requires requires (TypesIterator types, InputIt1 i1, InputIt2 i2, Compare cmp) {
    { cmp(*types, *i1, *i2) } -> std::same_as<std::strong_ordering>;
}
std::strong_ordering lexicographical_tri_compare(TypesIterator types_first, TypesIterator types_last,
        InputIt1 first1, InputIt1 last1,
        InputIt2 first2, InputIt2 last2,
        Compare comp,
        lexicographical_relation relation1 = lexicographical_relation::before_all_strictly_prefixed,
        lexicographical_relation relation2 = lexicographical_relation::before_all_strictly_prefixed) {
    while (types_first != types_last && first1 != last1 && first2 != last2) {
        auto c = comp(*types_first, *first1, *first2);
        if (c != 0) {
            return c;
        }
        ++first1;
        ++first2;
        ++types_first;
    }
    bool e1 = first1 == last1;
    bool e2 = first2 == last2;
    if (e1 && e2) {
        return static_cast<int>(relation1) <=> static_cast<int>(relation2);
    }
    if (e2) {
        return relation2 == lexicographical_relation::after_all_prefixed ? std::strong_ordering::less : std::strong_ordering::greater;
    } else if (e1) {
        return relation1 == lexicographical_relation::after_all_prefixed ? std::strong_ordering::greater : std::strong_ordering::less;
    } else {
        return std::strong_ordering::equal;
    }
}

// A trichotomic comparator for prefix equality total ordering.
// In this ordering, two sequences are equal iff any of them is a prefix
// of the another. Otherwise, lexicographical ordering determines the order.
//
// 'comp' is an abstract_type-aware trichotomic comparator, which takes the
// type as first argument.
//
template <typename TypesIterator, typename InputIt1, typename InputIt2, typename Compare>
requires requires (TypesIterator ti, InputIt1 i1, InputIt2 i2, Compare c) {
    { c(*ti, *i1, *i2) } -> std::same_as<std::strong_ordering>;
}
std::strong_ordering prefix_equality_tri_compare(TypesIterator types, InputIt1 first1, InputIt1 last1,
        InputIt2 first2, InputIt2 last2, Compare comp) {
    while (first1 != last1 && first2 != last2) {
        auto c = comp(*types, *first1, *first2);
        if (c != 0) {
            return c;
        }
        ++first1;
        ++first2;
        ++types;
    }
    return std::strong_ordering::equal;
}

// Returns true iff the second sequence is a prefix of the first sequence
// Equality is an abstract_type-aware equality checker which takes the type as first argument.
template <typename TypesIterator, typename InputIt1, typename InputIt2, typename Equality>
bool is_prefixed_by(TypesIterator types, InputIt1 first1, InputIt1 last1,
        InputIt2 first2, InputIt2 last2, Equality equality) {
    while (first1 != last1 && first2 != last2) {
        if (!equality(*types, *first1, *first2)) {
            return false;
        }
        ++first1;
        ++first2;
        ++types;
    }
    return first2 == last2;
}



namespace tasks {

using task_id = utils::tagged_uuid<struct task_id_tag>;

struct task_info {
    task_id id;
    unsigned shard;

    task_info() noexcept : id(task_id::create_null_id()) {}
    task_info(task_id id, unsigned parent_shard) noexcept : id(id), shard(parent_shard) {}

    operator bool() const noexcept {
        return bool(id);
    }
};

}


#include <optional>
#include <boost/functional/hash.hpp>
#include <iosfwd>
#include <sstream>

#include <seastar/core/sstring.hh>
#include <seastar/core/shared_ptr.hh>
#include <seastar/net/byteorder.hh>
#include <seastar/net/ipv4_address.hh>
#include <seastar/net/ipv6_address.hh>
#include <seastar/net/inet_address.hh>
#include <seastar/util/backtrace.hh>

class tuple_type_impl;
class big_decimal;

namespace utils {

class multiprecision_int;

}

namespace cql3 {

class cql3_type;

}

int64_t timestamp_from_string(sstring_view s);

struct runtime_exception : public std::exception {
    sstring _why;
public:
    runtime_exception(sstring why) : _why(sstring("runtime error: ") + why) {}
    virtual const char* what() const noexcept override { return _why.c_str(); }
};

struct empty_t {};

class empty_value_exception : public std::exception {
public:
    virtual const char* what() const noexcept override {
        return "Unexpected empty value";
    }
};

[[noreturn]] void on_types_internal_error(std::exception_ptr ex);

// Cassandra has a notion of empty values even for scalars (i.e. int).  This is
// distinct from NULL which means deleted or never set.  It is serialized
// as a zero-length byte array (whereas NULL is serialized as a negative-length
// byte array).
template <typename T>
requires std::is_default_constructible_v<T>
class emptyable {
    // We don't use optional<>, to avoid lots of ifs during the copy and move constructors
    bool _is_empty = false;
    T _value;
public:
    // default-constructor defaults to a non-empty value, since empty is the
    // exception rather than the rule
    emptyable() : _value{} {}
    emptyable(const T& x) : _value(x) {}
    emptyable(T&& x) : _value(std::move(x)) {}
    emptyable(empty_t) : _is_empty(true) {}
    template <typename... U>
    emptyable(U&&... args) : _value(std::forward<U>(args)...) {}
    bool empty() const { return _is_empty; }
    operator const T& () const { verify(); return _value; }
    operator T&& () && { verify(); return std::move(_value); }
    const T& get() const & { verify(); return _value; }
    T&& get() && { verify(); return std::move(_value); }
private:
    void verify() const {
        if (_is_empty) {
            throw empty_value_exception();
        }
    }
};

template <typename T>
inline
bool
operator==(const emptyable<T>& me1, const emptyable<T>& me2) {
    if (me1.empty() && me2.empty()) {
        return true;
    }
    if (me1.empty() != me2.empty()) {
        return false;
    }
    return me1.get() == me2.get();
}

template <typename T>
inline
bool
operator<(const emptyable<T>& me1, const emptyable<T>& me2) {
    if (me1.empty()) {
        if (me2.empty()) {
            return false;
        } else {
            return true;
        }
    }
    if (me2.empty()) {
        return false;
    } else {
        return me1.get() < me2.get();
    }
}

// Checks whether T::empty() const exists and returns bool
template <typename T>
concept has_empty = requires (T obj) {
    { obj.empty() } -> std::same_as<bool>;
};

template <typename T>
using maybe_empty =
        std::conditional_t<has_empty<T>, T, emptyable<T>>;

class abstract_type;
class data_value;

struct ascii_native_type {
    using primary_type = sstring;
    primary_type string;
};

struct simple_date_native_type {
    using primary_type = uint32_t;
    primary_type days;
};

struct date_type_native_type {
    using primary_type = db_clock::time_point;
    primary_type tp;
};

struct time_native_type {
    using primary_type = int64_t;
    primary_type nanoseconds;
};

struct timeuuid_native_type {
    using primary_type = utils::UUID;
    primary_type uuid;
};

using data_type = shared_ptr<const abstract_type>;

template <typename T>
const T& value_cast(const data_value& value);

template <typename T>
T&& value_cast(data_value&& value);

struct empty_type_representation {
};

class data_value {
    void* _value;  // FIXME: use "small value optimization" for small types
    data_type _type;
private:
    data_value(void* value, data_type type) : _value(value), _type(std::move(type)) {}
    template <typename T>
    static data_value make_new(data_type type, T&& value);
public:
    ~data_value();
    data_value(const data_value&);
    data_value(data_value&& x) noexcept : _value(x._value), _type(std::move(x._type)) {
        x._value = nullptr;
    }
    // common conversions from C++ types to database types
    // note: somewhat dangerous, consider a factory function instead
    explicit data_value(bytes);

    data_value(sstring&&);
    data_value(std::string_view);
    // We need the following overloads just to avoid ambiguity because
    // seastar::net::inet_address is implicitly constructible from a
    // const sstring&.
    data_value(const char*);
    data_value(const std::string&);
    data_value(const sstring&);

    // Do not allow construction of a data_value from nullptr. The reason is
    // that this is error prone, for example: it conflicts with `const char*` overload
    // which tries to allocate a value from it and will cause UB.
    //
    // We want the null value semantics here instead. So the user will be forced
    // to explicitly call `make_null()` instead.
    data_value(std::nullptr_t) = delete;

    data_value(ascii_native_type);
    data_value(bool);
    data_value(int8_t);
    data_value(int16_t);
    data_value(int32_t);
    data_value(int64_t);
    data_value(utils::UUID);
    data_value(float);
    data_value(double);
    data_value(net::ipv4_address);
    data_value(net::ipv6_address);
    data_value(seastar::net::inet_address);
    data_value(simple_date_native_type);
    data_value(db_clock::time_point);
    data_value(time_native_type);
    data_value(timeuuid_native_type);
    data_value(date_type_native_type);
    data_value(utils::multiprecision_int);
    data_value(big_decimal);
    data_value(cql_duration);
    data_value(empty_type_representation);
    explicit data_value(std::optional<bytes>);
    template <typename NativeType>
    data_value(std::optional<NativeType>);
    template <typename NativeType>
    data_value(const std::unordered_set<NativeType>&);

    data_value& operator=(const data_value&);
    data_value& operator=(data_value&&);
    const data_type& type() const {
        return _type;
    }
    bool is_null() const {   // may return false negatives for strings etc.
        return !_value;
    }
    size_t serialized_size() const;
    void serialize(bytes::iterator& out) const;
    bytes_opt serialize() const;
    bytes serialize_nonnull() const;
    friend bool operator==(const data_value& x, const data_value& y);
    friend class abstract_type;
    static data_value make_null(data_type type) {
        return data_value(nullptr, std::move(type));
    }
    template <typename T>
    static data_value make(data_type type, std::unique_ptr<T> value) {
        return data_value(value.release(), std::move(type));
    }
    friend class empty_type_impl;
    template <typename T> friend const T& value_cast(const data_value&);
    template <typename T> friend T&& value_cast(data_value&&);
    friend std::ostream& operator<<(std::ostream&, const data_value&);
    friend data_value make_tuple_value(data_type, maybe_empty<std::vector<data_value>>);
    friend data_value make_set_value(data_type, maybe_empty<std::vector<data_value>>);
    friend data_value make_list_value(data_type, maybe_empty<std::vector<data_value>>);
    friend data_value make_map_value(data_type, maybe_empty<std::vector<std::pair<data_value, data_value>>>);
    friend data_value make_user_value(data_type, std::vector<data_value>);
    template <typename Func>
    friend inline auto visit(const data_value& v, Func&& f);
    // Prints a value of this type in a way which is parsable back from CQL.
    // Differs from operator<< for collections.
    sstring to_parsable_string() const;
};

template<typename T>
inline bytes serialized(T v) {
    return data_value(v).serialize_nonnull();
}

class serialized_compare;
class serialized_tri_compare;
class user_type_impl;

// Unsafe to access across shards unless otherwise noted.
class abstract_type : public enable_shared_from_this<abstract_type> {
    sstring _name;
    std::optional<uint32_t> _value_length_if_fixed;
public:
    enum class kind : int8_t {
        ascii,
        boolean,
        byte,
        bytes,
        counter,
        date,
        decimal,
        double_kind,
        duration,
        empty,
        float_kind,
        inet,
        int32,
        list,
        long_kind,
        map,
        reversed,
        set,
        short_kind,
        simple_date,
        time,
        timestamp,
        timeuuid,
        tuple,
        user,
        utf8,
        uuid,
        varint,
        last = varint,
    };
private:
    kind _kind;
public:
    kind get_kind() const { return _kind; }

    abstract_type(kind k, sstring name, std::optional<uint32_t> value_length_if_fixed)
        : _name(name), _value_length_if_fixed(std::move(value_length_if_fixed)), _kind(k) {}
    virtual ~abstract_type() {}
    bool less(bytes_view v1, bytes_view v2) const { return compare(v1, v2) < 0; }
    // returns a callable that can be called with two byte_views, and calls this->less() on them.
    serialized_compare as_less_comparator() const ;
    serialized_tri_compare as_tri_comparator() const ;
    static data_type parse_type(const sstring& name);
    size_t hash(bytes_view v) const;
    size_t hash(managed_bytes_view v) const;
    bool equal(bytes_view v1, bytes_view v2) const;
    bool equal(managed_bytes_view v1, managed_bytes_view v2) const;
    bool equal(managed_bytes_view v1, bytes_view v2) const;
    bool equal(bytes_view v1, managed_bytes_view v2) const;
    std::strong_ordering compare(bytes_view v1, bytes_view v2) const;
    std::strong_ordering compare(managed_bytes_view v1, managed_bytes_view v2) const;
    std::strong_ordering compare(managed_bytes_view v1, bytes_view v2) const;
    std::strong_ordering compare(bytes_view v1, managed_bytes_view v2) const;
private:
    // Explicitly instantiated in .cc
    template <FragmentedView View> data_value deserialize_impl(View v) const;
public:
    template <FragmentedView View> data_value deserialize(View v) const {
        if (v.size_bytes() == v.current_fragment().size()) [[likely]] {
            return deserialize_impl(single_fragmented_view(v.current_fragment()));
        } else {
            return deserialize_impl(v);
        }
    }
    data_value deserialize(bytes_view v) const {
        return deserialize_impl(single_fragmented_view(v));
    }
    data_value deserialize(const managed_bytes& v) const {
        return deserialize(managed_bytes_view(v));
    }
    template <FragmentedView View> data_value deserialize_value(View v) const {
        return deserialize(v);
    }
    data_value deserialize_value(bytes_view v) const {
        return deserialize_impl(single_fragmented_view(v));
    };
    // Explicitly instantiated in .cc
    template <FragmentedView View> void validate(const View& v) const;
    void validate(bytes_view view) const;
    bool is_compatible_with(const abstract_type& previous) const;
    /*
     * Types which are wrappers over other types return the inner type.
     * For example the reversed_type returns the type it is reversing.
     */
    shared_ptr<const abstract_type> underlying_type() const;

    /**
     * Returns true if values of the other AbstractType can be read and "reasonably" interpreted by the this
     * AbstractType. Note that this is a weaker version of isCompatibleWith, as it does not require that both type
     * compare values the same way.
     *
     * The restriction on the other type being "reasonably" interpreted is to prevent, for example, IntegerType from
     * being compatible with all other types.  Even though any byte string is a valid IntegerType value, it doesn't
     * necessarily make sense to interpret a UUID or a UTF8 string as an integer.
     *
     * Note that a type should be compatible with at least itself.
     */
    bool is_value_compatible_with(const abstract_type& other) const;
    bool references_user_type(const sstring& keyspace, const bytes& name) const;

    // For types that contain (or are equal to) the given user type (e.g., a set of elements of this type),
    // updates them with the new version of the type ('updated'). For other types does nothing.
    std::optional<data_type> update_user_type(const shared_ptr<const user_type_impl> updated) const;

    bool references_duration() const;
    std::optional<uint32_t> value_length_if_fixed() const {
        return _value_length_if_fixed;
    }
public:
    bytes decompose(const data_value& value) const;
    // Safe to call across shards
    const sstring& name() const {
        return _name;
    }

    /**
     * When returns true then equal values have the same byte representation and if byte
     * representation is different, the values are not equal.
     *
     * When returns false, nothing can be inferred.
     */
    bool is_byte_order_equal() const;
    sstring get_string(const bytes& b) const;
    sstring to_string(bytes_view bv) const {
        return to_string_impl(deserialize(bv));
    }
    sstring to_string(const bytes& b) const {
        return to_string(bytes_view(b));
    }
    sstring to_string_impl(const data_value& v) const;
    bytes from_string(sstring_view text) const;
    bool is_counter() const;
    bool is_string() const;
    bool is_collection() const;
    bool is_map() const { return _kind == kind::map; }
    bool is_set() const { return _kind == kind::set; }
    bool is_list() const { return _kind == kind::list; }
    // Lists and sets are similar: they are both represented as std::vector<data_value>
    // @sa listlike_collection_type_impl
    bool is_listlike() const { return _kind == kind::list || _kind == kind::set; }
    bool is_multi_cell() const;
    bool is_atomic() const { return !is_multi_cell(); }
    bool is_reversed() const { return _kind == kind::reversed; }
    bool is_tuple() const;
    bool is_user_type() const { return _kind == kind::user; }
    bool is_native() const;
    cql3::cql3_type as_cql3_type() const;
    const sstring& cql3_type_name() const;
    virtual shared_ptr<const abstract_type> freeze() const { return shared_from_this(); }

    const abstract_type& without_reversed() const {
        return is_reversed() ? *underlying_type() : *this;
    }

    // Checks whether there can be a set or map somewhere inside a value of this type.
    bool contains_set_or_map() const;

    // Checks whether there can be a collection somewhere inside a value of this type.
    bool contains_collection() const;

    // Checks whether a bound value of this type has to be reserialized.
    // This can be for example because there is a set inside that needs to be sorted.
    bool bound_value_needs_to_be_reserialized() const;

    friend class list_type_impl;
private:
    mutable sstring _cql3_type_name;
protected:
    bool _contains_set_or_map = false;
    bool _contains_collection = false;

    // native_value_* methods are virualized versions of native_type's
    // sizeof/alignof/copy-ctor/move-ctor etc.
    void* native_value_clone(const void* from) const;
    const std::type_info& native_typeid() const;
    // abstract_type is a friend of data_value, but derived classes are not.
    static const void* get_value_ptr(const data_value& v) {
        return v._value;
    }
    friend void write_collection_value(bytes::iterator& out, data_type type, const data_value& value);
    friend class tuple_type_impl;
    friend class data_value;
    friend class reversed_type_impl;
    template <typename T> friend const T& value_cast(const data_value& value);
    template <typename T> friend T&& value_cast(data_value&& value);
    friend bool operator==(const abstract_type& x, const abstract_type& y);
};

inline bool operator==(const abstract_type& x, const abstract_type& y)
{
     return &x == &y;
}

template <typename T>
inline
data_value
data_value::make_new(data_type type, T&& v) {
    maybe_empty<std::remove_reference_t<T>> value(std::forward<T>(v));
    return data_value(type->native_value_clone(&value), type);
}

template <typename T>
const T& value_cast(const data_value& value) {
    return value_cast<T>(const_cast<data_value&&>(value));
}

template <typename T>
T&& value_cast(data_value&& value) {
    if (typeid(maybe_empty<T>) != value.type()->native_typeid()) {
        throw std::bad_cast();
    }
    if (value.is_null()) {
        throw std::runtime_error("value is null");
    }
    return std::move(*reinterpret_cast<maybe_empty<T>*>(value._value));
}

/// Special case: sometimes we cast uuid to timeuuid so we can correctly compare timestamps.  See #7729.
template <>
inline timeuuid_native_type&& value_cast<timeuuid_native_type>(data_value&& value) {
    static thread_local timeuuid_native_type value_holder; // Static so it survives return from this function.
    value_holder.uuid = value_cast<utils::UUID>(value);
    return std::move(value_holder);
}

// CRTP: implements translation between a native_type (C++ type) to abstract_type
// AbstractType is parametrized because we want a
//    abstract_type -> collection_type_impl -> map_type
// type hierarchy, and native_type is only known at the last step.
template <typename NativeType, typename AbstractType = abstract_type>
class concrete_type : public AbstractType {
public:
    using native_type = maybe_empty<NativeType>;
    using AbstractType::AbstractType;
public:
    data_value make_value(std::unique_ptr<native_type> value) const {
        return data_value::make(this->shared_from_this(), std::move(value));
    }
    data_value make_value(native_type value) const {
        return make_value(std::make_unique<native_type>(std::move(value)));
    }
    data_value make_null() const {
        return data_value::make_null(this->shared_from_this());
    }
    data_value make_empty() const {
        return make_value(native_type(empty_t()));
    }
    const native_type& from_value(const void* v) const {
        return *reinterpret_cast<const native_type*>(v);
    }
    const native_type& from_value(const data_value& v) const {
        return this->from_value(AbstractType::get_value_ptr(v));
    }

    friend class abstract_type;
};

bool operator==(const data_value& x, const data_value& y);

using bytes_view_opt = std::optional<bytes_view>;
using managed_bytes_view_opt = std::optional<managed_bytes_view>;

static inline
bool optional_less_compare(data_type t, bytes_view_opt e1, bytes_view_opt e2) {
    if (bool(e1) != bool(e2)) {
        return bool(e2);
    }
    if (!e1) {
        return false;
    }
    return t->less(*e1, *e2);
}

static inline
bool optional_equal(data_type t, bytes_view_opt e1, bytes_view_opt e2) {
    if (bool(e1) != bool(e2)) {
        return false;
    }
    if (!e1) {
        return true;
    }
    return t->equal(*e1, *e2);
}

static inline
bool less_compare(data_type t, bytes_view e1, bytes_view e2) {
    return t->less(e1, e2);
}

static inline
std::strong_ordering tri_compare(data_type t, managed_bytes_view e1, managed_bytes_view e2) {
    return t->compare(e1, e2);
}

inline
std::strong_ordering
tri_compare_opt(data_type t, managed_bytes_view_opt v1, managed_bytes_view_opt v2) {
    if (!v1 || !v2) {
        return bool(v1) <=> bool(v2);
    } else {
        return tri_compare(std::move(t), *v1, *v2);
    }
}

static inline
bool equal(data_type t, managed_bytes_view e1, managed_bytes_view e2) {
    return t->equal(e1, e2);
}

class row_tombstone;

class collection_type_impl;
using collection_type = shared_ptr<const collection_type_impl>;

template <typename... T>
struct simple_tuple_hash;

template <>
struct simple_tuple_hash<> {
    size_t operator()() const { return 0; }
};

template <typename Arg0, typename... Args >
struct simple_tuple_hash<std::vector<Arg0>, Args...> {
    size_t operator()(const std::vector<Arg0>& vec, const Args&... args) const {
        size_t h0 = 0;
        size_t h1;
        for (auto&& i : vec) {
            h1 = std::hash<Arg0>()(i);
            h0 = h0 ^ ((h1 << 7) | (h1 >> (std::numeric_limits<size_t>::digits - 7)));
        }
        h1 = simple_tuple_hash<Args...>()(args...);
        return h0 ^ ((h1 << 7) | (h1 >> (std::numeric_limits<size_t>::digits - 7)));
    }
};

template <typename Arg0, typename... Args>
struct simple_tuple_hash<Arg0, Args...> {
    size_t operator()(const Arg0& arg0, const Args&... args) const {
        size_t h0 = std::hash<Arg0>()(arg0);
        size_t h1 = simple_tuple_hash<Args...>()(args...);
        return h0 ^ ((h1 << 7) | (h1 >> (std::numeric_limits<size_t>::digits - 7)));
    }
};

template <typename InternedType, typename... BaseTypes>
class type_interning_helper {
    using key_type = std::tuple<BaseTypes...>;
    using value_type = shared_ptr<const InternedType>;
    struct hash_type {
        size_t operator()(const key_type& k) const {
            return apply(simple_tuple_hash<BaseTypes...>(), k);
        }
    };
    using map_type = std::unordered_map<key_type, value_type, hash_type>;
    static thread_local map_type _instances;
public:
    static shared_ptr<const InternedType> get_instance(BaseTypes... keys) {
        auto key = std::make_tuple(keys...);
        auto i = _instances.find(key);
        if (i == _instances.end()) {
            auto v = ::make_shared<InternedType>(std::move(keys)...);
            i = _instances.insert(std::make_pair(std::move(key), std::move(v))).first;
        }
        return i->second;
    }
};

template <typename InternedType, typename... BaseTypes>
thread_local typename type_interning_helper<InternedType, BaseTypes...>::map_type
    type_interning_helper<InternedType, BaseTypes...>::_instances;

class reversed_type_impl : public abstract_type {
    using intern = type_interning_helper<reversed_type_impl, data_type>;
    friend struct shared_ptr_make_helper<reversed_type_impl, true>;

    data_type _underlying_type;
    reversed_type_impl(data_type t)
        : abstract_type(kind::reversed, "org.apache.cassandra.db.marshal.ReversedType(" + t->name() + ")",
                        t->value_length_if_fixed())
        , _underlying_type(t)
    {
        _contains_set_or_map = _underlying_type->contains_set_or_map();
        _contains_collection = _underlying_type->contains_collection();
    }
public:
    const data_type& underlying_type() const {
        return _underlying_type;
    }

    static shared_ptr<const reversed_type_impl> get_instance(data_type type);
};
using reversed_type = shared_ptr<const reversed_type_impl>;

// Reverse the sort order of the type by wrapping in or stripping reversed_type,
// as needed.
data_type reversed(data_type);

class map_type_impl;
using map_type = shared_ptr<const map_type_impl>;

class set_type_impl;
using set_type = shared_ptr<const set_type_impl>;

class list_type_impl;
using list_type = shared_ptr<const list_type_impl>;

inline
size_t hash_value(const shared_ptr<const abstract_type>& x) {
    return std::hash<const abstract_type*>()(x.get());
}

struct no_match_for_native_data_type {};

template <typename T>
inline constexpr auto data_type_for_v = no_match_for_native_data_type();

template <typename Type>
inline
shared_ptr<const abstract_type> data_type_for() {
    return data_type_for_v<Type>;
}

class serialized_compare {
    data_type _type;
public:
    serialized_compare(data_type type) : _type(type) {}
    bool operator()(const bytes& v1, const bytes& v2) const {
        return _type->less(v1, v2);
    }
    bool operator()(const managed_bytes& v1, const managed_bytes& v2) const {
        return _type->compare(v1, v2) < 0;
    }
};

inline
serialized_compare
abstract_type::as_less_comparator() const {
    return serialized_compare(shared_from_this());
}

class serialized_tri_compare {
    data_type _type;
public:
    serialized_tri_compare(data_type type) : _type(type) {}
    std::strong_ordering operator()(const bytes_view& v1, const bytes_view& v2) const {
        return _type->compare(v1, v2);
    }
    std::strong_ordering operator()(const managed_bytes_view& v1, const managed_bytes_view& v2) const {
        return _type->compare(v1, v2);
    }
};

inline
serialized_tri_compare
abstract_type::as_tri_comparator() const {
    return serialized_tri_compare(shared_from_this());
}

using key_compare = serialized_compare;

// Remember to update type_codec in transport/server.cc and cql3/cql3_type.cc
extern thread_local const shared_ptr<const abstract_type> byte_type;
extern thread_local const shared_ptr<const abstract_type> short_type;
extern thread_local const shared_ptr<const abstract_type> int32_type;
extern thread_local const shared_ptr<const abstract_type> long_type;
extern thread_local const shared_ptr<const abstract_type> ascii_type;
extern thread_local const shared_ptr<const abstract_type> bytes_type;
extern thread_local const shared_ptr<const abstract_type> utf8_type;
extern thread_local const shared_ptr<const abstract_type> boolean_type;
extern thread_local const shared_ptr<const abstract_type> date_type;
extern thread_local const shared_ptr<const abstract_type> timeuuid_type;
extern thread_local const shared_ptr<const abstract_type> timestamp_type;
extern thread_local const shared_ptr<const abstract_type> simple_date_type;
extern thread_local const shared_ptr<const abstract_type> time_type;
extern thread_local const shared_ptr<const abstract_type> uuid_type;
extern thread_local const shared_ptr<const abstract_type> inet_addr_type;
extern thread_local const shared_ptr<const abstract_type> float_type;
extern thread_local const shared_ptr<const abstract_type> double_type;
extern thread_local const shared_ptr<const abstract_type> varint_type;
extern thread_local const shared_ptr<const abstract_type> decimal_type;
extern thread_local const shared_ptr<const abstract_type> counter_type;
extern thread_local const shared_ptr<const abstract_type> duration_type;
extern thread_local const data_type empty_type;

template <> inline thread_local const data_type& data_type_for_v<int8_t> = byte_type;
template <> inline thread_local const data_type& data_type_for_v<int16_t> = short_type;
template <> inline thread_local const data_type& data_type_for_v<int32_t> = int32_type;
template <> inline thread_local const data_type& data_type_for_v<int64_t> = long_type;
template <> inline thread_local const data_type& data_type_for_v<sstring> = utf8_type;
template <> inline thread_local const data_type& data_type_for_v<bytes> = bytes_type;
template <> inline thread_local const data_type& data_type_for_v<utils::UUID> = uuid_type;
template <> inline thread_local const data_type& data_type_for_v<date_type_native_type> = date_type;
template <> inline thread_local const data_type& data_type_for_v<simple_date_native_type> = simple_date_type;
template <> inline thread_local const data_type& data_type_for_v<db_clock::time_point> = timestamp_type;
template <> inline thread_local const data_type& data_type_for_v<ascii_native_type> = ascii_type;
template <> inline thread_local const data_type& data_type_for_v<time_native_type> = time_type;
template <> inline thread_local const data_type& data_type_for_v<timeuuid_native_type> = timeuuid_type;
template <> inline thread_local const data_type& data_type_for_v<net::inet_address> = inet_addr_type;
template <> inline thread_local const data_type& data_type_for_v<bool> = boolean_type;
template <> inline thread_local const data_type& data_type_for_v<float> = float_type;
template <> inline thread_local const data_type& data_type_for_v<double> = double_type;
template <> inline thread_local const data_type& data_type_for_v<utils::multiprecision_int> = varint_type;
template <> inline thread_local const data_type& data_type_for_v<big_decimal> = decimal_type;
template <> inline thread_local const data_type& data_type_for_v<cql_duration> = duration_type;

namespace std {

template <>
struct hash<shared_ptr<const abstract_type>> : boost::hash<shared_ptr<abstract_type>> {
};

}

// FIXME: make more explicit
inline
bytes
to_bytes(const char* x) {
    return bytes(reinterpret_cast<const int8_t*>(x), std::strlen(x));
}

// FIXME: make more explicit
inline
bytes
to_bytes(const std::string& x) {
    return bytes(reinterpret_cast<const int8_t*>(x.data()), x.size());
}

inline
bytes_view
to_bytes_view(const std::string& x) {
    return bytes_view(reinterpret_cast<const int8_t*>(x.data()), x.size());
}

inline
bytes
to_bytes(bytes_view x) {
    return bytes(x.begin(), x.size());
}

inline
bytes_opt
to_bytes_opt(bytes_view_opt bv) {
    if (bv) {
        return to_bytes(*bv);
    }
    return std::nullopt;
}

inline
bytes_view_opt
as_bytes_view_opt(const bytes_opt& bv) {
    if (bv) {
        return bytes_view{*bv};
    }
    return std::nullopt;
}

// FIXME: make more explicit
inline
bytes
to_bytes(const sstring& x) {
    return bytes(reinterpret_cast<const int8_t*>(x.c_str()), x.size());
}

// FIXME: make more explicit
inline
managed_bytes
to_managed_bytes(const sstring& x) {
    return managed_bytes(reinterpret_cast<const int8_t*>(x.c_str()), x.size());
}

inline
bytes_view
to_bytes_view(const sstring& x) {
    return bytes_view(reinterpret_cast<const int8_t*>(x.c_str()), x.size());
}

inline
bytes
to_bytes(const utils::UUID& uuid) {
    struct {
        uint64_t msb;
        uint64_t lsb;
    } tmp = { net::hton(uint64_t(uuid.get_most_significant_bits())),
        net::hton(uint64_t(uuid.get_least_significant_bits())) };
    return bytes(reinterpret_cast<int8_t*>(&tmp), 16);
}

inline bool
less_unsigned(bytes_view v1, bytes_view v2) {
    return compare_unsigned(v1, v2) < 0;
}

template<typename Type>
static inline
typename Type::value_type deserialize_value(Type& t, bytes_view v) {
    return t.deserialize_value(v);
}

template<typename T>
T read_simple(bytes_view& v) {
    if (v.size() < sizeof(T)) {
        throw_with_backtrace<marshal_exception>(format("read_simple - not enough bytes (expected {:d}, got {:d})", sizeof(T), v.size()));
    }
    auto p = v.begin();
    v.remove_prefix(sizeof(T));
    return net::ntoh(read_unaligned<T>(p));
}

template<typename T>
T read_simple_exactly(bytes_view v) {
    if (v.size() != sizeof(T)) {
        throw_with_backtrace<marshal_exception>(format("read_simple_exactly - size mismatch (expected {:d}, got {:d})", sizeof(T), v.size()));
    }
    auto p = v.begin();
    return net::ntoh(read_unaligned<T>(p));
}

inline
bytes_view
read_simple_bytes(bytes_view& v, size_t n) {
    if (v.size() < n) {
        throw_with_backtrace<marshal_exception>(format("read_simple_bytes - not enough bytes (requested {:d}, got {:d})", n, v.size()));
    }
    bytes_view ret(v.begin(), n);
    v.remove_prefix(n);
    return ret;
}

template<FragmentedView View>
View read_simple_bytes(View& v, size_t n) {
    if (v.size_bytes() < n) {
        throw_with_backtrace<marshal_exception>(format("read_simple_bytes - not enough bytes (requested {:d}, got {:d})", n, v.size_bytes()));
    }
    auto prefix = v.prefix(n);
    v.remove_prefix(n);
    return prefix;
}

inline sstring read_simple_short_string(bytes_view& v) {
    uint16_t len = read_simple<uint16_t>(v);
    if (v.size() < len) {
        throw_with_backtrace<marshal_exception>(format("read_simple_short_string - not enough bytes ({:d})", v.size()));
    }
    sstring ret = uninitialized_string(len);
    std::copy(v.begin(), v.begin() + len, ret.begin());
    v.remove_prefix(len);
    return ret;
}

size_t collection_size_len();
size_t collection_value_len();
void write_collection_size(bytes::iterator& out, int size);
void write_collection_size(managed_bytes_mutable_view&, int size);
void write_collection_value(bytes::iterator& out, bytes_view_opt val_bytes);
void write_collection_value(managed_bytes_mutable_view&, bytes_view_opt val_bytes);
void write_collection_value(managed_bytes_mutable_view&, const managed_bytes_view_opt& val_bytes);
void write_int32(bytes::iterator& out, int32_t value);

// Splits a serialized collection into a vector of elements, but does not recursively deserialize the elements.
// Does not perform validation.
template <FragmentedView View>
utils::chunked_vector<managed_bytes_opt> partially_deserialize_listlike(View in);
template <FragmentedView View>
std::vector<std::pair<managed_bytes, managed_bytes>> partially_deserialize_map(View in);

using user_type = shared_ptr<const user_type_impl>;
using tuple_type = shared_ptr<const tuple_type_impl>;

inline
data_value::data_value(std::optional<bytes> v)
        : data_value(v ? data_value(*v) : data_value::make_null(data_type_for<bytes>())) {
}

template <typename NativeType>
data_value::data_value(std::optional<NativeType> v)
        : data_value(v ? data_value(*v) : data_value::make_null(data_type_for<NativeType>())) {
}

template<>
struct appending_hash<data_type> {
    template<typename Hasher>
    void operator()(Hasher& h, const data_type& v) const {
        feed_hash(h, v->name());
    }
};

#include <iosfwd>
#include <algorithm>
#include <vector>
#include <span>
#include <boost/range/iterator_range.hpp>
#include <boost/range/adaptor/transformed.hpp>
#include <seastar/util/backtrace.hh>

enum class allow_prefixes { no, yes };

template<allow_prefixes AllowPrefixes = allow_prefixes::no>
class compound_type final {
private:
    const std::vector<data_type> _types;
    const bool _byte_order_equal;
    const bool _byte_order_comparable;
    const bool _is_reversed;
public:
    static constexpr bool is_prefixable = AllowPrefixes == allow_prefixes::yes;
    using prefix_type = compound_type<allow_prefixes::yes>;
    using value_type = std::vector<bytes>;
    using size_type = uint16_t;

    compound_type(std::vector<data_type> types)
        : _types(std::move(types))
        , _byte_order_equal(std::all_of(_types.begin(), _types.end(), [] (const auto& t) {
                return t->is_byte_order_equal();
            }))
        , _byte_order_comparable(false)
        , _is_reversed(_types.size() == 1 && _types[0]->is_reversed())
    { }

    compound_type(compound_type&&) = default;

    auto const& types() const {
        return _types;
    }

    bool is_singular() const {
        return _types.size() == 1;
    }

    prefix_type as_prefix() {
        return prefix_type(_types);
    }
private:
    /*
     * Format:
     *   <len(value1)><value1><len(value2)><value2>...<len(value_n)><value_n>
     *
     */
    template<typename RangeOfSerializedComponents, FragmentedMutableView Out>
    static void serialize_value(RangeOfSerializedComponents&& values, Out out) {
        for (auto&& val : values) {
            using val_type = std::remove_cvref_t<decltype(val)>;
            if constexpr (FragmentedView<val_type>) {
                assert(val.size_bytes() <= std::numeric_limits<size_type>::max());
                write<size_type>(out, size_type(val.size_bytes()));
                write_fragmented(out, val);
            } else if constexpr (std::same_as<val_type, managed_bytes>) {
                assert(val.size() <= std::numeric_limits<size_type>::max());
                write<size_type>(out, size_type(val.size()));
                write_fragmented(out, managed_bytes_view(val));
            } else {
                assert(val.size() <= std::numeric_limits<size_type>::max());
                write<size_type>(out, size_type(val.size()));
                write_fragmented(out, single_fragmented_view(val));
            }
        }
    }
    template <typename RangeOfSerializedComponents>
    static size_t serialized_size(RangeOfSerializedComponents&& values) {
        size_t len = 0;
        for (auto&& val : values) {
            using val_type = std::remove_cvref_t<decltype(val)>;
            if constexpr (FragmentedView<val_type>) {
                len += sizeof(size_type) + val.size_bytes();
            } else {
                len += sizeof(size_type) + val.size();
            }
        }
        return len;
    }
public:
    managed_bytes serialize_single(const managed_bytes& v) const {
        return serialize_value(boost::make_iterator_range(&v, 1+&v));
    }
    managed_bytes serialize_single(const bytes& v) const {
        return serialize_value(boost::make_iterator_range(&v, 1+&v));
    }
    template<typename RangeOfSerializedComponents>
    static managed_bytes serialize_value(RangeOfSerializedComponents&& values) {
        auto size = serialized_size(values);
        if (size > std::numeric_limits<size_type>::max()) {
            throw std::runtime_error(format("Key size too large: {:d} > {:d}", size, std::numeric_limits<size_type>::max()));
        }
        managed_bytes b(managed_bytes::initialized_later(), size);
        serialize_value(values, managed_bytes_mutable_view(b));
        return b;
    }
    template<typename T>
    static managed_bytes serialize_value(std::initializer_list<T> values) {
        return serialize_value(boost::make_iterator_range(values.begin(), values.end()));
    }
    managed_bytes serialize_optionals(std::span<const bytes_opt> values) const {
        return serialize_value(boost::make_iterator_range(values.begin(), values.end()) | boost::adaptors::transformed([] (const bytes_opt& bo) -> bytes_view {
            if (!bo) {
                throw std::logic_error("attempted to create key component from empty optional");
            }
            return *bo;
        }));
    }
    managed_bytes serialize_optionals(std::span<const managed_bytes_opt> values) const {
        return serialize_value(boost::make_iterator_range(values.begin(), values.end()) | boost::adaptors::transformed([] (const managed_bytes_opt& bo) -> managed_bytes_view {
            if (!bo) {
                throw std::logic_error("attempted to create key component from empty optional");
            }
            return managed_bytes_view(*bo);
        }));
    }
    managed_bytes serialize_value_deep(const std::vector<data_value>& values) const {
        // TODO: Optimize
        std::vector<bytes> partial;
        partial.reserve(values.size());
        auto i = _types.begin();
        for (auto&& component : values) {
            assert(i != _types.end());
            partial.push_back((*i++)->decompose(component));
        }
        return serialize_value(partial);
    }
    managed_bytes decompose_value(const value_type& values) const {
        return serialize_value(values);
    }
    class iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = const managed_bytes_view;
        using difference_type = std::ptrdiff_t;
        using pointer = const value_type*;
        using reference = const value_type&;
    private:
        managed_bytes_view _v;
        managed_bytes_view _current;
        size_t _remaining = 0;
    private:
        void read_current() {
            _remaining = _v.size_bytes();
            size_type len;
            {
                if (_v.empty()) {
                    return;
                }
                len = read_simple<size_type>(_v);
                if (_v.size() < len) {
                    throw_with_backtrace<marshal_exception>(format("compound_type iterator - not enough bytes, expected {:d}, got {:d}", len, _v.size()));
                }
            }
            _current = _v.prefix(len);
            _v.remove_prefix(_current.size_bytes());
        }
    public:
        struct end_iterator_tag {};
        iterator(const managed_bytes_view& v) : _v(v) {
            read_current();
        }
        iterator(end_iterator_tag, const managed_bytes_view& v) : _v() {}
        iterator() {}
        iterator& operator++() {
            read_current();
            return *this;
        }
        iterator operator++(int) {
            iterator i(*this);
            ++(*this);
            return i;
        }
        const value_type& operator*() const { return _current; }
        const value_type* operator->() const { return &_current; }
        bool operator==(const iterator& i) const { return _remaining == i._remaining; }
    };
    static iterator begin(managed_bytes_view v) {
        return iterator(v);
    }
    static iterator end(managed_bytes_view v) {
        return iterator(typename iterator::end_iterator_tag(), v);
    }
    static boost::iterator_range<iterator> components(managed_bytes_view v) {
        return { begin(v), end(v) };
    }
    value_type deserialize_value(managed_bytes_view v) const {
        std::vector<bytes> result;
        result.reserve(_types.size());
        std::transform(begin(v), end(v), std::back_inserter(result), [] (auto&& v) {
            return to_bytes(v);
        });
        return result;
    }
    bool less(managed_bytes_view b1, managed_bytes_view b2) const {
        return with_linearized(b1, [&] (bytes_view bv1) {
            return with_linearized(b2, [&] (bytes_view bv2) {
                return less(bv1, bv2);
            });
        });
    }
    bool less(bytes_view b1, bytes_view b2) const {
        return compare(b1, b2) < 0;
    }
    size_t hash(managed_bytes_view v) const{
        return with_linearized(v, [&] (bytes_view v) {
            return hash(v);
        });
    }
    size_t hash(bytes_view v) const {
        if (_byte_order_equal) {
            return std::hash<bytes_view>()(v);
        }
        auto t = _types.begin();
        size_t h = 0;
        for (auto&& value : components(v)) {
            h ^= (*t)->hash(value);
            ++t;
        }
        return h;
    }
    std::strong_ordering compare(managed_bytes_view b1, managed_bytes_view b2) const {
        return with_linearized(b1, [&] (bytes_view bv1) {
            return with_linearized(b2, [&] (bytes_view bv2) {
                return compare(bv1, bv2);
            });
        });
    }
    std::strong_ordering compare(bytes_view b1, bytes_view b2) const {
        if (_byte_order_comparable) {
            if (_is_reversed) {
                return compare_unsigned(b2, b1);
            } else {
                return compare_unsigned(b1, b2);
            }
        }
        return lexicographical_tri_compare(_types.begin(), _types.end(),
            begin(b1), end(b1), begin(b2), end(b2), [] (auto&& type, auto&& v1, auto&& v2) {
                return type->compare(v1, v2);
            });
    }
    // Retruns true iff given prefix has no missing components
    bool is_full(managed_bytes_view v) const {
        assert(AllowPrefixes == allow_prefixes::yes);
        return std::distance(begin(v), end(v)) == (ssize_t)_types.size();
    }
    bool is_empty(managed_bytes_view v) const {
        return v.empty();
    }
    bool is_empty(const managed_bytes& v) const {
        return v.empty();
    }
    bool is_empty(bytes_view v) const {
        return begin(v) == end(v);
    }
    void validate(managed_bytes_view v) const {
        std::vector<managed_bytes_view> values(begin(v), end(v));
        if (AllowPrefixes == allow_prefixes::no && values.size() < _types.size()) {
            throw marshal_exception(fmt::format("compound::validate(): non-prefixable compound cannot be a prefix"));
        }
        if (values.size() > _types.size()) {
            throw marshal_exception(fmt::format("compound::validate(): cannot have more values than types, have {} values but only {} types",
                        values.size(), _types.size()));
        }
        for (size_t i = 0; i != values.size(); ++i) {
            //FIXME: is it safe to assume internal serialization-format format?
            _types[i]->validate(values[i]);
        }
    }
    bool equal(managed_bytes_view v1, managed_bytes_view v2) const {
        return with_linearized(v1, [&] (bytes_view bv1) {
            return with_linearized(v2, [&] (bytes_view bv2) {
                return equal(bv1, bv2);
            });
        });
    }
    bool equal(bytes_view v1, bytes_view v2) const {
        if (_byte_order_equal) {
            return compare_unsigned(v1, v2) == 0;
        }
        // FIXME: call equal() on each component
        return compare(v1, v2) == 0;
    }
};

using compound_prefix = compound_type<allow_prefixes::yes>;


#include <map>
#include <set>

#include <seastar/core/future.hh>
#include <seastar/core/shared_ptr.hh>
#include <seastar/core/sstring.hh>



class compressor {
    sstring _name;
public:
    compressor(sstring);

    virtual ~compressor() {}

    /**
     * Unpacks data in "input" to output. If output_len is of insufficient size,
     * exception is thrown. I.e. you should keep track of the uncompressed size.
     */
    virtual size_t uncompress(const char* input, size_t input_len, char* output,
                    size_t output_len) const = 0;
    /**
     * Packs data in "input" to output. If output_len is of insufficient size,
     * exception is thrown. Maximum required size is obtained via "compress_max_size"
     */
    virtual size_t compress(const char* input, size_t input_len, char* output,
                    size_t output_len) const = 0;
    /**
     * Returns the maximum output size for compressing data on "input_len" size.
     */
    virtual size_t compress_max_size(size_t input_len) const = 0;

    /**
     * Returns accepted option names for this compressor
     */
    virtual std::set<sstring> option_names() const;
    /**
     * Returns original options used in instantiating this compressor
     */
    virtual std::map<sstring, sstring> options() const;

    /**
     * Compressor class name.
     */
    const sstring& name() const {
        return _name;
    }

    // to cheaply bridge sstable compression options / maps
    using opt_string = std::optional<sstring>;
    using opt_getter = std::function<opt_string(const sstring&)>;
    using ptr_type = shared_ptr<compressor>;

    static ptr_type create(const sstring& name, const opt_getter&);
    static ptr_type create(const std::map<sstring, sstring>&);

    static thread_local const ptr_type lz4;
    static thread_local const ptr_type snappy;
    static thread_local const ptr_type deflate;

    static const sstring namespace_prefix;
};

template<typename BaseType, typename... Args>
class class_registry;

using compressor_ptr = compressor::ptr_type;
using compressor_registry = class_registry<compressor, const typename compressor::opt_getter&>;

class compression_parameters {
public:
    static constexpr int32_t DEFAULT_CHUNK_LENGTH = 4 * 1024;
    static constexpr double DEFAULT_CRC_CHECK_CHANCE = 1.0;

    static const sstring SSTABLE_COMPRESSION;
    static const sstring CHUNK_LENGTH_KB;
    static const sstring CHUNK_LENGTH_KB_ERR;
    static const sstring CRC_CHECK_CHANCE;
private:
    compressor_ptr _compressor;
    std::optional<int> _chunk_length;
    std::optional<double> _crc_check_chance;
public:
    compression_parameters() {}
    compression_parameters(compressor_ptr) {}
    compression_parameters(const std::map<sstring, sstring>& options) {}
    ~compression_parameters() {}

    compressor_ptr get_compressor() const { return _compressor; }
    int32_t chunk_length() const { return _chunk_length.value_or(int(DEFAULT_CHUNK_LENGTH)); }
    double crc_check_chance() const { return _crc_check_chance.value_or(double(DEFAULT_CRC_CHECK_CHANCE)); }

    void validate();
    std::map<sstring, sstring> get_options() const { return {}; }
    bool operator==(const compression_parameters& other) const;

    static compression_parameters no_compression() {
        return compression_parameters(nullptr);
    }
private:
    void validate_options(const std::map<sstring, sstring>&);
};


namespace sstables {

enum class compaction_strategy_type {
    null,
    size_tiered,
    leveled,
    date_tiered,
    time_window,
};

enum class reshape_mode { strict, relaxed };
}


#include <seastar/core/sstring.hh>
#include <map>

class schema;

class caching_options {
    // For Origin, the default value for the row is "NONE". However, since our
    // row_cache will cache both keys and rows, we will default to ALL.
    //
    // FIXME: We don't yet make any changes to our caching policies based on
    // this (and maybe we shouldn't)
    static constexpr auto default_key = "ALL";
    static constexpr auto default_row = "ALL";

    sstring _key_cache;
    sstring _row_cache;
    bool _enabled = true;
    caching_options(sstring k, sstring r, bool enabled);

    friend class schema;
    caching_options();
public:
    bool enabled() const {
        return _enabled;
    }

    std::map<sstring, sstring> to_map() const;

    sstring to_sstring() const;

    static caching_options get_disabled_caching_options();
    static caching_options from_map(const std::map<sstring, sstring>& map);
    static caching_options from_sstring(const sstring& str);

    bool operator==(const caching_options& other) const = default;
};



class schema;
class partition_key;
struct atomic_cell_view;
struct tombstone;

namespace db::view {
struct clustering_or_static_row;
struct view_key_and_action;
}

class column_computation;
using column_computation_ptr = std::unique_ptr<column_computation>;

/*
 * Column computation represents a computation performed in order to obtain a value for a computed column.
 * Computed columns description is also available at docs/dev/system_schema_keyspace.md. They hold values
 * not provided directly by the user, but rather computed: from other column values and possibly other sources.
 * This class is able to serialize/deserialize column computations and perform the computation itself,
 * based on given schema, and partition key. Responsibility for providing enough data
 * in the clustering row in order for computation to succeed belongs to the caller. In particular,
 * generating a value might involve performing a read-before-write if the computation is performed
 * on more values than are present in the update request.
 */
class column_computation {
public:
    virtual ~column_computation() = default;

    static column_computation_ptr deserialize(bytes_view raw);

    virtual column_computation_ptr clone() const = 0;

    virtual bytes serialize() const = 0;
    virtual bytes compute_value(const schema& schema, const partition_key& key) const = 0;
    /*
     * depends_on_non_primary_key_column for a column computation is needed to
     * detect a case where the primary key of a materialized view depends on a
     * non primary key column from the base table, but at the same time, the view
     * itself doesn't have non-primary key columns. This is an issue, since as
     * for now, it was assumed that no non-primary key columns in view schema
     * meant that the update cannot change the primary key of the view, and
     * therefore the update path can be simplified.
     */
    virtual bool depends_on_non_primary_key_column() const {
        return false;
    }
};

/*
 * Computes token value of partition key and returns it as bytes.
 *
 * Should NOT be used (use token_column_computation), because ordering
 * of bytes is different than ordering of tokens (signed vs unsigned comparison).
 *
 * The type name stored for computations of this class is "token" - this was
 * the original implementation. (now depracated for new tables)
 */
class legacy_token_column_computation : public column_computation {
public:
    virtual column_computation_ptr clone() const override {
        return std::make_unique<legacy_token_column_computation>(*this);
    }
    virtual bytes serialize() const override;
    virtual bytes compute_value(const schema& schema, const partition_key& key) const override;
};


/*
 * Computes token value of partition key and returns it as long_type.
 * The return type means that it can be trivially sorted (for example
 * if computed column using this computation is a clustering key),
 * preserving the correct order of tokens (using signed comparisons).
 *
 * Please use this class instead of legacy_token_column_computation.
 * 
 * The type name stored for computations of this class is "token_v2".
 * (the name "token" refers to the depracated legacy_token_column_computation)
 */
class token_column_computation : public column_computation {
public:
    virtual column_computation_ptr clone() const override {
        return std::make_unique<token_column_computation>(*this);
    }
    virtual bytes serialize() const override;
    virtual bytes compute_value(const schema& schema, const partition_key& key) const override;
};

/*
 * collection_column_computation is used for a secondary index on a collection
 * column. In this case we don't have a single value to compute, but rather we
 * want to return multiple values (e.g., all the keys in the collection).
 * So this class does not implement the base class's compute_value() -
 * instead it implements a new method compute_collection_values(), which
 * can return multiple values. This new method is currently called only from
 * the materialized-view code which uses collection_column_computation.
 */
class collection_column_computation final : public column_computation {
    enum class kind {
        keys,
        values,
        entries,
    };
    const bytes _collection_name;
    const kind _kind;
    collection_column_computation(const bytes& collection_name, kind kind) : _collection_name(collection_name), _kind(kind) {}

    using collection_kv = std::pair<bytes_view, atomic_cell_view>;
    void operate_on_collection_entries(
            std::invocable<collection_kv*, collection_kv*, tombstone> auto&& old_and_new_row_func, const schema& schema,
            const partition_key& key, const db::view::clustering_or_static_row& update, const std::optional<db::view::clustering_or_static_row>& existing) const;

public:
    static collection_column_computation for_keys(const bytes& collection_name) {
        return {collection_name, kind::keys};
    }
    static collection_column_computation for_values(const bytes& collection_name) {
        return {collection_name, kind::values};
    }
    static collection_column_computation for_entries(const bytes& collection_name) {
        return {collection_name, kind::entries};
    }
    static column_computation_ptr for_target_type(std::string_view type, const bytes& collection_name);

    virtual bytes serialize() const override;
    virtual bytes compute_value(const schema& schema, const partition_key& key) const override;
    virtual column_computation_ptr clone() const override {
        return std::make_unique<collection_column_computation>(*this);
    }
    virtual bool depends_on_non_primary_key_column() const override {
        return true;
    }

    std::vector<db::view::view_key_and_action> compute_values_with_action(const schema& schema, const partition_key& key,
            const db::view::clustering_or_static_row& row, const std::optional<db::view::clustering_or_static_row>& existing) const;
};

#include <cstdint>
#include <limits>
#include <chrono>
#include <string>

namespace api {

using timestamp_type = int64_t;
timestamp_type constexpr missing_timestamp = std::numeric_limits<timestamp_type>::min();
timestamp_type constexpr min_timestamp = std::numeric_limits<timestamp_type>::min() + 1;
timestamp_type constexpr max_timestamp = std::numeric_limits<timestamp_type>::max();

// Used for generating server-side mutation timestamps.
// Same epoch as Java's System.currentTimeMillis() for compatibility.
// Satisfies requirements of Clock.
class timestamp_clock final {
    using base = std::chrono::system_clock;
public:
    using rep = timestamp_type;
    using duration = std::chrono::microseconds;
    using period = typename duration::period;
    using time_point = std::chrono::time_point<timestamp_clock, duration>;

    static constexpr bool is_steady = base::is_steady;

    static time_point now() {
        return time_point(std::chrono::duration_cast<duration>(base::now().time_since_epoch())) + get_clocks_offset();
    }
};

static inline
timestamp_type new_timestamp() {
    return timestamp_clock::now().time_since_epoch().count();
}

}

/* For debugging and log messages. */
std::string format_timestamp(api::timestamp_type);



#include <map>
#include <chrono>
#include <seastar/core/sstring.hh>

enum class tombstone_gc_mode : uint8_t { timeout, disabled, immediate, repair };

class tombstone_gc_options {
private:
    tombstone_gc_mode _mode = tombstone_gc_mode::timeout;
    std::chrono::seconds _propagation_delay_in_seconds = std::chrono::seconds(3600);
public:
    tombstone_gc_options() = default;
    const tombstone_gc_mode& mode() const { return _mode; }
    explicit tombstone_gc_options(const std::map<seastar::sstring, seastar::sstring>& map);
    const std::chrono::seconds& propagation_delay_in_seconds() const {
        return _propagation_delay_in_seconds;
    }
    std::map<seastar::sstring, seastar::sstring> to_map() const;
    seastar::sstring to_sstring() const;
    bool operator==(const tombstone_gc_options&) const = default;
};

std::ostream& operator<<(std::ostream& os, const tombstone_gc_mode& m);


#include <optional>
#include <map>

#include <seastar/core/sstring.hh>


using namespace seastar;

namespace db {

class per_partition_rate_limit_options final {
private:
    static const char* max_writes_per_second_key;
    static const char* max_reads_per_second_key;

private:
    std::optional<uint32_t> _max_writes_per_second;
    std::optional<uint32_t> _max_reads_per_second;

public:
    per_partition_rate_limit_options() = default;
    per_partition_rate_limit_options(std::map<sstring, sstring> map);

    std::map<sstring, sstring> to_map() const;

    inline std::optional<uint32_t> get_max_ops_per_second(operation_type op_type) const {
        switch (op_type) {
        case operation_type::write:
            return _max_writes_per_second;
        case operation_type::read:
            return _max_reads_per_second;
        }
        std::abort(); // compiler will error before we reach here
    }

    inline void set_max_writes_per_second(std::optional<uint32_t> v) {
        _max_writes_per_second = v;
    }

    inline std::optional<uint32_t> get_max_writes_per_second() const {
        return _max_writes_per_second;
    }

    inline void set_max_reads_per_second(std::optional<uint32_t> v) {
        _max_reads_per_second = v;
    }

    inline std::optional<uint32_t> get_max_reads_per_second() const {
        return _max_reads_per_second;
    }
};

}

#include <ostream>

namespace replica {
class database;
}

namespace data_dictionary {

/**
 * `keyspace_element` is a common interface used to describe elements of keyspace. 
 * It is used in `describe_statement`.
 *
 * Currently the elements of keyspace are:
 * - keyspace
 * - user-defined types
 * - user-defined functions/aggregates
 * - tables, views and indexes
*/
class keyspace_element {
public:
    virtual seastar::sstring keypace_name() const = 0;
    virtual seastar::sstring element_name() const = 0;

    // Override one of these element_type() overloads.
    virtual seastar::sstring element_type() const { return ""; }
    virtual seastar::sstring element_type(replica::database& db) const { return element_type(); }

    // Override one of these describe() overloads.
    virtual std::ostream& describe(std::ostream& os) const { return os; }
    virtual std::ostream& describe(replica::database& db, std::ostream& os, bool with_internals) const { return describe(os); }
};

}

namespace cql3 {

class column_identifier;

class column_specification final {
public:
    const sstring ks_name;
    const sstring cf_name;
    const ::shared_ptr<column_identifier> name;
    const data_type type;

    column_specification(std::string_view ks_name_, std::string_view cf_name_, ::shared_ptr<column_identifier> name_, data_type type_);

    /**
     * Returns a new <code>ColumnSpecification</code> for the same column but with the specified alias.
     *
     * @param alias the column alias
     * @return a new <code>ColumnSpecification</code> for the same column but with the specified alias.
     */
    lw_shared_ptr<column_specification> with_alias(::shared_ptr<column_identifier> alias) {
        return make_lw_shared<column_specification>(ks_name, cf_name, alias, type);
    }
    
    bool is_reversed_type() const {
        return ::dynamic_pointer_cast<const reversed_type_impl>(type) != nullptr;
    }

    static bool all_in_same_table(const std::vector<lw_shared_ptr<column_specification>>& names);
};

}

#include <functional>
#include <optional>
#include <unordered_map>
#include <boost/range/iterator_range.hpp>
#include <boost/range/join.hpp>
#include <boost/lexical_cast.hpp>
#include <boost/dynamic_bitset.hpp>

#include <seastar/core/shared_ptr.hh>
#include <seastar/util/backtrace.hh>

namespace dht {

class i_partitioner;
class sharder;

}

namespace cdc {
class options;
}

namespace replica {
class database;
}

using column_count_type = uint32_t;

// Column ID, unique within column_kind
using column_id = column_count_type;

// Column ID unique within a schema. Enum class to avoid
// mixing wtih column id.
enum class ordinal_column_id: column_count_type {};

std::ostream& operator<<(std::ostream& os, ordinal_column_id id);

// Maintains a set of columns used in a query. The columns are
// identified by ordinal_id.
//
// @sa column_definition::ordinal_id.
class column_set {
public:
    using bitset = boost::dynamic_bitset<uint64_t>;
    using size_type = bitset::size_type;

    // column_count_type is more narrow than size_type, but truncating a size_type max value does
    // give column_count_type max value. This is used to avoid extra branching in
    // find_first()/find_next().
    static_assert(static_cast<column_count_type>(boost::dynamic_bitset<uint64_t>::npos) == ~static_cast<column_count_type>(0));
    static constexpr ordinal_column_id npos = static_cast<ordinal_column_id>(bitset::npos);

    explicit column_set(column_count_type num_bits = 0)
        : _mask(num_bits)
    {
    }

    void resize(column_count_type num_bits) {
        _mask.resize(num_bits);
    }

    // Set the appropriate bit for column id.
    void set(ordinal_column_id id) {
        column_count_type bit = static_cast<column_count_type>(id);
        _mask.set(bit);
    }
    // Test the mask for use of a given column id.
    bool test(ordinal_column_id id) const {
        column_count_type bit = static_cast<column_count_type>(id);
        return _mask.test(bit);
    }
    // @sa boost::dynamic_bistet docs
    size_type count() const { return _mask.count(); }
    ordinal_column_id find_first() const {
        return static_cast<ordinal_column_id>(_mask.find_first());
    }
    ordinal_column_id find_next(ordinal_column_id pos) const {
        return static_cast<ordinal_column_id>(_mask.find_next(static_cast<column_count_type>(pos)));
    }
    // Logical or
    void union_with(const column_set& with) {
        _mask |= with._mask;
    }

private:
    bitset _mask;
};

class schema_registry_entry;
class schema_builder;

// Useful functions to manipulate the schema's comparator field
namespace cell_comparator {
sstring to_sstring(const schema& s);
bool check_compound(sstring comparator);
void read_collections(schema_builder& builder, sstring comparator);
}

namespace db {
class extensions;
}
// make sure these match the order we like columns back from schema
enum class column_kind { partition_key, clustering_key, static_column, regular_column };

enum class column_view_virtual { no, yes };

sstring to_sstring(column_kind k);
bool is_compatible(column_kind k1, column_kind k2);

enum class cf_type : uint8_t {
    standard,
    super,
};

inline sstring cf_type_to_sstring(cf_type t) {
    if (t == cf_type::standard) {
        return "Standard";
    } else if (t == cf_type::super) {
        return "Super";
    }
    throw std::invalid_argument(format("unknown type: {:d}\n", uint8_t(t)));
}

inline cf_type sstring_to_cf_type(sstring name) {
    if (name == "Standard") {
        return cf_type::standard;
    } else if (name == "Super") {
        return cf_type::super;
    }
    throw std::invalid_argument(format("unknown type: {}\n", name));
}

struct speculative_retry {
    enum class type {
        NONE, CUSTOM, PERCENTILE, ALWAYS
    };
private:
    type _t;
    double _v;
public:
    speculative_retry(type t, double v) : _t(t), _v(v) {}

    sstring to_sstring() const {
        if (_t == type::NONE) {
            return "NONE";
        } else if (_t == type::ALWAYS) {
            return "ALWAYS";
        } else if (_t == type::CUSTOM) {
            return format("{:.2f}ms", _v);
        } else if (_t == type::PERCENTILE) {
            return format("{:.1f}PERCENTILE", 100 * _v);
        } else {
            throw std::invalid_argument(format("unknown type: {:d}\n", uint8_t(_t)));
        }
    }
    static speculative_retry from_sstring(sstring str) {
        std::transform(str.begin(), str.end(), str.begin(), ::toupper);

        sstring ms("MS");
        sstring percentile("PERCENTILE");

        auto convert = [&str] (sstring& t) {
            try {
                return boost::lexical_cast<double>(str.substr(0, str.size() - t.size()));
            } catch (boost::bad_lexical_cast& e) {
                throw std::invalid_argument(format("cannot convert {} to speculative_retry\n", str));
            }
        };

        type t;
        double v = 0;
        if (str == "NONE") {
            t = type::NONE;
        } else if (str == "ALWAYS") {
            t = type::ALWAYS;
        } else if (str.compare(str.size() - ms.size(), ms.size(), ms) == 0) {
            t = type::CUSTOM;
            v = convert(ms);
        } else if (str.compare(str.size() - percentile.size(), percentile.size(), percentile) == 0) {
            t = type::PERCENTILE;
            v = convert(percentile) / 100;
        } else {
            throw std::invalid_argument(format("cannot convert {} to speculative_retry\n", str));
        }
        return speculative_retry(t, v);
    }
    type get_type() const {
        return _t;
    }
    double get_value() const {
        return _v;
    }
    bool operator==(const speculative_retry& other) const = default;
};

typedef std::unordered_map<sstring, sstring> index_options_map;

enum class index_metadata_kind {
    keys,
    custom,
    composites,
};

class index_metadata final {
public:
    struct is_local_index_tag {};
    using is_local_index = bool_class<is_local_index_tag>;
private:
    table_id _id;
    sstring _name;
    index_metadata_kind _kind;
    index_options_map _options;
    bool _local;
public:
    index_metadata(const sstring& name, const index_options_map& options, index_metadata_kind kind, is_local_index local);
    bool operator==(const index_metadata& other) const;
    bool equals_noname(const index_metadata& other) const;
    const table_id& id() const;
    const sstring& name() const;
    const index_metadata_kind kind() const;
    const index_options_map& options() const;
    bool local() const;
    static sstring get_default_index_name(const sstring& cf_name, std::optional<sstring> root);
};

class column_definition final {
public:
    struct name_comparator {
        data_type type;
        name_comparator(data_type type) : type(type) {}
        bool operator()(const column_definition& cd1, const column_definition& cd2) const {
            return type->less(cd1.name(), cd2.name());
        }
    };
private:
    bytes _name;
    api::timestamp_type _dropped_at;
    bool _is_atomic;
    bool _is_counter;
    column_view_virtual _is_view_virtual;
    column_computation_ptr _computation;

    struct thrift_bits {
        thrift_bits()
            : is_on_all_components(0)
        {}
        uint8_t is_on_all_components : 1;
        // more...?
    };

    thrift_bits _thrift_bits;
    friend class schema;
public:
    column_definition(bytes name, data_type type, column_kind kind,
        column_id component_index = 0,
        column_view_virtual view_virtual = column_view_virtual::no,
        column_computation_ptr = nullptr,
        api::timestamp_type dropped_at = api::missing_timestamp);

    data_type type;

    // Unique within (kind, schema instance).
    // schema::position() and component_index() depend on the fact that for PK columns this is
    // equivalent to component index.
    column_id id;

    // Unique within schema instance
    ordinal_column_id ordinal_id;

    column_kind kind;
    lw_shared_ptr<cql3::column_specification> column_specification;

    // NOTICE(sarna): This copy constructor is hand-written instead of default,
    // because it involves deep copying of the computation object.
    // Computation has a strict ownership policy provided by
    // unique_ptr, and as such cannot rely on default copying.
    column_definition(const column_definition& other)
            : _name(other._name)
            , _dropped_at(other._dropped_at)
            , _is_atomic(other._is_atomic)
            , _is_counter(other._is_counter)
            , _is_view_virtual(other._is_view_virtual)
            , _computation(other.get_computation_ptr())
            , _thrift_bits(other._thrift_bits)
            , type(other.type)
            , id(other.id)
            , ordinal_id(other.ordinal_id)
            , kind(other.kind)
            , column_specification(other.column_specification)
        {}

    column_definition& operator=(const column_definition& other) {
        if (this == &other) {
            return *this;
        }
        column_definition tmp(other);
        *this = std::move(tmp);
        return *this;
    }

    column_definition& operator=(column_definition&& other) = default;

    bool is_static() const { return kind == column_kind::static_column; }
    bool is_regular() const { return kind == column_kind::regular_column; }
    bool is_partition_key() const { return kind == column_kind::partition_key; }
    bool is_clustering_key() const { return kind == column_kind::clustering_key; }
    bool is_primary_key() const { return kind == column_kind::partition_key || kind == column_kind::clustering_key; }
    bool is_atomic() const { return _is_atomic; }
    bool is_multi_cell() const { return !_is_atomic; }
    bool is_counter() const { return _is_counter; }
    // "virtual columns" appear in a materialized view as placeholders for
    // unselected columns, with liveness information but without data, and
    // allow view rows to remain alive despite having no data (issue #3362).
    // These columns should be hidden from the user's SELECT queries.
    bool is_view_virtual() const { return _is_view_virtual == column_view_virtual::yes; }
    column_view_virtual view_virtual() const { return _is_view_virtual; }
    // Computed column values are generated from other columns (and possibly other sources) during updates.
    // Their values are still stored on disk, same as a regular columns.
    bool is_computed() const { return bool(_computation); }
    const column_computation& get_computation() const { return *_computation; }
    column_computation_ptr get_computation_ptr() const {
        return _computation ? _computation->clone() : nullptr;
    }
    void set_computed(column_computation_ptr computation) { _computation = std::move(computation); }
    // Columns hidden from CQL cannot be in any way retrieved by the user,
    // either explicitly or via the '*' operator, or functions, aggregates, etc.
    bool is_hidden_from_cql() const { return is_view_virtual(); }
    const sstring& name_as_text() const;
    const bytes& name() const;
    sstring name_as_cql_string() const;
    friend std::ostream& operator<<(std::ostream& os, const column_definition& cd);
    bool has_component_index() const {
        return is_primary_key();
    }
    uint32_t component_index() const {
        assert(has_component_index());
        return id;
    }
    uint32_t position() const {
        if (has_component_index()) {
            return component_index();
        }
        return 0;
    }
    bool is_on_all_components() const;
    bool is_part_of_cell_name() const {
        return is_regular() || is_static();
    }
    api::timestamp_type dropped_at() const { return _dropped_at; }
    friend bool operator==(const column_definition&, const column_definition&);
};

class schema_builder;

/*
 * Sub-schema for thrift aspects. Should be kept isolated (and starved)
 */
class thrift_schema {
    bool _compound = true;
    bool _is_dynamic = false;
public:
    bool has_compound_comparator() const;
    bool is_dynamic() const;
    friend class schema;
};

bool operator==(const column_definition&, const column_definition&);

static constexpr int DEFAULT_MIN_COMPACTION_THRESHOLD = 4;
static constexpr int DEFAULT_MAX_COMPACTION_THRESHOLD = 32;
static constexpr int DEFAULT_MIN_INDEX_INTERVAL = 128;
static constexpr int DEFAULT_GC_GRACE_SECONDS = 864000;

// Unsafe to access across shards.
// Safe to copy across shards.
class column_mapping_entry {
    bytes _name;
    data_type _type;
    bool _is_atomic;
public:
    column_mapping_entry(bytes name, data_type type)
        : _name(std::move(name)), _type(std::move(type)), _is_atomic(_type->is_atomic()) { }
    column_mapping_entry(bytes name, sstring type_name);
    column_mapping_entry(const column_mapping_entry&);
    column_mapping_entry& operator=(const column_mapping_entry&);
    column_mapping_entry(column_mapping_entry&&) = default;
    column_mapping_entry& operator=(column_mapping_entry&&) = default;
    const bytes& name() const { return _name; }
    const data_type& type() const { return _type; }
    const sstring& type_name() const { return _type->name(); }
    bool is_atomic() const { return _is_atomic; }
};

bool operator==(const column_mapping_entry& lhs, const column_mapping_entry& rhs);

// Encapsulates information needed for converting mutations between different schema versions.
//
// Unsafe to access across shards.
// Safe to copy across shards.
class column_mapping {
private:
    // Contains _n_static definitions for static columns followed by definitions for regular columns,
    // both ordered by consecutive column_ids.
    // Primary key column sets are not mutable so we don't need to map them.
    std::vector<column_mapping_entry> _columns;
    column_count_type _n_static = 0;
public:
    column_mapping() {}
    column_mapping(std::vector<column_mapping_entry> columns, column_count_type n_static)
            : _columns(std::move(columns))
            , _n_static(n_static)
    { }
    const std::vector<column_mapping_entry>& columns() const { return _columns; }
    column_count_type n_static() const { return _n_static; }
    const column_mapping_entry& column_at(column_kind kind, column_id id) const {
        assert(kind == column_kind::regular_column || kind == column_kind::static_column);
        return kind == column_kind::regular_column ? regular_column_at(id) : static_column_at(id);
    }
    const column_mapping_entry& static_column_at(column_id id) const {
        if (id >= _n_static) {
            throw std::out_of_range(format("static column id {:d} >= {:d}", id, _n_static));
        }
        return _columns[id];
    }
    const column_mapping_entry& regular_column_at(column_id id) const {
        auto n_regular = _columns.size() - _n_static;
        if (id >= n_regular) {
            throw std::out_of_range(format("regular column id {:d} >= {:d}", id, n_regular));
        }
        return _columns[id + _n_static];
    }
    friend std::ostream& operator<<(std::ostream& out, const column_mapping& cm);
};

bool operator==(const column_mapping& lhs, const column_mapping& rhs);

/**
 * Augments a schema with fields related to materialized views.
 * Effectively immutable.
 */
class raw_view_info final {
    table_id _base_id;
    sstring _base_name;
    bool _include_all_columns;
    sstring _where_clause;
public:
    raw_view_info(table_id base_id, sstring base_name, bool include_all_columns, sstring where_clause);

    const table_id& base_id() const {
        return _base_id;
    }

    const sstring& base_name() const {
        return _base_name;
    }

    bool include_all_columns() const {
        return _include_all_columns;
    }

    const sstring& where_clause() const {
        return _where_clause;
    }

    friend bool operator==(const raw_view_info&, const raw_view_info&);
    friend std::ostream& operator<<(std::ostream& os, const raw_view_info& view);
};

bool operator==(const raw_view_info&, const raw_view_info&);
std::ostream& operator<<(std::ostream& os, const raw_view_info& view);

class view_info;

// Represents a column set which is compactible with Cassandra 3.x.
//
// This layout differs from the layout Scylla uses in schema/schema_builder for static compact tables.
// For such tables, Scylla expects all columns to be of regular type and no clustering columns,
// whereas in v3 those columns are static and there is a clustering column with type matching the
// cell name comparator and a regular column with type matching the default validator.
// See issues #2555 and #1474.
class v3_columns {
    bool _is_dense = false;
    bool _is_compound = false;
    std::vector<column_definition> _columns;
    std::unordered_map<bytes, const column_definition*> _columns_by_name;
public:
    v3_columns(std::vector<column_definition> columns, bool is_dense, bool is_compound);
    v3_columns() = default;
    v3_columns(v3_columns&&) = default;
    v3_columns& operator=(v3_columns&&) = default;
    v3_columns(const v3_columns&) = delete;
    static v3_columns from_v2_schema(const schema&);
public:
    const std::vector<column_definition>& all_columns() const;
    const std::unordered_map<bytes, const column_definition*>& columns_by_name() const;
    bool is_static_compact() const;
    bool is_compact() const;
    void apply_to(schema_builder&) const;
};

namespace query {
class partition_slice;
}

/**
 * Schema extension. An opaque type representing
 * entries in the "extensions" part of a table/view (see schema_tables).
 *
 * An extension has a name (the mapping key), and it can re-serialize
 * itself to bytes again, when we write back into schema tables.
 *
 * Code using a particular extension can locate it by name in the schema map,
 * and barring the "is_placeholder" says true, cast it to whatever might
 * be the expeceted implementation.
 *
 * We allow placeholder object since an extension written to schema tables
 * might be unavailable on next boot/other node. To avoid loosing the config data,
 * a placeholder object is put into schema map, which at least can
 * re-serialize the data back.
 *
 */
class schema_extension {
public:
    virtual ~schema_extension() {};
    virtual bytes serialize() const = 0;
    virtual bool is_placeholder() const {
        return false;
    }
};

// To break some cyclic data dependencies in the early stages of a node boot process,
// the system tables are loaded in two phases. This allows to use the tables
// from the first phase to load the tables from the second phase.
// For example, we need system.scylla_local table to load raft tables, since it
// stores the enabled features, and SCHEMA_COMMITLOG feature is used to choose
// what commitlog (regular or schema) will be used for raft tables.
enum class system_table_load_phase {
    phase1,
    phase2
};
constexpr system_table_load_phase all_system_table_load_phases[] = {
    system_table_load_phase::phase1,
    system_table_load_phase::phase2
};

struct schema_static_props {
    bool use_null_sharder = false; // use a sharder that puts everything on shard 0
    bool wait_for_sync_to_commitlog = false; // true if all writes using this schema have to be synced immediately by commitlog
    bool use_schema_commitlog = false;
    system_table_load_phase load_phase = system_table_load_phase::phase1;
};

/*
 * Effectively immutable.
 * Not safe to access across cores because of shared_ptr's.
 * Use global_schema_ptr for safe across-shard access.
 */
class schema final : public enable_lw_shared_from_this<schema>, public data_dictionary::keyspace_element {
    friend class v3_columns;
public:
    struct dropped_column {
        data_type type;
        api::timestamp_type timestamp;
        bool operator==(const dropped_column& rhs) const {
            return type == rhs.type && timestamp == rhs.timestamp;
        }
    };
    using extensions_map = std::map<sstring, ::shared_ptr<schema_extension>>;
private:
    // More complex fields are derived from these inside rebuild().
    // Contains only fields which can be safely default-copied.
    struct raw_schema {
        raw_schema(table_id id);
        table_id _id;
        sstring _ks_name;
        sstring _cf_name;
        // regular columns are sorted by name
        // static columns are sorted by name, but present only when there's any clustering column
        std::vector<column_definition> _columns;
        sstring _comment;
        gc_clock::duration _default_time_to_live = gc_clock::duration::zero();
        data_type _regular_column_name_type;
        data_type _default_validation_class = bytes_type;
        double _bloom_filter_fp_chance = 0.01;
        compression_parameters _compressor_params;
        extensions_map _extensions;
        bool _is_dense = false;
        bool _is_compound = true;
        bool _is_counter = false;
        cf_type _type = cf_type::standard;
        int32_t _gc_grace_seconds = DEFAULT_GC_GRACE_SECONDS;
        std::optional<int32_t> _paxos_grace_seconds;
        double _dc_local_read_repair_chance = 0.0;
        double _read_repair_chance = 0.0;
        double _crc_check_chance = 1;
        db::per_partition_rate_limit_options _per_partition_rate_limit_options;
        int32_t _min_compaction_threshold = DEFAULT_MIN_COMPACTION_THRESHOLD;
        int32_t _max_compaction_threshold = DEFAULT_MAX_COMPACTION_THRESHOLD;
        int32_t _min_index_interval = DEFAULT_MIN_INDEX_INTERVAL;
        int32_t _max_index_interval = 2048;
        int32_t _memtable_flush_period = 0;
        ::speculative_retry _speculative_retry = ::speculative_retry(speculative_retry::type::PERCENTILE, 0.99);
        // This is the compaction strategy that will be used by default on tables which don't have one explicitly specified.
        sstables::compaction_strategy_type _compaction_strategy = sstables::compaction_strategy_type::size_tiered;
        std::map<sstring, sstring> _compaction_strategy_options;
        bool _compaction_enabled = true;
        ::caching_options _caching_options;
        table_schema_version _version;
        std::unordered_map<sstring, dropped_column> _dropped_columns;
        std::map<bytes, data_type> _collections;
        std::unordered_map<sstring, index_metadata> _indices_by_name;
        std::reference_wrapper<const dht::i_partitioner> _partitioner;
        // Sharding info is not stored in the schema mutation and does not affect
        // schema digest. It is also not set locally on a schema tables.
        std::reference_wrapper<const dht::sharder> _sharder;
    };
    raw_schema _raw;
    schema_static_props _static_props;
    thrift_schema _thrift;
    v3_columns _v3_columns;
    mutable schema_registry_entry* _registry_entry = nullptr;
    std::unique_ptr<::view_info> _view_info;

    const std::array<column_count_type, 3> _offsets;

    inline column_count_type column_offset(column_kind k) const {
        return k == column_kind::partition_key ? 0 : _offsets[column_count_type(k) - 1];
    }

    std::unordered_map<bytes, const column_definition*> _columns_by_name;
    lw_shared_ptr<compound_type<allow_prefixes::no>> _partition_key_type;
    lw_shared_ptr<compound_type<allow_prefixes::yes>> _clustering_key_type;
    column_mapping _column_mapping;
    shared_ptr<query::partition_slice> _full_slice;
    column_count_type _clustering_key_size;
    column_count_type _regular_column_count;
    column_count_type _static_column_count;

    extensions_map& extensions() {
        return _raw._extensions;
    }

    friend class db::extensions;
    friend class schema_builder;
public:
    using row_column_ids_are_ordered_by_name = std::true_type;

    typedef std::vector<column_definition> columns_type;
    typedef typename columns_type::iterator iterator;
    typedef typename columns_type::const_iterator const_iterator;
    typedef boost::iterator_range<iterator> iterator_range_type;
    typedef boost::iterator_range<const_iterator> const_iterator_range_type;

    static constexpr int32_t NAME_LENGTH = 48;


    struct column {
        bytes name;
        data_type type;
    };
private:
    struct reversed_tag { };

    lw_shared_ptr<cql3::column_specification> make_column_specification(const column_definition& def) const;
    void rebuild();
    schema(const schema&, const std::function<void(schema&)>&);
    class private_tag{};
public:
    schema(private_tag, const raw_schema&, std::optional<raw_view_info>, const schema_static_props& props);
    schema(const schema&);
    // See \ref make_reversed().
    schema(reversed_tag, const schema&);
    ~schema();
    const schema_static_props& static_props() const {
        return _static_props;
    }
    table_schema_version version() const {
        return _raw._version;
    }
    double bloom_filter_fp_chance() const {
        return _raw._bloom_filter_fp_chance;
    }
    sstring thrift_key_validator() const;
    const compression_parameters& get_compressor_params() const {
        return _raw._compressor_params;
    }
    const extensions_map& extensions() const {
        return _raw._extensions;
    }
    bool is_dense() const {
        return _raw._is_dense;
    }

    bool is_compound() const {
        return _raw._is_compound;
    }

    bool is_cql3_table() const {
        return !is_super() && !is_dense() && is_compound();
    }
    bool is_compact_table() const {
        return !is_cql3_table();
    }
    bool is_static_compact_table() const {
        return !is_super() && !is_dense() && !is_compound();
    }

    thrift_schema& thrift() {
        return _thrift;
    }
    const thrift_schema& thrift() const {
        return _thrift;
    }
    const table_id& id() const {
        return _raw._id;
    }
    const sstring& comment() const {
        return _raw._comment;
    }
    bool is_counter() const {
        return _raw._is_counter;
    }

    const cf_type type() const {
        return _raw._type;
    }

    bool is_super() const {
        return _raw._type == cf_type::super;
    }

    gc_clock::duration gc_grace_seconds() const {
        auto seconds = std::chrono::seconds(_raw._gc_grace_seconds);
        return std::chrono::duration_cast<gc_clock::duration>(seconds);
    }

    gc_clock::duration paxos_grace_seconds() const;

    double dc_local_read_repair_chance() const {
        return _raw._dc_local_read_repair_chance;
    }

    double read_repair_chance() const {
        return _raw._read_repair_chance;
    }
    double crc_check_chance() const {
        return _raw._crc_check_chance;
    }

    int32_t min_compaction_threshold() const {
        return _raw._min_compaction_threshold;
    }

    int32_t max_compaction_threshold() const {
        return _raw._max_compaction_threshold;
    }

    int32_t min_index_interval() const {
        return _raw._min_index_interval;
    }

    int32_t max_index_interval() const {
        return _raw._max_index_interval;
    }

    int32_t memtable_flush_period() const {
        return _raw._memtable_flush_period;
    }

    sstables::compaction_strategy_type configured_compaction_strategy() const {
        return _raw._compaction_strategy;
    }

    sstables::compaction_strategy_type compaction_strategy() const {
        return _raw._compaction_enabled ? _raw._compaction_strategy : sstables::compaction_strategy_type::null;
    }

    const std::map<sstring, sstring>& compaction_strategy_options() const {
        return _raw._compaction_strategy_options;
    }

    bool compaction_enabled() const {
        return _raw._compaction_enabled;
    }

    const cdc::options& cdc_options() const;

    const ::tombstone_gc_options& tombstone_gc_options() const;

    const db::per_partition_rate_limit_options& per_partition_rate_limit_options() const {
        return _raw._per_partition_rate_limit_options;
    }

    const ::speculative_retry& speculative_retry() const {
        return _raw._speculative_retry;
    }

    const ::caching_options& caching_options() const {
        return _raw._caching_options;
    }

    static void set_default_partitioner(const sstring& class_name, unsigned ignore_msb = 0);
    const dht::i_partitioner& get_partitioner() const;
    const dht::sharder& get_sharder() const;
    bool has_custom_partitioner() const;

    const column_definition* get_column_definition(const bytes& name) const;
    const column_definition& column_at(column_kind, column_id) const;
    // Find a column definition given column ordinal id in the schema
    const column_definition& column_at(ordinal_column_id ordinal_id) const;
    const_iterator regular_begin() const;
    const_iterator regular_end() const;
    const_iterator regular_lower_bound(const bytes& name) const;
    const_iterator regular_upper_bound(const bytes& name) const;
    const_iterator static_begin() const;
    const_iterator static_end() const;
    const_iterator static_lower_bound(const bytes& name) const;
    const_iterator static_upper_bound(const bytes& name) const;
    static data_type column_name_type(const column_definition& def, const data_type& regular_column_name_type);
    data_type column_name_type(const column_definition& def) const;
    const column_definition& clustering_column_at(column_id id) const;
    const column_definition& regular_column_at(column_id id) const;
    const column_definition& static_column_at(column_id id) const;
    bool is_last_partition_key(const column_definition& def) const;
    bool has_multi_cell_collections() const;
    bool has_static_columns() const;
    column_count_type columns_count(column_kind kind) const;
    column_count_type partition_key_size() const;
    column_count_type clustering_key_size() const { return _clustering_key_size; }
    column_count_type static_columns_count() const { return _static_column_count; }
    column_count_type regular_columns_count() const { return _regular_column_count; }
    column_count_type all_columns_count() const { return _raw._columns.size(); }
    // Returns a range of column definitions
    const_iterator_range_type partition_key_columns() const;
    // Returns a range of column definitions
    const_iterator_range_type clustering_key_columns() const;
    // Returns a range of column definitions
    const_iterator_range_type static_columns() const;
    // Returns a range of column definitions
    const_iterator_range_type regular_columns() const;
    // Returns a range of column definitions
    const_iterator_range_type columns(column_kind) const;
    // Returns a range of column definitions

    typedef boost::range::joined_range<const_iterator_range_type, const_iterator_range_type>
        select_order_range;

    select_order_range all_columns_in_select_order() const;
    uint32_t position(const column_definition& column) const;

    const columns_type& all_columns() const {
        return _raw._columns;
    }

    const std::unordered_map<bytes, const column_definition*>& columns_by_name() const {
        return _columns_by_name;
    }

    const auto& dropped_columns() const {
        return _raw._dropped_columns;
    }

    const auto& collections() const {
        return _raw._collections;
    }

    gc_clock::duration default_time_to_live() const {
        return _raw._default_time_to_live;
    }

    data_type make_legacy_default_validator() const;

    const sstring& ks_name() const {
        return _raw._ks_name;
    }
    const sstring& cf_name() const {
        return _raw._cf_name;
    }
    const lw_shared_ptr<compound_type<allow_prefixes::no>>& partition_key_type() const {
        return _partition_key_type;
    }
    const lw_shared_ptr<compound_type<allow_prefixes::yes>>& clustering_key_type() const {
        return _clustering_key_type;
    }
    const lw_shared_ptr<compound_type<allow_prefixes::yes>>& clustering_key_prefix_type() const {
        return _clustering_key_type;
    }
    const data_type& regular_column_name_type() const {
        return _raw._regular_column_name_type;
    }
    const data_type& static_column_name_type() const {
        return utf8_type;
    }
    const std::unique_ptr<::view_info>& view_info() const {
        return _view_info;
    }
    bool is_view() const {
        return bool(_view_info);
    }
    const query::partition_slice& full_slice() const {
        return *_full_slice;
    }
    // Returns all index names of this schema.
    std::vector<sstring> index_names() const;
    // Returns all indices of this schema.
    std::vector<index_metadata> indices() const;
    const std::unordered_map<sstring, index_metadata>& all_indices() const;
    // Search for an index with a given name.
    bool has_index(const sstring& index_name) const;
    // Search for an existing index with same kind and options.
    std::optional<index_metadata> find_index_noname(const index_metadata& target) const;
    friend std::ostream& operator<<(std::ostream& os, const schema& s);
    virtual sstring keypace_name() const override { return ks_name(); }
    virtual sstring element_name() const override { return cf_name(); }
    virtual sstring element_type(replica::database& db) const override;
    /*!
     * \brief stream the CQL DESCRIBE output.
     *
     * The output of DESCRIBE is the CQL command to create the described table with its indexes and views.
     *
     * For tables with Indexes or Materialized Views, the CQL DESCRIBE is split between the base and view tables.
     * Calling the describe method on the base table schema would result with the CQL "CREATE TABLE"
     * command for creating that table only.
     *
     * Calling the describe method on a view schema would result with the appropriate "CREATE MATERIALIZED VIEW"
     * or "CREATE INDEX" depends on the type of index that schema describes (ie. Materialized View, Global
     * Index or Local Index).
     *
     * When `with_internals` is true, the description is extended with table's id and dropped columns.
     * The dropped columns are present in column definitions and also the `ALTER DROP` statement 
     * (and `ALTER ADD` if the column has been readded) to the description.
     */
    virtual std::ostream& describe(replica::database& db, std::ostream& os, bool with_internals) const override;
    friend bool operator==(const schema&, const schema&);
    const column_mapping& get_column_mapping() const;
    friend class schema_registry_entry;
    // May be called from different shard
    schema_registry_entry* registry_entry() const noexcept;
    // Returns true iff this schema version was synced with on current node.
    // Schema version is said to be synced with when its mutations were merged
    // into current node's schema, so that current node's schema is at least as
    // recent as this version.
    bool is_synced() const;
    bool equal_columns(const schema&) const;
    bool wait_for_sync_to_commitlog() const {
        return _static_props.wait_for_sync_to_commitlog;
    }
public:
    const v3_columns& v3() const {
        return _v3_columns;
    }

    // Make a copy of the schema with reversed clustering order.
    //
    // The reversing is revertible, so that:
    //
    //      s->make_reversed()->make_reversed()->version() == s->version()
    //
    // But note that: `s != s->make_reversed()->make_reversed()` (they are two
    // different C++ objects).
    // The schema's version is also reversed using UUID_gen::negate().
    schema_ptr make_reversed() const;

    // Get the reversed counterpart of this schema from the schema registry.
    //
    // If not present in the registry, create one (via \ref make_reversed()) and
    // load it. Unlike \ref make_reversed(), this method guarantees that double
    // reversing will return the very same C++ object:
    //
    //      auto schema = make_schema();
    //      auto reverse_schema = schema->get_reversed();
    //      assert(reverse_schema->get_reversed().get() == schema.get());
    //      assert(schema->get_reversed().get() == reverse_schema.get());
    //
    schema_ptr get_reversed() const;
};

lw_shared_ptr<const schema> make_shared_schema(std::optional<table_id> id, std::string_view ks_name, std::string_view cf_name,
    std::vector<schema::column> partition_key, std::vector<schema::column> clustering_key, std::vector<schema::column> regular_columns,
    std::vector<schema::column> static_columns, data_type regular_column_name_type, sstring comment = "");

bool operator==(const schema&, const schema&);

/**
 * Wrapper for schema_ptr used by functions that expect an engaged view_info field.
 */
class view_ptr final {
    schema_ptr _schema;
public:
    view_ptr() = default;
    explicit view_ptr(schema_ptr schema) noexcept : _schema(schema) {
        if (schema) {
            assert(_schema->is_view());
        }
    }

    const schema& operator*() const noexcept { return *_schema; }
    const schema* operator->() const noexcept { return _schema.operator->(); }
    const schema* get() const noexcept { return _schema.get(); }

    operator schema_ptr() const noexcept {
        return _schema;
    }

    explicit operator bool() const noexcept {
        return bool(_schema);
    }

    friend std::ostream& operator<<(std::ostream& os, const view_ptr& s);
};

std::ostream& operator<<(std::ostream& os, const view_ptr& view);

table_id generate_legacy_id(const sstring& ks_name, const sstring& cf_name);


// Thrown when attempted to access a schema-dependent object using
// an incompatible version of the schema object.
class schema_mismatch_error : public std::runtime_error {
public:
    schema_mismatch_error(table_schema_version expected, const schema& access);
};

// Throws schema_mismatch_error when a schema-dependent object of "expected" version
// cannot be accessed using "access" schema.
inline void check_schema_version(table_schema_version expected, const schema& access) {
    if (expected != access.version()) {
        throw_with_backtrace<schema_mismatch_error>(expected, access);
    }
}

#include <seastar/util/log.hh>

namespace logging {

//
// Seastar changed the names of some of these types. Maintain the old names here to avoid too much churn.
//

using log_level = seastar::log_level;
using logger = seastar::logger;
using registry = seastar::logger_registry;

inline registry& logger_registry() noexcept {
    return seastar::global_logger_registry();
}

using settings = seastar::logging_settings;

inline void apply_settings(const settings& s) {
    seastar::apply_logging_settings(s);
}

using seastar::pretty_type_name;
using seastar::level_name;

}

#include <seastar/util/bool_class.hh>
#include <absl/container/btree_set.h>
#include <seastar/core/shared_ptr.hh>
#include <seastar/core/on_internal_error.hh>

namespace seastar {
extern logger seastar_logger;
}

namespace ser {

template<typename T>
void set_size(seastar::measuring_output_stream& os, const T& obj) {
    serialize(os, uint32_t(0));
}

template<typename Stream, typename T>
void set_size(Stream& os, const T& obj) {
    serialize(os, get_sizeof(obj));
}


template<typename Output>
void safe_serialize_as_uint32(Output& out, uint64_t data) {
    if (data > std::numeric_limits<uint32_t>::max()) {
        throw std::runtime_error("Size is too big for serialization");
    }
    serialize(out, uint32_t(data));
}

template<typename T>
constexpr bool can_serialize_fast() {
    return !std::is_same<T, bool>::value && std::is_integral<T>::value && (sizeof(T) == 1 || __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__);
}

template<bool Fast, typename T>
struct serialize_array_helper;

template<typename T>
struct serialize_array_helper<true, T> {
    template<typename Container, typename Output>
    static void doit(Output& out, const Container& v) {
        out.write(reinterpret_cast<const char*>(v.data()), v.size() * sizeof(T));
    }
};

template<typename T>
struct serialize_array_helper<false, T> {
    template<typename Container, typename Output>
    static void doit(Output& out, const Container& v) {
        for (auto&& e : v) {
            serialize(out, e);
        }
    }
};

template<typename T, typename Container, typename Output>
static inline void serialize_array(Output& out, const Container& v) {
    serialize_array_helper<can_serialize_fast<T>(), T>::doit(out, v);
}

template<typename Container>
struct container_traits;

template<typename T>
struct container_traits<absl::btree_set<T>> {
    struct back_emplacer {
        absl::btree_set<T>& c;
        back_emplacer(absl::btree_set<T>& c_) : c(c_) {}
        void operator()(T&& v) {
            c.emplace(std::move(v));
        }
    };
};

template<typename T, typename... Args>
struct container_traits<std::unordered_set<T, Args...>> {
    struct back_emplacer {
        std::unordered_set<T, Args...>& c;
        back_emplacer(std::unordered_set<T, Args...>& c_) : c(c_) {}
        void operator()(T&& v) {
            c.emplace(std::move(v));
        }
    };
};

template<typename T>
struct container_traits<std::list<T>> {
    struct back_emplacer {
        std::list<T>& c;
        back_emplacer(std::list<T>& c_) : c(c_) {}
        void operator()(T&& v) {
            c.emplace_back(std::move(v));
        }
    };
    void resize(std::list<T>& c, size_t size) {
        c.resize(size);
    }
};

template<typename T>
struct container_traits<std::vector<T>> {
    struct back_emplacer {
        std::vector<T>& c;
        back_emplacer(std::vector<T>& c_) : c(c_) {}
        void operator()(T&& v) {
            c.emplace_back(std::move(v));
        }
    };
    void resize(std::vector<T>& c, size_t size) {
        c.resize(size);
    }
};

template<typename T, size_t N>
struct container_traits<utils::small_vector<T, N>> {
    struct back_emplacer {
        utils::small_vector<T, N>& c;
        back_emplacer(utils::small_vector<T, N>& c_) : c(c_) {}
        void operator()(T&& v) {
            c.emplace_back(std::move(v));
        }
    };
    void resize(utils::small_vector<T, N>& c, size_t size) {
        c.resize(size);
    }
};

template<typename T>
struct container_traits<utils::chunked_vector<T>> {
    struct back_emplacer {
        utils::chunked_vector<T>& c;
        back_emplacer(utils::chunked_vector<T>& c_) : c(c_) {}
        void operator()(T&& v) {
            c.emplace_back(std::move(v));
        }
    };
    void resize(utils::chunked_vector<T>& c, size_t size) {
        c.resize(size);
    }
};

template<typename T, size_t N>
struct container_traits<std::array<T, N>> {
    struct back_emplacer {
        std::array<T, N>& c;
        size_t idx = 0;
        back_emplacer(std::array<T, N>& c_) : c(c_) {}
        void operator()(T&& v) {
            c[idx++] = std::move(v);
        }
    };
    void resize(std::array<T, N>& c, size_t size) {}
};

template<bool Fast, typename T>
struct deserialize_array_helper;

template<typename T>
struct deserialize_array_helper<true, T> {
    template<typename Input, typename Container>
    static void doit(Input& in, Container& v, size_t sz) {
        container_traits<Container> t;
        t.resize(v, sz);
        in.read(reinterpret_cast<char*>(v.data()), v.size() * sizeof(T));
    }
    template<typename Input>
    static void skip(Input& in, size_t sz) {
        in.skip(sz * sizeof(T));
    }
};

template<typename T>
struct deserialize_array_helper<false, T> {
    template<typename Input, typename Container>
    static void doit(Input& in, Container& v, size_t sz) {
        typename container_traits<Container>::back_emplacer be(v);
        while (sz--) {
            be(deserialize(in, boost::type<T>()));
        }
    }
    template<typename Input>
    static void skip(Input& in, size_t sz) {
        while (sz--) {
            serializer<T>::skip(in);
        }
    }
};

template<typename T, typename Input, typename Container>
static inline void deserialize_array(Input& in, Container& v, size_t sz) {
    deserialize_array_helper<can_serialize_fast<T>(), T>::doit(in, v, sz);
}

template<typename T, typename Input>
static inline void skip_array(Input& in, size_t sz) {
    deserialize_array_helper<can_serialize_fast<T>(), T>::skip(in, sz);
}

namespace idl::serializers::internal {

template<typename Vector>
struct vector_serializer {
    using value_type = typename Vector::value_type;
    template<typename Input>
    static Vector read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        Vector v;
        v.reserve(sz);
        deserialize_array<value_type>(in, v, sz);
        return v;
    }
    template<typename Output>
    static void write(Output& out, const Vector& v) {
        safe_serialize_as_uint32(out, v.size());
        serialize_array<value_type>(out, v);
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        skip_array<value_type>(in, sz);
    }
};

}

template<typename T>
struct serializer<std::list<T>> {
    template<typename Input>
    static std::list<T> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        std::list<T> v;
        deserialize_array_helper<false, T>::doit(in, v, sz);
        return v;
    }
    template<typename Output>
    static void write(Output& out, const std::list<T>& v) {
        safe_serialize_as_uint32(out, v.size());
        serialize_array_helper<false, T>::doit(out, v);
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        skip_array<T>(in, sz);
    }
};

template<typename T>
struct serializer<absl::btree_set<T>> {
    template<typename Input>
    static absl::btree_set<T> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        absl::btree_set<T> v;
        deserialize_array_helper<false, T>::doit(in, v, sz);
        return v;
    }
    template<typename Output>
    static void write(Output& out, const absl::btree_set<T>& v) {
        safe_serialize_as_uint32(out, v.size());
        serialize_array_helper<false, T>::doit(out, v);
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        skip_array<T>(in, sz);
    }
};

template<typename T, typename... Args>
struct serializer<std::unordered_set<T, Args...>> {
    template<typename Input>
    static std::unordered_set<T, Args...> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        std::unordered_set<T, Args...> v;
        v.reserve(sz);
        deserialize_array_helper<false, T>::doit(in, v, sz);
        return v;
    }
    template<typename Output>
    static void write(Output& out, const std::unordered_set<T, Args...>& v) {
        safe_serialize_as_uint32(out, v.size());
        serialize_array_helper<false, T>::doit(out, v);
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        skip_array<T>(in, sz);
    }
};

template<typename T>
struct serializer<std::vector<T>>
    : idl::serializers::internal::vector_serializer<std::vector<T>>
{ };

template<typename T>
struct serializer<utils::chunked_vector<T>>
    : idl::serializers::internal::vector_serializer<utils::chunked_vector<T>>
{ };

template<typename T, size_t N>
struct serializer<utils::small_vector<T, N>>
    : idl::serializers::internal::vector_serializer<utils::small_vector<T, N>>
{ };

template<typename T, typename Ratio>
struct serializer<std::chrono::duration<T, Ratio>> {
    template<typename Input>
    static std::chrono::duration<T, Ratio> read(Input& in) {
        return std::chrono::duration<T, Ratio>(deserialize(in, boost::type<T>()));
    }
    template<typename Output>
    static void write(Output& out, const std::chrono::duration<T, Ratio>& d) {
        serialize(out, d.count());
    }
    template<typename Input>
    static void skip(Input& in) {
        read(in);
    }
};

template<typename Clock, typename Duration>
struct serializer<std::chrono::time_point<Clock, Duration>> {
    using value_type = std::chrono::time_point<Clock, Duration>;

    template<typename Input>
    static value_type read(Input& in) {
        return typename Clock::time_point(Duration(deserialize(in, boost::type<uint64_t>())));
    }
    template<typename Output>
    static void write(Output& out, const value_type& v) {
        serialize(out, uint64_t(v.time_since_epoch().count()));
    }
    template<typename Input>
    static void skip(Input& in) {
        read(in);
    }
};

template<size_t N, typename T>
struct serializer<std::array<T, N>> {
    template<typename Input>
    static std::array<T, N> read(Input& in) {
        std::array<T, N> v;
        deserialize_array<T>(in, v, N);
        return v;
    }
    template<typename Output>
    static void write(Output& out, const std::array<T, N>& v) {
        serialize_array<T>(out, v);
    }
    template<typename Input>
    static void skip(Input& in) {
        skip_array<T>(in, N);
    }
};

template<typename K, typename V>
struct serializer<std::map<K, V>> {
    template<typename Input>
    static std::map<K, V> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        std::map<K, V> m;
        while (sz--) {
            K k = deserialize(in, boost::type<K>());
            V v = deserialize(in, boost::type<V>());
            m[k] = v;
        }
        return m;
    }
    template<typename Output>
    static void write(Output& out, const std::map<K, V>& v) {
        safe_serialize_as_uint32(out, v.size());
        for (auto&& e : v) {
            serialize(out, e.first);
            serialize(out, e.second);
        }
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        while (sz--) {
            serializer<K>::skip(in);
            serializer<V>::skip(in);
        }
    }
};

template<typename K, typename V>
struct serializer<std::unordered_map<K, V>> {
    template<typename Input>
    static std::unordered_map<K, V> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        std::unordered_map<K, V> m;
        m.reserve(sz);
        while (sz--) {
            auto k = deserialize(in, boost::type<K>());
            auto v = deserialize(in, boost::type<V>());
            m.emplace(std::move(k), std::move(v));
        }
        return m;
    }
    template<typename Output>
    static void write(Output& out, const std::unordered_map<K, V>& v) {
        safe_serialize_as_uint32(out, v.size());
        for (auto&& e : v) {
            serialize(out, e.first);
            serialize(out, e.second);
        }
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        while (sz--) {
            serializer<K>::skip(in);
            serializer<V>::skip(in);
        }
    }
};

template<typename Tag>
struct serializer<bool_class<Tag>> {
    template<typename Input>
    static bool_class<Tag> read(Input& in) {
        return bool_class<Tag>(deserialize(in, boost::type<bool>()));
    }

    template<typename Output>
    static void write(Output& out, bool_class<Tag> v) {
        serialize(out, bool(v));
    }

    template<typename Input>
    static void skip(Input& in) {
        read(in);
    }
};

template<typename Stream>
class deserialized_bytes_proxy {
    Stream _stream;

    template<typename OtherStream>
    friend class deserialized_bytes_proxy;
public:
    explicit deserialized_bytes_proxy(Stream stream)
        : _stream(std::move(stream)) { }

    template<typename OtherStream>
    requires std::convertible_to<OtherStream, Stream>
    deserialized_bytes_proxy(deserialized_bytes_proxy<OtherStream> proxy)
        : _stream(std::move(proxy._stream)) { }

    auto view() const {
      if constexpr (std::is_same_v<Stream, simple_input_stream>) {
        return bytes_view(reinterpret_cast<const int8_t*>(_stream.begin()), _stream.size());
      } else {
        using iterator_type = typename Stream::iterator_type ;
        static_assert(FragmentRange<buffer_view<iterator_type>>);
        return seastar::with_serialized_stream(_stream, seastar::make_visitor(
            [&] (typename seastar::memory_input_stream<iterator_type >::simple stream) {
                return buffer_view<iterator_type>(bytes_view(reinterpret_cast<const int8_t*>(stream.begin()),
                                                        stream.size()));
            },
            [&] (typename seastar::memory_input_stream<iterator_type >::fragmented stream) {
                return buffer_view<iterator_type>(bytes_view(reinterpret_cast<const int8_t*>(stream.first_fragment_data()),
                                                        stream.first_fragment_size()),
                                             stream.size(), stream.fragment_iterator());
            }
        ));
      }
    }

    [[gnu::always_inline]]
    operator bytes() && {
        bytes v(bytes::initialized_later(), _stream.size());
        _stream.read(reinterpret_cast<char*>(v.begin()), _stream.size());
        return v;
    }

    [[gnu::always_inline]]
    operator managed_bytes() && {
        managed_bytes mb(managed_bytes::initialized_later(), _stream.size());
        for (bytes_mutable_view frag : fragment_range(managed_bytes_mutable_view(mb))) {
            _stream.read(reinterpret_cast<char*>(frag.data()), frag.size());
        }
        return mb;
    }

    [[gnu::always_inline]]
    operator bytes_ostream() && {
        bytes_ostream v;
        _stream.copy_to(v);
        return v;
    }
};

template<>
struct serializer<bytes> {
    template<typename Input>
    static deserialized_bytes_proxy<Input> read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        return deserialized_bytes_proxy<Input>(in.read_substream(sz));
    }
    template<typename Output>
    static void write(Output& out, bytes_view v) {
        safe_serialize_as_uint32(out, uint32_t(v.size()));
        out.write(reinterpret_cast<const char*>(v.begin()), v.size());
    }
    template<typename Output>
    static void write(Output& out, const bytes& v) {
        write(out, static_cast<bytes_view>(v));
    }
    template<typename Output>
    static void write(Output& out, const managed_bytes& mb) {
        safe_serialize_as_uint32(out, uint32_t(mb.size()));
        for (bytes_view frag : fragment_range(managed_bytes_view(mb))) {
            out.write(reinterpret_cast<const char*>(frag.data()), frag.size());
        }
    }
    template<typename Output>
    static void write(Output& out, const bytes_ostream& v) {
        safe_serialize_as_uint32(out, uint32_t(v.size()));
        for (bytes_view frag : v.fragments()) {
            out.write(reinterpret_cast<const char*>(frag.begin()), frag.size());
        }
    }
    template<typename Output, typename FragmentedBuffer>
    requires FragmentRange<FragmentedBuffer>
    static void write_fragmented(Output& out, FragmentedBuffer&& fragments) {
        safe_serialize_as_uint32(out, uint32_t(fragments.size_bytes()));
        for (bytes_view frag : fragments) {
            out.write(reinterpret_cast<const char*>(frag.begin()), frag.size());
        }
    }
    template<typename Input>
    static void skip(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        in.skip(sz);
    }
};

template<typename Output>
void serialize(Output& out, const bytes_view& v) {
    serializer<bytes>::write(out, v);
}
template<typename Output>
void serialize(Output& out, const managed_bytes& v) {
    serializer<bytes>::write(out, v);
}
template<typename Output>
void serialize(Output& out, const bytes_ostream& v) {
    serializer<bytes>::write(out, v);
}
template<typename Input>
bytes_ostream deserialize(Input& in, boost::type<bytes_ostream>) {
    return serializer<bytes>::read(in);
}
template<typename Output, typename FragmentedBuffer>
requires FragmentRange<FragmentedBuffer>
void serialize_fragmented(Output& out, FragmentedBuffer&& v) {
    serializer<bytes>::write_fragmented(out, std::forward<FragmentedBuffer>(v));
}

template<typename T>
struct serializer<std::optional<T>> {
    template<typename Input>
    static std::optional<T> read(Input& in) {
        std::optional<T> v;
        auto b = deserialize(in, boost::type<bool>());
        if (b) {
            v.emplace(deserialize(in, boost::type<T>()));
        }
        return v;
    }
    template<typename Output>
    static void write(Output& out, const std::optional<T>& v) {
        serialize(out, bool(v));
        if (v) {
            serialize(out, v.value());
        }
    }
    template<typename Input>
    static void skip(Input& in) {
        auto present = deserialize(in, boost::type<bool>());
        if (present) {
            serializer<T>::skip(in);
        }
    }
};

extern logging::logger serlog;

// Warning: assumes that pointer is never null
template<typename T>
struct serializer<seastar::lw_shared_ptr<T>> {
    template<typename Input>
    static seastar::lw_shared_ptr<T> read(Input& in) {
        return seastar::make_lw_shared<T>(deserialize(in, boost::type<T>()));
    }
    template<typename Output>
    static void write(Output& out, const seastar::lw_shared_ptr<T>& v) {
        if (!v) {
            on_internal_error(serlog, "Unexpected nullptr while serializing a pointer");
        }
        serialize(out, *v);
    }
    template<typename Input>
    static void skip(Input& in) {
        serializer<T>::skip(in);
    }
};

template<>
struct serializer<sstring> {
    template<typename Input>
    static sstring read(Input& in) {
        auto sz = deserialize(in, boost::type<uint32_t>());
        sstring v = uninitialized_string(sz);
        in.read(v.data(), sz);
        return v;
    }
    template<typename Output>
    static void write(Output& out, const sstring& v) {
        safe_serialize_as_uint32(out, uint32_t(v.size()));
        out.write(v.data(), v.size());
    }
    template<typename Input>
    static void skip(Input& in) {
        in.skip(deserialize(in, boost::type<size_type>()));
    }
};

template<typename T>
struct serializer<std::unique_ptr<T>> {
    template<typename Input>
    static std::unique_ptr<T> read(Input& in) {
        std::unique_ptr<T> v;
        auto b = deserialize(in, boost::type<bool>());
        if (b) {
            v = std::make_unique<T>(deserialize(in, boost::type<T>()));
        }
        return v;
    }
    template<typename Output>
    static void write(Output& out, const std::unique_ptr<T>& v) {
        serialize(out, bool(v));
        if (v) {
            serialize(out, *v);
        }
    }
    template<typename Input>
    static void skip(Input& in) {
        auto present = deserialize(in, boost::type<bool>());
        if (present) {
            serializer<T>::skip(in);
        }
    }
};

template<typename Enum>
struct serializer<enum_set<Enum>> {
    template<typename Input>
    static enum_set<Enum> read(Input& in) {
        return enum_set<Enum>::from_mask(deserialize(in, boost::type<uint64_t>()));
    }
    template<typename Output>
    static void write(Output& out, enum_set<Enum> v) {
        serialize(out, uint64_t(v.mask()));
    }
    template<typename Input>
    static void skip(Input& in) {
        read(in);
    }
};

template<>
struct serializer<std::monostate> {
    template<typename Input>
    static std::monostate read(Input& in) {
        return std::monostate{};
    }
    template<typename Output>
    static void write(Output& out, std::monostate v) {}
    template<typename Input>
    static void skip(Input& in) {
    }
};

template<typename T>
size_type get_sizeof(const T& obj) {
    seastar::measuring_output_stream ms;
    serialize(ms, obj);
    auto size = ms.size();
    if (size > std::numeric_limits<size_type>::max()) {
        throw std::runtime_error("Object is too big for get_sizeof");
    }
    return size;
}

template<typename Buffer, typename T>
Buffer serialize_to_buffer(const T& v, size_t head_space) {
    seastar::measuring_output_stream measure;
    ser::serialize(measure, v);
    Buffer ret(typename Buffer::initialized_later(), measure.size() + head_space);
    seastar::simple_output_stream out(reinterpret_cast<char*>(ret.begin()), ret.size(), head_space);
    ser::serialize(out, v);
    return ret;
}

template<typename T, typename Buffer>
T deserialize_from_buffer(const Buffer& buf, boost::type<T> type, size_t head_space) {
    seastar::simple_input_stream in(reinterpret_cast<const char*>(buf.begin() + head_space), buf.size() - head_space);
    return deserialize(in, std::move(type));
}

inline
utils::input_stream as_input_stream(bytes_view b) {
    return utils::input_stream::simple(reinterpret_cast<const char*>(b.begin()), b.size());
}

inline
utils::input_stream as_input_stream(const bytes_ostream& b) {
    if (b.is_linearized()) {
        return as_input_stream(b.view());
    }
    return utils::input_stream::fragmented(b.fragments().begin(), b.size());
}

template<typename Output, typename ...T>
void serialize(Output& out, const boost::variant<T...>& v) {}

template<typename Input, typename ...T>
boost::variant<T...> deserialize(Input& in, boost::type<boost::variant<T...>>) {
    return boost::variant<T...>();
}

template<typename Output, typename ...T>
void serialize(Output& out, const std::variant<T...>& v) {
    static_assert(std::variant_size_v<std::variant<T...>> < 256);
    size_t type_index = v.index();
    serialize(out, uint8_t(type_index));
    std::visit([&out] (const auto& member) {
        serialize(out, member);
    }, v);
}

template<typename Input, typename T, size_t... I>
T deserialize_std_variant(Input& in, boost::type<T> t,  size_t idx, std::index_sequence<I...>) {
    T v;
    (void)((I == idx ? v.template emplace<I>(deserialize(in, boost::type<std::variant_alternative_t<I, T>>())), true : false) || ...);
    return v;
}

template<typename Input, typename ...T>
std::variant<T...> deserialize(Input& in, boost::type<std::variant<T...>> v) {
    size_t idx = deserialize(in, boost::type<uint8_t>());
    return deserialize_std_variant(in, v, idx, std::make_index_sequence<sizeof...(T)>());
}

template<typename Output>
void serialize(Output& out, const unknown_variant_type& v) {
    out.write(v.data.begin(), v.data.size());
}
template<typename Input>
unknown_variant_type deserialize(Input& in, boost::type<unknown_variant_type>) {
    return seastar::with_serialized_stream(in, [] (auto& in) {
        auto size = deserialize(in, boost::type<size_type>());
        auto index = deserialize(in, boost::type<size_type>());
        auto sz = size - sizeof(size_type) * 2;
        sstring v = uninitialized_string(sz);
        in.read(v.data(), sz);
        return unknown_variant_type{ index, std::move(v) };
    });
}

// Class for iteratively deserializing a frozen vector
// using a range.
// Use begin() and end() to iterate through the frozen vector,
// deserializing (or skipping) one element at a time.
template <typename T, bool IsForward=true>
class vector_deserializer {
public:
    using value_type = T;
    using input_stream = utils::input_stream;

private:
    input_stream _in;
    size_t _size;
    utils::chunked_vector<input_stream> _substreams;

    void fill_substreams() requires (!IsForward) {
        input_stream in = _in;
        input_stream in2 = _in;
        for (size_t i = 0; i < size(); ++i) {
            size_t old_size = in.size();
            serializer<T>::skip(in);
            size_t new_size = in.size();

            _substreams.push_back(in2.read_substream(old_size - new_size));
        }
    }

    struct forward_iterator_data {
        input_stream _in = simple_input_stream();
        void skip() {
            serializer<T>::skip(_in);
        }
        value_type deserialize_next() {
            return deserialize(_in, boost::type<T>());
        }
    };
    struct reverse_iterator_data {
        std::reverse_iterator<utils::chunked_vector<input_stream>::const_iterator> _substream_it;
        void skip() {
            ++_substream_it;
        }
        value_type deserialize_next() {
            input_stream is = *_substream_it;
            ++_substream_it;
            return deserialize(is, boost::type<T>());
        }
    };


public:
    vector_deserializer() noexcept
        : _in(simple_input_stream())
        , _size(0)
    { }

    explicit vector_deserializer(input_stream in)
        : _in(std::move(in))
        , _size(deserialize(_in, boost::type<uint32_t>()))
    {
        if constexpr (!IsForward) {
            fill_substreams();
        }
    }

    // Get the number of items in the vector
    size_t size() const noexcept {
        return _size;
    }

    bool empty() const noexcept {
        return _size == 0;
    }

    // Input iterator
    class iterator {
        // _idx is the distance from .begin(). It is used only for comparing iterators.
        size_t _idx = 0;
        bool _consumed = false;
        std::conditional_t<IsForward, forward_iterator_data, reverse_iterator_data> _data;

        iterator(input_stream in, size_t idx) noexcept requires(IsForward)
            : _idx(idx)
            , _data{in}
        { }
        iterator(decltype(reverse_iterator_data::_substream_it) substreams, size_t idx) noexcept requires(!IsForward)
            : _idx(idx)
            , _data{substreams}
        { }

        friend class vector_deserializer;
   public:
        using iterator_category = std::input_iterator_tag;
        using value_type = T;
        using pointer = value_type*;
        using reference = value_type&;
        using difference_type = ssize_t;

        iterator() noexcept = default;

        bool operator==(const iterator& it) const noexcept {
            return _idx == it._idx;
        }

        // Deserializes and returns the item, effectively incrementing the iterator..
        value_type operator*() const {
            auto zis = const_cast<iterator*>(this);
            zis->_idx++;
            zis->_consumed = true;
            return zis->_data.deserialize_next();
        }

        iterator& operator++() {
            if (!_consumed) {
                _data.skip();
                ++_idx;
            } else {
                _consumed = false;
            }
            return *this;
        }
        iterator operator++(int) {
            auto pre = *this;
            ++*this;
            return pre;
        }

        ssize_t operator-(const iterator& it) const noexcept {
            return _idx - it._idx;
        }
    };

    using const_iterator = iterator;

    static_assert(std::input_iterator<iterator>);
    static_assert(std::sentinel_for<iterator, iterator>);

    iterator begin() noexcept requires(IsForward) {
        return {_in, 0};
    }
    const_iterator begin() const noexcept requires(IsForward) {
        return {_in, 0};
    }
    const_iterator cbegin() const noexcept requires(IsForward) {
        return {_in, 0};
    }

    iterator end() noexcept requires(IsForward) {
        return {_in, _size};
    }
    const_iterator end() const noexcept requires(IsForward) {
        return {_in, _size};
    }
    const_iterator cend() const noexcept requires(IsForward) {
        return {_in, _size};
    }

    iterator begin() noexcept requires(!IsForward) {
        return {_substreams.crbegin(), 0};
    }
    const_iterator begin() const noexcept requires(!IsForward) {
        return {_substreams.crbegin(), 0};
    }
    const_iterator cbegin() const noexcept requires(!IsForward) {
        return {_substreams.crbegin(), 0};
    }

    iterator end() noexcept requires(!IsForward) {
        return {_substreams.crend(), _size};
    }
    const_iterator end() const noexcept requires(!IsForward) {
        return {_substreams.crend(), _size};
    }
    const_iterator cend() const noexcept requires(!IsForward) {
        return {_substreams.crend(), _size};
    }

};

static_assert(std::ranges::range<vector_deserializer<int>>);

}



namespace cdc {

class cdc_extension : public schema_extension {
    cdc::options _cdc_options;
public:
    static constexpr auto NAME = "cdc";

    cdc_extension() = default;
    cdc_extension(const options& opts) : _cdc_options(opts) {}
    explicit cdc_extension(std::map<sstring, sstring> tags) : _cdc_options(std::move(tags)) {}
    explicit cdc_extension(const bytes& b) : _cdc_options(cdc_extension::deserialize(b)) {}
    explicit cdc_extension(const sstring& s) {
        throw std::logic_error("Cannot create cdc info from string");
    }
    bytes serialize() const override {
        return ser::serialize_to_buffer<bytes>(_cdc_options.to_map());
    }
    static std::map<sstring, sstring> deserialize(const bytes_view& buffer) {
        return ser::deserialize_from_buffer(buffer, boost::type<std::map<sstring, sstring>>());
    }
    const options& get_options() const {
        return _cdc_options;
    }
};

}


#include <seastar/core/sstring.hh>
#include <iosfwd>
#include <functional>

namespace db {

sstring system_keyspace_name();

namespace functions {

class function_name final {
public:
    sstring keyspace;
    sstring name;

    static function_name native_function(sstring name) {
        return function_name(db::system_keyspace_name(), name);
    }

    function_name() = default; // for ANTLR
    function_name(sstring keyspace, sstring name)
            : keyspace(std::move(keyspace)), name(std::move(name)) {
    }

    function_name as_native_function() const {
        return native_function(name);
    }

    bool has_keyspace() const {
        return !keyspace.empty();
    }

    bool operator==(const function_name& x) const {
        return keyspace == x.keyspace && name == x.name;
    }
};

}
}

template <>
struct fmt::formatter<db::functions::function_name> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const db::functions::function_name& fn, FormatContext& ctx) const {
        auto out = ctx.out();
        if (fn.has_keyspace()) {
            out = fmt::format_to(out, "{}.", fn.keyspace);
        }
        return fmt::format_to(out, "{}", fn.name);
    }
};

namespace std {

template <>
struct hash<db::functions::function_name> {
    size_t operator()(const db::functions::function_name& x) const {
        return std::hash<sstring>()(x.keyspace) ^ std::hash<sstring>()(x.name);
    }
};

}


#include <vector>
#include <optional>

namespace db {
namespace functions {

class function_name;

class function {
public:
    using opt_bytes = std::optional<bytes>;
    virtual ~function() {}
    virtual const function_name& name() const = 0;
    virtual const std::vector<data_type>& arg_types() const = 0;
    virtual const data_type& return_type() const = 0;

    /**
     * Checks whether the function is a pure function (as in doesn't depend on, nor produce side effects) or not.
     *
     * @return <code>true</code> if the function is a pure function, <code>false</code> otherwise.
     */
    virtual bool is_pure() const = 0;

    /**
     * Checks whether the function is a native/hard coded one or not.
     *
     * @return <code>true</code> if the function is a native/hard coded one, <code>false</code> otherwise.
     */
    virtual bool is_native() const = 0;

    virtual bool requires_thread() const = 0;

    /**
     * Checks whether the function is an aggregate function or not.
     *
     * @return <code>true</code> if the function is an aggregate function, <code>false</code> otherwise.
     */
    virtual bool is_aggregate() const = 0;

    virtual void print(std::ostream& os) const = 0;

    /**
     * Returns the name of the function to use within a ResultSet.
     *
     * @param column_names the names of the columns used to call the function
     * @return the name of the function to use within a ResultSet
     */
    virtual sstring column_name(const std::vector<sstring>& column_names) const = 0;

    friend class function_call;
    friend std::ostream& operator<<(std::ostream& os, const function& f);
};

inline
std::ostream&
operator<<(std::ostream& os, const function& f) {
    f.print(os);
    return os;
}

}
}

#include <span>

namespace db::functions {

class scalar_function : public virtual function {
public:
    /**
     * Applies this function to the specified parameter.
     *
     * @param parameters the input parameters
     * @return the result of applying this function to the parameter
     * @throws InvalidRequestException if this function cannot not be applied to the parameter
     */
    virtual bytes_opt execute(std::span<const bytes_opt> parameters) = 0;
};


}


#include <optional>

namespace db::functions {

struct stateless_aggregate_function final {
    function_name name;
    std::optional<sstring> column_name_override; // if unset, column name is synthesized from name and argument names

    data_type state_type;
    data_type result_type;
    std::vector<data_type> argument_types;

    bytes_opt initial_state;

    // aggregates another input
    // signature: (state_type, argument_types...) -> state_type
    shared_ptr<scalar_function> aggregation_function;

    // converts the state type to a result
    // signature: (state_type) -> result_type
    shared_ptr<scalar_function> state_to_result_function;

    // optional: reduces states computed in parallel
    // signature: (state_type, state_type) -> state_type
    shared_ptr<scalar_function> state_reduction_function;
};

}


#include <optional>

namespace db {
namespace functions {


/**
 * Performs a calculation on a set of values and return a single value.
 */
class aggregate_function : public virtual function {
protected:
    stateless_aggregate_function _agg;
private:
    shared_ptr<aggregate_function> _reducible;
private:
    static shared_ptr<aggregate_function> make_reducible_variant(stateless_aggregate_function saf);
public:
    explicit aggregate_function(stateless_aggregate_function saf, bool reducible_variant = false);

    /**
     * Creates a new <code>Aggregate</code> instance.
     *
     * @return a new <code>Aggregate</code> instance.
     */
    const stateless_aggregate_function& get_aggregate() const;

    /**
     * Checks wheather the function can be distributed and is able to reduce states.
     *
     * @return <code>true</code> if the function is reducible, <code>false</code> otherwise.
     */
    bool is_reducible() const;

    /**
     * Creates a <code>Aggregate Function</code> that can be reduced.
     * Such <code>Aggregate Function</code> returns <code>Aggregate</code>,
     * which returns not finalized output, but its accumulator that can be 
     * later reduced with other one.
     *
     * Reducible aggregate function can be obtained only if <code>is_reducible()</code>
     * return true. Otherwise, it should return <code>nullptr</code>.
     *
     * @return a reducible <code>Aggregate Function</code>.
     */
    ::shared_ptr<aggregate_function> reducible_aggregate_function();

    virtual const function_name& name() const override;
    virtual const std::vector<data_type>& arg_types() const override;
    virtual const data_type& return_type() const override;
    virtual bool is_pure() const override;
    virtual bool is_native() const override;
    virtual bool requires_thread() const override;
    virtual bool is_aggregate() const override;
    virtual void print(std::ostream& os) const override;
    virtual sstring column_name(const std::vector<sstring>& column_names) const override;
};

}
}


#include <stdexcept>
#include <type_traits>
#include <seastar/core/sstring.hh>
#include <seastar/core/enum.hh>

namespace sstables {

enum class sstable_version_types { ka, la, mc, md, me };
enum class sstable_format_types { big };

constexpr std::array<sstable_version_types, 5> all_sstable_versions = {
    sstable_version_types::ka,
    sstable_version_types::la,
    sstable_version_types::mc,
    sstable_version_types::md,
    sstable_version_types::me,
};

constexpr std::array<sstable_version_types, 3> writable_sstable_versions = {
    sstable_version_types::mc,
    sstable_version_types::md,
    sstable_version_types::me,
};

constexpr sstable_version_types oldest_writable_sstable_format = sstable_version_types::mc;

template <size_t S1, size_t S2>
constexpr bool check_sstable_versions(const std::array<sstable_version_types, S1>& all_sstable_versions,
        const std::array<sstable_version_types, S2>& writable_sstable_versions, sstable_version_types oldest_writable_sstable_format) {
    for (auto v : writable_sstable_versions) {
        if (v < oldest_writable_sstable_format) {
            return false;
        }
    }
    size_t expected = 0;
    for (auto v : all_sstable_versions) {
        if (v >= oldest_writable_sstable_format) {
            ++expected;
        }
    }
    return expected == S2;
}

static_assert(check_sstable_versions(all_sstable_versions, writable_sstable_versions, oldest_writable_sstable_format));

inline auto get_highest_sstable_version() {
    return all_sstable_versions[all_sstable_versions.size() - 1];
}

sstable_version_types version_from_string(std::string_view s);
sstable_format_types format_from_string(std::string_view s);

extern const std::unordered_map<sstable_version_types, seastar::sstring, seastar::enum_hash<sstable_version_types>> version_string;
extern const std::unordered_map<sstable_format_types, seastar::sstring, seastar::enum_hash<sstable_format_types>> format_string;


inline int operator<=>(sstable_version_types a, sstable_version_types b) {
    auto to_int = [] (sstable_version_types x) {
        return static_cast<std::underlying_type_t<sstable_version_types>>(x);
    };
    return to_int(a) - to_int(b);
}

}

template <>
struct fmt::formatter<sstables::sstable_version_types> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const sstables::sstable_version_types& version, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", sstables::version_string.at(version));
    }
};

template <>
struct fmt::formatter<sstables::sstable_format_types> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const sstables::sstable_format_types& format, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", sstables::format_string.at(format));
    }
};



#include <boost/range/algorithm/copy.hpp>
#include <boost/range/adaptor/transformed.hpp>
#include <compare>

//FIXME: de-inline methods and define this as static in a .cc file.
extern logging::logger compound_logger;

//
// This header provides adaptors between the representation used by our compound_type<>
// and representation used by Origin.
//
// For single-component keys the legacy representation is equivalent
// to the only component's serialized form. For composite keys it the following
// (See org.apache.cassandra.db.marshal.CompositeType):
//
//   <representation> ::= ( <component> )+
//   <component>      ::= <length> <value> <EOC>
//   <length>         ::= <uint16_t>
//   <EOC>            ::= <uint8_t>
//
//  <value> is component's value in serialized form. <EOC> is always 0 for partition key.
//

// Given a representation serialized using @CompoundType, provides a view on the
// representation of the same components as they would be serialized by Origin.
//
// The view is exposed in a form of a byte range. For example of use see to_legacy() function.
template <typename CompoundType>
class legacy_compound_view {
    static_assert(!CompoundType::is_prefixable, "Legacy view not defined for prefixes");
    CompoundType& _type;
    managed_bytes_view _packed;
public:
    legacy_compound_view(CompoundType& c, managed_bytes_view packed)
        : _type(c)
        , _packed(packed)
    { }

    class iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = bytes::value_type;
        using difference_type = std::ptrdiff_t;
        using pointer = bytes::value_type*;
        using reference = bytes::value_type&;
    private:
        bool _singular;
        // Offset within virtual output space of a component.
        //
        // Offset: -2             -1             0  ...  LEN-1 LEN
        // Field:  [ length MSB ] [ length LSB ] [   VALUE   ] [ EOC ]
        //
        int32_t _offset;
        typename CompoundType::iterator _i;
    public:
        struct end_tag {};

        iterator(const legacy_compound_view& v)
            : _singular(v._type.is_singular())
            , _offset(_singular ? 0 : -2)
            , _i(_singular && !v._type.begin(v._packed)->size() ?
                    v._type.end(v._packed) : v._type.begin(v._packed))
        { }

        iterator(const legacy_compound_view& v, end_tag)
            : _offset(v._type.is_singular() && !v._type.begin(v._packed)->size() ? 0 : -2)
            , _i(v._type.end(v._packed))
        { }

        // Default constructor is incorrectly needed for c++20
        // weakly_incrementable concept requires for ranges.
        // Will be fixed by https://wg21.link/P2325R3 but still
        // needed for now.
        iterator() {}

        value_type operator*() const {
            int32_t component_size = _i->size();
            if (_offset == -2) {
                return (component_size >> 8) & 0xff;
            } else if (_offset == -1) {
                return component_size & 0xff;
            } else if (_offset < component_size) {
                return (*_i)[_offset];
            } else { // _offset == component_size
                return 0; // EOC field
            }
        }

        iterator& operator++() {
            auto component_size = (int32_t) _i->size();
            if (_offset < component_size
                // When _singular, we skip the EOC byte.
                && (!_singular || _offset != (component_size - 1)))
            {
                ++_offset;
            } else {
                ++_i;
                _offset = -2;
            }
            return *this;
        }

        iterator operator++(int) {
            iterator i(*this);
            ++(*this);
            return i;
        }

        bool operator==(const iterator& other) const {
            return _offset == other._offset && other._i == _i;
        }
    };

    // A trichotomic comparator defined on @CompoundType representations which
    // orders them according to lexicographical ordering of their corresponding
    // legacy representations.
    //
    //   tri_comparator(t)(k1, k2)
    //
    // ...is equivalent to:
    //
    //   compare_unsigned(to_legacy(t, k1), to_legacy(t, k2))
    //
    // ...but more efficient.
    //
    struct tri_comparator {
        const CompoundType& _type;

        tri_comparator(const CompoundType& type)
            : _type(type)
        { }

        // @k1 and @k2 must be serialized using @type, which was passed to the constructor.
        std::strong_ordering operator()(managed_bytes_view k1, managed_bytes_view k2) const {
            if (_type.is_singular()) {
                return compare_unsigned(*_type.begin(k1), *_type.begin(k2));
            }
            return std::lexicographical_compare_three_way(
                _type.begin(k1), _type.end(k1),
                _type.begin(k2), _type.end(k2),
                [] (const managed_bytes_view& c1, const managed_bytes_view& c2) -> std::strong_ordering {
                    if (c1.size() != c2.size() || !c1.size()) {
                        return c1.size() < c2.size() ? std::strong_ordering::less : c1.size() ? std::strong_ordering::greater : std::strong_ordering::equal;
                    }
                    return compare_unsigned(c1, c2);
                });
        }
    };

    // Equivalent to std::distance(begin(), end()), but computes faster
    size_t size() const {
        if (_type.is_singular()) {
            return _type.begin(_packed)->size();
        }
        size_t s = 0;
        for (auto&& component : _type.components(_packed)) {
            s += 2 /* length field */ + component.size() + 1 /* EOC */;
        }
        return s;
    }

    iterator begin() const {
        return iterator(*this);
    }

    iterator end() const {
        return iterator(*this, typename iterator::end_tag());
    }
};

// Converts compound_type<> representation to legacy representation
// @packed is assumed to be serialized using supplied @type.
template <typename CompoundType>
static inline
bytes to_legacy(CompoundType& type, managed_bytes_view packed) {
    legacy_compound_view<CompoundType> lv(type, packed);
    bytes legacy_form(bytes::initialized_later(), lv.size());
    std::copy(lv.begin(), lv.end(), legacy_form.begin());
    return legacy_form;
}

class composite_view;

// Represents a value serialized according to Origin's CompositeType.
// If is_compound is true, then the value is one or more components encoded as:
//
//   <representation> ::= ( <component> )+
//   <component>      ::= <length> <value> <EOC>
//   <length>         ::= <uint16_t>
//   <EOC>            ::= <uint8_t>
//
// If false, then it encodes a single value, without a prefix length or a suffix EOC.
class composite final {
    bytes _bytes;
    bool _is_compound;
public:
    composite(bytes&& b, bool is_compound)
            : _bytes(std::move(b))
            , _is_compound(is_compound)
    { }

    explicit composite(bytes&& b)
            : _bytes(std::move(b))
            , _is_compound(true)
    { }

    explicit composite(const composite_view& v);

    composite()
            : _bytes()
            , _is_compound(true)
    { }

    using size_type = uint16_t;
    using eoc_type = int8_t;

    /*
     * The 'end-of-component' byte should always be 0 for actual column name.
     * However, it can set to 1 for query bounds. This allows to query for the
     * equivalent of 'give me the full range'. That is, if a slice query is:
     *   start = <3><"foo".getBytes()><0>
     *   end   = <3><"foo".getBytes()><1>
     * then we'll return *all* the columns whose first component is "foo".
     * If for a component, the 'end-of-component' is != 0, there should not be any
     * following component. The end-of-component can also be -1 to allow
     * non-inclusive query. For instance:
     *   end = <3><"foo".getBytes()><-1>
     * allows to query everything that is smaller than <3><"foo".getBytes()>, but
     * not <3><"foo".getBytes()> itself.
     */
    enum class eoc : eoc_type {
        start = -1,
        none = 0,
        end = 1
    };

    using component = std::pair<bytes, eoc>;
    using component_view = std::pair<bytes_view, eoc>;
private:
    template<typename Value>
    requires (!std::same_as<const data_value, std::decay_t<Value>>)
    static size_t size(const Value& val) {
        return val.size();
    }
    static size_t size(const data_value& val) {
        return val.serialized_size();
    }
    template<typename Value, typename CharOutputIterator>
    requires (!std::same_as<const data_value, std::decay_t<Value>>)
    static void write_value(Value&& val, CharOutputIterator& out) {
        out = std::copy(val.begin(), val.end(), out);
    }
    template<typename CharOutputIterator>
    static void write_value(managed_bytes_view val, CharOutputIterator& out) {
        for (bytes_view frag : fragment_range(val)) {
            out = std::copy(frag.begin(), frag.end(), out);
        }
    }
    template <typename CharOutputIterator>
    static void write_value(const data_value& val, CharOutputIterator& out) {
        val.serialize(out);
    }
    template<typename RangeOfSerializedComponents, typename CharOutputIterator>
    static void serialize_value(RangeOfSerializedComponents&& values, CharOutputIterator& out, bool is_compound) {
        if (!is_compound) {
            auto it = values.begin();
            write_value(std::forward<decltype(*it)>(*it), out);
            return;
        }

        for (auto&& val : values) {
            write<size_type>(out, static_cast<size_type>(size(val)));
            write_value(std::forward<decltype(val)>(val), out);
            // Range tombstones are not keys. For collections, only frozen
            // values can be keys. Therefore, for as long as it is safe to
            // assume that this code will be used to create keys, it is safe
            // to assume the trailing byte is always zero.
            write<eoc_type>(out, eoc_type(eoc::none));
        }
    }
    template <typename RangeOfSerializedComponents>
    static size_t serialized_size(RangeOfSerializedComponents&& values, bool is_compound) {
        size_t len = 0;
        auto it = values.begin();
        if (it != values.end()) {
            // CQL3 uses a specific prefix (0xFFFF) to encode "static columns"
            // (CASSANDRA-6561). This does mean the maximum size of the first component of a
            // composite is 65534, not 65535 (or we wouldn't be able to detect if the first 2
            // bytes is the static prefix or not).
            auto value_size = size(*it);
            if (value_size > static_cast<size_type>(std::numeric_limits<size_type>::max() - uint8_t(is_compound))) {
                throw std::runtime_error(format("First component size too large: {:d} > {:d}", value_size, std::numeric_limits<size_type>::max() - is_compound));
            }
            if (!is_compound) {
                return value_size;
            }
            len += sizeof(size_type) + value_size + sizeof(eoc_type);
            ++it;
        }
        for ( ; it != values.end(); ++it) {
            auto value_size = size(*it);
            if (value_size > std::numeric_limits<size_type>::max()) {
                throw std::runtime_error(format("Component size too large: {:d} > {:d}", value_size, std::numeric_limits<size_type>::max()));
            }
            len += sizeof(size_type) + value_size + sizeof(eoc_type);
        }
        return len;
    }
public:
    template <typename Describer>
    auto describe_type(sstables::sstable_version_types v, Describer f) const {
        return f(const_cast<bytes&>(_bytes));
    }

    // marker is ignored if !is_compound
    template<typename RangeOfSerializedComponents>
    static composite serialize_value(RangeOfSerializedComponents&& values, bool is_compound = true, eoc marker = eoc::none) {
        auto size = serialized_size(values, is_compound);
        bytes b(bytes::initialized_later(), size);
        auto i = b.begin();
        serialize_value(std::forward<decltype(values)>(values), i, is_compound);
        if (is_compound && !b.empty()) {
            b.back() = eoc_type(marker);
        }
        return composite(std::move(b), is_compound);
    }

    template<typename RangeOfSerializedComponents>
    static composite serialize_static(const schema& s, RangeOfSerializedComponents&& values) {
        // FIXME: Optimize
        auto b = bytes(size_t(2), bytes::value_type(0xff));
        std::vector<bytes_view> sv(s.clustering_key_size());
        b += composite::serialize_value(boost::range::join(sv, std::forward<RangeOfSerializedComponents>(values)), true).release_bytes();
        return composite(std::move(b));
    }

    static eoc to_eoc(int8_t eoc_byte) {
        return eoc_byte == 0 ? eoc::none : (eoc_byte < 0 ? eoc::start : eoc::end);
    }

    class iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = const component_view;
        using difference_type = std::ptrdiff_t;
        using pointer = const component_view*;
        using reference = const component_view&;
    private:
        bytes_view _v;
        component_view _current;
        bool _strict_mode = true;
    private:
        void do_read_current() {
            size_type len;
            {
                if (_v.empty()) {
                    _v = bytes_view(nullptr, 0);
                    return;
                }
                len = read_simple<size_type>(_v);
                if (_v.size() < len) {
                    throw_with_backtrace<marshal_exception>(format("composite iterator - not enough bytes, expected {:d}, got {:d}", len, _v.size()));
                }
            }
            auto value = bytes_view(_v.begin(), len);
            _v.remove_prefix(len);
            _current = component_view(std::move(value), to_eoc(read_simple<eoc_type>(_v)));
        }
        void read_current() {
            try {
                do_read_current();
            } catch (marshal_exception&) {
                if (_strict_mode) {
                    on_internal_error(compound_logger, std::current_exception());
                } else {
                    throw;
                }
            }
        }

        struct end_iterator_tag {};

        // In strict-mode de-serialization errors will invoke `on_internal_error()`.
        iterator(const bytes_view& v, bool is_compound, bool is_static, bool strict_mode = true)
                : _v(v), _strict_mode(strict_mode) {
            if (is_static) {
                _v.remove_prefix(2);
            }
            if (is_compound) {
                read_current();
            } else {
                _current = component_view(_v, eoc::none);
                _v.remove_prefix(_v.size());
            }
        }

        iterator(end_iterator_tag) : _v(nullptr, 0) {}

    public:
        iterator() : iterator(end_iterator_tag()) {}
        iterator& operator++() {
            read_current();
            return *this;
        }

        iterator operator++(int) {
            iterator i(*this);
            ++(*this);
            return i;
        }

        const value_type& operator*() const { return _current; }
        const value_type* operator->() const { return &_current; }
        bool operator==(const iterator& i) const { return _v.begin() == i._v.begin(); }

        friend class composite;
        friend class composite_view;
    };

    iterator begin() const {
        return iterator(_bytes, _is_compound, is_static());
    }

    iterator end() const {
        return iterator(iterator::end_iterator_tag());
    }

    boost::iterator_range<iterator> components() const & {
        return { begin(), end() };
    }

    auto values() const & {
        return components() | boost::adaptors::transformed([](auto&& c) { return c.first; });
    }

    std::vector<component> components() const && {
        std::vector<component> result;
        std::transform(begin(), end(), std::back_inserter(result), [](auto&& p) {
            return component(bytes(p.first.begin(), p.first.end()), p.second);
        });
        return result;
    }

    std::vector<bytes> values() const && {
        std::vector<bytes> result;
        boost::copy(components() | boost::adaptors::transformed([](auto&& c) { return to_bytes(c.first); }), std::back_inserter(result));
        return result;
    }

    const bytes& get_bytes() const {
        return _bytes;
    }

    bytes release_bytes() && {
        return std::move(_bytes);
    }

    size_t size() const {
        return _bytes.size();
    }

    bool empty() const {
        return _bytes.empty();
    }

    static bool is_static(bytes_view bytes, bool is_compound) {
        return is_compound && bytes.size() > 2 && (bytes[0] & bytes[1] & 0xff) == 0xff;
    }

    bool is_static() const {
        return is_static(_bytes, _is_compound);
    }

    bool is_compound() const {
        return _is_compound;
    }

    template <typename ClusteringElement>
    static composite from_clustering_element(const schema& s, const ClusteringElement& ce) {
        return serialize_value(ce.components(s), s.is_compound());
    }

    static composite from_exploded(const std::vector<bytes_view>& v, bool is_compound, eoc marker = eoc::none) {
        if (v.size() == 0) {
            return composite(bytes(size_t(1), bytes::value_type(marker)), is_compound);
        }
        return serialize_value(v, is_compound, marker);
    }

    static composite static_prefix(const schema& s) {
        return serialize_static(s, std::vector<bytes_view>());
    }

    explicit operator bytes_view() const {
        return _bytes;
    }

    template <typename Component>
    friend inline std::ostream& operator<<(std::ostream& os, const std::pair<Component, eoc>& c) {
        fmt::print(os, "{}", c);
        return os;
    }

    struct tri_compare {
        const std::vector<data_type>& _types;
        tri_compare(const std::vector<data_type>& types) : _types(types) {}
        std::strong_ordering operator()(const composite&, const composite&) const;
        std::strong_ordering operator()(composite_view, composite_view) const;
    };
};

template <typename Component>
struct fmt::formatter<std::pair<Component, composite::eoc>> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const std::pair<Component, composite::eoc>& c, FormatContext& ctx) const {
        if constexpr (std::same_as<Component, bytes_view>) {
            return fmt::format_to(ctx.out(), "{{value={}; eoc={:#02x}}}",
                                  fmt_hex(c.first), composite::eoc_type(c.second) & 0xff);
        } else {
            return fmt::format_to(ctx.out(), "{{value={}; eoc={:#02x}}}",
                                  c.first, composite::eoc_type(c.second) & 0xff);
        }
    }
};

class composite_view final {
    friend class composite;
    bytes_view _bytes;
    bool _is_compound;
public:
    composite_view(bytes_view b, bool is_compound = true)
            : _bytes(b)
            , _is_compound(is_compound)
    { }

    composite_view(const composite& c)
            : composite_view(static_cast<bytes_view>(c), c.is_compound())
    { }

    composite_view()
            : _bytes(nullptr, 0)
            , _is_compound(true)
    { }

    std::vector<bytes_view> explode() const {
        if (!_is_compound) {
            return { _bytes };
        }

        std::vector<bytes_view> ret;
        ret.reserve(8);
        for (auto it = begin(), e = end(); it != e; ) {
            ret.push_back(it->first);
            auto marker = it->second;
            ++it;
            if (it != e && marker != composite::eoc::none) {
                throw runtime_exception(format("non-zero component divider found ({:d}) mid", format("0x{:02x}", composite::eoc_type(marker) & 0xff)));
            }
        }
        return ret;
    }

    composite::iterator begin() const {
        return composite::iterator(_bytes, _is_compound, is_static());
    }

    composite::iterator end() const {
        return composite::iterator(composite::iterator::end_iterator_tag());
    }

    boost::iterator_range<composite::iterator> components() const {
        return { begin(), end() };
    }

    composite::eoc last_eoc() const {
        if (!_is_compound || _bytes.empty()) {
            return composite::eoc::none;
        }
        bytes_view v(_bytes);
        v.remove_prefix(v.size() - 1);
        return composite::to_eoc(read_simple<composite::eoc_type>(v));
    }

    auto values() const {
        return components() | boost::adaptors::transformed([](auto&& c) { return c.first; });
    }

    size_t size() const {
        return _bytes.size();
    }

    bool empty() const {
        return _bytes.empty();
    }

    bool is_static() const {
        return composite::is_static(_bytes, _is_compound);
    }

    bool is_valid() const {
        try {
            auto it = composite::iterator(_bytes, _is_compound, is_static(), false);
            const auto end = composite::iterator(composite::iterator::end_iterator_tag());
            size_t s = 0;
            for (; it != end; ++it) {
                auto& c = *it;
                s += c.first.size() + sizeof(composite::size_type) + sizeof(composite::eoc_type);
            }
            return s == _bytes.size();
        } catch (marshal_exception&) {
            return false;
        }
    }

    explicit operator bytes_view() const {
        return _bytes;
    }

    bool operator==(const composite_view& k) const { return k._bytes == _bytes && k._is_compound == _is_compound; }

    friend fmt::formatter<composite_view>;
};

template <>
struct fmt::formatter<composite_view> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const composite_view& v, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{{{}, compound={}, static={}}}",
                              fmt::join(v.components(), ", "), v._is_compound, v.is_static());
    }
};

inline
composite::composite(const composite_view& v)
    : composite(bytes(v._bytes), v._is_compound)
{ }

template <>
struct fmt::formatter<composite> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const composite& v, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", composite_view(v));
    }
};

inline
std::strong_ordering composite::tri_compare::operator()(const composite& v1, const composite& v2) const {
    return (*this)(composite_view(v1), composite_view(v2));
}

inline
std::strong_ordering composite::tri_compare::operator()(composite_view v1, composite_view v2) const {
    // See org.apache.cassandra.db.composites.AbstractCType#compare
    if (v1.empty()) {
        return v2.empty() ? std::strong_ordering::equal : std::strong_ordering::less;
    }
    if (v2.empty()) {
        return std::strong_ordering::greater;
    }
    if (v1.is_static() != v2.is_static()) {
        return v1.is_static() ? std::strong_ordering::less : std::strong_ordering::greater;
    }
    auto a_values = v1.components();
    auto b_values = v2.components();
    auto cmp = [&](const data_type& t, component_view c1, component_view c2) {
        // First by value, then by EOC
        auto r = t->compare(c1.first, c2.first);
        if (r != 0) {
            return r;
        }
        return (static_cast<int>(c1.second) - static_cast<int>(c2.second)) <=> 0;
    };
    return lexicographical_tri_compare(_types.begin(), _types.end(),
        a_values.begin(), a_values.end(),
        b_values.begin(), b_values.end(),
        cmp);
}



#include <cstdint>

namespace utils {

namespace utf8 {

namespace internal {

struct partial_validation_results {
    bool error;
    size_t unvalidated_tail;
    size_t bytes_needed_for_tail;
};

partial_validation_results validate_partial(const uint8_t* data, size_t len);

}


bool validate(const uint8_t *data, size_t len);

inline bool validate(bytes_view string) {
    const uint8_t *data = reinterpret_cast<const uint8_t*>(string.data());
    size_t len = string.size();

    return validate(data, len);
}

// If data represents a correct UTF-8 string, return std::nullopt,
// otherwise return a position of first error byte.
std::optional<size_t> validate_with_error_position(const uint8_t *data, size_t len);

inline std::optional<size_t> validate_with_error_position(bytes_view string) {
    const uint8_t *data = reinterpret_cast<const uint8_t*>(string.data());
    size_t len = string.size();

    return validate_with_error_position(data, len);	
}

inline std::optional<size_t> validate_with_error_position_fragmented(single_fragmented_view fv) {
    return validate_with_error_position(fv.current_fragment());
}

std::optional<size_t> validate_with_error_position_fragmented(FragmentedView auto fv) {
    uint8_t partial_codepoint[4];
    size_t partial_filled = 0;
    size_t partial_more_needed = 0;
    size_t bytes_validated = 0;
    for (bytes_view frag : fragment_range(fv)) {
        auto data = reinterpret_cast<const uint8_t*>(frag.data());
        auto len = frag.size();
        if (partial_more_needed) {
            // Tiny loop (often zero iterations), don't call memcpy
            while (partial_more_needed && len) {
                partial_codepoint[partial_filled++] = *data++;
                --partial_more_needed;
                --len;
            }
            if (!partial_more_needed) {
                // We accumulated a codepoint that straddled two or more fragments,
                // validate it now.
                auto pvr = internal::validate_partial(partial_codepoint, partial_filled);
                if (pvr.error) {
                    return {bytes_validated};
                }
                bytes_validated += partial_filled;
                partial_filled = partial_more_needed = 0;
            }
            if (!len) {
                continue;
            }
        }
        auto pvr = internal::validate_partial(data, len);
        if (pvr.error) {
            return bytes_validated + *validate_with_error_position(data, len);
        }
        bytes_validated += len - pvr.unvalidated_tail;
        data += len - pvr.unvalidated_tail;
        len = pvr.unvalidated_tail;
        while (len) {
            partial_codepoint[partial_filled++] = *data++;
            --len;
        }
        partial_more_needed = pvr.bytes_needed_for_tail;
    }
    if (partial_more_needed) {
        return bytes_validated;
    }
    return std::nullopt;
}

} // namespace utf8

} // namespace utils



namespace replica {

// replica/database.hh
class database;
class keyspace;
class table;
using column_family = table;
class memtable_list;

}


// mutation.hh
class mutation;
class mutation_partition;

// schema/schema.hh
class schema;
class column_definition;
class column_mapping;

// schema_mutations.hh
class schema_mutations;

// keys.hh
class exploded_clustering_prefix;
class partition_key;
class partition_key_view;
class clustering_key_prefix;
class clustering_key_prefix_view;
using clustering_key = clustering_key_prefix;
using clustering_key_view = clustering_key_prefix_view;

// memtable.hh
namespace replica {
class memtable;
}


#include <boost/iterator/zip_iterator.hpp>
#include <boost/range/adaptor/transformed.hpp>
#include <boost/range/iterator_range_core.hpp>
#include <compare>
#include <span>

//
// This header defines type system for primary key holders.
//
// We distinguish partition keys and clustering keys. API-wise they are almost
// the same, but they're separate type hierarchies.
//
// Clustering keys are further divided into prefixed and non-prefixed (full).
// Non-prefixed keys always have full component set, as defined by schema.
// Prefixed ones can have any number of trailing components missing. They may
// differ in underlying representation.
//
// The main classes are:
//
//   partition_key           - full partition key
//   clustering_key          - full clustering key
//   clustering_key_prefix   - clustering key prefix
//
// These classes wrap only the minimum information required to store the key
// (the key value itself). Any information which can be inferred from schema
// is not stored. Therefore accessors need to be provided with a pointer to
// schema, from which information about structure is extracted.

// Abstracts a view to serialized compound.
template <typename TopLevelView>
class compound_view_wrapper {
protected:
    managed_bytes_view _bytes;
protected:
    compound_view_wrapper(managed_bytes_view v)
        : _bytes(v)
    { }

    static inline const auto& get_compound_type(const schema& s) {
        return TopLevelView::get_compound_type(s);
    }
public:
    std::vector<bytes> explode(const schema& s) const {
        return get_compound_type(s)->deserialize_value(_bytes);
    }

    managed_bytes_view representation() const {
        return _bytes;
    }

    struct less_compare {
        typename TopLevelView::compound _t;
        less_compare(const schema& s) : _t(get_compound_type(s)) {}
        bool operator()(const TopLevelView& k1, const TopLevelView& k2) const {
            return _t->less(k1.representation(), k2.representation());
        }
    };

    struct tri_compare {
        typename TopLevelView::compound _t;
        tri_compare(const schema &s) : _t(get_compound_type(s)) {}
        std::strong_ordering operator()(const TopLevelView& k1, const TopLevelView& k2) const {
            return _t->compare(k1.representation(), k2.representation());
        }
    };

    struct hashing {
        typename TopLevelView::compound _t;
        hashing(const schema& s) : _t(get_compound_type(s)) {}
        size_t operator()(const TopLevelView& o) const {
            return _t->hash(o.representation());
        }
    };

    struct equality {
        typename TopLevelView::compound _t;
        equality(const schema& s) : _t(get_compound_type(s)) {}
        bool operator()(const TopLevelView& o1, const TopLevelView& o2) const {
            return _t->equal(o1.representation(), o2.representation());
        }
    };

    bool equal(const schema& s, const TopLevelView& other) const {
        return get_compound_type(s)->equal(representation(), other.representation());
    }

    // begin() and end() return iterators over components of this compound. The iterator yields a managed_bytes_view to the component.
    // The iterators satisfy InputIterator concept.
    auto begin() const {
        return TopLevelView::compound::element_type::begin(representation());
    }

    // See begin()
    auto end() const {
        return TopLevelView::compound::element_type::end(representation());
    }

    // begin() and end() return iterators over components of this compound. The iterator yields a managed_bytes_view to the component.
    // The iterators satisfy InputIterator concept.
    auto begin(const schema& s) const {
        return begin();
    }

    // See begin()
    auto end(const schema& s) const {
        return end();
    }

    // Returns a range of managed_bytes_view
    auto components() const {
        return TopLevelView::compound::element_type::components(representation());
    }

    // Returns a range of managed_bytes_view
    auto components(const schema& s) const {
        return components();
    }

    bool is_empty() const {
        return _bytes.empty();
    }

    explicit operator bool() const {
        return !is_empty();
    }

    // For backward compatibility with existing code.
    bool is_empty(const schema& s) const {
        return is_empty();
    }
};

template <typename TopLevel, typename TopLevelView>
class compound_wrapper {
protected:
    managed_bytes _bytes;
protected:
    compound_wrapper(managed_bytes&& b) : _bytes(std::move(b)) {}

    static inline const auto& get_compound_type(const schema& s) {
        return TopLevel::get_compound_type(s);
    }
private:
    static const data_type& get_singular_type(const schema& s) {
        const auto& ct = get_compound_type(s);
        if (!ct->is_singular()) {
            throw std::invalid_argument("compound is not singular");
        }
        return ct->types()[0];
    }
public:
    struct with_schema_wrapper {
        with_schema_wrapper(const schema& s, const TopLevel& key) : s(s), key(key) {}
        const schema& s;
        const TopLevel& key;
    };

    with_schema_wrapper with_schema(const schema& s) const {
        return with_schema_wrapper(s, *static_cast<const TopLevel*>(this));
    }

    static TopLevel make_empty() {
        return from_exploded(std::vector<bytes>());
    }

    static TopLevel make_empty(const schema&) {
        return make_empty();
    }

    template<typename RangeOfSerializedComponents>
    static TopLevel from_exploded(RangeOfSerializedComponents&& v) {
        return TopLevel::from_range(std::forward<RangeOfSerializedComponents>(v));
    }

    static TopLevel from_exploded(const schema& s, const std::vector<bytes>& v) {
        return from_exploded(v);
    }
    static TopLevel from_exploded(const schema& s, const std::vector<managed_bytes>& v) {
        return from_exploded(v);
    }
    static TopLevel from_exploded_view(const std::vector<bytes_view>& v) {
        return from_exploded(v);
    }

    // We don't allow optional values, but provide this method as an efficient adaptor
    static TopLevel from_optional_exploded(const schema& s, std::span<const bytes_opt> v) {
        return TopLevel::from_bytes(get_compound_type(s)->serialize_optionals(v));
    }
    static TopLevel from_optional_exploded(const schema& s, std::span<const managed_bytes_opt> v) {
        return TopLevel::from_bytes(get_compound_type(s)->serialize_optionals(v));
    }

    static TopLevel from_deeply_exploded(const schema& s, const std::vector<data_value>& v) {
        return TopLevel::from_bytes(get_compound_type(s)->serialize_value_deep(v));
    }

    static TopLevel from_single_value(const schema& s, const bytes& v) {
        return TopLevel::from_bytes(get_compound_type(s)->serialize_single(v));
    }

    static TopLevel from_single_value(const schema& s, const managed_bytes& v) {
        return TopLevel::from_bytes(get_compound_type(s)->serialize_single(v));
    }

    template <typename T>
    static
    TopLevel from_singular(const schema& s, const T& v) {
        const auto& type = get_singular_type(s);
        return from_single_value(s, type->decompose(v));
    }

    static TopLevel from_singular_bytes(const schema& s, const bytes& b) {
        get_singular_type(s); // validation
        return from_single_value(s, b);
    }

    TopLevelView view() const {
        return TopLevelView::from_bytes(_bytes);
    }

    operator TopLevelView() const {
        return view();
    }

    // FIXME: return views
    std::vector<bytes> explode(const schema& s) const {
        return get_compound_type(s)->deserialize_value(_bytes);
    }

    std::vector<bytes> explode() const {
        std::vector<bytes> result;
        for (managed_bytes_view c : components()) {
            result.emplace_back(to_bytes(c));
        }
        return result;
    }

    std::vector<managed_bytes> explode_fragmented() const {
        std::vector<managed_bytes> result;
        for (managed_bytes_view c : components()) {
            result.emplace_back(managed_bytes(c));
        }
        return result;
    }

    struct tri_compare {
        typename TopLevel::compound _t;
        tri_compare(const schema& s) : _t(get_compound_type(s)) {}
        std::strong_ordering operator()(const TopLevel& k1, const TopLevel& k2) const {
            return _t->compare(k1.representation(), k2.representation());
        }
        std::strong_ordering operator()(const TopLevelView& k1, const TopLevel& k2) const {
            return _t->compare(k1.representation(), k2.representation());
        }
        std::strong_ordering operator()(const TopLevel& k1, const TopLevelView& k2) const {
            return _t->compare(k1.representation(), k2.representation());
        }
    };

    struct less_compare {
        typename TopLevel::compound _t;
        less_compare(const schema& s) : _t(get_compound_type(s)) {}
        bool operator()(const TopLevel& k1, const TopLevel& k2) const {
            return _t->less(k1.representation(), k2.representation());
        }
        bool operator()(const TopLevelView& k1, const TopLevel& k2) const {
            return _t->less(k1.representation(), k2.representation());
        }
        bool operator()(const TopLevel& k1, const TopLevelView& k2) const {
            return _t->less(k1.representation(), k2.representation());
        }
    };

    struct hashing {
        typename TopLevel::compound _t;
        hashing(const schema& s) : _t(get_compound_type(s)) {}
        size_t operator()(const TopLevel& o) const {
            return _t->hash(o.representation());
        }
        size_t operator()(const TopLevelView& o) const {
            return _t->hash(o.representation());
        }
    };

    struct equality {
        typename TopLevel::compound _t;
        equality(const schema& s) : _t(get_compound_type(s)) {}
        bool operator()(const TopLevel& o1, const TopLevel& o2) const {
            return _t->equal(o1.representation(), o2.representation());
        }
        bool operator()(const TopLevelView& o1, const TopLevel& o2) const {
            return _t->equal(o1.representation(), o2.representation());
        }
        bool operator()(const TopLevel& o1, const TopLevelView& o2) const {
            return _t->equal(o1.representation(), o2.representation());
        }
    };

    bool equal(const schema& s, const TopLevel& other) const {
        return get_compound_type(s)->equal(representation(), other.representation());
    }

    bool equal(const schema& s, const TopLevelView& other) const {
        return get_compound_type(s)->equal(representation(), other.representation());
    }

    operator managed_bytes_view() const
    {
        return _bytes;
    }

    const managed_bytes& representation() const {
        return _bytes;
    }

    // begin() and end() return iterators over components of this compound. The iterator yields a managed_bytes_view to the component.
    // The iterators satisfy InputIterator concept.
    auto begin(const schema& s) const {
        return get_compound_type(s)->begin(_bytes);
    }

    // See begin()
    auto end(const schema& s) const {
        return get_compound_type(s)->end(_bytes);
    }

    bool is_empty() const {
        return _bytes.empty();
    }

    explicit operator bool() const {
        return !is_empty();
    }

    // For backward compatibility with existing code.
    bool is_empty(const schema& s) const {
        return is_empty();
    }

    // Returns a range of managed_bytes_view
    auto components() const {
        return TopLevelView::compound::element_type::components(representation());
    }

    // Returns a range of managed_bytes_view
    auto components(const schema& s) const {
        return components();
    }

    managed_bytes_view get_component(const schema& s, size_t idx) const {
        auto it = begin(s);
        std::advance(it, idx);
        return *it;
    }

    // Returns the number of components of this compound.
    size_t size(const schema& s) const {
        return std::distance(begin(s), end(s));
    }

    size_t minimal_external_memory_usage() const {
        return _bytes.minimal_external_memory_usage();
    }

    size_t external_memory_usage() const noexcept {
        return _bytes.external_memory_usage();
    }

    size_t memory_usage() const noexcept {
        return sizeof(*this) + external_memory_usage();
    }
};

template <typename TopLevel, typename PrefixTopLevel>
class prefix_view_on_full_compound {
public:
    using iterator = typename compound_type<allow_prefixes::no>::iterator;
private:
    bytes_view _b;
    unsigned _prefix_len;
    iterator _begin;
    iterator _end;
public:
    prefix_view_on_full_compound(const schema& s, bytes_view b, unsigned prefix_len)
        : _b(b)
        , _prefix_len(prefix_len)
        , _begin(TopLevel::get_compound_type(s)->begin(_b))
        , _end(_begin)
    {
        std::advance(_end, prefix_len);
    }

    iterator begin() const { return _begin; }
    iterator end() const { return _end; }

    struct less_compare_with_prefix {
        typename PrefixTopLevel::compound prefix_type;

        less_compare_with_prefix(const schema& s)
            : prefix_type(PrefixTopLevel::get_compound_type(s))
        { }

        bool operator()(const prefix_view_on_full_compound& k1, const PrefixTopLevel& k2) const {
            return lexicographical_tri_compare(
                prefix_type->types().begin(), prefix_type->types().end(),
                k1.begin(), k1.end(),
                prefix_type->begin(k2), prefix_type->end(k2),
                tri_compare) < 0;
        }

        bool operator()(const PrefixTopLevel& k1, const prefix_view_on_full_compound& k2) const {
            return lexicographical_tri_compare(
                prefix_type->types().begin(), prefix_type->types().end(),
                prefix_type->begin(k1), prefix_type->end(k1),
                k2.begin(), k2.end(),
                tri_compare) < 0;
        }
    };
};

template <typename TopLevel>
class prefix_view_on_prefix_compound {
public:
    using iterator = typename compound_type<allow_prefixes::yes>::iterator;
private:
    bytes_view _b;
    unsigned _prefix_len;
    iterator _begin;
    iterator _end;
public:
    prefix_view_on_prefix_compound(const schema& s, bytes_view b, unsigned prefix_len)
        : _b(b)
        , _prefix_len(prefix_len)
        , _begin(TopLevel::get_compound_type(s)->begin(_b))
        , _end(_begin)
    {
        std::advance(_end, prefix_len);
    }

    iterator begin() const { return _begin; }
    iterator end() const { return _end; }

    struct less_compare_with_prefix {
        typename TopLevel::compound prefix_type;

        less_compare_with_prefix(const schema& s)
            : prefix_type(TopLevel::get_compound_type(s))
        { }

        bool operator()(const prefix_view_on_prefix_compound& k1, const TopLevel& k2) const {
            return lexicographical_tri_compare(
                prefix_type->types().begin(), prefix_type->types().end(),
                k1.begin(), k1.end(),
                prefix_type->begin(k2), prefix_type->end(k2),
                tri_compare) < 0;
        }

        bool operator()(const TopLevel& k1, const prefix_view_on_prefix_compound& k2) const {
            return lexicographical_tri_compare(
                prefix_type->types().begin(), prefix_type->types().end(),
                prefix_type->begin(k1), prefix_type->end(k1),
                k2.begin(), k2.end(),
                tri_compare) < 0;
        }
    };
};

template <typename TopLevel, typename TopLevelView, typename PrefixTopLevel>
class prefixable_full_compound : public compound_wrapper<TopLevel, TopLevelView> {
    using base = compound_wrapper<TopLevel, TopLevelView>;
protected:
    prefixable_full_compound(bytes&& b) : base(std::move(b)) {}
public:
    using prefix_view_type = prefix_view_on_full_compound<TopLevel, PrefixTopLevel>;

    bool is_prefixed_by(const schema& s, const PrefixTopLevel& prefix) const {
        const auto& t = base::get_compound_type(s);
        const auto& prefix_type = PrefixTopLevel::get_compound_type(s);
        return ::is_prefixed_by(t->types().begin(),
            t->begin(*this), t->end(*this),
            prefix_type->begin(prefix), prefix_type->end(prefix),
            ::equal);
    }

    struct less_compare_with_prefix {
        typename PrefixTopLevel::compound prefix_type;
        typename TopLevel::compound full_type;

        less_compare_with_prefix(const schema& s)
            : prefix_type(PrefixTopLevel::get_compound_type(s))
            , full_type(TopLevel::get_compound_type(s))
        { }

        bool operator()(const TopLevel& k1, const PrefixTopLevel& k2) const {
            return lexicographical_tri_compare(
                prefix_type->types().begin(), prefix_type->types().end(),
                full_type->begin(k1), full_type->end(k1),
                prefix_type->begin(k2), prefix_type->end(k2),
                tri_compare) < 0;
        }

        bool operator()(const PrefixTopLevel& k1, const TopLevel& k2) const {
            return lexicographical_tri_compare(
                prefix_type->types().begin(), prefix_type->types().end(),
                prefix_type->begin(k1), prefix_type->end(k1),
                full_type->begin(k2), full_type->end(k2),
                tri_compare) < 0;
        }
    };

    // In prefix equality two sequences are equal if any of them is a prefix
    // of the other. Otherwise lexicographical ordering is applied.
    // Note: full compounds sorted according to lexicographical ordering are also
    // sorted according to prefix equality ordering.
    struct prefix_equality_less_compare {
        typename PrefixTopLevel::compound prefix_type;
        typename TopLevel::compound full_type;

        prefix_equality_less_compare(const schema& s)
            : prefix_type(PrefixTopLevel::get_compound_type(s))
            , full_type(TopLevel::get_compound_type(s))
        { }

        bool operator()(const TopLevel& k1, const PrefixTopLevel& k2) const {
            return prefix_equality_tri_compare(prefix_type->types().begin(),
                full_type->begin(k1), full_type->end(k1),
                prefix_type->begin(k2), prefix_type->end(k2),
                tri_compare) < 0;
        }

        bool operator()(const PrefixTopLevel& k1, const TopLevel& k2) const {
            return prefix_equality_tri_compare(prefix_type->types().begin(),
                prefix_type->begin(k1), prefix_type->end(k1),
                full_type->begin(k2), full_type->end(k2),
                tri_compare) < 0;
        }
    };

    prefix_view_type prefix_view(const schema& s, unsigned prefix_len) const {
        return { s, this->representation(), prefix_len };
    }
};

template <typename TopLevel, typename FullTopLevel>
class prefix_compound_view_wrapper : public compound_view_wrapper<TopLevel> {
    using base = compound_view_wrapper<TopLevel>;
protected:
    prefix_compound_view_wrapper(managed_bytes_view v)
        : compound_view_wrapper<TopLevel>(v)
    { }

public:
    bool is_full(const schema& s) const {
        return TopLevel::get_compound_type(s)->is_full(base::_bytes);
    }
};

template <typename TopLevel, typename TopLevelView, typename FullTopLevel>
class prefix_compound_wrapper : public compound_wrapper<TopLevel, TopLevelView> {
    using base = compound_wrapper<TopLevel, TopLevelView>;
protected:
    prefix_compound_wrapper(managed_bytes&& b) : base(std::move(b)) {}
public:
    using prefix_view_type = prefix_view_on_prefix_compound<TopLevel>;

    prefix_view_type prefix_view(const schema& s, unsigned prefix_len) const {
        return { s, this->representation(), prefix_len };
    }

    bool is_full(const schema& s) const {
        return TopLevel::get_compound_type(s)->is_full(base::_bytes);
    }

    // Can be called only if is_full()
    FullTopLevel to_full(const schema& s) const {
        return FullTopLevel::from_exploded(s, base::explode(s));
    }

    bool is_prefixed_by(const schema& s, const TopLevel& prefix) const {
        const auto& t = base::get_compound_type(s);
        return ::is_prefixed_by(t->types().begin(),
            t->begin(*this), t->end(*this),
            t->begin(prefix), t->end(prefix),
            equal);
    }

    // In prefix equality two sequences are equal if any of them is a prefix
    // of the other. Otherwise lexicographical ordering is applied.
    // Note: full compounds sorted according to lexicographical ordering are also
    // sorted according to prefix equality ordering.
    struct prefix_equality_less_compare {
        typename TopLevel::compound prefix_type;

        prefix_equality_less_compare(const schema& s)
            : prefix_type(TopLevel::get_compound_type(s))
        { }

        bool operator()(const TopLevel& k1, const TopLevel& k2) const {
            return prefix_equality_tri_compare(prefix_type->types().begin(),
                prefix_type->begin(k1.representation()), prefix_type->end(k1.representation()),
                prefix_type->begin(k2.representation()), prefix_type->end(k2.representation()),
                tri_compare) < 0;
        }
    };

    // See prefix_equality_less_compare.
    struct prefix_equal_tri_compare {
        typename TopLevel::compound prefix_type;

        prefix_equal_tri_compare(const schema& s)
            : prefix_type(TopLevel::get_compound_type(s))
        { }

        std::strong_ordering operator()(const TopLevel& k1, const TopLevel& k2) const {
            return prefix_equality_tri_compare(prefix_type->types().begin(),
                prefix_type->begin(k1.representation()), prefix_type->end(k1.representation()),
                prefix_type->begin(k2.representation()), prefix_type->end(k2.representation()),
                tri_compare);
        }
    };
};

class partition_key_view : public compound_view_wrapper<partition_key_view> {
public:
    using c_type = compound_type<allow_prefixes::no>;
private:
    partition_key_view(managed_bytes_view v)
        : compound_view_wrapper<partition_key_view>(v)
    { }
public:
    using compound = lw_shared_ptr<c_type>;

    static partition_key_view from_bytes(managed_bytes_view v) {
        return { v };
    }

    static const compound& get_compound_type(const schema& s) {
        return s.partition_key_type();
    }

    // Returns key's representation which is compatible with Origin.
    // The result is valid as long as the schema is live.
    const legacy_compound_view<c_type> legacy_form(const schema& s) const;

    // A trichotomic comparator for ordering compatible with Origin.
    std::strong_ordering legacy_tri_compare(const schema& s, partition_key_view o) const;

    // Checks if keys are equal in a way which is compatible with Origin.
    bool legacy_equal(const schema& s, partition_key_view o) const {
        return legacy_tri_compare(s, o) == 0;
    }

    void validate(const schema& s) const {
        return s.partition_key_type()->validate(representation());
    }

    // A trichotomic comparator which orders keys according to their ordering on the ring.
    std::strong_ordering ring_order_tri_compare(const schema& s, partition_key_view o) const;
};

template <>
struct fmt::formatter<partition_key_view> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const partition_key_view& pk, FormatContext& ctx) const {
        return with_linearized(pk.representation(), [&] (bytes_view v) {
            return fmt::format_to(ctx.out(), "pk{{{}}}", fmt_hex(v));
        });
    }
};

class partition_key : public compound_wrapper<partition_key, partition_key_view> {
    explicit partition_key(managed_bytes&& b)
        : compound_wrapper<partition_key, partition_key_view>(std::move(b))
    { }
public:
    using c_type = compound_type<allow_prefixes::no>;

    template<typename RangeOfSerializedComponents>
    static partition_key from_range(RangeOfSerializedComponents&& v) {
        return partition_key(managed_bytes(c_type::serialize_value(std::forward<RangeOfSerializedComponents>(v))));
    }

    /*!
     * \brief create a partition_key from a nodetool style string
     * takes a nodetool style string representation of a partition key and returns a partition_key.
     * With composite keys, columns are concatenate using ':'.
     * For example if a composite key is has two columns (col1, col2) to get the partition key that
     * have col1=val1 and col2=val2 use the string 'val1:val2'
     */
    static partition_key from_nodetool_style_string(const schema_ptr s, const sstring& key);

    partition_key(std::vector<bytes> v)
        : compound_wrapper(managed_bytes(c_type::serialize_value(std::move(v))))
    { }
    partition_key(std::initializer_list<bytes> v) : partition_key(std::vector(v)) {}    

    partition_key(partition_key&& v) = default;
    partition_key(const partition_key& v) = default;
    partition_key(partition_key& v) = default;
    partition_key& operator=(const partition_key&) = default;
    partition_key& operator=(partition_key&) = default;
    partition_key& operator=(partition_key&&) = default;

    partition_key(partition_key_view key)
        : partition_key(managed_bytes(key.representation()))
    { }

    using compound = lw_shared_ptr<c_type>;

    static partition_key from_bytes(managed_bytes_view b) {
        return partition_key(managed_bytes(b));
    }
    static partition_key from_bytes(managed_bytes&& b) {
        return partition_key(std::move(b));
    }
    static partition_key from_bytes(bytes_view b) {
        return partition_key(managed_bytes(b));
    }

    static const compound& get_compound_type(const schema& s) {
        return s.partition_key_type();
    }

    // Returns key's representation which is compatible with Origin.
    // The result is valid as long as the schema is live.
    const legacy_compound_view<c_type> legacy_form(const schema& s) const {
        return view().legacy_form(s);
    }

    // A trichotomic comparator for ordering compatible with Origin.
    std::strong_ordering legacy_tri_compare(const schema& s, const partition_key& o) const {
        return view().legacy_tri_compare(s, o);
    }

    // Checks if keys are equal in a way which is compatible with Origin.
    bool legacy_equal(const schema& s, const partition_key& o) const {
        return view().legacy_equal(s, o);
    }

    void validate(const schema& s) const {
        return s.partition_key_type()->validate(representation());
    }
};

template <>
struct fmt::formatter<partition_key> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const partition_key& pk, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "pk{{{}}}", managed_bytes_view(pk.representation()));
    }
};

namespace detail {

template <typename WithSchemaWrapper, typename FormatContext>
auto format_pk(const WithSchemaWrapper& pk, FormatContext& ctx) {
    const auto& [schema, key] = pk;
    const auto& types = key.get_compound_type(schema)->types();
    const auto components = key.components(schema);
    return fmt::format_to(
            ctx.out(), "{}",
            fmt::join(boost::make_iterator_range(
                          boost::make_zip_iterator(boost::make_tuple(types.begin(),
                                                                     components.begin())),
                          boost::make_zip_iterator(boost::make_tuple(types.end(),
                                                                     components.end()))) |
                      boost::adaptors::transformed([](const auto& type_and_component) {
                          auto& [type, component] = type_and_component;
                          auto key = type->to_string(to_bytes(component));
                          if (!utils::utf8::validate((const uint8_t *) key.data(), key.size())) {
                              return sstring("<non-utf8-key>");
                          }
                          return key;
                      }),
                      ":"));

    }
} // namespace detail

template <>
struct fmt::formatter<partition_key::with_schema_wrapper> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const partition_key::with_schema_wrapper& pk, FormatContext& ctx) const {
        return ::detail::format_pk(pk, ctx);
    }
};

class exploded_clustering_prefix {
    std::vector<bytes> _v;
public:
    exploded_clustering_prefix(std::vector<bytes>&& v) : _v(std::move(v)) {}
    exploded_clustering_prefix() {}
    size_t size() const {
        return _v.size();
    }
    auto const& components() const {
        return _v;
    }
    explicit operator bool() const {
        return !_v.empty();
    }
    bool is_full(const schema& s) const {
        return _v.size() == s.clustering_key_size();
    }
    friend std::ostream& operator<<(std::ostream& os, const exploded_clustering_prefix& ecp);
};

class clustering_key_prefix_view : public prefix_compound_view_wrapper<clustering_key_prefix_view, clustering_key> {
    clustering_key_prefix_view(managed_bytes_view v)
        : prefix_compound_view_wrapper<clustering_key_prefix_view, clustering_key>(v)
    { }
public:
    static clustering_key_prefix_view from_bytes(const managed_bytes& v) {
        return { v };
    }
    static clustering_key_prefix_view from_bytes(managed_bytes_view v) {
        return { v };
    }
    static clustering_key_prefix_view from_bytes(bytes_view v) {
        return { v };
    }

    using compound = lw_shared_ptr<compound_type<allow_prefixes::yes>>;

    static const compound& get_compound_type(const schema& s) {
        return s.clustering_key_prefix_type();
    }

    static clustering_key_prefix_view make_empty() {
        return { bytes_view() };
    }
};

class clustering_key_prefix : public prefix_compound_wrapper<clustering_key_prefix, clustering_key_prefix_view, clustering_key> {
    explicit clustering_key_prefix(managed_bytes&& b)
            : prefix_compound_wrapper<clustering_key_prefix, clustering_key_prefix_view, clustering_key>(std::move(b))
    { }
public:
    template<typename RangeOfSerializedComponents>
    static clustering_key_prefix from_range(RangeOfSerializedComponents&& v) {
        return clustering_key_prefix(compound::element_type::serialize_value(std::forward<RangeOfSerializedComponents>(v)));
    }

    clustering_key_prefix(std::vector<bytes> v)
        : prefix_compound_wrapper(compound::element_type::serialize_value(std::move(v)))
    { }
    clustering_key_prefix(std::vector<managed_bytes> v)
        : prefix_compound_wrapper(compound::element_type::serialize_value(std::move(v)))
    { }
    clustering_key_prefix(std::initializer_list<bytes> v) : clustering_key_prefix(std::vector(v)) {}

    clustering_key_prefix(clustering_key_prefix&& v) = default;
    clustering_key_prefix(const clustering_key_prefix& v) = default;
    clustering_key_prefix(clustering_key_prefix& v) = default;
    clustering_key_prefix& operator=(const clustering_key_prefix&) = default;
    clustering_key_prefix& operator=(clustering_key_prefix&) = default;
    clustering_key_prefix& operator=(clustering_key_prefix&&) = default;

    clustering_key_prefix(clustering_key_prefix_view v)
        : clustering_key_prefix(managed_bytes(v.representation()))
    { }

    using compound = lw_shared_ptr<compound_type<allow_prefixes::yes>>;

    static clustering_key_prefix from_bytes(const managed_bytes& b) { return clustering_key_prefix(managed_bytes(b)); }
    static clustering_key_prefix from_bytes(managed_bytes&& b) { return clustering_key_prefix(std::move(b)); }
    static clustering_key_prefix from_bytes(managed_bytes_view b) { return clustering_key_prefix(managed_bytes(b)); }
    static clustering_key_prefix from_bytes(bytes_view b) {
        return clustering_key_prefix(managed_bytes(b));
    }

    static const compound& get_compound_type(const schema& s) {
        return s.clustering_key_prefix_type();
    }

    static clustering_key_prefix from_clustering_prefix(const schema& s, const exploded_clustering_prefix& prefix) {
        return from_exploded(s, prefix.components());
    }

    /* This function makes the passed clustering key full by filling its
     * missing trailing components with empty values.
     * This is used to represesent clustering keys of rows in compact tables that may be non-full.
     * Returns whether a key wasn't full before the call.
     */
    static bool make_full(const schema& s, clustering_key_prefix& ck) {
        if (!ck.is_full(s)) {
            // TODO: avoid unnecessary copy here
            auto full_ck_size = s.clustering_key_columns().size();
            auto exploded = ck.explode(s);
            exploded.resize(full_ck_size);
            ck = clustering_key_prefix::from_exploded(std::move(exploded));
            return true;
        }
        return false;
    }
};

template <>
struct fmt::formatter<clustering_key_prefix> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const clustering_key_prefix& ckp, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "ckp{{{}}}", managed_bytes_view(ckp.representation()));
    }
};

template <>
struct fmt::formatter<clustering_key_prefix::with_schema_wrapper> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const clustering_key_prefix::with_schema_wrapper& pk, FormatContext& ctx) const {
        return ::detail::format_pk(pk, ctx);
    }
};

template<>
struct appending_hash<partition_key_view> {
    template<typename Hasher>
    void operator()(Hasher& h, const partition_key_view& pk, const schema& s) const {
        for (managed_bytes_view v : pk.components(s)) {
            ::feed_hash(h, v);
        }
    }
};

template<>
struct appending_hash<partition_key> {
    template<typename Hasher>
    void operator()(Hasher& h, const partition_key& pk, const schema& s) const {
        appending_hash<partition_key_view>()(h, pk.view(), s);
    }
};

template<>
struct appending_hash<clustering_key_prefix_view> {
    template<typename Hasher>
    void operator()(Hasher& h, const clustering_key_prefix_view& ck, const schema& s) const {
        for (managed_bytes_view v : ck.components(s)) {
            ::feed_hash(h, v);
        }
    }
};

template<>
struct appending_hash<clustering_key_prefix> {
    template<typename Hasher>
    void operator()(Hasher& h, const clustering_key_prefix& ck, const schema& s) const {
        appending_hash<clustering_key_prefix_view>()(h, ck.view(), s);
    }
};

#include <list>
#include <vector>
#include <optional>
#include <iosfwd>
#include <boost/range/algorithm/copy.hpp>
#include <boost/range/adaptor/sliced.hpp>
#include <boost/range/adaptor/transformed.hpp>
#include <compare>
#include <fmt/format.h>

template <typename Comparator, typename T>
concept IntervalComparatorFor = requires (T a, T b, Comparator& cmp) {
    { cmp(a, b) } -> std::same_as<std::strong_ordering>;
};

template <typename LessComparator, typename T>
concept IntervalLessComparatorFor = requires (T a, T b, LessComparator& cmp) {
    { cmp(a, b) } -> std::same_as<bool>;
};

inline
bool
require_ordering_and_on_equal_return(
        std::strong_ordering input,
        std::strong_ordering match_to_return_true,
        bool return_if_equal) {
    if (input == match_to_return_true) {
        return true;
    } else if (input == std::strong_ordering::equal) {
        return return_if_equal;
    } else {
        return false;
    }
}

template<typename T>
class interval_bound {
    T _value;
    bool _inclusive;
public:
    interval_bound(T value, bool inclusive = true)
              : _value(std::move(value))
              , _inclusive(inclusive)
    { }
    const T& value() const & { return _value; }
    T&& value() && { return std::move(_value); }
    bool is_inclusive() const { return _inclusive; }
    bool operator==(const interval_bound& other) const {
        return (_value == other._value) && (_inclusive == other._inclusive);
    }
    bool equal(const interval_bound& other, IntervalComparatorFor<T> auto&& cmp) const {
        return _inclusive == other._inclusive && cmp(_value, other._value) == 0;
    }
};

template<typename T>
class nonwrapping_interval;

template <typename T>
using interval = nonwrapping_interval<T>;

// An interval which can have inclusive, exclusive or open-ended bounds on each end.
// The end bound can be smaller than the start bound.
template<typename T>
class wrapping_interval {
    template <typename U>
    using optional = std::optional<U>;
public:
    using bound = interval_bound<T>;

    template <typename Transformer>
    using transformed_type = typename std::remove_cv_t<std::remove_reference_t<std::result_of_t<Transformer(T)>>>;
private:
    optional<bound> _start;
    optional<bound> _end;
    bool _singular;
public:
    wrapping_interval(optional<bound> start, optional<bound> end, bool singular = false)
        : _start(std::move(start))
        , _singular(singular) {
        if (!_singular) {
            _end = std::move(end);
        }
    }
    wrapping_interval(T value)
        : _start(bound(std::move(value), true))
        , _end()
        , _singular(true)
    { }
    wrapping_interval() : wrapping_interval({}, {}) { }
private:
    // Bound wrappers for compile-time dispatch and safety.
    struct start_bound_ref { const optional<bound>& b; };
    struct end_bound_ref { const optional<bound>& b; };

    start_bound_ref start_bound() const { return { start() }; }
    end_bound_ref end_bound() const { return { end() }; }

    static bool greater_than_or_equal(end_bound_ref end, start_bound_ref start, IntervalComparatorFor<T> auto&& cmp) {
        return !end.b || !start.b || require_ordering_and_on_equal_return(
                cmp(end.b->value(), start.b->value()),
                std::strong_ordering::greater,
                end.b->is_inclusive() && start.b->is_inclusive());
    }

    static bool less_than(end_bound_ref end, start_bound_ref start, IntervalComparatorFor<T> auto&& cmp) {
        return !greater_than_or_equal(end, start, cmp);
    }

    static bool less_than_or_equal(start_bound_ref first, start_bound_ref second, IntervalComparatorFor<T> auto&& cmp) {
        return !first.b || (second.b && require_ordering_and_on_equal_return(
                cmp(first.b->value(), second.b->value()),
                std::strong_ordering::less,
                first.b->is_inclusive() || !second.b->is_inclusive()));
    }

    static bool less_than(start_bound_ref first, start_bound_ref second, IntervalComparatorFor<T> auto&& cmp) {
        return second.b && (!first.b || require_ordering_and_on_equal_return(
                cmp(first.b->value(), second.b->value()),
                std::strong_ordering::less,
                first.b->is_inclusive() && !second.b->is_inclusive()));
    }

    static bool greater_than_or_equal(end_bound_ref first, end_bound_ref second, IntervalComparatorFor<T> auto&& cmp) {
        return !first.b || (second.b && require_ordering_and_on_equal_return(
                cmp(first.b->value(), second.b->value()),
                std::strong_ordering::greater,
                first.b->is_inclusive() || !second.b->is_inclusive()));
    }
public:
    // the point is before the interval (works only for non wrapped intervals)
    // Comparator must define a total ordering on T.
    bool before(const T& point, IntervalComparatorFor<T> auto&& cmp) const {
        assert(!is_wrap_around(cmp));
        if (!start()) {
            return false; //open start, no points before
        }
        auto r = cmp(point, start()->value());
        if (r < 0) {
            return true;
        }
        if (!start()->is_inclusive() && r == 0) {
            return true;
        }
        return false;
    }
    // the point is after the interval (works only for non wrapped intervals)
    // Comparator must define a total ordering on T.
    bool after(const T& point, IntervalComparatorFor<T> auto&& cmp) const {
        assert(!is_wrap_around(cmp));
        if (!end()) {
            return false; //open end, no points after
        }
        auto r = cmp(end()->value(), point);
        if (r < 0) {
            return true;
        }
        if (!end()->is_inclusive() && r == 0) {
            return true;
        }
        return false;
    }
    // check if two intervals overlap.
    // Comparator must define a total ordering on T.
    bool overlaps(const wrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        bool this_wraps = is_wrap_around(cmp);
        bool other_wraps = other.is_wrap_around(cmp);

        if (this_wraps && other_wraps) {
            return true;
        } else if (this_wraps) {
            auto unwrapped = unwrap();
            return other.overlaps(unwrapped.first, cmp) || other.overlaps(unwrapped.second, cmp);
        } else if (other_wraps) {
            auto unwrapped = other.unwrap();
            return overlaps(unwrapped.first, cmp) || overlaps(unwrapped.second, cmp);
        }

        // No interval should reach this point as wrap around.
        assert(!this_wraps);
        assert(!other_wraps);

        // if both this and other have an open start, the two intervals will overlap.
        if (!start() && !other.start()) {
            return true;
        }

        return greater_than_or_equal(end_bound(), other.start_bound(), cmp)
            && greater_than_or_equal(other.end_bound(), start_bound(), cmp);
    }
    static wrapping_interval make(bound start, bound end) {
        return wrapping_interval({std::move(start)}, {std::move(end)});
    }
    static wrapping_interval make_open_ended_both_sides() {
        return {{}, {}};
    }
    static wrapping_interval make_singular(T value) {
        return {std::move(value)};
    }
    static wrapping_interval make_starting_with(bound b) {
        return {{std::move(b)}, {}};
    }
    static wrapping_interval make_ending_with(bound b) {
        return {{}, {std::move(b)}};
    }
    bool is_singular() const {
        return _singular;
    }
    bool is_full() const {
        return !_start && !_end;
    }
    void reverse() {
        if (!_singular) {
            std::swap(_start, _end);
        }
    }
    const optional<bound>& start() const {
        return _start;
    }
    const optional<bound>& end() const {
        return _singular ? _start : _end;
    }
    // Range is a wrap around if end value is smaller than the start value
    // or they're equal and at least one bound is not inclusive.
    // Comparator must define a total ordering on T.
    bool is_wrap_around(IntervalComparatorFor<T> auto&& cmp) const {
        if (_end && _start) {
            auto r = cmp(end()->value(), start()->value());
            return r < 0
                   || (r == 0 && (!start()->is_inclusive() || !end()->is_inclusive()));
        } else {
            return false; // open ended interval or singular interval don't wrap around
        }
    }
    // Converts a wrap-around interval to two non-wrap-around intervals.
    // The returned intervals are not overlapping and ordered.
    // Call only when is_wrap_around().
    std::pair<wrapping_interval, wrapping_interval> unwrap() const {
        return {
            { {}, end() },
            { start(), {} }
        };
    }
    // the point is inside the interval
    // Comparator must define a total ordering on T.
    bool contains(const T& point, IntervalComparatorFor<T> auto&& cmp) const {
        if (is_wrap_around(cmp)) {
            auto unwrapped = unwrap();
            return unwrapped.first.contains(point, cmp)
                   || unwrapped.second.contains(point, cmp);
        } else {
            return !before(point, cmp) && !after(point, cmp);
        }
    }
    // Returns true iff all values contained by other are also contained by this.
    // Comparator must define a total ordering on T.
    bool contains(const wrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        bool this_wraps = is_wrap_around(cmp);
        bool other_wraps = other.is_wrap_around(cmp);

        if (this_wraps && other_wraps) {
            return require_ordering_and_on_equal_return(
                            cmp(start()->value(), other.start()->value()),
                            std::strong_ordering::less,
                            start()->is_inclusive() || !other.start()->is_inclusive())
                && require_ordering_and_on_equal_return(
                            cmp(end()->value(), other.end()->value()),
                            std::strong_ordering::greater,
                            end()->is_inclusive() || !other.end()->is_inclusive());
        }

        if (!this_wraps && !other_wraps) {
            return less_than_or_equal(start_bound(), other.start_bound(), cmp)
                    && greater_than_or_equal(end_bound(), other.end_bound(), cmp);
        }

        if (other_wraps) { // && !this_wraps
            return !start() && !end();
        }

        // !other_wraps && this_wraps
        return (other.start() && require_ordering_and_on_equal_return(
                                    cmp(start()->value(), other.start()->value()),
                                    std::strong_ordering::less,
                                    start()->is_inclusive() || !other.start()->is_inclusive()))
                || (other.end() && (require_ordering_and_on_equal_return(
                                        cmp(end()->value(), other.end()->value()),
                                        std::strong_ordering::greater,
                                        end()->is_inclusive() || !other.end()->is_inclusive())));
    }
    // Returns intervals which cover all values covered by this interval but not covered by the other interval.
    // Ranges are not overlapping and ordered.
    // Comparator must define a total ordering on T.
    std::vector<wrapping_interval> subtract(const wrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        std::vector<wrapping_interval> result;
        std::list<wrapping_interval> left;
        std::list<wrapping_interval> right;

        if (is_wrap_around(cmp)) {
            auto u = unwrap();
            left.emplace_back(std::move(u.first));
            left.emplace_back(std::move(u.second));
        } else {
            left.push_back(*this);
        }

        if (other.is_wrap_around(cmp)) {
            auto u = other.unwrap();
            right.emplace_back(std::move(u.first));
            right.emplace_back(std::move(u.second));
        } else {
            right.push_back(other);
        }

        // left and right contain now non-overlapping, ordered intervals

        while (!left.empty() && !right.empty()) {
            auto& r1 = left.front();
            auto& r2 = right.front();
            if (less_than(r2.end_bound(), r1.start_bound(), cmp)) {
                right.pop_front();
            } else if (less_than(r1.end_bound(), r2.start_bound(), cmp)) {
                result.emplace_back(std::move(r1));
                left.pop_front();
            } else { // Overlap
                auto tmp = std::move(r1);
                left.pop_front();
                if (!greater_than_or_equal(r2.end_bound(), tmp.end_bound(), cmp)) {
                    left.push_front({bound(r2.end()->value(), !r2.end()->is_inclusive()), tmp.end()});
                }
                if (!less_than_or_equal(r2.start_bound(), tmp.start_bound(), cmp)) {
                    left.push_front({tmp.start(), bound(r2.start()->value(), !r2.start()->is_inclusive())});
                }
            }
        }

        boost::copy(left, std::back_inserter(result));

        // TODO: Merge adjacent intervals (optimization)
        return result;
    }
    // split interval in two around a split_point. split_point has to be inside the interval
    // split_point will belong to first interval
    // Comparator must define a total ordering on T.
    std::pair<wrapping_interval<T>, wrapping_interval<T>> split(const T& split_point, IntervalComparatorFor<T> auto&& cmp) const {
        assert(contains(split_point, std::forward<decltype(cmp)>(cmp)));
        wrapping_interval left(start(), bound(split_point));
        wrapping_interval right(bound(split_point, false), end());
        return std::make_pair(std::move(left), std::move(right));
    }
    // Create a sub-interval including values greater than the split_point. Returns std::nullopt if
    // split_point is after the end (but not included in the interval, in case of wraparound intervals)
    // Comparator must define a total ordering on T.
    std::optional<wrapping_interval<T>> split_after(const T& split_point, IntervalComparatorFor<T> auto&& cmp) const {
        if (contains(split_point, std::forward<decltype(cmp)>(cmp))
                && (!end() || cmp(split_point, end()->value()) != 0)) {
            return wrapping_interval(bound(split_point, false), end());
        } else if (end() && cmp(split_point, end()->value()) >= 0) {
            // whether to return std::nullopt or the full interval is not
            // well-defined for wraparound intervals; we return nullopt
            // if split_point is after the end.
            return std::nullopt;
        } else {
            return *this;
        }
    }
    template<typename Bound, typename Transformer, typename U = transformed_type<Transformer>>
    static std::optional<typename wrapping_interval<U>::bound> transform_bound(Bound&& b, Transformer&& transformer) {
        if (b) {
            return { { transformer(std::forward<Bound>(b).value().value()), b->is_inclusive() } };
        };
        return {};
    }
    // Transforms this interval into a new interval of a different value type
    // Supplied transformer should transform value of type T (the old type) into value of type U (the new type).
    template<typename Transformer, typename U = transformed_type<Transformer>>
    wrapping_interval<U> transform(Transformer&& transformer) && {
        return wrapping_interval<U>(transform_bound(std::move(_start), transformer), transform_bound(std::move(_end), transformer), _singular);
    }
    template<typename Transformer, typename U = transformed_type<Transformer>>
    wrapping_interval<U> transform(Transformer&& transformer) const & {
        return wrapping_interval<U>(transform_bound(_start, transformer), transform_bound(_end, transformer), _singular);
    }
    bool equal(const wrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        return bool(_start) == bool(other._start)
               && bool(_end) == bool(other._end)
               && (!_start || _start->equal(*other._start, cmp))
               && (!_end || _end->equal(*other._end, cmp))
               && _singular == other._singular;
    }
    bool operator==(const wrapping_interval& other) const {
        return (_start == other._start) && (_end == other._end) && (_singular == other._singular);
    }

    template<typename U>
    friend std::ostream& operator<<(std::ostream& out, const wrapping_interval<U>& r);
private:
    friend class nonwrapping_interval<T>;
};

template<typename U>
std::ostream& operator<<(std::ostream& out, const wrapping_interval<U>& r) {
    if (r.is_singular()) {
        fmt::print(out, "{{{}}}", r.start()->value());
        return out;
    }

    if (!r.start()) {
        out << "(-inf, ";
    } else {
        if (r.start()->is_inclusive()) {
            out << "[";
        } else {
            out << "(";
        }
        fmt::print(out, "{},", r.start()->value());
    }

    if (!r.end()) {
        out << "+inf)";
    } else {
        fmt::print(out, "{}", r.end()->value());
        if (r.end()->is_inclusive()) {
            out << "]";
        } else {
            out << ")";
        }
    }

    return out;
}

// An interval which can have inclusive, exclusive or open-ended bounds on each end.
// The end bound can never be smaller than the start bound.
template<typename T>
class nonwrapping_interval {
    template <typename U>
    using optional = std::optional<U>;
public:
    using bound = interval_bound<T>;

    template <typename Transformer>
    using transformed_type = typename wrapping_interval<T>::template transformed_type<Transformer>;
private:
    wrapping_interval<T> _interval;
public:
    nonwrapping_interval(T value)
        : _interval(std::move(value))
    { }
    nonwrapping_interval() : nonwrapping_interval({}, {}) { }
    // Can only be called if start <= end. IDL ctor.
    nonwrapping_interval(optional<bound> start, optional<bound> end, bool singular = false)
        : _interval(std::move(start), std::move(end), singular)
    { }
    // Can only be called if !r.is_wrap_around().
    explicit nonwrapping_interval(wrapping_interval<T>&& r)
        : _interval(std::move(r))
    { }
    // Can only be called if !r.is_wrap_around().
    explicit nonwrapping_interval(const wrapping_interval<T>& r)
        : _interval(r)
    { }
    operator wrapping_interval<T>() const & {
        return _interval;
    }
    operator wrapping_interval<T>() && {
        return std::move(_interval);
    }

    // the point is before the interval.
    // Comparator must define a total ordering on T.
    bool before(const T& point, IntervalComparatorFor<T> auto&& cmp) const {
        return _interval.before(point, std::forward<decltype(cmp)>(cmp));
    }
    // the point is after the interval.
    // Comparator must define a total ordering on T.
    bool after(const T& point, IntervalComparatorFor<T> auto&& cmp) const {
        return _interval.after(point, std::forward<decltype(cmp)>(cmp));
    }
    // check if two intervals overlap.
    // Comparator must define a total ordering on T.
    bool overlaps(const nonwrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        // if both this and other have an open start, the two intervals will overlap.
        if (!start() && !other.start()) {
            return true;
        }

        return wrapping_interval<T>::greater_than_or_equal(_interval.end_bound(), other._interval.start_bound(), cmp)
            && wrapping_interval<T>::greater_than_or_equal(other._interval.end_bound(), _interval.start_bound(), cmp);
    }
    static nonwrapping_interval make(bound start, bound end) {
        return nonwrapping_interval({std::move(start)}, {std::move(end)});
    }
    static nonwrapping_interval make_open_ended_both_sides() {
        return {{}, {}};
    }
    static nonwrapping_interval make_singular(T value) {
        return {std::move(value)};
    }
    static nonwrapping_interval make_starting_with(bound b) {
        return {{std::move(b)}, {}};
    }
    static nonwrapping_interval make_ending_with(bound b) {
        return {{}, {std::move(b)}};
    }
    bool is_singular() const {
        return _interval.is_singular();
    }
    bool is_full() const {
        return _interval.is_full();
    }
    const optional<bound>& start() const {
        return _interval.start();
    }
    const optional<bound>& end() const {
        return _interval.end();
    }
    // the point is inside the interval
    // Comparator must define a total ordering on T.
    bool contains(const T& point, IntervalComparatorFor<T> auto&& cmp) const {
        return !before(point, cmp) && !after(point, cmp);
    }
    // Returns true iff all values contained by other are also contained by this.
    // Comparator must define a total ordering on T.
    bool contains(const nonwrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        return wrapping_interval<T>::less_than_or_equal(_interval.start_bound(), other._interval.start_bound(), cmp)
                && wrapping_interval<T>::greater_than_or_equal(_interval.end_bound(), other._interval.end_bound(), cmp);
    }
    // Returns intervals which cover all values covered by this interval but not covered by the other interval.
    // Ranges are not overlapping and ordered.
    // Comparator must define a total ordering on T.
    std::vector<nonwrapping_interval> subtract(const nonwrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        auto subtracted = _interval.subtract(other._interval, std::forward<decltype(cmp)>(cmp));
        return boost::copy_range<std::vector<nonwrapping_interval>>(subtracted | boost::adaptors::transformed([](auto&& r) {
            return nonwrapping_interval(std::move(r));
        }));
    }
    // split interval in two around a split_point. split_point has to be inside the interval
    // split_point will belong to first interval
    // Comparator must define a total ordering on T.
    std::pair<nonwrapping_interval<T>, nonwrapping_interval<T>> split(const T& split_point, IntervalComparatorFor<T> auto&& cmp) const {
        assert(contains(split_point, std::forward<decltype(cmp)>(cmp)));
        nonwrapping_interval left(start(), bound(split_point));
        nonwrapping_interval right(bound(split_point, false), end());
        return std::make_pair(std::move(left), std::move(right));
    }
    // Create a sub-interval including values greater than the split_point. If split_point is after
    // the end, returns std::nullopt.
    std::optional<nonwrapping_interval> split_after(const T& split_point, IntervalComparatorFor<T> auto&& cmp) const {
        if (end() && cmp(split_point, end()->value()) >= 0) {
            return std::nullopt;
        } else if (start() && cmp(split_point, start()->value()) < 0) {
            return *this;
        } else {
            return nonwrapping_interval(interval_bound<T>(split_point, false), end());
        }
    }
    // Creates a new sub-interval which is the intersection of this interval and an interval starting with "start".
    // If there is no overlap, returns std::nullopt.
    std::optional<nonwrapping_interval> trim_front(std::optional<bound>&& start, IntervalComparatorFor<T> auto&& cmp) const {
        return intersection(nonwrapping_interval(std::move(start), {}), cmp);
    }
    // Transforms this interval into a new interval of a different value type
    // Supplied transformer should transform value of type T (the old type) into value of type U (the new type).
    template<typename Transformer, typename U = transformed_type<Transformer>>
    nonwrapping_interval<U> transform(Transformer&& transformer) && {
        return nonwrapping_interval<U>(std::move(_interval).transform(std::forward<Transformer>(transformer)));
    }
    template<typename Transformer, typename U = transformed_type<Transformer>>
    nonwrapping_interval<U> transform(Transformer&& transformer) const & {
        return nonwrapping_interval<U>(_interval.transform(std::forward<Transformer>(transformer)));
    }
    bool equal(const nonwrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        return _interval.equal(other._interval, std::forward<decltype(cmp)>(cmp));
    }
    bool operator==(const nonwrapping_interval& other) const {
        return _interval == other._interval;
    }
    // Takes a vector of possibly overlapping intervals and returns a vector containing
    // a set of non-overlapping intervals covering the same values.
    template<IntervalComparatorFor<T> Comparator, typename IntervalVec>
    requires requires (IntervalVec vec) {
        { vec.begin() } -> std::random_access_iterator;
        { vec.end() } -> std::random_access_iterator;
        { vec.reserve(1) };
        { vec.front() } -> std::same_as<nonwrapping_interval&>;
    }
    static IntervalVec deoverlap(IntervalVec intervals, Comparator&& cmp) {
        auto size = intervals.size();
        if (size <= 1) {
            return intervals;
        }

        std::sort(intervals.begin(), intervals.end(), [&](auto&& r1, auto&& r2) {
            return wrapping_interval<T>::less_than(r1._interval.start_bound(), r2._interval.start_bound(), cmp);
        });

        IntervalVec deoverlapped_intervals;
        deoverlapped_intervals.reserve(size);

        auto&& current = intervals[0];
        for (auto&& r : intervals | boost::adaptors::sliced(1, intervals.size())) {
            bool includes_end = wrapping_interval<T>::greater_than_or_equal(r._interval.end_bound(), current._interval.start_bound(), cmp)
                                && wrapping_interval<T>::greater_than_or_equal(current._interval.end_bound(), r._interval.end_bound(), cmp);
            if (includes_end) {
                continue; // last.start <= r.start <= r.end <= last.end
            }
            bool includes_start = wrapping_interval<T>::greater_than_or_equal(current._interval.end_bound(), r._interval.start_bound(), cmp);
            if (includes_start) {
                current = nonwrapping_interval(std::move(current.start()), std::move(r.end()));
            } else {
                deoverlapped_intervals.emplace_back(std::move(current));
                current = std::move(r);
            }
        }

        deoverlapped_intervals.emplace_back(std::move(current));
        return deoverlapped_intervals;
    }

private:
    // These private functions optimize the case where a sequence supports the
    // lower and upper bound operations more efficiently, as is the case with
    // some boost containers.
    struct std_ {};
    struct built_in_ : std_ {};

    template<typename Range, IntervalLessComparatorFor<T> LessComparator,
             typename = decltype(std::declval<Range>().lower_bound(std::declval<T>(), std::declval<LessComparator>()))>
    typename std::remove_reference<Range>::type::const_iterator do_lower_bound(const T& value, Range&& r, LessComparator&& cmp, built_in_) const {
        return r.lower_bound(value, std::forward<LessComparator>(cmp));
    }

    template<typename Range, IntervalLessComparatorFor<T> LessComparator,
             typename = decltype(std::declval<Range>().upper_bound(std::declval<T>(), std::declval<LessComparator>()))>
    typename std::remove_reference<Range>::type::const_iterator do_upper_bound(const T& value, Range&& r, LessComparator&& cmp, built_in_) const {
        return r.upper_bound(value, std::forward<LessComparator>(cmp));
    }

    template<typename Range, IntervalLessComparatorFor<T> LessComparator>
    typename std::remove_reference<Range>::type::const_iterator do_lower_bound(const T& value, Range&& r, LessComparator&& cmp, std_) const {
        return std::lower_bound(r.begin(), r.end(), value, std::forward<LessComparator>(cmp));
    }

    template<typename Range, IntervalLessComparatorFor<T> LessComparator>
    typename std::remove_reference<Range>::type::const_iterator do_upper_bound(const T& value, Range&& r, LessComparator&& cmp, std_) const {
        return std::upper_bound(r.begin(), r.end(), value, std::forward<LessComparator>(cmp));
    }
public:
    // Return the lower bound of the specified sequence according to these bounds.
    template<typename Range, IntervalLessComparatorFor<T> LessComparator>
    typename std::remove_reference<Range>::type::const_iterator lower_bound(Range&& r, LessComparator&& cmp) const {
        return start()
            ? (start()->is_inclusive()
                ? do_lower_bound(start()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_())
                : do_upper_bound(start()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_()))
            : std::cbegin(r);
    }
    // Return the upper bound of the specified sequence according to these bounds.
    template<typename Range, IntervalLessComparatorFor<T> LessComparator>
    typename std::remove_reference<Range>::type::const_iterator upper_bound(Range&& r, LessComparator&& cmp) const {
        return end()
             ? (end()->is_inclusive()
                ? do_upper_bound(end()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_())
                : do_lower_bound(end()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_()))
             : (is_singular()
                ? do_upper_bound(start()->value(), std::forward<Range>(r), std::forward<LessComparator>(cmp), built_in_())
                : std::cend(r));
    }
    // Returns a subset of the range that is within these bounds.
    template<typename Range, IntervalLessComparatorFor<T> LessComparator>
    boost::iterator_range<typename std::remove_reference<Range>::type::const_iterator>
    slice(Range&& range, LessComparator&& cmp) const {
        return boost::make_iterator_range(lower_bound(range, cmp), upper_bound(range, cmp));
    }

    // Returns the intersection between this interval and other.
    std::optional<nonwrapping_interval> intersection(const nonwrapping_interval& other, IntervalComparatorFor<T> auto&& cmp) const {
        auto p = std::minmax(_interval, other._interval, [&cmp] (auto&& a, auto&& b) {
            return wrapping_interval<T>::less_than(a.start_bound(), b.start_bound(), cmp);
        });
        if (wrapping_interval<T>::greater_than_or_equal(p.first.end_bound(), p.second.start_bound(), cmp)) {
            auto end = std::min(p.first.end_bound(), p.second.end_bound(), [&cmp] (auto&& a, auto&& b) {
                return !wrapping_interval<T>::greater_than_or_equal(a, b, cmp);
            });
            return nonwrapping_interval(p.second.start(), end.b);
        }
        return {};
    }

    template<typename U>
    friend std::ostream& operator<<(std::ostream& out, const nonwrapping_interval<U>& r);
};

template<typename U>
std::ostream& operator<<(std::ostream& out, const nonwrapping_interval<U>& r) {
    return out << r._interval;
}

template<template<typename> typename T, typename U>
concept Interval = std::is_same<T<U>, wrapping_interval<U>>::value || std::is_same<T<U>, nonwrapping_interval<U>>::value;

// Allow using interval<T> in a hash table. The hash function 31 * left +
// right is the same one used by Cassandra's AbstractBounds.hashCode().
namespace std {

template<typename T>
struct hash<wrapping_interval<T>> {
    using argument_type = wrapping_interval<T>;
    using result_type = decltype(std::hash<T>()(std::declval<T>()));
    result_type operator()(argument_type const& s) const {
        auto hash = std::hash<T>();
        auto left = s.start() ? hash(s.start()->value()) : 0;
        auto right = s.end() ? hash(s.end()->value()) : 0;
        return 31 * left + right;
    }
};

template<typename T>
struct hash<nonwrapping_interval<T>> {
    using argument_type = nonwrapping_interval<T>;
    using result_type = decltype(std::hash<T>()(std::declval<T>()));
    result_type operator()(argument_type const& s) const {
        return hash<wrapping_interval<T>>()(s);
    }
};

}


// range.hh is deprecated and should be replaced with interval.hh


template <typename T>
using range_bound = interval_bound<T>;

template <typename T>
using nonwrapping_range = interval<T>;

template <typename T>
using wrapping_range = wrapping_interval<T>;

template <typename T>
using range = wrapping_interval<T>;

template <template<typename> typename T, typename U>
concept Range = Interval<T, U>;

#include <seastar/core/shared_ptr.hh>
#include <seastar/core/sstring.hh>
#include <seastar/util/optimized_optional.hh>
#include <memory>
#include <random>
#include <utility>
#include <vector>
#include <compare>
#include <byteswap.h>



#include <seastar/net/byteorder.hh>
#include <fmt/format.h>
#include <array>
#include <functional>
#include <utility>
#include <compare>

namespace dht {

class token;

enum class token_kind {
    before_all_keys,
    key,
    after_all_keys,
};

class token {
    // INT64_MIN is not a legal token, but a special value used to represent
    // infinity in token intervals.
    // If a token with value INT64_MIN is generated by the hashing algorithm,
    // the result is coerced into INT64_MAX.
    // (So INT64_MAX is twice as likely as every other token.)
    static inline int64_t normalize(int64_t t) {
        return t == std::numeric_limits<int64_t>::min() ? std::numeric_limits<int64_t>::max() : t;
    }
public:
    using kind = token_kind;
    kind _kind;
    int64_t _data;

    token() : _kind(kind::before_all_keys) {
    }

    token(kind k, int64_t d)
        : _kind(std::move(k))
        , _data(normalize(d)) { }

    // This constructor seems redundant with the bytes_view constructor, but
    // it's necessary for IDL, which passes a deserialized_bytes_proxy here.
    // (deserialized_bytes_proxy is convertible to bytes&&, but not bytes_view.)
    token(kind k, const bytes& b) : _kind(std::move(k)) {
        if (_kind != kind::key) {
            _data = 0;
        } else {
            if (b.size() != sizeof(_data)) {
                throw std::runtime_error(fmt::format("Wrong token bytes size: expected {} but got {}", sizeof(_data), b.size()));
            }
            _data = net::ntoh(read_unaligned<int64_t>(b.begin()));
        }
    }

    token(kind k, bytes_view b) : _kind(std::move(k)) {
        if (_kind != kind::key) {
            _data = 0;
        } else {
            if (b.size() != sizeof(_data)) {
                throw std::runtime_error(fmt::format("Wrong token bytes size: expected {} but got {}", sizeof(_data), b.size()));
            }
            _data = net::ntoh(read_unaligned<int64_t>(b.begin()));
        }
    }

    bool is_minimum() const noexcept {
        return _kind == kind::before_all_keys;
    }

    bool is_maximum() const noexcept {
        return _kind == kind::after_all_keys;
    }

    size_t external_memory_usage() const {
        return 0;
    }

    size_t memory_usage() const {
        return sizeof(token);
    }

    bytes data() const {
        bytes b(bytes::initialized_later(), sizeof(_data));
        write_unaligned<int64_t>(b.begin(), net::hton(_data));
        return b;
    }

    /**
     * @return a string representation of this token
     */
    sstring to_sstring() const;

    /**
     * Calculate a token representing the approximate "middle" of the given
     * range.
     *
     * @return The approximate midpoint between left and right.
     */
    static token midpoint(const token& left, const token& right);

    /**
     * @return a randomly generated token
     */
    static token get_random_token();

    /**
     * @return a token from string representation
     */
    static dht::token from_sstring(const sstring& t);

    /**
     * @return a token from its byte representation
     */
    static dht::token from_bytes(bytes_view bytes);

    /**
     * Returns int64_t representation of the token
     */
    static int64_t to_int64(token);

    /**
     * Creates token from its int64_t representation
     */
    static dht::token from_int64(int64_t);

    /**
     * Calculate the deltas between tokens in the ring in order to compare
     *  relative sizes.
     *
     * @param sortedtokens a sorted List of tokens
     * @return the mapping from 'token' to 'percentage of the ring owned by that token'.
     */
    static std::map<token, float> describe_ownership(const std::vector<token>& sorted_tokens);

    static data_type get_token_validator();

    /**
     * Gets the first shard of the minimum token.
     */
    static unsigned shard_of_minimum_token() {
        return 0;  // hardcoded for now; unlikely to change
    }

    int64_t raw() const noexcept {
        if (is_minimum()) {
            return std::numeric_limits<int64_t>::min();
        }
        if (is_maximum()) {
            return std::numeric_limits<int64_t>::max();
        }

        return _data;
    }
};

static inline std::strong_ordering tri_compare_raw(const int64_t l1, const int64_t l2) noexcept {
    if (l1 == l2) {
        return std::strong_ordering::equal;
    } else {
        return l1 < l2 ? std::strong_ordering::less : std::strong_ordering::greater;
    }
}

template <typename T>
concept TokenCarrier = requires (const T& v) {
    { v.token() } noexcept -> std::same_as<const token&>;
};

struct raw_token_less_comparator {
    bool operator()(const int64_t k1, const int64_t k2) const noexcept {
        return dht::tri_compare_raw(k1, k2) < 0;
    }

    template <typename Key>
    requires TokenCarrier<Key>
    bool operator()(const Key& k1, const int64_t k2) const noexcept {
        return dht::tri_compare_raw(k1.token().raw(), k2) < 0;
    }

    template <typename Key>
    requires TokenCarrier<Key>
    bool operator()(const int64_t k1, const Key& k2) const noexcept {
        return dht::tri_compare_raw(k1, k2.token().raw()) < 0;
    }

    template <typename Key>
    requires TokenCarrier<Key>
    int64_t simplify_key(const Key& k) const noexcept {
        return k.token().raw();
    }

    int64_t simplify_key(int64_t k) const noexcept {
        return k;
    }
};

const token& minimum_token() noexcept;
const token& maximum_token() noexcept;
std::strong_ordering operator<=>(const token& t1, const token& t2);
inline bool operator==(const token& t1, const token& t2) { return t1 <=> t2 == 0; }
std::ostream& operator<<(std::ostream& out, const token& t);

// Returns a successor for token t.
// The caller must ensure there is a next token, otherwise
// the result is unspecified.
//
// Precondition: t.kind() == dht::token::kind::key
inline
token next_token(const token& t) {
    return {dht::token::kind::key, t._data + 1};
}

// Returns the smallest token in the ring which can be associated with a partition key.
inline
token first_token() {
    // dht::token::normalize() does not allow std::numeric_limits<int64_t>::min()
    return dht::token(dht::token_kind::key, std::numeric_limits<int64_t>::min() + 1);
}

uint64_t unbias(const token& t);
token bias(uint64_t n);
size_t compaction_group_of(unsigned most_significant_bits, const token& t);
token last_token_of_compaction_group(unsigned most_significant_bits, size_t group);

} // namespace dht

template <>
struct fmt::formatter<dht::token> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const dht::token& t, FormatContext& ctx) const {
        if (t.is_maximum()) {
            return fmt::format_to(ctx.out(), "maximum token");
        } else if (t.is_minimum()) {
            return fmt::format_to(ctx.out(), "minimum token");
        } else {
            return fmt::format_to(ctx.out(), "{}", dht::token::to_int64(t));
        }
    }
};


#include <seastar/core/smp.hh>

namespace dht {

inline sstring cpu_sharding_algorithm_name() {
    return "biased-token-round-robin";
}

std::vector<uint64_t> init_zero_based_shard_start(unsigned shards, unsigned sharding_ignore_msb_bits);

unsigned shard_of(unsigned shard_count, unsigned sharding_ignore_msb_bits, const token& t);

token token_for_next_shard(const std::vector<uint64_t>& shard_start, unsigned shard_count, unsigned sharding_ignore_msb_bits, const token& t, shard_id shard, unsigned spans);

class sharder {
protected:
    unsigned _shard_count;
    unsigned _sharding_ignore_msb_bits;
    std::vector<uint64_t> _shard_start;
public:
    sharder(unsigned shard_count = smp::count, unsigned sharding_ignore_msb_bits = 0);
    virtual ~sharder() = default;
    /**
     * Calculates the shard that handles a particular token.
     */
    virtual unsigned shard_of(const token& t) const;

    /**
     * Gets the first token greater than `t` that is in shard `shard`, and is a shard boundary (its first token).
     *
     * If the `spans` parameter is greater than zero, the result is the same as if the function
     * is called `spans` times, each time applied to its return value, but efficiently. This allows
     * selecting ranges that include multiple round trips around the 0..smp::count-1 shard span:
     *
     *     token_for_next_shard(t, shard, spans) == token_for_next_shard(token_for_next_shard(t, shard, 1), spans - 1)
     *
     * On overflow, maximum_token() is returned.
     */
    virtual token token_for_next_shard(const token& t, shard_id shard, unsigned spans = 1) const;

    /**
     * @return number of shards configured for this partitioner
     */
    unsigned shard_count() const {
        return _shard_count;
    }

    unsigned sharding_ignore_msb() const {
        return _sharding_ignore_msb_bits;
    }

    bool operator==(const sharder& o) const {
        return _shard_count == o._shard_count && _sharding_ignore_msb_bits == o._sharding_ignore_msb_bits;
    }
};

inline std::ostream& operator<<(std::ostream& os, const sharder& sharder) {
    os << "sharder[shard_count=" << sharder.shard_count()
       << ", ignore_msb_bits="<< sharder.sharding_ignore_msb() << "]";
    return os;
}

/*
 * Finds the first token in token range (`start`, `end`] that belongs to shard shard_idx.
 *
 * If there is no token that belongs to shard shard_idx in this range,
 * `end` is returned.
 *
 * The first token means the one that appears first on the ring when going
 * from `start` to `end`.
 * 'first token' is not always the smallest.
 * For example, if in vnode (100, 10] only tokens 110 and 1 belong to
 * shard shard_idx then token 110 is the first because it appears first
 * when going from 100 to 10 on the ring.
 */
dht::token find_first_token_for_shard(
        const dht::sharder& sharder, dht::token start, dht::token end, size_t shard_idx);

} //namespace dht

#include <seastar/core/thread.hh>
#include <seastar/util/bool_class.hh>

namespace utils {

class can_yield_tag;
using can_yield = seastar::bool_class<can_yield_tag>;

inline void maybe_yield(can_yield can_yield) {
    if (can_yield) {
        seastar::thread::maybe_yield();
    }
}

} // namespace utils


namespace sstables {

class key_view;
class decorated_key_view;

}

namespace dht {

//
// Origin uses a complex class hierarchy where Token is an abstract class,
// and various subclasses use different implementations (LongToken vs.
// BigIntegerToken vs. StringToken), plus other variants to to signify the
// the beginning of the token space etc.
//
// We'll fold all of that into the token class and push all of the variations
// into its users.

class decorated_key;
class ring_position;

using partition_range = nonwrapping_range<ring_position>;
using token_range = nonwrapping_range<token>;

using partition_range_vector = std::vector<partition_range>;
using token_range_vector = std::vector<token_range>;

// Wraps partition_key with its corresponding token.
//
// Total ordering defined by comparators is compatible with Origin's ordering.
class decorated_key {
public:
    dht::token _token;
    partition_key _key;

    decorated_key(dht::token t, partition_key k)
        : _token(std::move(t))
        , _key(std::move(k)) {
    }

    struct less_comparator {
        schema_ptr s;
        less_comparator(schema_ptr s);
        bool operator()(const decorated_key& k1, const decorated_key& k2) const;
        bool operator()(const decorated_key& k1, const ring_position& k2) const;
        bool operator()(const ring_position& k1, const decorated_key& k2) const;
    };

    bool equal(const schema& s, const decorated_key& other) const;

    bool less_compare(const schema& s, const decorated_key& other) const;
    bool less_compare(const schema& s, const ring_position& other) const;

    // Trichotomic comparators defining total ordering on the union of
    // decorated_key and ring_position objects.
    std::strong_ordering tri_compare(const schema& s, const decorated_key& other) const;
    std::strong_ordering tri_compare(const schema& s, const ring_position& other) const;

    const dht::token& token() const noexcept {
        return _token;
    }

    const partition_key& key() const {
        return _key;
    }

    size_t external_memory_usage() const {
        return _key.external_memory_usage() + _token.external_memory_usage();
    }

    size_t memory_usage() const {
        return sizeof(decorated_key) + external_memory_usage();
    }
};


class decorated_key_equals_comparator {
    const schema& _schema;
public:
    explicit decorated_key_equals_comparator(const schema& schema) : _schema(schema) {}
    bool operator()(const dht::decorated_key& k1, const dht::decorated_key& k2) const {
        return k1.equal(_schema, k2);
    }
};

using decorated_key_opt = std::optional<decorated_key>;

class i_partitioner {
public:
    using ptr_type = std::unique_ptr<i_partitioner>;

    i_partitioner() = default;
    virtual ~i_partitioner() {}

    /**
     * Transform key to object representation of the on-disk format.
     *
     * @param key the raw, client-facing key
     * @return decorated version of key
     */
    decorated_key decorate_key(const schema& s, const partition_key& key) const {
        return { get_token(s, key), key };
    }

    /**
     * Transform key to object representation of the on-disk format.
     *
     * @param key the raw, client-facing key
     * @return decorated version of key
     */
    decorated_key decorate_key(const schema& s, partition_key&& key) const {
        auto token = get_token(s, key);
        return { std::move(token), std::move(key) };
    }

    /**
     * @return a token that can be used to route a given key
     * (This is NOT a method to create a token from its string representation;
     * for that, use tokenFactory.fromString.)
     */
    virtual token get_token(const schema& s, partition_key_view key) const = 0;
    virtual token get_token(const sstables::key_view& key) const = 0;

    // FIXME: token.tokenFactory
    //virtual token.tokenFactory gettokenFactory() = 0;

    /**
     * @return name of partitioner.
     */
    virtual const sstring name() const = 0;

    bool operator==(const i_partitioner& o) const {
        return name() == o.name();
    }
};

//
// Represents position in the ring of partitions, where partitions are ordered
// according to decorated_key ordering (first by token, then by key value).
// Intended to be used for defining partition ranges.
//
// The 'key' part is optional. When it's absent, this object represents a position
// which is either before or after all keys sharing given token. That's determined
// by relation_to_keys().
//
// For example for the following data:
//
//   tokens: |    t1   | t2 |
//           +----+----+----+
//   keys:   | k1 | k2 | k3 |
//
// The ordering is:
//
//   ring_position(t1, token_bound::start) < ring_position(k1)
//   ring_position(k1)                     < ring_position(k2)
//   ring_position(k1)                     == decorated_key(k1)
//   ring_position(k2)                     == decorated_key(k2)
//   ring_position(k2)                     < ring_position(t1, token_bound::end)
//   ring_position(k2)                     < ring_position(k3)
//   ring_position(t1, token_bound::end)   < ring_position(t2, token_bound::start)
//
// Maps to org.apache.cassandra.db.RowPosition and its derivatives in Origin.
//
class ring_position {
public:
    enum class token_bound : int8_t { start = -1, end = 1 };
private:
    friend class ring_position_comparator;
    friend class ring_position_ext;
    dht::token _token;
    token_bound _token_bound{}; // valid when !_key
    std::optional<partition_key> _key;
public:
    static ring_position min() noexcept {
        return { minimum_token(), token_bound::start };
    }

    static ring_position max() noexcept {
        return { maximum_token(), token_bound::end };
    }

    bool is_min() const noexcept {
        return _token.is_minimum();
    }

    bool is_max() const noexcept {
        return _token.is_maximum();
    }

    static ring_position starting_at(dht::token token) {
        return { std::move(token), token_bound::start };
    }

    static ring_position ending_at(dht::token token) {
        return { std::move(token), token_bound::end };
    }

    ring_position(dht::token token, token_bound bound)
        : _token(std::move(token))
        , _token_bound(bound)
    { }

    ring_position(dht::token token, partition_key key)
        : _token(std::move(token))
        , _key(std::make_optional(std::move(key)))
    { }

    ring_position(dht::token token, token_bound bound, std::optional<partition_key> key)
        : _token(std::move(token))
        , _token_bound(bound)
        , _key(std::move(key))
    { }

    ring_position(const dht::decorated_key& dk)
        : _token(dk._token)
        , _key(std::make_optional(dk._key))
    { }

    ring_position(dht::decorated_key&& dk)
        : _token(std::move(dk._token))
        , _key(std::make_optional(std::move(dk._key)))
    { }

    const dht::token& token() const noexcept {
        return _token;
    }

    // Valid when !has_key()
    token_bound bound() const {
        return _token_bound;
    }

    // Returns -1 if smaller than keys with the same token, +1 if greater.
    int relation_to_keys() const {
        return _key ? 0 : static_cast<int>(_token_bound);
    }

    const std::optional<partition_key>& key() const {
        return _key;
    }

    bool has_key() const {
        return bool(_key);
    }

    // Call only when has_key()
    dht::decorated_key as_decorated_key() const {
        return { _token, *_key };
    }

    bool equal(const schema&, const ring_position&) const;

    // Trichotomic comparator defining a total ordering on ring_position objects
    std::strong_ordering tri_compare(const schema&, const ring_position&) const;

    // "less" comparator corresponding to tri_compare()
    bool less_compare(const schema&, const ring_position&) const;

    friend std::ostream& operator<<(std::ostream&, const ring_position&);
};

// Non-owning version of ring_position and ring_position_ext.
//
// Unlike ring_position, it can express positions which are right after and right before the keys.
// ring_position still can not because it is sent between nodes and such a position
// would not be (yet) properly interpreted by old nodes. That's why any ring_position
// can be converted to ring_position_view, but not the other way.
//
// It is possible to express a partition_range using a pair of two ring_position_views v1 and v2,
// where v1 = ring_position_view::for_range_start(r) and v2 = ring_position_view::for_range_end(r).
// Such range includes all keys k such that v1 <= k < v2, with order defined by ring_position_comparator.
//
class ring_position_view {
    friend std::strong_ordering ring_position_tri_compare(const schema& s, ring_position_view lh, ring_position_view rh);
    friend class ring_position_comparator;
    friend class ring_position_comparator_for_sstables;
    friend class ring_position_ext;

    // Order is lexicographical on (_token, _key) tuples, where _key part may be missing, and
    // _weight affecting order between tuples if one is a prefix of the other (including being equal).
    // A positive weight puts the position after all strictly prefixed by it, while a non-positive
    // weight puts it before them. If tuples are equal, the order is further determined by _weight.
    //
    // For example {_token=t1, _key=nullptr, _weight=1} is ordered after {_token=t1, _key=k1, _weight=0},
    // but {_token=t1, _key=nullptr, _weight=-1} is ordered before it.
    //
    const dht::token* _token; // always not nullptr
    const partition_key* _key; // Can be nullptr
    int8_t _weight;
private:
    ring_position_view() noexcept : _token(nullptr), _key(nullptr), _weight(0) { }
    explicit operator bool() const noexcept { return bool(_token); }
public:
    using token_bound = ring_position::token_bound;
    struct after_key_tag {};
    using after_key = bool_class<after_key_tag>;

    static ring_position_view min() noexcept {
        return { minimum_token(), nullptr, -1 };
    }

    static ring_position_view max() noexcept {
        return { maximum_token(), nullptr, 1 };
    }

    bool is_min() const noexcept {
        return _token->is_minimum();
    }

    bool is_max() const noexcept {
        return _token->is_maximum();
    }

    static ring_position_view for_range_start(const partition_range& r) {
        return r.start() ? ring_position_view(r.start()->value(), after_key(!r.start()->is_inclusive())) : min();
    }

    static ring_position_view for_range_end(const partition_range& r) {
        return r.end() ? ring_position_view(r.end()->value(), after_key(r.end()->is_inclusive())) : max();
    }

    static ring_position_view for_after_key(const dht::decorated_key& dk) {
        return ring_position_view(dk, after_key::yes);
    }

    static ring_position_view for_after_key(dht::ring_position_view view) {
        return ring_position_view(after_key_tag(), view);
    }

    static ring_position_view starting_at(const dht::token& t) {
        return ring_position_view(t, token_bound::start);
    }

    static ring_position_view ending_at(const dht::token& t) {
        return ring_position_view(t, token_bound::end);
    }

    ring_position_view(const dht::ring_position& pos, after_key after = after_key::no)
        : _token(&pos.token())
        , _key(pos.has_key() ? &*pos.key() : nullptr)
        , _weight(pos.has_key() ? bool(after) : pos.relation_to_keys())
    { }

    ring_position_view(const ring_position_view& pos) = default;
    ring_position_view& operator=(const ring_position_view& other) = default;

    ring_position_view(after_key_tag, const ring_position_view& v)
        : _token(v._token)
        , _key(v._key)
        , _weight(v._key ? 1 : v._weight)
    { }

    ring_position_view(const dht::decorated_key& key, after_key after_key = after_key::no)
        : _token(&key.token())
        , _key(&key.key())
        , _weight(bool(after_key))
    { }

    ring_position_view(const dht::token& token, const partition_key* key, int8_t weight)
        : _token(&token)
        , _key(key)
        , _weight(weight)
    { }

    explicit ring_position_view(const dht::token& token, token_bound bound = token_bound::start)
        : _token(&token)
        , _key(nullptr)
        , _weight(static_cast<std::underlying_type_t<token_bound>>(bound))
    { }

    const dht::token& token() const noexcept { return *_token; }
    const partition_key* key() const { return _key; }

    // Only when key() == nullptr
    token_bound get_token_bound() const { return token_bound(_weight); }
    // Only when key() != nullptr
    after_key is_after_key() const { return after_key(_weight == 1); }

    friend std::ostream& operator<<(std::ostream&, ring_position_view);
    friend class optimized_optional<ring_position_view>;
};

using ring_position_ext_view = ring_position_view;
using ring_position_view_opt = optimized_optional<ring_position_view>;

//
// Represents position in the ring of partitions, where partitions are ordered
// according to decorated_key ordering (first by token, then by key value).
// Intended to be used for defining partition ranges.
//
// Unlike ring_position, it can express positions which are right after and right before the keys.
// ring_position still can not because it is sent between nodes and such a position
// would not be (yet) properly interpreted by old nodes. That's why any ring_position
// can be converted to ring_position_ext, but not the other way.
//
// It is possible to express a partition_range using a pair of two ring_position_exts v1 and v2,
// where v1 = ring_position_ext::for_range_start(r) and v2 = ring_position_ext::for_range_end(r).
// Such range includes all keys k such that v1 <= k < v2, with order defined by ring_position_comparator.
//
class ring_position_ext {
    // Order is lexicographical on (_token, _key) tuples, where _key part may be missing, and
    // _weight affecting order between tuples if one is a prefix of the other (including being equal).
    // A positive weight puts the position after all strictly prefixed by it, while a non-positive
    // weight puts it before them. If tuples are equal, the order is further determined by _weight.
    //
    // For example {_token=t1, _key=nullptr, _weight=1} is ordered after {_token=t1, _key=k1, _weight=0},
    // but {_token=t1, _key=nullptr, _weight=-1} is ordered before it.
    //
    dht::token _token;
    std::optional<partition_key> _key;
    int8_t _weight;
public:
    using token_bound = ring_position::token_bound;
    struct after_key_tag {};
    using after_key = bool_class<after_key_tag>;

    static ring_position_ext min() noexcept {
        return { minimum_token(), std::nullopt, -1 };
    }

    static ring_position_ext max() noexcept {
        return { maximum_token(), std::nullopt, 1 };
    }

    bool is_min() const noexcept {
        return _token.is_minimum();
    }

    bool is_max() const noexcept {
        return _token.is_maximum();
    }

    static ring_position_ext for_range_start(const partition_range& r) {
        return r.start() ? ring_position_ext(r.start()->value(), after_key(!r.start()->is_inclusive())) : min();
    }

    static ring_position_ext for_range_end(const partition_range& r) {
        return r.end() ? ring_position_ext(r.end()->value(), after_key(r.end()->is_inclusive())) : max();
    }

    static ring_position_ext for_after_key(const dht::decorated_key& dk) {
        return ring_position_ext(dk, after_key::yes);
    }

    static ring_position_ext for_after_key(dht::ring_position_ext view) {
        return ring_position_ext(after_key_tag(), view);
    }

    static ring_position_ext starting_at(const dht::token& t) {
        return ring_position_ext(t, token_bound::start);
    }

    static ring_position_ext ending_at(const dht::token& t) {
        return ring_position_ext(t, token_bound::end);
    }

    ring_position_ext(const dht::ring_position& pos, after_key after = after_key::no)
        : _token(pos.token())
        , _key(pos.key())
        , _weight(pos.has_key() ? bool(after) : pos.relation_to_keys())
    { }

    ring_position_ext(const ring_position_ext& pos) = default;
    ring_position_ext& operator=(const ring_position_ext& other) = default;

    ring_position_ext(ring_position_view v)
        : _token(*v._token)
        , _key(v._key ? std::make_optional(*v._key) : std::nullopt)
        , _weight(v._weight)
    { }

    ring_position_ext(after_key_tag, const ring_position_ext& v)
        : _token(v._token)
        , _key(v._key)
        , _weight(v._key ? 1 : v._weight)
    { }

    ring_position_ext(const dht::decorated_key& key, after_key after_key = after_key::no)
        : _token(key.token())
        , _key(key.key())
        , _weight(bool(after_key))
    { }

    ring_position_ext(dht::token token, std::optional<partition_key> key, int8_t weight) noexcept
        : _token(std::move(token))
        , _key(std::move(key))
        , _weight(weight)
    { }

    ring_position_ext(ring_position&& pos) noexcept
        : _token(std::move(pos._token))
        , _key(std::move(pos._key))
        , _weight(pos.relation_to_keys())
    { }

    explicit ring_position_ext(const dht::token& token, token_bound bound = token_bound::start)
        : _token(token)
        , _key(std::nullopt)
        , _weight(static_cast<std::underlying_type_t<token_bound>>(bound))
    { }

    const dht::token& token() const noexcept { return _token; }
    const std::optional<partition_key>& key() const { return _key; }
    int8_t weight() const { return _weight; }

    // Only when key() == std::nullopt
    token_bound get_token_bound() const { return token_bound(_weight); }

    // Only when key() != std::nullopt
    after_key is_after_key() const { return after_key(_weight == 1); }

    operator ring_position_view() const { return { _token, _key ? &*_key : nullptr, _weight }; }

    friend std::ostream& operator<<(std::ostream&, const ring_position_ext&);
};

std::strong_ordering ring_position_tri_compare(const schema& s, ring_position_view lh, ring_position_view rh);

template <typename T>
requires std::is_convertible<T, ring_position_view>::value
ring_position_view ring_position_view_to_compare(const T& val) {
    return val;
}

// Trichotomic comparator for ring order
struct ring_position_comparator {
    const schema& s;
    ring_position_comparator(const schema& s_) : s(s_) {}

    std::strong_ordering operator()(ring_position_view lh, ring_position_view rh) const {
        return ring_position_tri_compare(s, lh, rh);
    }

    template <typename T>
    std::strong_ordering operator()(const T& lh, ring_position_view rh) const {
        return ring_position_tri_compare(s, ring_position_view_to_compare(lh), rh);
    }

    template <typename T>
    std::strong_ordering operator()(ring_position_view lh, const T& rh) const {
        return ring_position_tri_compare(s, lh, ring_position_view_to_compare(rh));
    }

    template <typename T1, typename T2>
    std::strong_ordering operator()(const T1& lh, const T2& rh) const {
        return ring_position_tri_compare(s, ring_position_view_to_compare(lh), ring_position_view_to_compare(rh));
    }
};

struct ring_position_comparator_for_sstables {
    const schema& s;
    ring_position_comparator_for_sstables(const schema& s_) : s(s_) {}
    std::strong_ordering operator()(ring_position_view, sstables::decorated_key_view) const;
    std::strong_ordering operator()(sstables::decorated_key_view, ring_position_view) const;
};

// "less" comparator giving the same order as ring_position_comparator
struct ring_position_less_comparator {
    ring_position_comparator tri;

    ring_position_less_comparator(const schema& s) : tri(s) {}

    template<typename T, typename U>
    bool operator()(const T& lh, const U& rh) const {
        return tri(lh, rh) < 0;
    }
};

struct token_comparator {
    // Return values are those of a trichotomic comparison.
    std::strong_ordering operator()(const token& t1, const token& t2) const;
};

std::ostream& operator<<(std::ostream& out, const decorated_key& t);

std::ostream& operator<<(std::ostream& out, const i_partitioner& p);

class partition_ranges_view {
    const dht::partition_range* _data = nullptr;
    size_t _size = 0;

public:
    partition_ranges_view() = default;
    partition_ranges_view(const dht::partition_range& range) : _data(&range), _size(1) {}
    partition_ranges_view(const dht::partition_range_vector& ranges) : _data(ranges.data()), _size(ranges.size()) {}
    bool empty() const { return _size == 0; }
    size_t size() const { return _size; }
    const dht::partition_range& front() const { return *_data; }
    const dht::partition_range& back() const { return *(_data + _size - 1); }
    const dht::partition_range* begin() const { return _data; }
    const dht::partition_range* end() const { return _data + _size; }
};
std::ostream& operator<<(std::ostream& out, partition_ranges_view v);

unsigned shard_of(const schema&, const token&);
inline decorated_key decorate_key(const schema& s, const partition_key& key) {
    return s.get_partitioner().decorate_key(s, key);
}
inline decorated_key decorate_key(const schema& s, partition_key&& key) {
    return s.get_partitioner().decorate_key(s, std::move(key));
}

inline token get_token(const schema& s, partition_key_view key) {
    return s.get_partitioner().get_token(s, key);
}

dht::partition_range to_partition_range(dht::token_range);
dht::partition_range_vector to_partition_ranges(const dht::token_range_vector& ranges, utils::can_yield can_yield = utils::can_yield::no);

// Each shard gets a sorted, disjoint vector of ranges
std::map<unsigned, dht::partition_range_vector>
split_range_to_shards(dht::partition_range pr, const schema& s);

// Intersect a partition_range with a shard and return the the resulting sub-ranges, in sorted order
future<utils::chunked_vector<partition_range>> split_range_to_single_shard(const schema& s, const dht::partition_range& pr, shard_id shard);

std::unique_ptr<dht::i_partitioner> make_partitioner(sstring name);

// Returns a sorted and deoverlapped list of ranges that are
// the result of subtracting all ranges from ranges_to_subtract.
// ranges_to_subtract must be sorted and deoverlapped.
future<dht::partition_range_vector> subtract_ranges(const schema& schema, const dht::partition_range_vector& ranges, dht::partition_range_vector ranges_to_subtract);

// Returns a token_range vector split based on the given number of most-significant bits
dht::token_range_vector split_token_range_msb(unsigned most_significant_bits);

} // dht

namespace std {
template<>
struct hash<dht::token> {
    size_t operator()(const dht::token& t) const {
        // We have to reverse the bytes here to keep compatibility with
        // the behaviour that was here when tokens were represented as
        // sequence of bytes.
        return bswap_64(t._data);
    }
};

template <>
struct hash<dht::decorated_key> {
    size_t operator()(const dht::decorated_key& k) const {
        auto h_token = hash<dht::token>();
        return h_token(k.token());
    }
};


}

#include <seastar/net/ipv4_address.hh>
#include <seastar/net/inet_address.hh>
#include <seastar/net/socket_defs.hh>
#include <iosfwd>
#include <optional>
#include <functional>


namespace gms {

class inet_address {
private:
    net::inet_address _addr;
public:
    inet_address() = default;
    inet_address(int32_t ip) noexcept
        : inet_address(uint32_t(ip)) {
    }
    explicit inet_address(uint32_t ip) noexcept
        : _addr(net::ipv4_address(ip)) {
    }
    inet_address(const net::inet_address& addr) noexcept : _addr(addr) {}
    inet_address(const socket_address& sa) noexcept
        : inet_address(sa.addr())
    {}
    const net::inet_address& addr() const noexcept {
        return _addr;
    }

    inet_address(const inet_address&) = default;

    operator const seastar::net::inet_address&() const noexcept {
        return _addr;
    }

    // throws std::invalid_argument if sstring is invalid
    inet_address(const sstring& addr) {
        // FIXME: We need a real DNS resolver
        if (addr == "localhost") {
            _addr = net::ipv4_address("127.0.0.1");
        } else {
            _addr = net::inet_address(addr);
        }
    }
    bytes_view bytes() const noexcept {
        return bytes_view(reinterpret_cast<const int8_t*>(_addr.data()), _addr.size());
    }
    // TODO remove
    uint32_t raw_addr() const {
        return addr().as_ipv4_address().ip;
    }
    sstring to_sstring() const;
    friend inline bool operator==(const inet_address& x, const inet_address& y) noexcept = default;
    friend inline bool operator<(const inet_address& x, const inet_address& y) noexcept {
        return x.bytes() < y.bytes();
    }
    friend struct std::hash<inet_address>;

    using opt_family = std::optional<net::inet_address::family>;

    static future<inet_address> lookup(sstring, opt_family family = {}, opt_family preferred = {});
};

std::ostream& operator<<(std::ostream& os, const inet_address& x);

}

namespace std {
template<>
struct hash<gms::inet_address> {
    size_t operator()(gms::inet_address a) const noexcept { return std::hash<net::inet_address>()(a._addr); }
};
}

template <>
struct fmt::formatter<gms::inet_address> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const ::gms::inet_address& x, FormatContext& ctx) const {
        if (x.addr().is_ipv4()) {
            return fmt::format_to(ctx.out(), "{}", x.addr());
        }
        // print 2 bytes in a group, and use ':' as the delimeter
        fmt::format_to(ctx.out(), "{:2:}", fmt_hex(x.bytes()));
        if (x.addr().scope() != seastar::net::inet_address::invalid_scope) {
            return fmt::format_to(ctx.out(), "%{}", x.addr().scope());
        }
        return ctx.out();
    }
};


#include <vector>
#include <atomic>
#include <random>
#include <seastar/core/sharded.hh>
#include <seastar/core/sstring.hh>
#include <seastar/core/metrics_registration.hh>

namespace cql3 { class query_processor; }

namespace tracing {

using elapsed_clock = std::chrono::steady_clock;

class trace_state_ptr;
class tracing;

enum class trace_type : uint8_t {
    NONE,
    QUERY,
    REPAIR,
};

extern std::vector<sstring> trace_type_names;

inline const sstring& type_to_string(trace_type t) {
    return trace_type_names.at(static_cast<int>(t));
}

/**
 * Returns a TTL for a given trace type
 * @param t trace type
 *
 * @return TTL
 */
inline std::chrono::seconds ttl_by_type(const trace_type t) {
    switch (t) {
    case trace_type::NONE:
    case trace_type::QUERY:
        return std::chrono::seconds(86400);  // 1 day
    case trace_type::REPAIR:
        return std::chrono::seconds(604800); // 7 days
    default:
        // unknown type value - must be a SW bug
        throw std::invalid_argument("unknown trace type: " + std::to_string(int(t)));
    }
}

/**
 * @brief represents an ID of a single tracing span.
 *
 * Currently span ID is a random 64-bit integer.
 */
class span_id {
private:
    uint64_t _id = illegal_id;

public:
    static constexpr uint64_t illegal_id = 0;

public:
    span_id() = default;
    uint64_t get_id() const { return _id; }
    span_id(uint64_t id) : _id(id) {}

    /**
     * @return New span_id with a random legal value
     */
    static span_id make_span_id();
};

std::ostream& operator<<(std::ostream& os, const span_id& id);

// !!!!IMPORTANT!!!!
//
// The enum_set based on this enum is serialized using IDL, therefore new items
// should always be added to the end of this enum - never before the existing
// ones.
//
// Otherwise this may break IDL's backward compatibility.
enum class trace_state_props {
    write_on_close, primary, log_slow_query, full_tracing, ignore_events
};

using trace_state_props_set = enum_set<super_enum<trace_state_props,
    trace_state_props::write_on_close,
    trace_state_props::primary,
    trace_state_props::log_slow_query,
    trace_state_props::full_tracing,
    trace_state_props::ignore_events>>;

class trace_info {
public:
    utils::UUID session_id;
    trace_type type;
    bool write_on_close;
    trace_state_props_set state_props;
    uint32_t slow_query_threshold_us; // in microseconds
    uint32_t slow_query_ttl_sec; // in seconds
    span_id parent_id;
    uint64_t start_ts_us = 0u; // sentinel value (== "unset")

public:
    trace_info(utils::UUID sid, trace_type t, bool w_o_c, trace_state_props_set s_p, uint32_t slow_query_threshold, uint32_t slow_query_ttl, span_id p_id, uint64_t s_t_u)
        : session_id(std::move(sid))
        , type(t)
        , write_on_close(w_o_c)
        , state_props(s_p)
        , slow_query_threshold_us(slow_query_threshold)
        , slow_query_ttl_sec(slow_query_ttl)
        , parent_id(std::move(p_id))
        , start_ts_us(s_t_u)
    {
        state_props.set_if<trace_state_props::write_on_close>(write_on_close);
    }
};

struct one_session_records;
using records_bulk = std::deque<lw_shared_ptr<one_session_records>>;

struct backend_session_state_base {
    virtual ~backend_session_state_base() {};
};

struct i_tracing_backend_helper {
    using wall_clock = std::chrono::system_clock;

protected:
    tracing& _local_tracing;

public:
    using ptr_type = std::unique_ptr<i_tracing_backend_helper>;

    i_tracing_backend_helper(tracing& tr) : _local_tracing(tr) {}
    virtual ~i_tracing_backend_helper() {}
    virtual future<> start(cql3::query_processor& qp) = 0;
    virtual future<> stop() = 0;

    /**
     * Write a bulk of tracing records.
     *
     * This function has to clear a scheduled state of each one_session_records object
     * in the @param bulk after it has been actually passed to the backend for writing.
     *
     * @param bulk a bulk of records
     */
    virtual void write_records_bulk(records_bulk& bulk) = 0;

    virtual std::unique_ptr<backend_session_state_base> allocate_session_state() const = 0;

private:
    friend class tracing;
};

struct event_record {
    sstring message;
    elapsed_clock::duration elapsed;
    i_tracing_backend_helper::wall_clock::time_point event_time_point;

    event_record(sstring message_, elapsed_clock::duration elapsed_, i_tracing_backend_helper::wall_clock::time_point event_time_point_)
        : message(std::move(message_))
        , elapsed(elapsed_)
        , event_time_point(event_time_point_) {}
};

struct session_record {
    gms::inet_address client;
    // Keep the containers below sorted since some backends require that and
    // it's very cheap to always do that because the amount of elements in a
    // container is very small.
    std::map<sstring, sstring> parameters;
    std::set<sstring> tables;
    sstring username;
    sstring request;
    size_t request_size = 0;
    size_t response_size = 0;
    std::chrono::system_clock::time_point started_at;
    trace_type command = trace_type::NONE;
    elapsed_clock::duration elapsed;
    std::chrono::seconds slow_query_record_ttl;

private:
    bool _consumed = false;

public:
    session_record(trace_type cmd, std::chrono::seconds ttl)
        : username("<unauthenticated request>")
        , command(cmd)
        , elapsed(-1)
        , slow_query_record_ttl(ttl)
    {}

    bool ready() const {
        return elapsed.count() >= 0 && !_consumed;
    }

    void set_consumed() {
        _consumed = true;
    }
};

class one_session_records {
private:
    shared_ptr<tracing> _local_tracing_ptr;
public:
    utils::UUID session_id;
    session_record session_rec;
    std::chrono::seconds ttl;
    std::deque<event_record> events_recs;
    std::unique_ptr<backend_session_state_base> backend_state_ptr;
    bool do_log_slow_query = false;

    // A pointer to the records counter of the corresponding state new records
    // of this tracing session should consume from (e.g. "cached" or "pending
    // for write").
    uint64_t* budget_ptr;

    // Each tracing session object represents a single tracing span.
    //
    // Each span has a span ID. In order to be able to build a full tree of all
    // spans of the same query we need a parent span ID as well.
    span_id parent_id;
    span_id my_span_id;

    one_session_records(trace_type type, std::chrono::seconds slow_query_ttl, std::chrono::seconds slow_query_rec_ttl,
            std::optional<utils::UUID> session_id = std::nullopt, span_id parent_id = span_id::illegal_id);

    /**
     * Consume a single record from the per-shard budget.
     */
    void consume_from_budget() {
        ++(*budget_ptr);
    }

    /**
     * Drop all pending records and return the budget.
     */
    void drop_records() {
        (*budget_ptr) -= size();
        events_recs.clear();
        session_rec.set_consumed();
    }

    /**
     * Should be called when a record is scheduled for write.
     * From that point till data_consumed() call all new records will be written
     * in the next write event.
     */
    inline void set_pending_for_write();

    /**
     * Should be called after all data pending to be written in this record has
     * been processed.
     * From that point on new records are cached internally and have to be
     * explicitly committed for write in order to be written during the write event.
     */
    inline void data_consumed();

    bool is_pending_for_write() const {
        return _is_pending_for_write;
    }

    uint64_t size() const {
        return events_recs.size() + session_rec.ready();
    }

private:
    bool _is_pending_for_write = false;
};

class tracing : public seastar::async_sharded_service<tracing> {
public:
    static const gc_clock::duration write_period;
    // maximum number of sessions pending for write per shard
    static constexpr int max_pending_sessions = 1000;
    // expectation of an average number of trace records per session
    static constexpr int exp_trace_events_per_session = 10;
    // maximum allowed pending records per-shard
    static constexpr int max_pending_trace_records = max_pending_sessions * exp_trace_events_per_session;
    // number of pending sessions that would trigger a write event
    static constexpr int write_event_sessions_threshold = 100;
    // number of pending records that would trigger a write event
    static constexpr int write_event_records_threshold = write_event_sessions_threshold * exp_trace_events_per_session;
    // Number of events when an info message is printed
    static constexpr int log_warning_period = 10000;

    static const std::chrono::microseconds default_slow_query_duraion_threshold;
    static const std::chrono::seconds default_slow_query_record_ttl;

    struct stats {
        uint64_t dropped_sessions = 0;
        uint64_t dropped_records = 0;
        uint64_t trace_records_count = 0;
        uint64_t trace_errors = 0;
    } stats;

private:
    // A number of currently active tracing sessions
    uint64_t _active_sessions = 0;

    // Below are 3 counters that describe the total amount of tracing records on
    // this shard. Each counter describes a state in which a record may be.
    //
    // Each record may only be in a specific state at every point of time and
    // thereby it must be accounted only in one and only one of the three
    // counters below at any given time.
    //
    // The sum of all three counters should not be greater than
    // (max_pending_trace_records + write_event_records_threshold) at any time
    // (actually it can get as high as a value above plus (max_pending_sessions)
    // if all sessions are primary but we won't take this into an account for
    // simplicity).
    //
    // The same is about the number of outstanding sessions: it may not be
    // greater than (max_pending_sessions + write_event_sessions_threshold) at
    // any time.
    //
    // If total number of tracing records is greater or equal to the limit
    // above, the new trace point is going to be dropped.
    //
    // If current number or records plus the expected number of trace records
    // per session (exp_trace_events_per_session) is greater than the limit
    // above new sessions will be dropped. A new session will also be dropped if
    // there are too many active sessions.
    //
    // When the record or a session is dropped the appropriate statistics
    // counters are updated and there is a rate-limited warning message printed
    // to the log.
    //
    // Every time a number of records pending for write is greater or equal to
    // (write_event_records_threshold) or a number of sessions pending for
    // write is greater or equal to (write_event_sessions_threshold) a write
    // event is issued.
    //
    // Every 2 seconds a timer would write all pending for write records
    // available so far.

    // Total number of records cached in the active sessions that are not going
    // to be written in the next write event
    uint64_t _cached_records = 0;
    // Total number of records that are currently being written to I/O
    uint64_t _flushing_records = 0;
    // Total number of records in the _pending_for_write_records_bulk. All of
    // them are going to be written to the I/O during the next write event.
    uint64_t _pending_for_write_records_count = 0;

    records_bulk _pending_for_write_records_bulk;
    timer<lowres_clock> _write_timer;
    // _down becomes FALSE after the local service is fully initialized and
    // tracing records are allowed to be created and collected. It becomes TRUE
    // after the shutdown() call and prevents further write attempts to I/O
    // backend.
    bool _down = true;
    // If _slow_query_logging_enabled is enabled, a query processor keeps all
    // trace events related to the query until in the end it can decide
    // if the query was slow to be saved.
    bool _slow_query_logging_enabled = false;
    // If _ignore_trace_events is enabled, tracing::trace ignores all tracing
    // events as well as creating trace_state descendants with trace_info to
    // track tracing sessions only. This is used to implement lightweight
    // slow query tracing.
    bool _ignore_trace_events = false;
    std::unique_ptr<i_tracing_backend_helper> _tracing_backend_helper_ptr;
    sstring _thread_name;
    sstring _tracing_backend_helper_class_name;
    seastar::metrics::metric_groups _metrics;
    double _trace_probability = 0.0; // keep this one for querying purposes
    uint64_t _normalized_trace_probability = 0;
    std::ranlux48_base _gen;
    std::chrono::microseconds _slow_query_duration_threshold;
    std::chrono::seconds _slow_query_record_ttl;

public:
    uint64_t get_next_rand_uint64() {
        return _gen();
    }

    i_tracing_backend_helper& backend_helper() {
        return *_tracing_backend_helper_ptr;
    }

    const sstring& get_thread_name() const {
        return _thread_name;
    }

    static seastar::sharded<tracing>& tracing_instance() {
        // FIXME: leaked intentionally to avoid shutdown problems, see #293
        static seastar::sharded<tracing>* tracing_inst = new seastar::sharded<tracing>();

        return *tracing_inst;
    }

    static tracing& get_local_tracing_instance() {
        return tracing_instance().local();
    }

    bool started() const {
        return !_down;
    }

    static future<> create_tracing(sstring tracing_backend_helper_class_name);
    static future<> start_tracing(sharded<cql3::query_processor>& qp);
    static future<> stop_tracing();
    tracing(sstring tracing_backend_helper_class_name);

    // Initialize a tracing backend (e.g. tracing_keyspace or logstash)
    future<> start(cql3::query_processor& qp);

    future<> stop();

    /**
     * Waits until all pending tracing records are flushed to the backend an
     * shuts down the backend. The following calls to
     * write_session_record()/write_event_record() methods of a backend instance
     * should be a NOOP.
     *
     * @return a ready future when the shutdown is complete
     */
    future<> shutdown();

    void write_pending_records() {
        if (_pending_for_write_records_bulk.size()) {
            _flushing_records += _pending_for_write_records_count;
            stats.trace_records_count += _pending_for_write_records_count;
            _pending_for_write_records_count = 0;
            _tracing_backend_helper_ptr->write_records_bulk(_pending_for_write_records_bulk);
            _pending_for_write_records_bulk.clear();
        }
    }

    void write_complete(uint64_t nr = 1) {
        if (nr > _flushing_records) {
            throw std::logic_error(seastar::format("completing more records ({:d}) than there are pending ({:d})", nr, _flushing_records));
        }
        _flushing_records -= nr;
    }


    void write_maybe() {
        if (_pending_for_write_records_count >= write_event_records_threshold || _pending_for_write_records_bulk.size() >= write_event_sessions_threshold) {
            write_pending_records();
        }
    }

    void end_session() {
        --_active_sessions;
    }

    void write_session_records(lw_shared_ptr<one_session_records> records, bool write_now) {
        // if service is down - drop the records and return
        if (_down) {
            return;
        }

        try {
            schedule_for_write(std::move(records));
        } catch (...) {
            // OOM: bump up the error counter and ignore
            ++stats.trace_errors;
            return;
        }

        if (write_now) {
            write_pending_records();
        } else {
            write_maybe();
        }
    }

    /**
     * Sets a probability for tracing a CQL request.
     *
     * @param p a new tracing probability - a floating point value in a [0,1]
     *          range. It would effectively define a portion of CQL requests
     *          initiated on the current Node that will be traced.
     * @throw std::invalid_argument if @ref p is out of range
     */
    void set_trace_probability(double p);
    double get_trace_probability() const {
        return _trace_probability;
    }

    bool trace_next_query() {
        return _normalized_trace_probability != 0 && _gen() < _normalized_trace_probability;
    }

    std::unique_ptr<backend_session_state_base> allocate_backend_session_state() const {
        return _tracing_backend_helper_ptr->allocate_session_state();
    }

    /**
     * Checks if there is enough budget for the @param nr new records
     * @param nr number of new records
     *
     * @return TRUE if there is enough budget, FLASE otherwise
     */
    bool have_records_budget(uint64_t nr = 1) {
        // We don't want the total amount of pending, active and flushing records to
        // bypass the maximum number of pending records plus the number of
        // records that are possibly being written write now.
        //
        // If either records are being created too fast or a backend doesn't
        // keep up we want to start dropping records.
        // In any case, this should be rare.
        if (_pending_for_write_records_count + _cached_records + _flushing_records + nr > max_pending_trace_records + write_event_records_threshold) {
            return false;
        }

        return true;
    }

    uint64_t* get_pending_records_ptr() {
        return &_pending_for_write_records_count;
    }

    uint64_t* get_cached_records_ptr() {
        return &_cached_records;
    }

    void schedule_for_write(lw_shared_ptr<one_session_records> records) {
        if (records->is_pending_for_write()) {
            return;
        }

        _pending_for_write_records_bulk.emplace_back(records);
        records->set_pending_for_write();

        // move the current records from a "cached" to "pending for write" state
        auto current_records_num = records->size();
        _cached_records -= current_records_num;
        _pending_for_write_records_count += current_records_num;
    }

    void set_slow_query_enabled(bool enable = true) {
        _slow_query_logging_enabled = enable;
    }

    bool slow_query_tracing_enabled() const {
        return _slow_query_logging_enabled;
    }

    void set_ignore_trace_events(bool enable = true) {
        _ignore_trace_events = enable;
    }

    bool ignore_trace_events_enabled() const {
        return _ignore_trace_events;
    }

    /**
     * Set the slow query threshold
     *
     * We limit the number of microseconds in the threshold by a maximal unsigned 32-bit
     * integer.
     *
     * If a new threshold value exceeds the above limitation we will override it
     * with the value based on a limit above.
     *
     * @param new_threshold new threshold value
     */
    void set_slow_query_threshold(std::chrono::microseconds new_threshold) {
        if (new_threshold.count() > std::numeric_limits<uint32_t>::max()) {
            _slow_query_duration_threshold = std::chrono::microseconds(std::numeric_limits<uint32_t>::max());
            return;
        }

        _slow_query_duration_threshold = new_threshold;
    }

    std::chrono::microseconds slow_query_threshold() const {
        return _slow_query_duration_threshold;
    }

    /**
     * Set the slow query record TTL
     *
     * We limit the number of seconds in the TTL by a maximal signed 32-bit
     * integer.
     *
     * If a new TTL value exceeds the above limitation we will override it
     * with the value based on a limit above.
     *
     * @param new_ttl new TTL
     */
    void set_slow_query_record_ttl(std::chrono::seconds new_ttl) {
        if (new_ttl.count() > std::numeric_limits<int32_t>::max()) {
            _slow_query_record_ttl = std::chrono::seconds(std::numeric_limits<int32_t>::max());
            return;
        }

        _slow_query_record_ttl = new_ttl;
    }

    std::chrono::seconds slow_query_record_ttl() const {
        return _slow_query_record_ttl;
    }

private:
    void write_timer_callback();

    /**
     * Check if we may create a new tracing session.
     *
     * @return TRUE if conditions are allowing creating a new tracing session
     */
    bool may_create_new_session(const std::optional<utils::UUID>& session_id = std::nullopt);
};

void one_session_records::set_pending_for_write() {
    _is_pending_for_write = true;
    budget_ptr = _local_tracing_ptr->get_pending_records_ptr();
}

void one_session_records::data_consumed() {
    if (session_rec.ready()) {
        session_rec.set_consumed();
    }

    _is_pending_for_write = false;
    budget_ptr = _local_tracing_ptr->get_cached_records_ptr();
}

inline span_id span_id::make_span_id() {
    // make sure the value is always greater than 0
    return 1 + (tracing::get_local_tracing_instance().get_next_rand_uint64() << 1);
}
}

#include <cinttypes>

namespace ser {

template <typename T>
class serializer;

};


namespace query {

/*
 * This struct is used in two incompatible ways.
 *
 * SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT cluster feature determines which way is
 * used.
 *
 * 1. If SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT is not enabled on the cluster then
 *    `page_size` field is ignored. Depending on the query type the meaning of
 *    the remaining two fields is:
 *
 *    a. For unpaged queries or for reverse queries:
 *
 *          * `soft_limit` is used to warn about queries that result exceeds
 *            this limit. If the limit is exceeded, a warning will be written to
 *            the log.
 *
 *          * `hard_limit` is used to terminate a query which result exceeds
 *            this limit. If the limit is exceeded, the operation will end with
 *            an exception.
 *
 *    b. For all other queries, `soft_limit` == `hard_limit` and their value is
 *       really a page_size in bytes. If the page is not previously cut by the
 *       page row limit then reaching the size of `soft_limit`/`hard_limit`
 *       bytes will cause a page to be finished.
 *
 * 2. If SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT is enabled on the cluster then all
 *    three fields are always set. They are used in different places:
 *
 *    a. `soft_limit` and `hard_limit` are used for unpaged queries and in a
 *       reversing reader used for reading KA/LA sstables. Their meaning is the
 *       same as in (1.a) above.
 *
 *    b. all other queries use `page_size` field only and the meaning of the
 *       field is the same ase in (1.b) above.
 *
 * Two interpretations of the `max_result_size` struct are not compatible so we
 * need to take care of handling a mixed clusters.
 *
 * As long as SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT cluster feature is not
 * supported by all nodes in the clustser, new nodes will always use the
 * interpretation described in the point (1). `soft_limit` and `hard_limit`
 * fields will be set appropriately to the query type and `page_size` field
 * will be set to 0. Old nodes will ignare `page_size` anyways and new nodes
 * will know to ignore it as well when it's set to 0. Old nodes will never set
 * `page_size` and that means new nodes will give it a default value of 0 and
 * ignore it for messages that miss this field.
 *
 * Once SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT cluster feature becomes supported by
 * the whole cluster, new nodes will start to set `page_size` to the right value
 * according to the interpretation described in the point (2).
 *
 * For each request, only the coordinator looks at
 * SEPARATE_PAGE_SIZE_AND_SAFETY_LIMIT and based on it decides for this request
 * whether it will be handled with interpretation (1) or (2). Then all the
 * replicas can check the decision based only on the message they receive.
 * If page_size is set to 0 or not set at all then the request will be handled
 * using the interpretation (1). Otherwise, interpretation (2) will be used.
 */
struct max_result_size {
    uint64_t soft_limit;
    uint64_t hard_limit;
private:
    uint64_t page_size = 0;
public:

    max_result_size() = delete;
    explicit max_result_size(uint64_t max_size) : soft_limit(max_size), hard_limit(max_size) { }
    explicit max_result_size(uint64_t soft_limit, uint64_t hard_limit) : soft_limit(soft_limit), hard_limit(hard_limit) { }
    max_result_size(uint64_t soft_limit, uint64_t hard_limit, uint64_t page_size)
            : soft_limit(soft_limit)
            , hard_limit(hard_limit)
            , page_size(page_size)
    { }
    uint64_t get_page_size() const {
        return page_size == 0 ? hard_limit : page_size;
    }
    friend bool operator==(const max_result_size&, const max_result_size&);
    friend class ser::serializer<query::max_result_size>;
};

inline bool operator==(const max_result_size& a, const max_result_size& b) {
    return a.soft_limit == b.soft_limit && a.hard_limit == b.hard_limit && a.page_size == b.page_size;
}

}

#include <cstdint>
#include <variant>
#include <seastar/util/bool_class.hh>

namespace db {

using allow_per_partition_rate_limit = seastar::bool_class<class allow_per_partition_rate_limit_tag>;

namespace per_partition_rate_limit {

// Tells the replica to account the operation (increase the corresponding counter)
// and accept it regardless from the value of the counter.
//
// Used when the coordinator IS a replica (correct node and shard).
struct account_only {};

// Tells the replica to account the operation and decide whether to reject
// or not, based on the random variable sent by the coordinator.
//
// Used when the coordinator IS NOT a replica (wrong node or shard).
struct account_and_enforce {
    // A random 32-bit number generated by the coordinator.
    // Replicas are supposed to use it in order to decide whether
    // to accept or reject.
    uint32_t random_variable;

    inline double get_random_variable_as_double() const {
        return double(random_variable) / double(1LL << 32);
    }
};

// std::monostate -> do not count to the rate limit and never reject
// account_and_enforce -> account to the rate limit and optionally reject
using info = std::variant<std::monostate, account_only, account_and_enforce>;

} // namespace per_partition_rate_limit

} // namespace db



using query_id = utils::tagged_uuid<struct query_id_tag>;

#include <iostream>
#include <cstdint>
#include <exception>

using cql_protocol_version_type = uint8_t;

// Abstraction of transport protocol-dependent serialization format
// Protocols v1, v2 used 16 bits for collection sizes, while v3 and
// above use 32 bits.  But letting every bit of the code know what
// transport protocol we're using (and in some cases, we aren't using
// any transport -- it's for internal storage) is bad, so abstract it
// away here.

class cql_serialization_format {
    cql_protocol_version_type _version;
public:
    static constexpr cql_protocol_version_type latest_version = 4;
    explicit cql_serialization_format(cql_protocol_version_type version) : _version(version) {}
    static cql_serialization_format latest() { return cql_serialization_format{latest_version}; }
    cql_protocol_version_type protocol_version() const { return _version; }
    void ensure_supported() const {
        if (_version < 3) {
            throw std::runtime_error("cql protocol version must be 3 or later");
        }
    }
};


#include <memory>
#include <optional>


class position_in_partition_view;
class position_in_partition;
class partition_slice_builder;

namespace query {

using column_id_vector = utils::small_vector<column_id, 8>;

template <typename T>
using range = wrapping_range<T>;

using ring_position = dht::ring_position;

// Note: the bounds of a  clustering range don't necessarily satisfy `rb.end()->value() >= lb.end()->value()`,
// where `lb`, `rb` are the left and right bound respectively, if the bounds use non-full clustering
// key prefixes. Inclusiveness of the range's bounds must be taken into account during comparisons.
// For example, consider clustering key type consisting of two ints. Then [0:1, 0:] is a valid non-empty range
// (e.g. it includes the key 0:2) even though 0: < 0:1 w.r.t the clustering prefix order.
using clustering_range = nonwrapping_range<clustering_key_prefix>;

// If `range` was supposed to be used with a comparator `cmp`, then
// `reverse(range)` is supposed to be used with a reversed comparator `c`.
// For instance, if it does make sense to do
//   range.contains(point, cmp);
// then it also makes sense to do
//   reversed(range).contains(point, [](auto x, auto y) { return cmp(y, x); });
// but it doesn't make sense to do
//   reversed(range).contains(point, cmp);
clustering_range reverse(const clustering_range& range);

extern const dht::partition_range full_partition_range;
extern const clustering_range full_clustering_range;

inline
bool is_single_partition(const dht::partition_range& range) {
    return range.is_singular() && range.start()->value().has_key();
}

inline
bool is_single_row(const schema& s, const query::clustering_range& range) {
    return range.is_singular() && range.start()->value().is_full(s);
}

typedef std::vector<clustering_range> clustering_row_ranges;

/// Trim the clustering ranges.
///
/// Equivalent of intersecting each clustering range with [pos, +inf) position
/// in partition range, or (-inf, pos] position in partition range if
/// reversed == true. Ranges that do not intersect are dropped. Ranges that
/// partially overlap are trimmed.
/// Result: each range will overlap fully with [pos, +inf), or (-int, pos] if
/// reversed is true.
void trim_clustering_row_ranges_to(const schema& s, clustering_row_ranges& ranges, position_in_partition pos, bool reversed = false);

/// Trim the clustering ranges.
///
/// Equivalent of intersecting each clustering range with (key, +inf) clustering
/// range, or (-inf, key) clustering range if reversed == true. Ranges that do
/// not intersect are dropped. Ranges that partially overlap are trimmed.
/// Result: each range will overlap fully with (key, +inf), or (-int, key) if
/// reversed is true.
void trim_clustering_row_ranges_to(const schema& s, clustering_row_ranges& ranges, const clustering_key& key, bool reversed = false);

class specific_ranges {
public:
    specific_ranges(partition_key pk, clustering_row_ranges ranges)
            : _pk(std::move(pk)), _ranges(std::move(ranges)) {
    }
    specific_ranges(const specific_ranges&) = default;

    void add(const schema& s, partition_key pk, clustering_row_ranges ranges) {
        if (!_pk.equal(s, pk)) {
            throw std::runtime_error("Only single specific range supported currently");
        }
        _pk = std::move(pk);
        _ranges = std::move(ranges);
    }
    bool contains(const schema& s, const partition_key& pk) {
        return _pk.equal(s, pk);
    }
    size_t size() const {
        return 1;
    }
    const clustering_row_ranges* range_for(const schema& s, const partition_key& key) const {
        if (_pk.equal(s, key)) {
            return &_ranges;
        }
        return nullptr;
    }
    const partition_key& pk() const {
        return _pk;
    }
    const clustering_row_ranges& ranges() const {
        return _ranges;
    }
    clustering_row_ranges& ranges() {
        return _ranges;
    }
private:
    friend std::ostream& operator<<(std::ostream& out, const specific_ranges& r);

    partition_key _pk;
    clustering_row_ranges _ranges;
};

constexpr auto max_rows = std::numeric_limits<uint64_t>::max();
constexpr auto partition_max_rows = std::numeric_limits<uint64_t>::max();
constexpr auto max_rows_if_set = std::numeric_limits<uint32_t>::max();

// Specifies subset of rows, columns and cell attributes to be returned in a query.
// Can be accessed across cores.
// Schema-dependent.
//
// COMPATIBILITY NOTE: the partition-slice for reverse queries has two different
// format:
// * legacy format
// * native format
// The wire format uses the legacy format. See docs/dev/reverse-reads.md
// for more details on the formats.
class partition_slice {
    friend class ::partition_slice_builder;
public:
    enum class option {
        send_clustering_key,
        send_partition_key,
        send_timestamp,
        send_expiry,
        reversed,
        distinct,
        collections_as_maps,
        send_ttl,
        allow_short_read,
        with_digest,
        bypass_cache,
        // Normally, we don't return static row if the request has clustering
        // key restrictions and the partition doesn't have any rows matching
        // the restrictions, see #589. This flag overrides this behavior.
        always_return_static_content,
        // Use the new data range scan variant, which builds query::result
        // directly, bypassing the intermediate reconcilable_result format used
        // in pre 4.5 range scans.
        range_scan_data_variant,
    };
    using option_set = enum_set<super_enum<option,
        option::send_clustering_key,
        option::send_partition_key,
        option::send_timestamp,
        option::send_expiry,
        option::reversed,
        option::distinct,
        option::collections_as_maps,
        option::send_ttl,
        option::allow_short_read,
        option::with_digest,
        option::bypass_cache,
        option::always_return_static_content,
        option::range_scan_data_variant>>;
    clustering_row_ranges _row_ranges;
public:
    column_id_vector static_columns; // TODO: consider using bitmap
    column_id_vector regular_columns;  // TODO: consider using bitmap
    option_set options;
private:
    std::unique_ptr<specific_ranges> _specific_ranges;
    uint32_t _partition_row_limit_low_bits;
    uint32_t _partition_row_limit_high_bits;
public:
    partition_slice(clustering_row_ranges row_ranges, column_id_vector static_columns,
        column_id_vector regular_columns, option_set options,
        std::unique_ptr<specific_ranges> specific_ranges,
        cql_serialization_format,
        uint32_t partition_row_limit_low_bits,
        uint32_t partition_row_limit_high_bits);
    partition_slice(clustering_row_ranges row_ranges, column_id_vector static_columns,
        column_id_vector regular_columns, option_set options,
        std::unique_ptr<specific_ranges> specific_ranges = nullptr,
        uint64_t partition_row_limit = partition_max_rows);
    partition_slice(clustering_row_ranges ranges, const schema& schema, const column_set& mask, option_set options);
    partition_slice(const partition_slice&);
    partition_slice(partition_slice&&);
    ~partition_slice();

    partition_slice& operator=(partition_slice&& other) noexcept;

    const clustering_row_ranges& row_ranges(const schema&, const partition_key&) const;
    void set_range(const schema&, const partition_key&, clustering_row_ranges);
    void clear_range(const schema&, const partition_key&);
    void clear_ranges() {
        _specific_ranges = nullptr;
    }
    // FIXME: possibly make this function return a const ref instead.
    clustering_row_ranges get_all_ranges() const;

    const clustering_row_ranges& default_row_ranges() const {
        return _row_ranges;
    }
    const std::unique_ptr<specific_ranges>& get_specific_ranges() const {
        return _specific_ranges;
    }
    const cql_serialization_format cql_format() const {
        return cql_serialization_format(4); // For IDL compatibility
    }
    const uint32_t partition_row_limit_low_bits() const {
        return _partition_row_limit_low_bits;
    }
    const uint32_t partition_row_limit_high_bits() const {
        return _partition_row_limit_high_bits;
    }
    const uint64_t partition_row_limit() const {
        return (static_cast<uint64_t>(_partition_row_limit_high_bits) << 32) | _partition_row_limit_low_bits;
    }
    void set_partition_row_limit(uint64_t limit) {
        _partition_row_limit_low_bits = static_cast<uint64_t>(limit);
        _partition_row_limit_high_bits = static_cast<uint64_t>(limit >> 32);
    }

    [[nodiscard]]
    bool is_reversed() const {
        return options.contains<query::partition_slice::option::reversed>();
    }

    friend std::ostream& operator<<(std::ostream& out, const partition_slice& ps);
    friend std::ostream& operator<<(std::ostream& out, const specific_ranges& ps);
};

// See docs/dev/reverse-reads.md
// In the following functions, `schema` may be reversed or not (both work).
partition_slice legacy_reverse_slice_to_native_reverse_slice(const schema& schema, partition_slice slice);
partition_slice native_reverse_slice_to_legacy_reverse_slice(const schema& schema, partition_slice slice);
// Fully reverse slice (forward to native reverse or native reverse to forward).
// Also toggles the reversed bit in `partition_slice::options`.
partition_slice reverse_slice(const schema& schema, partition_slice slice);
// Half reverse slice (forwad to legacy reverse or legacy reverse to forward).
// Also toggles the reversed bit in `partition_slice::options`.
partition_slice half_reverse_slice(const schema&, partition_slice);

constexpr auto max_partitions = std::numeric_limits<uint32_t>::max();
constexpr auto max_tombstones = std::numeric_limits<uint64_t>::max();

// Tagged integers to disambiguate constructor arguments.
enum class row_limit : uint64_t { max = max_rows };
enum class partition_limit : uint32_t { max = max_partitions };
enum class tombstone_limit : uint64_t { max = max_tombstones };

using is_first_page = bool_class<class is_first_page_tag>;

// Full specification of a query to the database.
// Intended for passing across replicas.
// Can be accessed across cores.
class read_command {
public:
    table_id cf_id;
    table_schema_version schema_version; // TODO: This should be enough, drop cf_id
    partition_slice slice;
    uint32_t row_limit_low_bits;
    gc_clock::time_point timestamp;
    std::optional<tracing::trace_info> trace_info;
    uint32_t partition_limit; // The maximum number of live partitions to return.
    // The "query_uuid" field is useful in pages queries: It tells the replica
    // that when it finishes the read request prematurely, i.e., reached the
    // desired number of rows per page, it should not destroy the reader object,
    // rather it should keep it alive - at its current position - and save it
    // under the unique key "query_uuid". Later, when we want to resume
    // the read at exactly the same position (i.e., to request the next page)
    // we can pass this same unique id in that query's "query_uuid" field.
    query_id query_uuid;
    // Signal to the replica that this is the first page of a (maybe) paged
    // read request as far the replica is concerned. Can be used by the replica
    // to avoid doing work normally done on paged requests, e.g. attempting to
    // reused suspended readers.
    query::is_first_page is_first_page;
    // The maximum size of the query result, for all queries.
    // We use the entire value range, so we need an optional for the case when
    // the remote doesn't send it.
    std::optional<query::max_result_size> max_result_size;
    uint32_t row_limit_high_bits;
    // Cut the page after processing this many tombstones (even if the page is empty).
    uint64_t tombstone_limit;
    api::timestamp_type read_timestamp; // not serialized
    db::allow_per_partition_rate_limit allow_limit; // not serialized
public:
    // IDL constructor
    read_command(table_id cf_id,
                 table_schema_version schema_version,
                 partition_slice slice,
                 uint32_t row_limit_low_bits,
                 gc_clock::time_point now,
                 std::optional<tracing::trace_info> ti,
                 uint32_t partition_limit,
                 query_id query_uuid,
                 query::is_first_page is_first_page,
                 std::optional<query::max_result_size> max_result_size,
                 uint32_t row_limit_high_bits,
                 uint64_t tombstone_limit)
        : cf_id(std::move(cf_id))
        , schema_version(std::move(schema_version))
        , slice(std::move(slice))
        , row_limit_low_bits(row_limit_low_bits)
        , timestamp(now)
        , trace_info(std::move(ti))
        , partition_limit(partition_limit)
        , query_uuid(query_uuid)
        , is_first_page(is_first_page)
        , max_result_size(max_result_size)
        , row_limit_high_bits(row_limit_high_bits)
        , tombstone_limit(tombstone_limit)
        , read_timestamp(api::new_timestamp())
        , allow_limit(db::allow_per_partition_rate_limit::no)
    { }

    read_command(table_id cf_id,
            table_schema_version schema_version,
            partition_slice slice,
            query::max_result_size max_result_size,
            query::tombstone_limit tombstone_limit,
            query::row_limit row_limit = query::row_limit::max,
            query::partition_limit partition_limit = query::partition_limit::max,
            gc_clock::time_point now = gc_clock::now(),
            std::optional<tracing::trace_info> ti = std::nullopt,
            query_id query_uuid = query_id::create_null_id(),
            query::is_first_page is_first_page = query::is_first_page::no,
            api::timestamp_type rt = api::new_timestamp(),
            db::allow_per_partition_rate_limit allow_limit = db::allow_per_partition_rate_limit::no)
        : cf_id(std::move(cf_id))
        , schema_version(std::move(schema_version))
        , slice(std::move(slice))
        , row_limit_low_bits(static_cast<uint32_t>(row_limit))
        , timestamp(now)
        , trace_info(std::move(ti))
        , partition_limit(static_cast<uint32_t>(partition_limit))
        , query_uuid(query_uuid)
        , is_first_page(is_first_page)
        , max_result_size(max_result_size)
        , row_limit_high_bits(static_cast<uint32_t>(static_cast<uint64_t>(row_limit) >> 32))
        , tombstone_limit(static_cast<uint64_t>(tombstone_limit))
        , read_timestamp(rt)
        , allow_limit(allow_limit)
    { }


    uint64_t get_row_limit() const {
        return (static_cast<uint64_t>(row_limit_high_bits) << 32) | row_limit_low_bits;
    }
    void set_row_limit(uint64_t new_row_limit) {
        row_limit_low_bits = static_cast<uint32_t>(new_row_limit);
        row_limit_high_bits = static_cast<uint32_t>(new_row_limit >> 32);
    }
    friend std::ostream& operator<<(std::ostream& out, const read_command& r);
};

struct forward_request {
    enum class reduction_type {
        count,
        aggregate
    };
    struct aggregation_info {
        db::functions::function_name name;
        std::vector<sstring> column_names;
    };
    struct reductions_info { 
        // Used by selector_factries to prepare reductions information
        std::vector<reduction_type> types;
        std::vector<aggregation_info> infos;
    };

    std::vector<reduction_type> reduction_types;

    query::read_command cmd;
    dht::partition_range_vector pr;

    db::consistency_level cl;
    lowres_system_clock::time_point timeout;
    std::optional<std::vector<aggregation_info>> aggregation_infos;
};

std::ostream& operator<<(std::ostream& out, const forward_request& r);
std::ostream& operator<<(std::ostream& out, const forward_request::reduction_type& r);
std::ostream& operator<<(std::ostream& out, const forward_request::aggregation_info& a);

struct forward_result {
    // vector storing query result for each selected column
    std::vector<bytes_opt> query_results;

    struct printer {
        const std::vector<::shared_ptr<db::functions::aggregate_function>> functions;
        const query::forward_result& res;
    };
};

std::ostream& operator<<(std::ostream& out, const query::forward_result::printer&);
}



namespace query {

class clustering_key_filter_ranges {
    clustering_row_ranges _storage;
    std::reference_wrapper<const clustering_row_ranges> _ref;
public:
    clustering_key_filter_ranges(const clustering_row_ranges& ranges) : _ref(ranges) { }
    clustering_key_filter_ranges(clustering_row_ranges&& ranges)
        : _storage(std::make_move_iterator(ranges.begin()), std::make_move_iterator(ranges.end())), _ref(_storage) {}

    struct reversed { };
    clustering_key_filter_ranges(reversed, const clustering_row_ranges& ranges)
        : _storage(ranges.rbegin(), ranges.rend()), _ref(_storage) { }

    clustering_key_filter_ranges(clustering_key_filter_ranges&& other) noexcept
        : _storage(std::move(other._storage))
        , _ref(&other._ref.get() == &other._storage ? _storage : other._ref.get())
    { }

    clustering_key_filter_ranges& operator=(clustering_key_filter_ranges&& other) noexcept {
        if (this != &other) {
            _storage = std::move(other._storage);
            _ref = (&other._ref.get() == &other._storage) ? _storage : other._ref.get();
        }
        return *this;
    }

    auto begin() const { return _ref.get().begin(); }
    auto end() const { return _ref.get().end(); }
    bool empty() const { return _ref.get().empty(); }
    size_t size() const { return _ref.get().size(); }
    const clustering_row_ranges& ranges() const { return _ref; }

    // Returns all clustering ranges determined by `slice` inside partition determined by `key`.
    // If the slice contains the `reversed` option, we assume that it is given in 'half-reversed' format
    // (i.e. the ranges within are given in reverse order, but the ranges themselves are not reversed)
    // with respect to the table order.
    // The ranges will be returned in forward (increasing) order even if the slice is reversed.
    static clustering_key_filter_ranges get_ranges(const schema& schema, const query::partition_slice& slice, const partition_key& key) {
        const query::clustering_row_ranges& ranges = slice.row_ranges(schema, key);
        if (slice.is_reversed()) {
            return clustering_key_filter_ranges(clustering_key_filter_ranges::reversed{}, ranges);
        }
        return clustering_key_filter_ranges(ranges);
    }

    // Returns all clustering ranges determined by `slice` inside partition determined by `key`.
    // The ranges will be returned in the same order as stored in the slice.
    static clustering_key_filter_ranges get_native_ranges(const schema& schema, const query::partition_slice& slice, const partition_key& key) {
        const query::clustering_row_ranges& ranges = slice.row_ranges(schema, key);
        return clustering_key_filter_ranges(ranges);
    }
};

}


#include <algorithm>

// combine two sorted uniqued sequences into a single sorted sequence
// unique elements are copied, duplicate elements are merged with a
// binary function.
template <typename InputIterator1,
          typename InputIterator2,
          typename OutputIterator,
          typename Compare,
          typename Merge>
OutputIterator
combine(InputIterator1 begin1, InputIterator1 end1,
        InputIterator2 begin2, InputIterator2 end2,
        OutputIterator out,
        Compare compare,
        Merge merge) {
    while (begin1 != end1 && begin2 != end2) {
        auto& e1 = *begin1;
        auto& e2 = *begin2;
        if (compare(e1, e2)) {
            *out++ = e1;
            ++begin1;
        } else if (compare(e2, e1)) {
            *out++ = e2;
            ++begin2;
        } else {
            *out++ = merge(e1, e2);
            ++begin1;
            ++begin2;
        }
    }
    out = std::copy(begin1, end1, out);
    out = std::copy(begin2, end2, out);
    return out;
}




#include <seastar/net/inet_address.hh>


#include <functional>
#include <compare>

/**
 * Represents deletion operation. Can be commuted with other tombstones via apply() method.
 * Can be empty.
 */
struct tombstone final {
    api::timestamp_type timestamp;
    gc_clock::time_point deletion_time;

    tombstone(api::timestamp_type timestamp, gc_clock::time_point deletion_time)
        : timestamp(timestamp)
        , deletion_time(deletion_time)
    { }

    tombstone()
        : tombstone(api::missing_timestamp, {})
    { }

    std::strong_ordering operator<=>(const tombstone& t) const = default;
    bool operator==(const tombstone&) const = default;

    explicit operator bool() const {
        return timestamp != api::missing_timestamp;
    }

    void apply(const tombstone& t) noexcept {
        if (*this < t) {
            *this = t;
        }
    }

    // See reversibly_mergeable.hh
    void apply_reversibly(tombstone& t) noexcept {
        std::swap(*this, t);
        apply(t);
    }

    // See reversibly_mergeable.hh
    void revert(tombstone& t) noexcept {
        std::swap(*this, t);
    }

    tombstone operator+(const tombstone& t) {
        auto result = *this;
        result.apply(t);
        return result;
    }
};

template <>
struct fmt::formatter<tombstone> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const tombstone& t, FormatContext& ctx) const {
        if (t) {
            return fmt::format_to(ctx.out(),
                                  "{{tombstone: timestamp={}, deletion_time={}}}",
                                  t.timestamp, t.deletion_time.time_since_epoch().count());
        } else {
            return fmt::format_to(ctx.out(),
                                  "{{tombstone: none}}");
        }
     }
};

static inline std::ostream& operator<<(std::ostream& out, const tombstone& t) {
    fmt::print(out, "{}", t);
    return out;
}

template<>
struct appending_hash<tombstone> {
    template<typename Hasher>
    void operator()(Hasher& h, const tombstone& t) const {
        feed_hash(h, t.timestamp);
        feed_hash(h, t.deletion_time);
    }
};

// Determines whether tombstone may be GC-ed.
using can_gc_fn = std::function<bool(tombstone)>;

extern can_gc_fn always_gc;

#include <seastar/net//byteorder.hh>
#include <seastar/util/bool_class.hh>
#include <cstdint>
#include <iosfwd>
#include <concepts>

class abstract_type;
class collection_type_impl;
class atomic_cell_or_collection;

using atomic_cell_value = managed_bytes;
template <mutable_view is_mutable>
using atomic_cell_value_basic_view = managed_bytes_basic_view<is_mutable>;
using atomic_cell_value_view = atomic_cell_value_basic_view<mutable_view::no>;
using atomic_cell_value_mutable_view = atomic_cell_value_basic_view<mutable_view::yes>;

template <typename T>
requires std::is_trivial_v<T>
static void set_field(atomic_cell_value_mutable_view& out, unsigned offset, T val) {
    auto out_view = managed_bytes_mutable_view(out);
    out_view.remove_prefix(offset);
    write<T>(out_view, val);
}

template <typename T>
requires std::is_trivial_v<T>
static void set_field(atomic_cell_value& out, unsigned offset, T val) {
    auto out_view = atomic_cell_value_mutable_view(out);
    set_field(out_view, offset, val);
}

template <FragmentRange Buffer>
static void set_value(managed_bytes& b, unsigned value_offset, const Buffer& value) {
    auto v = managed_bytes_mutable_view(b).substr(value_offset, value.size_bytes());
    for (auto frag : value) {
        write_fragmented(v, single_fragmented_view(frag));
    }
}

template <typename T, FragmentedView Input>
requires std::is_trivial_v<T>
static T get_field(Input in, unsigned offset = 0) {
    in.remove_prefix(offset);
    return read_simple<T>(in);
}

/*
 * Represents atomic cell layout. Works on serialized form.
 *
 * Layout:
 *
 *  <live>  := <int8_t:flags><int64_t:timestamp>(<int64_t:expiry><int32_t:ttl>)?<value>
 *  <dead>  := <int8_t:    0><int64_t:timestamp><int64_t:deletion_time>
 */
class atomic_cell_type final {
private:
    static constexpr int8_t LIVE_FLAG = 0x01;
    static constexpr int8_t EXPIRY_FLAG = 0x02; // When present, expiry field is present. Set only for live cells
    static constexpr int8_t COUNTER_UPDATE_FLAG = 0x08; // Cell is a counter update.
    static constexpr unsigned flags_size = 1;
    static constexpr unsigned timestamp_offset = flags_size;
    static constexpr unsigned timestamp_size = 8;
    static constexpr unsigned expiry_offset = timestamp_offset + timestamp_size;
    static constexpr unsigned expiry_size = 8;
    static constexpr unsigned deletion_time_offset = timestamp_offset + timestamp_size;
    static constexpr unsigned deletion_time_size = 8;
    static constexpr unsigned ttl_offset = expiry_offset + expiry_size;
    static constexpr unsigned ttl_size = 4;
    friend class counter_cell_builder;
private:
    static bool is_counter_update(atomic_cell_value_view cell) {
        return cell.front() & COUNTER_UPDATE_FLAG;
    }
    static bool is_live(atomic_cell_value_view cell) {
        return cell.front() & LIVE_FLAG;
    }
    static bool is_live_and_has_ttl(atomic_cell_value_view cell) {
        return cell.front() & EXPIRY_FLAG;
    }
    static bool is_dead(atomic_cell_value_view cell) {
        return !is_live(cell);
    }
    // Can be called on live and dead cells
    static api::timestamp_type timestamp(atomic_cell_value_view cell) {
        return get_field<api::timestamp_type>(cell, timestamp_offset);
    }
    static void set_timestamp(atomic_cell_value_mutable_view& cell, api::timestamp_type ts) {
        set_field(cell, timestamp_offset, ts);
    }
    // Can be called on live cells only
private:
    template <mutable_view is_mutable>
    static managed_bytes_basic_view<is_mutable> do_get_value(managed_bytes_basic_view<is_mutable> cell) {
        auto expiry_field_size = bool(cell.front() & EXPIRY_FLAG) * (expiry_size + ttl_size);
        auto value_offset = flags_size + timestamp_size + expiry_field_size;
        cell.remove_prefix(value_offset);
        return cell;
    }
public:
    static atomic_cell_value_view value(managed_bytes_view cell) {
        return do_get_value(cell);
    }
    static atomic_cell_value_mutable_view value(managed_bytes_mutable_view cell) {
        return do_get_value(cell);
    }
    // Can be called on live counter update cells only
    static int64_t counter_update_value(atomic_cell_value_view cell) {
        return get_field<int64_t>(cell, flags_size + timestamp_size);
    }
    // Can be called only when is_dead() is true.
    static gc_clock::time_point deletion_time(atomic_cell_value_view cell) {
        assert(is_dead(cell));
        return gc_clock::time_point(gc_clock::duration(get_field<int64_t>(cell, deletion_time_offset)));
    }
    // Can be called only when is_live_and_has_ttl() is true.
    static gc_clock::time_point expiry(atomic_cell_value_view cell) {
        assert(is_live_and_has_ttl(cell));
        auto expiry = get_field<int64_t>(cell, expiry_offset);
        return gc_clock::time_point(gc_clock::duration(expiry));
    }
    // Can be called only when is_live_and_has_ttl() is true.
    static gc_clock::duration ttl(atomic_cell_value_view cell) {
        assert(is_live_and_has_ttl(cell));
        return gc_clock::duration(get_field<int32_t>(cell, ttl_offset));
    }
    static managed_bytes make_dead(api::timestamp_type timestamp, gc_clock::time_point deletion_time) {
        managed_bytes b(managed_bytes::initialized_later(), flags_size + timestamp_size + deletion_time_size);
        b[0] = 0;
        set_field(b, timestamp_offset, timestamp);
        set_field(b, deletion_time_offset, static_cast<int64_t>(deletion_time.time_since_epoch().count()));
        return b;
    }
    template <FragmentRange Buffer>
    static managed_bytes make_live(api::timestamp_type timestamp, const Buffer& value) {
        auto value_offset = flags_size + timestamp_size;
        managed_bytes b(managed_bytes::initialized_later(), value_offset + value.size_bytes());
        b[0] = LIVE_FLAG;
        set_field(b, timestamp_offset, timestamp);
        set_value(b, value_offset, value);
        return b;
    }
    static managed_bytes make_live_counter_update(api::timestamp_type timestamp, int64_t value) {
        auto value_offset = flags_size + timestamp_size;
        managed_bytes b(managed_bytes::initialized_later(), value_offset + sizeof(value));
        b[0] = LIVE_FLAG | COUNTER_UPDATE_FLAG;
        set_field(b, timestamp_offset, timestamp);
        set_field(b, value_offset, value);
        return b;
    }
    template <FragmentRange Buffer>
    static managed_bytes make_live(api::timestamp_type timestamp, const Buffer& value, gc_clock::time_point expiry, gc_clock::duration ttl) {
        auto value_offset = flags_size + timestamp_size + expiry_size + ttl_size;
        managed_bytes b(managed_bytes::initialized_later(), value_offset + value.size_bytes());
        b[0] = EXPIRY_FLAG | LIVE_FLAG;
        set_field(b, timestamp_offset, timestamp);
        set_field(b, expiry_offset, static_cast<int64_t>(expiry.time_since_epoch().count()));
        set_field(b, ttl_offset, static_cast<int32_t>(ttl.count()));
        set_value(b, value_offset, value);
        return b;
    }
    static managed_bytes make_live_uninitialized(api::timestamp_type timestamp, size_t size) {
        auto value_offset = flags_size + timestamp_size;
        managed_bytes b(managed_bytes::initialized_later(), value_offset + size);
        b[0] = LIVE_FLAG;
        set_field(b, timestamp_offset, timestamp);
        return b;
    }
    template <mutable_view is_mutable>
    friend class basic_atomic_cell_view;
    friend class atomic_cell;
};

/// View of an atomic cell
template<mutable_view is_mutable>
class basic_atomic_cell_view {
protected:
    managed_bytes_basic_view<is_mutable> _view;
	friend class atomic_cell;
protected:
    void set_view(managed_bytes_basic_view<is_mutable> v) {
        _view = v;
    }
    basic_atomic_cell_view() = default;
    explicit basic_atomic_cell_view(managed_bytes_basic_view<is_mutable> v) : _view(std::move(v)) { }
    friend class atomic_cell_or_collection;
public:
    operator basic_atomic_cell_view<mutable_view::no>() const noexcept {
        return basic_atomic_cell_view<mutable_view::no>(_view);
    }

    bool is_counter_update() const {
        return atomic_cell_type::is_counter_update(_view);
    }
    bool is_live() const {
        return atomic_cell_type::is_live(_view);
    }
    bool is_live(tombstone t, bool is_counter) const {
        return is_live() && !is_covered_by(t, is_counter);
    }
    bool is_live(tombstone t, gc_clock::time_point now, bool is_counter) const {
        return is_live() && !is_covered_by(t, is_counter) && !has_expired(now);
    }
    bool is_live_and_has_ttl() const {
        return atomic_cell_type::is_live_and_has_ttl(_view);
    }
    bool is_dead(gc_clock::time_point now) const {
        return atomic_cell_type::is_dead(_view) || has_expired(now);
    }
    bool is_covered_by(tombstone t, bool is_counter) const {
        return timestamp() <= t.timestamp || (is_counter && t.timestamp != api::missing_timestamp);
    }
    // Can be called on live and dead cells
    api::timestamp_type timestamp() const {
        return atomic_cell_type::timestamp(_view);
    }
    void set_timestamp(api::timestamp_type ts) {
        atomic_cell_type::set_timestamp(_view, ts);
    }
    // Can be called on live cells only
    atomic_cell_value_basic_view<is_mutable> value() const {
        return atomic_cell_type::value(_view);
    }
    // Can be called on live cells only
    size_t value_size() const {
        return atomic_cell_type::value(_view).size();
    }
    // Can be called on live counter update cells only
    int64_t counter_update_value() const {
        return atomic_cell_type::counter_update_value(_view);
    }
    // Can be called only when is_dead(gc_clock::time_point)
    gc_clock::time_point deletion_time() const {
        return !is_live() ? atomic_cell_type::deletion_time(_view) : expiry() - ttl();
    }
    // Can be called only when is_live_and_has_ttl()
    gc_clock::time_point expiry() const {
        return atomic_cell_type::expiry(_view);
    }
    // Can be called only when is_live_and_has_ttl()
    gc_clock::duration ttl() const {
        return atomic_cell_type::ttl(_view);
    }
    // Can be called on live and dead cells
    bool has_expired(gc_clock::time_point now) const {
        return is_live_and_has_ttl() && expiry() <= now;
    }

    managed_bytes_view serialize() const {
        return _view;
    }
};

class atomic_cell_view final : public basic_atomic_cell_view<mutable_view::no> {
    atomic_cell_view(managed_bytes_view v)
        : basic_atomic_cell_view(v) {}

    template<mutable_view is_mutable>
    atomic_cell_view(basic_atomic_cell_view<is_mutable> view)
        : basic_atomic_cell_view<mutable_view::no>(view) {}
    friend class atomic_cell;
public:
    static atomic_cell_view from_bytes(const abstract_type& t, managed_bytes_view v) {
        return atomic_cell_view(v);
    }
    static atomic_cell_view from_bytes(const abstract_type& t, bytes_view v) {
        return atomic_cell_view(managed_bytes_view(v));
    }

    friend std::ostream& operator<<(std::ostream& os, const atomic_cell_view& acv);

    class printer {
        const abstract_type& _type;
        const atomic_cell_view& _cell;
    public:
        printer(const abstract_type& type, const atomic_cell_view& cell) : _type(type), _cell(cell) {}
        friend std::ostream& operator<<(std::ostream& os, const printer& acvp);
    };
};

class atomic_cell_mutable_view final : public basic_atomic_cell_view<mutable_view::yes> {
    atomic_cell_mutable_view(managed_bytes_mutable_view data)
        : basic_atomic_cell_view(data) {}
public:
    static atomic_cell_mutable_view from_bytes(const abstract_type& t, managed_bytes_mutable_view v) {
        return atomic_cell_mutable_view(v);
    }

    friend class atomic_cell;
};

using atomic_cell_ref = atomic_cell_mutable_view;

class atomic_cell final : public basic_atomic_cell_view<mutable_view::yes> {
    managed_bytes _data;
    atomic_cell(managed_bytes b) : _data(std::move(b))  {
        set_view(_data);
    }

public:
    class collection_member_tag;
    using collection_member = bool_class<collection_member_tag>;

    atomic_cell(atomic_cell&& o) noexcept : _data(std::move(o._data)) {
        set_view(_data);
    }
    atomic_cell& operator=(const atomic_cell&) = delete;
    atomic_cell& operator=(atomic_cell&& o) {
        _data = std::move(o._data);
        set_view(_data);
        return *this;
    }
    operator atomic_cell_view() const { return atomic_cell_view(managed_bytes_view(_data)); }
    atomic_cell(const abstract_type& t, atomic_cell_view other);
    static atomic_cell make_dead(api::timestamp_type timestamp, gc_clock::time_point deletion_time);
    static atomic_cell make_live(const abstract_type& type, api::timestamp_type timestamp, bytes_view value,
                                 collection_member = collection_member::no);
    static atomic_cell make_live(const abstract_type& type, api::timestamp_type timestamp, managed_bytes_view value,
                                 collection_member = collection_member::no);
    static atomic_cell make_live(const abstract_type& type, api::timestamp_type timestamp, ser::buffer_view<bytes_ostream::fragment_iterator> value,
                                 collection_member = collection_member::no);
    static atomic_cell make_live(const abstract_type& type, api::timestamp_type timestamp, const fragmented_temporary_buffer::view& value,
                                 collection_member = collection_member::no);
    static atomic_cell make_live(const abstract_type& type, api::timestamp_type timestamp, const bytes& value,
                                 collection_member cm = collection_member::no) {
        return make_live(type, timestamp, bytes_view(value), cm);
    }
    static atomic_cell make_live_counter_update(api::timestamp_type timestamp, int64_t value);
    static atomic_cell make_live(const abstract_type&, api::timestamp_type timestamp, bytes_view value,
        gc_clock::time_point expiry, gc_clock::duration ttl, collection_member = collection_member::no);
    static atomic_cell make_live(const abstract_type&, api::timestamp_type timestamp, managed_bytes_view value,
        gc_clock::time_point expiry, gc_clock::duration ttl, collection_member = collection_member::no);
    static atomic_cell make_live(const abstract_type&, api::timestamp_type timestamp, ser::buffer_view<bytes_ostream::fragment_iterator> value,
        gc_clock::time_point expiry, gc_clock::duration ttl, collection_member = collection_member::no);
    static atomic_cell make_live(const abstract_type&, api::timestamp_type timestamp, const fragmented_temporary_buffer::view& value,
        gc_clock::time_point expiry, gc_clock::duration ttl, collection_member = collection_member::no);
    static atomic_cell make_live(const abstract_type& type, api::timestamp_type timestamp, const bytes& value,
                                 gc_clock::time_point expiry, gc_clock::duration ttl, collection_member cm = collection_member::no)
    {
        return make_live(type, timestamp, bytes_view(value), expiry, ttl, cm);
    }
    static atomic_cell make_live(const abstract_type& type, api::timestamp_type timestamp, bytes_view value, ttl_opt ttl, collection_member cm = collection_member::no) {
        if (!ttl) {
            return make_live(type, timestamp, value, cm);
        } else {
            return make_live(type, timestamp, value, gc_clock::now() + *ttl, *ttl, cm);
        }
    }
    static atomic_cell make_live(const abstract_type& type, api::timestamp_type timestamp, const managed_bytes_view& value, ttl_opt ttl, collection_member cm = collection_member::no) {
        if (!ttl) {
            return make_live(type, timestamp, value, cm);
        } else {
            return make_live(type, timestamp, value, gc_clock::now() + *ttl, *ttl, cm);
        }
    }
    static atomic_cell make_live_uninitialized(const abstract_type& type, api::timestamp_type timestamp, size_t size);
    friend class atomic_cell_or_collection;
    friend std::ostream& operator<<(std::ostream& os, const atomic_cell& ac);

    class printer : atomic_cell_view::printer {
    public:
        printer(const abstract_type& type, const atomic_cell_view& cell) : atomic_cell_view::printer(type, cell) {}
        friend std::ostream& operator<<(std::ostream& os, const printer& acvp);
    };
};

class column_definition;

std::strong_ordering compare_atomic_cell_for_merge(atomic_cell_view left, atomic_cell_view right);
void merge_column(const abstract_type& def,
        atomic_cell_or_collection& old,
        const atomic_cell_or_collection& neww);


#include <iosfwd>
#include <forward_list>

class abstract_type;
class compaction_garbage_collector;
class row_tombstone;

class collection_mutation;

// An auxiliary struct used to (de)construct collection_mutations.
// Unlike collection_mutation which is a serialized blob, this struct allows to inspect logical units of information
// (tombstone and cells) inside the mutation easily.
struct collection_mutation_description {
    tombstone tomb;
    // FIXME: use iterators?
    // we never iterate over `cells` more than once, so there is no need to store them in memory.
    // In some cases instead of constructing the `cells` vector, it would be more efficient to provide
    // a one-time-use forward iterator which returns the cells.
    utils::chunked_vector<std::pair<bytes, atomic_cell>> cells;

    // Expires cells based on query_time. Expires tombstones based on max_purgeable and gc_before.
    // Removes cells covered by tomb or this->tomb.
    bool compact_and_expire(column_id id, row_tombstone tomb, gc_clock::time_point query_time,
        can_gc_fn&, gc_clock::time_point gc_before, compaction_garbage_collector* collector = nullptr);

    // Packs the data to a serialized blob.
    collection_mutation serialize(const abstract_type&) const;
};

// Similar to collection_mutation_description, except that it doesn't store the cells' data, only observes it.
struct collection_mutation_view_description {
    tombstone tomb;
    // FIXME: use iterators? See the fixme in collection_mutation_description; the same considerations apply here.
    utils::chunked_vector<std::pair<bytes_view, atomic_cell_view>> cells;

    // Copies the observed data, storing it in a collection_mutation_description.
    collection_mutation_description materialize(const abstract_type&) const;

    // Packs the data to a serialized blob.
    collection_mutation serialize(const abstract_type&) const;
};

class collection_mutation_input_stream {
    std::forward_list<bytes> _linearized;
    managed_bytes_view _src;
public:
    collection_mutation_input_stream(const managed_bytes_view& src) : _src(src) {}
    template <Trivial T>
    T read_trivial() {
        return ::read_simple<T>(_src);
    }
    bytes_view read_linearized(size_t n);
    managed_bytes_view read_fragmented(size_t n);
    bool empty() const;
};

// Given a collection_mutation_view, returns an auxiliary struct allowing the inspection of each cell.
// The function needs to be given the type of stored data to reconstruct the structural information.
collection_mutation_view_description deserialize_collection_mutation(const abstract_type&, collection_mutation_input_stream&);

class collection_mutation_view {
public:
    managed_bytes_view data;

    // Is this a noop mutation?
    bool is_empty() const;

    // Is any of the stored cells live (not deleted nor expired) at the time point `tp`,
    // given the later of the tombstones `t` and the one stored in the mutation (if any)?
    // Requires a type to reconstruct the structural information.
    bool is_any_live(const abstract_type&, tombstone t = tombstone(), gc_clock::time_point tp = gc_clock::time_point::min()) const;

    // The maximum of timestamps of the mutation's cells and tombstone.
    api::timestamp_type last_update(const abstract_type&) const;

    // Given a function that operates on a collection_mutation_view_description,
    // calls it on the corresponding description of `this`.
    template <typename F>
    inline decltype(auto) with_deserialized(const abstract_type& type, F f) const {
        collection_mutation_input_stream stream(data);
        return f(deserialize_collection_mutation(type, stream));
    }

    class printer {
        const abstract_type& _type;
        const collection_mutation_view& _cmv;
    public:
        printer(const abstract_type& type, const collection_mutation_view& cmv)
                : _type(type), _cmv(cmv) {}
        friend std::ostream& operator<<(std::ostream& os, const printer& cmvp);
    };
};

// A serialized mutation of a collection of cells.
// Used to represent mutations of collections (lists, maps, sets) or non-frozen user defined types.
// It contains a sequence of cells, each representing a mutation of a single entry (element or field) of the collection.
// Each cell has an associated 'key' (or 'path'). The meaning of each (key, cell) pair is:
//  for sets: the key is the serialized set element, the cell contains no data (except liveness information),
//  for maps: the key is the serialized map element's key, the cell contains the serialized map element's value,
//  for lists: the key is a timeuuid identifying the list entry, the cell contains the serialized value,
//  for user types: the key is an index identifying the field, the cell contains the value of the field.
//  The mutation may also contain a collection-wide tombstone.
class collection_mutation {
public:
    managed_bytes _data;

    collection_mutation() {}
    collection_mutation(const abstract_type&, collection_mutation_view);
    collection_mutation(const abstract_type&, managed_bytes);
    operator collection_mutation_view() const;
};

collection_mutation merge(const abstract_type&, collection_mutation_view, collection_mutation_view);

collection_mutation difference(const abstract_type&, collection_mutation_view, collection_mutation_view);

// Serializes the given collection of cells to a sequence of bytes ready to be sent over the CQL protocol.
bytes_ostream serialize_for_cql(const abstract_type&, collection_mutation_view);


#include <seastar/core/shared_ptr.hh>
#include <seastar/core/sstring.hh>
#include <vector>

namespace cql3 {

class column_specification;

}

class collection_type_impl : public abstract_type {
    static logging::logger _logger;
public:
    static constexpr size_t max_elements = 65535;

protected:
    bool _is_multi_cell;
    explicit collection_type_impl(kind k, sstring name, bool is_multi_cell)
            : abstract_type(k, std::move(name), {}), _is_multi_cell(is_multi_cell) {
                _contains_collection = true;
            }
public:
    bool is_multi_cell() const { return _is_multi_cell; }
    virtual data_type name_comparator() const = 0;
    virtual data_type value_comparator() const = 0;
    lw_shared_ptr<cql3::column_specification> make_collection_receiver(const cql3::column_specification& collection, bool is_key) const;
    virtual bool is_compatible_with_frozen(const collection_type_impl& previous) const = 0;
    virtual bool is_value_compatible_with_frozen(const collection_type_impl& previous) const = 0;

    template <typename Iterator>
    requires requires (Iterator it) { {*it} -> std::convertible_to<bytes_view_opt>; }
    static bytes pack(Iterator start, Iterator finish, int elements);

    template <typename Iterator>
    requires requires (Iterator it) { {*it} -> std::convertible_to<managed_bytes_view_opt>; }
    static managed_bytes pack_fragmented(Iterator start, Iterator finish, int elements);

private:
    // Explicitly instantiated in types.cc
    template <FragmentedView View> data_value deserialize_impl(View v) const;
public:
    template <FragmentedView View> data_value deserialize_value(View v) const {
        return deserialize(v);
    }
    data_value deserialize_value(bytes_view v) const {
        return deserialize_impl(single_fragmented_view(v));
    }
};

// a list or a set
class listlike_collection_type_impl : public collection_type_impl {
protected:
    data_type _elements;
    explicit listlike_collection_type_impl(kind k, sstring name, data_type elements,bool is_multi_cell);
public:
    const data_type& get_elements_type() const { return _elements; }
    // A list or set value can be serialized as a vector<pair<timeuuid, data_value>> or
    // vector<pair<data_value, empty>> respectively. Compare this representation with
    // vector<data_value> without transforming either of the arguments. Since Cassandra doesn't
    // allow nested multi-cell collections this representation does not transcend to values, and we
    // don't need to worry about recursing.
    // @param this          type of the listlike value represented as vector<data_value>
    // @param map_type      type of the listlike value represented as vector<pair<data_value, data_value>>
    // @param list          listlike value, represented as vector<data_value>
    // @param map           listlike value represented as vector<pair<data_value, data_value>>
    //
    // This function is used to compare receiver with a literal or parameter marker during condition
    // evaluation.
    std::strong_ordering compare_with_map(const map_type_impl& map_type, bytes_view list, bytes_view map) const;
    // A list or set value can be represented as a vector<pair<timeuuid, data_value>> or
    // vector<pair<data_value, empty>> respectively. Serialize this representation
    // as a vector of values, not as a vector of pairs.
    bytes serialize_map(const map_type_impl& map_type, const data_value& value) const;

    // Verify that there are no NULL elements. Throws if there are.
    void validate_for_storage(const FragmentedView auto& value) const;
};

template <typename Iterator>
requires requires (Iterator it) { {*it} -> std::convertible_to<bytes_view_opt>; }
bytes
collection_type_impl::pack(Iterator start, Iterator finish, int elements) {
    size_t len = collection_size_len();
    size_t psz = collection_value_len();
    for (auto j = start; j != finish; j++) {
        auto v = bytes_view_opt(*j);
        len += (v ? v->size() : 0) + psz;
    }
    bytes out(bytes::initialized_later(), len);
    bytes::iterator i = out.begin();
    write_collection_size(i, elements);
    while (start != finish) {
        write_collection_value(i, *start++);
    }
    return out;
}

template <typename Iterator>
requires requires (Iterator it) { {*it} -> std::convertible_to<managed_bytes_view_opt>; }
managed_bytes
collection_type_impl::pack_fragmented(Iterator start, Iterator finish, int elements) {
    size_t len = collection_size_len();
    size_t psz = collection_value_len();
    for (auto j = start; j != finish; j++) {
        auto v = managed_bytes_view_opt(*j);
        len += (v ? v->size() : 0) + psz;
    }
    managed_bytes out(managed_bytes::initialized_later(), len);
    managed_bytes_mutable_view v(out);
    write_collection_size(v, elements);
    while (start != finish) {
        write_collection_value(v, *start++);
    }
    return out;
}

extern
template
void listlike_collection_type_impl::validate_for_storage(const managed_bytes_view& value) const;

extern
template
void listlike_collection_type_impl::validate_for_storage(const fragmented_temporary_buffer::view& value) const;

#include <seastar/core/shared_ptr.hh>
#include <seastar/core/sstring.hh>
#include <vector>


class user_type_impl;

namespace Json {
class Value;
}

class list_type_impl final : public concrete_type<std::vector<data_value>, listlike_collection_type_impl> {
    using list_type = shared_ptr<const list_type_impl>;
    using intern = type_interning_helper<list_type_impl, data_type, bool>;
public:
    static list_type get_instance(data_type elements, bool is_multi_cell);
    list_type_impl(data_type elements, bool is_multi_cell);
    virtual data_type name_comparator() const override;
    virtual data_type value_comparator() const override;
    virtual data_type freeze() const override;
    virtual bool is_compatible_with_frozen(const collection_type_impl& previous) const override;
    virtual bool is_value_compatible_with_frozen(const collection_type_impl& previous) const override;
    using abstract_type::deserialize;
    using collection_type_impl::deserialize;
    template <FragmentedView View> data_value deserialize(View v) const;
};

data_value make_list_value(data_type type, list_type_impl::native_type value);


#include <seastar/core/shared_ptr.hh>
#include <seastar/core/sstring.hh>
#include <vector>
#include <utility>


class user_type_impl;

namespace Json {
class Value;
}

class map_type_impl final : public concrete_type<std::vector<std::pair<data_value, data_value>>, collection_type_impl> {
    using map_type = shared_ptr<const map_type_impl>;
    using intern = type_interning_helper<map_type_impl, data_type, data_type, bool>;
    data_type _keys;
    data_type _values;
    data_type _key_value_pair_type;
public:
    static shared_ptr<const map_type_impl> get_instance(data_type keys, data_type values, bool is_multi_cell);
    map_type_impl(data_type keys, data_type values, bool is_multi_cell);
    const data_type& get_keys_type() const { return _keys; }
    const data_type& get_values_type() const { return _values; }
    virtual data_type name_comparator() const override { return _keys; }
    virtual data_type value_comparator() const override { return _values; }
    virtual data_type freeze() const override;
    virtual bool is_compatible_with_frozen(const collection_type_impl& previous) const override;
    virtual bool is_value_compatible_with_frozen(const collection_type_impl& previous) const override;
    static std::strong_ordering compare_maps(data_type keys_comparator, data_type values_comparator,
                        managed_bytes_view o1, managed_bytes_view o2);
    using abstract_type::deserialize;
    using collection_type_impl::deserialize;
    template <FragmentedView View> data_value deserialize(View v) const;
    static bytes serialize_partially_deserialized_form(const std::vector<std::pair<bytes_view, bytes_view>>& v);
    static managed_bytes serialize_partially_deserialized_form_fragmented(const std::vector<std::pair<managed_bytes_view, managed_bytes_view>>& v);

    // Serializes a map using the internal cql serialization format
    // Takes a range of pair<const bytes, bytes>
    template <std::ranges::range Range>
    requires std::convertible_to<std::ranges::range_value_t<Range>, std::pair<const bytes, bytes>>
    static bytes serialize_to_bytes(const Range& map_range);

    // Serializes a map using the internal cql serialization format
    // Takes a range of pair<const managed_bytes, managed_bytes>
    template <std::ranges::range Range>
    requires std::convertible_to<std::ranges::range_value_t<Range>, std::pair<const managed_bytes, managed_bytes>>
    static managed_bytes serialize_to_managed_bytes(const Range& map_range);
};

data_value make_map_value(data_type tuple_type, map_type_impl::native_type value);

template <std::ranges::range Range>
requires std::convertible_to<std::ranges::range_value_t<Range>, std::pair<const bytes, bytes>>
bytes map_type_impl::serialize_to_bytes(const Range& map_range) {
    size_t serialized_len = 4;
    size_t map_size = 0;
    for (const std::pair<const bytes, bytes>& elem : map_range) {
        serialized_len += 4 + elem.first.size() + 4 + elem.second.size();
        map_size += 1;
    }

    if (map_size > std::numeric_limits<int32_t>::max()) {
        throw exceptions::invalid_request_exception(
            fmt::format("Map size too large: {} > {}", map_size, std::numeric_limits<int32_t>::max()));
    }

    bytes result(bytes::initialized_later(), serialized_len);
    bytes::iterator out = result.begin();

    write_collection_size(out, map_size);
    for (const std::pair<const bytes, bytes>& elem : map_range) {
        if (elem.first.size() > std::numeric_limits<int32_t>::max()) {
            throw exceptions::invalid_request_exception(
                fmt::format("Map key size too large: {} bytes > {}", map_size, std::numeric_limits<int32_t>::max()));
        }

        if (elem.second.size() > std::numeric_limits<int32_t>::max()) {
            throw exceptions::invalid_request_exception(
                fmt::format("Map value size too large: {} bytes > {}", map_size, std::numeric_limits<int32_t>::max()));
        }

        write_collection_value(out, elem.first);
        write_collection_value(out, elem.second);
    }

    return result;
}

template <std::ranges::range Range>
requires std::convertible_to<std::ranges::range_value_t<Range>, std::pair<const managed_bytes, managed_bytes>>
managed_bytes map_type_impl::serialize_to_managed_bytes(const Range& map_range) {
    size_t serialized_len = 4;
    size_t map_size = 0;
    for (const std::pair<const managed_bytes, managed_bytes>& elem : map_range) {
        serialized_len += 4 + elem.first.size() + 4 + elem.second.size();
        map_size += 1;
    }

    if (map_size > std::numeric_limits<int32_t>::max()) {
        throw exceptions::invalid_request_exception(
            fmt::format("Map size too large: {} > {}", map_size, std::numeric_limits<int32_t>::max()));
    }

    managed_bytes result(managed_bytes::initialized_later(), serialized_len);
    managed_bytes_mutable_view out(result);

    write_collection_size(out, map_size);
    for (const std::pair<const managed_bytes, managed_bytes>& elem : map_range) {
        if (elem.first.size() > std::numeric_limits<int32_t>::max()) {
            throw exceptions::invalid_request_exception(
                fmt::format("Map key size too large: {} bytes > {}", map_size, std::numeric_limits<int32_t>::max()));
        }

        if (elem.second.size() > std::numeric_limits<int32_t>::max()) {
            throw exceptions::invalid_request_exception(
                fmt::format("Map value size too large: {} bytes > {}", map_size, std::numeric_limits<int32_t>::max()));
        }

        write_collection_value(out, elem.first);
        write_collection_value(out, elem.second);
    }

    return result;
}


#include <seastar/core/shared_ptr.hh>
#include <seastar/core/sstring.hh>
#include <vector>

class user_type_impl;

namespace Json {
class Value;
}

class set_type_impl final : public concrete_type<std::vector<data_value>, listlike_collection_type_impl> {
    using set_type = shared_ptr<const set_type_impl>;
    using intern = type_interning_helper<set_type_impl, data_type, bool>;
public:
    static set_type get_instance(data_type elements, bool is_multi_cell);
    set_type_impl(data_type elements, bool is_multi_cell);
    virtual data_type name_comparator() const override { return _elements; }
    virtual data_type value_comparator() const override;
    virtual data_type freeze() const override;
    virtual bool is_compatible_with_frozen(const collection_type_impl& previous) const override;
    virtual bool is_value_compatible_with_frozen(const collection_type_impl& previous) const override;
    using abstract_type::deserialize;
    using collection_type_impl::deserialize;
    template <FragmentedView View> data_value deserialize(View v) const;
    static bytes serialize_partially_deserialized_form(
            const std::vector<bytes_view>& v);
    static managed_bytes serialize_partially_deserialized_form_fragmented(
            const std::vector<managed_bytes_view_opt>& v);
};

data_value make_set_value(data_type tuple_type, set_type_impl::native_type value);

template <typename NativeType>
data_value::data_value(const std::unordered_set<NativeType>& v)
    : data_value(new set_type_impl::native_type(v.begin(), v.end()), set_type_impl::get_instance(data_type_for<NativeType>(), true))
{}


#include <iterator>
#include <vector>
#include <string>

#include <boost/range/numeric.hpp>
#include <boost/range/adaptor/transformed.hpp>
#include <boost/range/algorithm/for_each.hpp>

struct tuple_deserializing_iterator {
public:
    using iterator_category = std::input_iterator_tag;
    using value_type = const managed_bytes_view_opt;
    using difference_type = std::ptrdiff_t;
    using pointer = const managed_bytes_view_opt*;
    using reference = const managed_bytes_view_opt&;
private:
    managed_bytes_view _v;
    managed_bytes_view_opt _current;
public:
    struct end_tag {};
    tuple_deserializing_iterator(managed_bytes_view v) : _v(v) {
        parse();
    }
    tuple_deserializing_iterator(end_tag, managed_bytes_view v) : _v(v) {
        _v.remove_prefix(_v.size());
    }
    static tuple_deserializing_iterator start(managed_bytes_view v) {
        return tuple_deserializing_iterator(v);
    }
    static tuple_deserializing_iterator finish(managed_bytes_view v) {
        return tuple_deserializing_iterator(end_tag(), v);
    }
    const managed_bytes_view_opt& operator*() const {
        return _current;
    }
    const managed_bytes_view_opt* operator->() const {
        return &_current;
    }
    tuple_deserializing_iterator& operator++() {
        skip();
        parse();
        return *this;
    }
    void operator++(int) {
        skip();
        parse();
    }
    bool operator==(const tuple_deserializing_iterator& x) const {
        return _v == x._v;
    }
private:
    void parse() {
        _current = std::nullopt;
        if (_v.empty()) {
            return;
        }
        // we don't consume _v, otherwise operator==
        // or the copy constructor immediately after
        // parse() yields the wrong results.
        auto tmp = _v;
        auto s = read_simple<int32_t>(tmp);
        if (s < 0) {
            return;
        }
        _current = read_simple_bytes(tmp, s);
    }
    void skip() {
        _v.remove_prefix(4 + (_current ? _current->size() : 0));
    }
};

template <FragmentedView View>
std::optional<View> read_tuple_element(View& v) {
    auto s = read_simple<int32_t>(v);
    if (s < 0) {
        return std::nullopt;
    }
    return read_simple_bytes(v, s);
}

template <FragmentedView View>
managed_bytes_opt get_nth_tuple_element(View v, size_t n) {
    for (size_t i = 0; i < n; ++i) {
        if (v.empty()) {
            return std::nullopt;
        }
        read_tuple_element(v);
    }
    if (v.empty()) {
        return std::nullopt;
    }
    auto el = read_tuple_element(v);
    if (el) {
        return managed_bytes(*el);
    }
    return std::nullopt;
}

class tuple_type_impl : public concrete_type<std::vector<data_value>> {
    using intern = type_interning_helper<tuple_type_impl, std::vector<data_type>>;
protected:
    std::vector<data_type> _types;
    static boost::iterator_range<tuple_deserializing_iterator> make_range(managed_bytes_view v) {
        return { tuple_deserializing_iterator::start(v), tuple_deserializing_iterator::finish(v) };
    }
    tuple_type_impl(kind k, sstring name, std::vector<data_type> types, bool freeze_inner);
    tuple_type_impl(std::vector<data_type> types, bool freze_inner);
public:
    tuple_type_impl(std::vector<data_type> types);
    static shared_ptr<const tuple_type_impl> get_instance(std::vector<data_type> types);
    data_type type(size_t i) const {
        return _types[i];
    }
    size_t size() const {
        return _types.size();
    }
    const std::vector<data_type>& all_types() const {
        return _types;
    }
    std::vector<bytes_opt> split(FragmentedView auto v) const {
        std::vector<bytes_opt> elements;
        while (!v.empty()) {
            auto fragmented_element_optional = read_tuple_element(v);
            if (fragmented_element_optional) {
                elements.push_back(linearized(*fragmented_element_optional));
            } else {
                elements.push_back(std::nullopt);
            }
        }
        return elements;
    }
    std::vector<managed_bytes_opt> split_fragmented(FragmentedView auto v) const {
        std::vector<managed_bytes_opt> elements;
        while (!v.empty()) {
            auto fragmented_element_optional = read_tuple_element(v);
            if (fragmented_element_optional) {
                elements.push_back(managed_bytes(*fragmented_element_optional));
            } else {
                elements.push_back(std::nullopt);
            }
        }
        return elements;
    }
    template <typename RangeOf_bytes_opt>  // also accepts bytes_view_opt
    static bytes build_value(RangeOf_bytes_opt&& range) {
        auto item_size = [] (auto&& v) { return 4 + (v ? v->size() : 0); };
        auto size = boost::accumulate(range | boost::adaptors::transformed(item_size), 0);
        auto ret = bytes(bytes::initialized_later(), size);
        auto out = ret.begin();
        auto put = [&out] (auto&& v) {
            if (v) {
                using val_type = std::remove_cvref_t<decltype(*v)>;
                if constexpr (FragmentedView<val_type>) {
                    int32_t size = v->size_bytes();
                    write(out, size);
                    read_fragmented(*v, size, out);
                    out += size;
                } else {
                    write(out, int32_t(v->size()));
                    out = std::copy(v->begin(), v->end(), out);
                }
            } else {
                write(out, int32_t(-1));
            }
        };
        boost::range::for_each(range, put);
        return ret;
    }
    template <typename Range> // range of managed_bytes_opt or managed_bytes_view_opt
    requires requires (Range it) { {std::begin(it)->value()} -> std::convertible_to<managed_bytes_view>; }
    static managed_bytes build_value_fragmented(Range&& range) {
        size_t size = 0;
        for (auto&& v : range) {
            size += 4 + (v ? v->size() : 0);
        }
        auto ret = managed_bytes(managed_bytes::initialized_later(), size);
        auto out = managed_bytes_mutable_view(ret);
        for (auto&& v : range) {
            if (v) {
                write<int32_t>(out, v->size());
                write_fragmented(out, managed_bytes_view(*v));
            } else {
                write<int32_t>(out, -1);
            }
        }
        return ret;
    }
private:
    void set_contains_collections();
    static sstring make_name(const std::vector<data_type>& types);
    friend abstract_type;
};

data_value make_tuple_value(data_type tuple_type, tuple_type_impl::native_type value);




class user_type_impl : public tuple_type_impl, public data_dictionary::keyspace_element {
    using intern = type_interning_helper<user_type_impl, sstring, bytes, std::vector<bytes>, std::vector<data_type>, bool>;
public:
    const sstring _keyspace;
    const bytes _name;
private:
    const std::vector<bytes> _field_names;
    const std::vector<sstring> _string_field_names;
    const bool _is_multi_cell;
public:
    using native_type = std::vector<data_value>;
    user_type_impl(sstring keyspace, bytes name, std::vector<bytes> field_names, std::vector<data_type> field_types, bool is_multi_cell)
            : tuple_type_impl(kind::user, make_name(keyspace, name, field_names, field_types, is_multi_cell), field_types, false /* don't freeze inner */)
            , _keyspace(std::move(keyspace))
            , _name(std::move(name))
            , _field_names(std::move(field_names))
            , _string_field_names(boost::copy_range<std::vector<sstring>>(_field_names | boost::adaptors::transformed(
                    [] (const bytes& field_name) { return utf8_type->to_string(field_name); })))
            , _is_multi_cell(is_multi_cell) {
    }
    static shared_ptr<const user_type_impl> get_instance(sstring keyspace, bytes name,
            std::vector<bytes> field_names, std::vector<data_type> field_types, bool multi_cell);
    data_type field_type(size_t i) const { return type(i); }
    const std::vector<data_type>& field_types() const { return _types; }
    bytes_view field_name(size_t i) const { return _field_names[i]; }
    sstring field_name_as_string(size_t i) const { return _string_field_names[i]; }
    const std::vector<bytes>& field_names() const { return _field_names; }
    const std::vector<sstring>& string_field_names() const { return _string_field_names; }
    std::optional<size_t> idx_of_field(const bytes& name) const;
    bool is_multi_cell() const { return _is_multi_cell; }
    virtual data_type freeze() const override;
    bytes get_name() const { return _name; }
    sstring get_name_as_string() const;
    sstring get_name_as_cql_string() const;

    virtual sstring keypace_name() const override { return _keyspace; }
    virtual sstring element_name() const override { return get_name_as_string(); }
    virtual sstring element_type() const override { return "type"; }
    virtual std::ostream& describe(std::ostream& os) const override;

private:
    static sstring make_name(sstring keyspace,
                             bytes name,
                             std::vector<bytes> field_names,
                             std::vector<data_type> field_types,
                             bool is_multi_cell);
};

data_value make_user_value(data_type tuple_type, user_type_impl::native_type value);

constexpr size_t max_udt_fields = std::numeric_limits<int16_t>::max();

// The following two functions are used to translate field indices (used to identify fields inside non-frozen UDTs)
// from/to a serialized bytes representation to be stored in mutations and sstables.
// Refer to collection_mutation.hh for a detailed description on how the serialized indices are used inside mutations.
bytes serialize_field_index(size_t);
size_t deserialize_field_index(const bytes_view&);
size_t deserialize_field_index(managed_bytes_view);



#include <boost/multiprecision/cpp_int.hpp>
#include <iosfwd>
#include <compare>

namespace utils {

// multiprecision_int is a thin wrapper around boost::multiprecision::cpp_int.
// cpp_int is worth about 20,000 lines of header files that are rarely used.
// Forward-declaring cpp_int is very difficult as it is a complicated template,
// hence this wrapper.
//
// Because cpp_int uses a lot of expression templates, the code below contains
// many casts since the expression templates defeat regular C++ conversion rules.

class multiprecision_int final {
public:
    using cpp_int = boost::multiprecision::cpp_int;
private:
    cpp_int _v;
private:
    // maybe_unwrap() selectively unwraps multiprecision_int values (leaving
    // anything else unchanged), so avoid confusing boost::multiprecision.
    static const cpp_int& maybe_unwrap(const multiprecision_int& x) {
        return x._v;
    }
    template <typename T>
    static const T& maybe_unwrap(const T& x) {
        return x;
    }
public:
    multiprecision_int() = default;
    multiprecision_int(cpp_int x) : _v(std::move(x)) {}
    explicit multiprecision_int(int x) : _v(x) {}
    explicit multiprecision_int(unsigned x) : _v(x) {}
    explicit multiprecision_int(long x) : _v(x) {}
    explicit multiprecision_int(unsigned long x) : _v(x) {}
    explicit multiprecision_int(long long x) : _v(x) {}
    explicit multiprecision_int(unsigned long long x) : _v(x) {}
    explicit multiprecision_int(float x) : _v(x) {}
    explicit multiprecision_int(double x) : _v(x) {}
    explicit multiprecision_int(long double x) : _v(x) {}
    explicit multiprecision_int(const std::string x) : _v(x) {}
    explicit multiprecision_int(const char* x) : _v(x) {}
    operator const cpp_int&() const {
        return _v;
    }
    explicit operator signed char() const {
        return static_cast<signed char>(_v);
    }
    explicit operator unsigned char() const {
        return static_cast<unsigned char>(_v);
    }
    explicit operator short() const {
        return static_cast<short>(_v);
    }
    explicit operator unsigned short() const {
        return static_cast<unsigned short>(_v);
    }
    explicit operator int() const {
        return static_cast<int>(_v);
    }
    explicit operator unsigned() const {
        return static_cast<unsigned>(_v);
    }
    explicit operator long() const {
        return static_cast<long>(_v);
    }
    explicit operator unsigned long() const {
        return static_cast<unsigned long>(_v);
    }
    explicit operator long long() const {
        return static_cast<long long>(_v);
    }
    explicit operator unsigned long long() const {
        return static_cast<unsigned long long>(_v);
    }
    explicit operator float() const {
        return static_cast<float>(_v);
    }
    explicit operator double() const {
        return static_cast<double>(_v);
    }
    explicit operator long double() const {
        return static_cast<long double>(_v);
    }
    template <typename T>
    multiprecision_int& operator+=(const T& x) {
        _v += maybe_unwrap(x);
        return *this;
    }
    template <typename T>
    multiprecision_int& operator-=(const T& x) {
        _v -= maybe_unwrap(x);
        return *this;
    }
    template <typename T>
    multiprecision_int& operator*=(const T& x) {
        _v *= maybe_unwrap(x);
        return *this;
    }
    template <typename T>
    multiprecision_int& operator/=(const T& x) {
        _v /= maybe_unwrap(x);
        return *this;
    }
    template <typename T>
    multiprecision_int& operator%=(const T& x) {
        _v %= maybe_unwrap(x);
        return *this;
    }
    template <typename T>
    multiprecision_int& operator<<=(const T& x) {
        _v <<= maybe_unwrap(x);
        return *this;
    }
    template <typename T>
    multiprecision_int& operator>>=(const T& x) {
        _v >>= maybe_unwrap(x);
        return *this;
    }
    multiprecision_int operator-() const {
        return cpp_int(-_v);
    }
    multiprecision_int operator+(const multiprecision_int& x) const {
        return cpp_int(_v + x._v);
    }
    template <typename T>
    multiprecision_int operator+(const T& x) const {
        return cpp_int(_v + maybe_unwrap(x));
    }
    template <typename T>
    multiprecision_int operator-(const T& x) const {
        return cpp_int(_v - maybe_unwrap(x));
    }
    template <typename T>
    multiprecision_int operator*(const T& x) const {
        return cpp_int(_v * maybe_unwrap(x));
    }
    template <typename T>
    multiprecision_int operator/(const T& x) const {
        return cpp_int(_v / maybe_unwrap(x));
    }
    template <typename T>
    multiprecision_int operator%(const T& x) const {
        return cpp_int(_v % maybe_unwrap(x));
    }
    template <typename T>
    multiprecision_int operator<<(const T& x) const {
        return cpp_int(_v << maybe_unwrap(x));
    }
    template <typename T>
    multiprecision_int operator>>(const T& x) const {
        return cpp_int(_v >> maybe_unwrap(x));
    }
    std::strong_ordering operator<=>(const multiprecision_int& x) const = default;
    template <typename T>
    bool operator==(const T& x) const {
        return _v == maybe_unwrap(x);
    }
    template <typename T>
    bool operator>(const T& x) const {
        return _v > maybe_unwrap(x);
    }
    template <typename T>
    bool operator>=(const T& x) const {
        return _v >= maybe_unwrap(x);
    }
    template <typename T>
    bool operator<(const T& x) const {
        return _v < maybe_unwrap(x);
    }
    template <typename T>
    bool operator<=(const T& x) const {
        return _v <= maybe_unwrap(x);
    }
    template <typename T>
    friend multiprecision_int operator+(const T& x, const multiprecision_int& y) {
        return cpp_int(maybe_unwrap(x) + y._v);
    }
    template <typename T>
    friend multiprecision_int operator-(const T& x, const multiprecision_int& y) {
        return cpp_int(maybe_unwrap(x) - y._v);
    }
    template <typename T>
    friend multiprecision_int operator*(const T& x, const multiprecision_int& y) {
        return cpp_int(maybe_unwrap(x) * y._v);
    }
    template <typename T>
    friend multiprecision_int operator/(const T& x, const multiprecision_int& y) {
        return cpp_int(maybe_unwrap(x) / y._v);
    }
    template <typename T>
    friend multiprecision_int operator%(const T& x, const multiprecision_int& y) {
        return cpp_int(maybe_unwrap(x) % y._v);
    }
    template <typename T>
    friend multiprecision_int operator<<(const T& x, const multiprecision_int& y) {
        return cpp_int(maybe_unwrap(x) << y._v);
    }
    template <typename T>
    friend multiprecision_int operator>>(const T& x, const multiprecision_int& y) {
        return cpp_int(maybe_unwrap(x) >> y._v);
    }
    std::string str() const;
    friend std::ostream& operator<<(std::ostream& os, const multiprecision_int& x);
};


}

#include <boost/multiprecision/cpp_int.hpp>
#include <ostream>
#include <compare>
#include <concepts>


uint64_t from_varint_to_integer(const utils::multiprecision_int& varint);

class big_decimal {
private:
    int32_t _scale;
    boost::multiprecision::cpp_int _unscaled_value;
public:
    enum class rounding_mode {
        HALF_EVEN,
    };

    explicit big_decimal(sstring_view text);
    big_decimal();
    big_decimal(int32_t scale, boost::multiprecision::cpp_int unscaled_value);
    big_decimal(std::integral auto v) : big_decimal(0, v) {}

    int32_t scale() const { return _scale; }
    const boost::multiprecision::cpp_int& unscaled_value() const { return _unscaled_value; }
    boost::multiprecision::cpp_rational as_rational() const;

    sstring to_string() const;

    std::strong_ordering operator<=>(const big_decimal& other) const;

    big_decimal& operator+=(const big_decimal& other);
    big_decimal& operator-=(const big_decimal& other);
    big_decimal operator+(const big_decimal& other) const;
    big_decimal operator-(const big_decimal& other) const;
    big_decimal div(const ::uint64_t y, const rounding_mode mode) const;
};

template <>
struct fmt::formatter<big_decimal> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const big_decimal& v, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", v.to_string());
    }
};



struct empty_type_impl final : public abstract_type {
    using native_type = empty_type_representation;
    empty_type_impl();
};

struct counter_type_impl final : public abstract_type {
    counter_type_impl();
};

template <typename T>
struct simple_type_impl : public concrete_type<T> {
    simple_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed);
};

template<typename T>
struct integer_type_impl : public simple_type_impl<T> {
    integer_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed);
};

struct byte_type_impl final : public integer_type_impl<int8_t> {
    byte_type_impl();
};

struct short_type_impl final : public integer_type_impl<int16_t> {
    short_type_impl();
};

struct int32_type_impl final : public integer_type_impl<int32_t> {
    int32_type_impl();
};

struct long_type_impl final : public integer_type_impl<int64_t> {
    long_type_impl();
};

struct boolean_type_impl final : public simple_type_impl<bool> {
    boolean_type_impl();
};

template <typename T>
struct floating_type_impl : public simple_type_impl<T> {
    floating_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed);
};

struct double_type_impl final : public floating_type_impl<double> {
    double_type_impl();
};

struct float_type_impl final : public floating_type_impl<float> {
    float_type_impl();
};

struct decimal_type_impl final : public concrete_type<big_decimal> {
    decimal_type_impl();
};

struct duration_type_impl final : public concrete_type<cql_duration> {
    duration_type_impl();
};

struct timestamp_type_impl final : public simple_type_impl<db_clock::time_point> {
    timestamp_type_impl();
    static db_clock::time_point from_sstring(sstring_view s);
};

struct simple_date_type_impl final : public simple_type_impl<uint32_t> {
    simple_date_type_impl();
    static uint32_t from_sstring(sstring_view s);
};

struct time_type_impl final : public simple_type_impl<int64_t> {
    time_type_impl();
    static int64_t from_sstring(sstring_view s);
};

struct string_type_impl : public concrete_type<sstring> {
    string_type_impl(kind k, sstring name);
};

struct ascii_type_impl final : public string_type_impl {
    ascii_type_impl();
};

struct utf8_type_impl final : public string_type_impl {
    utf8_type_impl();
};

struct bytes_type_impl final : public concrete_type<bytes> {
    bytes_type_impl();
};

// This is the old version of timestamp_type_impl, but has been replaced as it
// wasn't comparing pre-epoch timestamps correctly. This is kept for backward
// compatibility but shouldn't be used in new code.
struct date_type_impl final : public concrete_type<db_clock::time_point> {
    date_type_impl();
};

using timestamp_date_base_class = concrete_type<db_clock::time_point>;

struct timeuuid_type_impl final : public concrete_type<utils::UUID> {
    timeuuid_type_impl();
    static utils::UUID from_sstring(sstring_view s);
};

struct varint_type_impl final : public concrete_type<utils::multiprecision_int> {
    varint_type_impl();
};

struct inet_addr_type_impl final : public concrete_type<seastar::net::inet_address> {
    inet_addr_type_impl();
    static sstring to_sstring(const seastar::net::inet_address& addr);
    static seastar::net::inet_address from_sstring(sstring_view s);
};

struct uuid_type_impl final : public concrete_type<utils::UUID> {
    uuid_type_impl();
    static utils::UUID from_sstring(sstring_view s);
};

template <typename Func> using visit_ret_type = std::invoke_result_t<Func, const ascii_type_impl&>;

template <typename Func> concept CanHandleAllTypes = requires(Func f) {
    { f(*static_cast<const ascii_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const boolean_type_impl*>(nullptr)) }     -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const byte_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const bytes_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const counter_type_impl*>(nullptr)) }     -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const date_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const decimal_type_impl*>(nullptr)) }     -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const double_type_impl*>(nullptr)) }      -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const duration_type_impl*>(nullptr)) }    -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const empty_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const float_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const inet_addr_type_impl*>(nullptr)) }   -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const int32_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const list_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const long_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const map_type_impl*>(nullptr)) }         -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const reversed_type_impl*>(nullptr)) }    -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const set_type_impl*>(nullptr)) }         -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const short_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const simple_date_type_impl*>(nullptr)) } -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const time_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const timestamp_type_impl*>(nullptr)) }   -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const timeuuid_type_impl*>(nullptr)) }    -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const tuple_type_impl*>(nullptr)) }       -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const user_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const utf8_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const uuid_type_impl*>(nullptr)) }        -> std::same_as<visit_ret_type<Func>>;
    { f(*static_cast<const varint_type_impl*>(nullptr)) }      -> std::same_as<visit_ret_type<Func>>;
};

template<typename Func>
requires CanHandleAllTypes<Func>
static inline visit_ret_type<Func> visit(const abstract_type& t, Func&& f) {
    switch (t.get_kind()) {
    case abstract_type::kind::ascii:
        return f(*static_cast<const ascii_type_impl*>(&t));
    case abstract_type::kind::boolean:
        return f(*static_cast<const boolean_type_impl*>(&t));
    case abstract_type::kind::byte:
        return f(*static_cast<const byte_type_impl*>(&t));
    case abstract_type::kind::bytes:
        return f(*static_cast<const bytes_type_impl*>(&t));
    case abstract_type::kind::counter:
        return f(*static_cast<const counter_type_impl*>(&t));
    case abstract_type::kind::date:
        return f(*static_cast<const date_type_impl*>(&t));
    case abstract_type::kind::decimal:
        return f(*static_cast<const decimal_type_impl*>(&t));
    case abstract_type::kind::double_kind:
        return f(*static_cast<const double_type_impl*>(&t));
    case abstract_type::kind::duration:
        return f(*static_cast<const duration_type_impl*>(&t));
    case abstract_type::kind::empty:
        return f(*static_cast<const empty_type_impl*>(&t));
    case abstract_type::kind::float_kind:
        return f(*static_cast<const float_type_impl*>(&t));
    case abstract_type::kind::inet:
        return f(*static_cast<const inet_addr_type_impl*>(&t));
    case abstract_type::kind::int32:
        return f(*static_cast<const int32_type_impl*>(&t));
    case abstract_type::kind::list:
        return f(*static_cast<const list_type_impl*>(&t));
    case abstract_type::kind::long_kind:
        return f(*static_cast<const long_type_impl*>(&t));
    case abstract_type::kind::map:
        return f(*static_cast<const map_type_impl*>(&t));
    case abstract_type::kind::reversed:
        return f(*static_cast<const reversed_type_impl*>(&t));
    case abstract_type::kind::set:
        return f(*static_cast<const set_type_impl*>(&t));
    case abstract_type::kind::short_kind:
        return f(*static_cast<const short_type_impl*>(&t));
    case abstract_type::kind::simple_date:
        return f(*static_cast<const simple_date_type_impl*>(&t));
    case abstract_type::kind::time:
        return f(*static_cast<const time_type_impl*>(&t));
    case abstract_type::kind::timestamp:
        return f(*static_cast<const timestamp_type_impl*>(&t));
    case abstract_type::kind::timeuuid:
        return f(*static_cast<const timeuuid_type_impl*>(&t));
    case abstract_type::kind::tuple:
        return f(*static_cast<const tuple_type_impl*>(&t));
    case abstract_type::kind::user:
        return f(*static_cast<const user_type_impl*>(&t));
    case abstract_type::kind::utf8:
        return f(*static_cast<const utf8_type_impl*>(&t));
    case abstract_type::kind::uuid:
        return f(*static_cast<const uuid_type_impl*>(&t));
    case abstract_type::kind::varint:
        return f(*static_cast<const varint_type_impl*>(&t));
    }
    __builtin_unreachable();
}

template <typename Func> struct data_value_visitor {
    const void* v;
    Func& f;
    auto operator()(const empty_type_impl& t) { return f(t, v); }
    auto operator()(const counter_type_impl& t) { return f(t, v); }
    auto operator()(const reversed_type_impl& t) { return f(t, v); }
    template <typename T> auto operator()(const T& t) {
        return f(t, reinterpret_cast<const typename T::native_type*>(v));
    }
};

// Given an abstract_type and a void pointer to an object of that
// type, call f with the runtime type of t and v casted to the
// corresponding native type.
// This takes an abstract_type and a void pointer instead of a
// data_value to support reversed_type_impl without requiring that
// each visitor create a new data_value just to recurse.
template <typename Func> inline auto visit(const abstract_type& t, const void* v, Func&& f) {
    return ::visit(t, data_value_visitor<Func>{v, f});
}

template <typename Func> inline auto visit(const data_value& v, Func&& f) {
    return ::visit(*v.type(), v._value, f);
}


#include <iterator>
#include <boost/regex.hpp>

#include <yaml-cpp/yaml.h>
#include <boost/any.hpp>

#include <seastar/core/coroutine.hh>
#include <seastar/core/smp.hh>


#include <seastar/util/noncopyable_function.hh>
#include <vector>
#include <boost/range/algorithm/replace.hpp>
#include <boost/range/algorithm/remove.hpp>

namespace utils {

// observable/observer - a publish/subscribe utility
//
// An observable is an object that can broadcast notifications
// about changes in some state. An observer listens for such notifications
// in a particular observable it is connected to. Multiple observers can
// observe a single observable.
//
// A connection between an observer and an observable is established when
// the observer is constructed (using observable::observe()); from then
// on their life cycles are separate, either can be moved or destroyed
// without affecting the other.
//
// During construction, the observer specifies how to react to a change
// in the observable's state by specifying a function to be called on
// a state change. An observable causes the function to be executed
// by calling its operator()() method.
//
// All observers are called without preemption, so an observer should have
// a small number of observers.

template <typename... Args>
class observable {
public:
    class observer;
private:
    std::vector<observer*> _observers;
public:
    class observer {
        friend class observable;
        observable* _observable;
        seastar::noncopyable_function<void (Args...)> _callback;
    private:
        void moved(observer* from) {
            if (_observable) {
                _observable->moved(from, this);
            }
        }
    public:
        observer(observable* o, seastar::noncopyable_function<void (Args...)> callback) noexcept
                : _observable(o), _callback(std::move(callback)) {
        }
        observer(observer&& o) noexcept
                : _observable(std::exchange(o._observable, nullptr))
                , _callback(std::move(o._callback)) {
            moved(&o);
        }
        observer& operator=(observer&& o) noexcept {
            if (this != &o) {
                disconnect();
                _observable = std::exchange(o._observable, nullptr);
                _callback = std::move(o._callback);
                moved(&o);
            }
            return *this;
        }
        ~observer() {
            disconnect();
        }
        // Stops observing the observable immediately, instead of
        // during destruction.
        void disconnect() {
            if (_observable) {
                _observable->destroyed(this);
            }
            _observable = nullptr;
        }
    };
    friend class observer;
private:
    void destroyed(observer* dead) {
        _observers.erase(boost::remove(_observers, dead), _observers.end());
    }
    void moved(observer* from, observer* to) {
        boost::replace(_observers, from, to);
    }
    void update_observers(observable* ob) {
        for (auto&& c : _observers) {
            c->_observable = ob;
        }
    }
public:
    observable() = default;
    observable(observable&& o) noexcept
            : _observers(std::move(o._observers)) {
        update_observers(this);
    }
    observable& operator=(observable&& o) noexcept {
        if (this != &o) {
            update_observers(nullptr);
            _observers = std::move(o._observers);
            update_observers(this);
        }
        return *this;
    }
    ~observable() {
        update_observers(nullptr);
    }
    // Send args to all connected observers
    void operator()(Args... args) const {
        std::exception_ptr e;
        for (auto&& ob : _observers) {
            try {
                ob->_callback(args...);
            } catch (...) {
                if (!e) {
                    e = std::current_exception();
                }
            }
        }
        if (e) {
            std::rethrow_exception(std::move(e));
        }
    }
    // Adds an observer to an observable
    observer observe(std::function<void (Args...)> callback) {
        observer ob(this, std::move(callback));
        _observers.push_back(&ob);
        return ob;
    }
};

// An observer<Args...> can receive notifications about changes
// in an observable<Args...>'s state.
template <typename... Args>
using observer = typename observable<Args...>::observer;

template <typename... Args>
inline observer<Args...> dummy_observer() {
    return observer<Args...>(nullptr, seastar::noncopyable_function<void(Args...)>());
}

}


#include <functional>
#include <seastar/core/semaphore.hh>
#include <seastar/core/future.hh>
#include <seastar/core/shared_future.hh>
#include <seastar/util/later.hh>
#include <seastar/core/abort_source.hh>

// An async action wrapper which ensures that at most one action
// is running at any time.
class serialized_action {
public:
    template <typename... T>
    using future = seastar::future<T...>;
private:
    std::function<future<>()> _func;
    seastar::shared_future<> _pending;
    seastar::semaphore _sem;
private:
    future<> do_trigger() {
        _pending = {};
        return futurize_invoke(_func);
    }
public:
    serialized_action(std::function<future<>()> func)
        : _func(std::move(func))
        , _sem(1)
    { }
    serialized_action(serialized_action&&) = delete;
    serialized_action(const serialized_action&) = delete;

    // Makes sure that a new action will be started after this call and
    // returns a future which resolves when that action completes.
    // At most one action can run at any given moment.
    // A single action is started on behalf of all earlier triggers.
    //
    // When action is not currently running, it is started immediately if !later or
    // at some point in time soon after current fiber defers when later is true.
    future<> trigger(bool later = false, seastar::abort_source* as = nullptr) {
        if (_pending.valid()) {
            return as ? _pending.get_future(*as) : _pending.get_future();
        }
        seastar::shared_promise<> pr;
        _pending = pr.get_shared_future();
        future<> ret = _pending;
        std::optional<future<>> abortable;

        if (as) {
            abortable = _pending.get_future(*as);
        }

        // run in background, synchronize using `ret`
        (void)_sem.wait().then([this, later] () mutable {
            if (later) {
                return seastar::yield().then([this] () mutable {
                    return do_trigger();
                });
            }
            return do_trigger();
        }).then_wrapped([pr = std::move(pr)] (auto&& f) mutable {
            if (f.failed()) {
                pr.set_exception(f.get_exception());
            } else {
                pr.set_value();
            }
        });

        ret = ret.finally([this] {
            _sem.signal();
        });

        if (abortable) {
            // exception will be delivered to each individual future as well
            (void)std::move(ret).handle_exception([] (std::exception_ptr ep) {});
            return std::move(*abortable);
        }

        return ret;
    }

    // Like trigger() but can be aborted
    future<> trigger(seastar::abort_source& as) {
        return trigger(false, &as);
    }

    // Like trigger(), but defers invocation of the action to allow for batching
    // more requests.
    future<> trigger_later() {
        return trigger(true);
    }

    // Waits for all invocations initiated in the past.
    future<> join() {
        return get_units(_sem, 1).discard_result();
    }

    // The adaptor is to be used as an argument to utils::observable.observe()
    // When the notification happens the adaptor just triggers the action
    // Note, that all arguments provided by the notification callback are lost,
    // its up to the action to get the needed values
    // Also, the future<> returned from .trigger() is ignored, the action code
    // runs in the background. The user should .join() the action if it needs
    // to wait for it to finish on stop/drain/shutdown
    class observing_adaptor {
        friend class serialized_action;
        serialized_action& _action;
        observing_adaptor(serialized_action& act) noexcept : _action(act) {}

    public:
        template <typename... Args>
        void operator()(Args&&...) { (void)_action.trigger(); };
    };

    observing_adaptor make_observer() noexcept {
        return observing_adaptor(*this);
    }
};


#include <seastar/core/future.hh>
#include <vector>
#include <functional>

namespace utils {

// This file contains two templates, updateable_value_source<T> and updateable_value<T>.
//
// The two are analogous to T and const T& respectively, with the following additional
// functionality:
//
//  - updateable_value contains a copy of T, so it can be accessed without indirection
//  - updateable_value and updateable_value_source track each other, so if they move,
//    the references are updated
//  - an observe() function is provided (to both) that can be used to attach a callback
//    that is called whenever the value changes

template <typename T>
class updateable_value_source;

class updateable_value_source_base;

// Base class for updateable_value<T>, containing functionality for tracking
// the update source. Used to reduce template bloat and not meant to be used
// directly.
class updateable_value_base {
protected:
    const updateable_value_source_base* _source = nullptr;
public:
    updateable_value_base() = default;
    explicit updateable_value_base(const updateable_value_source_base& source);
    ~updateable_value_base();
    updateable_value_base(const updateable_value_base&);
    updateable_value_base& operator=(const updateable_value_base&);
    updateable_value_base(updateable_value_base&&) noexcept;
    updateable_value_base& operator=(updateable_value_base&&) noexcept;
    updateable_value_base& operator=(std::nullptr_t);

    friend class updateable_value_source_base;
};


// A T that can be updated at runtime; uses updateable_value_base to track
// the source as the object is moved or copied. Copying across shards is supported
// unless #7316 is still open
template <typename T>
class updateable_value : public updateable_value_base {
    T _value = {};
private:
    const updateable_value_source<T>* source() const;
public:
    updateable_value() = default;
    explicit updateable_value(T value) : _value(std::move(value)) {}
    explicit updateable_value(const updateable_value_source<T>& source);
    updateable_value(const updateable_value& v);
    updateable_value& operator=(T value);
    updateable_value& operator=(const updateable_value&);
    updateable_value(updateable_value&&) noexcept;
    updateable_value& operator=(updateable_value&&) noexcept;
    const T& operator()() const { return _value; }
    operator const T& () const { return _value; }
    const T& get() const { return _value; }
    observer<T> observe(std::function<void (const T&)> callback) const;

    friend class updateable_value_source_base;
    template <typename U>
    friend class updateable_value_source;
};

// Contains the mechanisms to track updateable_value_base.  Used to reduce template
// bloat and not meant to be used directly.
class updateable_value_source_base {
protected:
    // This class contains two different types of state: values and
    // references to updateable_value_base. We consider adding and removing
    // such references const operations since they don't change the logical
    // state of the object (they don't allow changing the carried value).
    mutable std::vector<updateable_value_base*> _refs; // all connected updateable_values on this shard
    void for_each_ref(std::function<void (updateable_value_base* ref)> func);
protected:
    ~updateable_value_source_base();
    void add_ref(updateable_value_base* ref) const;
    void del_ref(updateable_value_base* ref) const;
    void update_ref(updateable_value_base* old_ref, updateable_value_base* new_ref) const;

    friend class updateable_value_base;
};

template <typename T>
class updateable_value_source : public updateable_value_source_base {
    T _value;
    mutable observable<T> _updater;
    void for_each_ref(std::function<void (updateable_value<T>*)> func) {
        updateable_value_source_base::for_each_ref([func = std::move(func)] (updateable_value_base* ref) {
            func(static_cast<updateable_value<T>*>(ref));
        });
    };
private:
    void add_ref(updateable_value<T>* ref) const {
        updateable_value_source_base::add_ref(ref);
    }
    void del_ref(updateable_value<T>* ref) const {
        updateable_value_source_base::del_ref(ref);
    }
    void update_ref(updateable_value<T>* old_ref, updateable_value<T>* new_ref) const {
        updateable_value_source_base::update_ref(old_ref, new_ref);
    }
public:
    explicit updateable_value_source(T value = T{})
            : _value(std::move(value)) {}
    updateable_value_source(const updateable_value_source& x) : updateable_value_source(x.get()) {
        // We can't copy x's _refs and therefore also _updater. So this is an imperfect copy.
        // This copy constructor therefore breaks updates made to the original copy; it only
        // exists because unit tests copy configs like mad.
    }
    void set(T value) {
        if (value == _value) {
            return;
        }
        _value = std::move(value);
        for_each_ref([&] (updateable_value<T>* ref) {
            ref->_value = _value;
        });
        _updater(_value);
    }
    const T& get() const {
        return _value;
    }
    const T& operator()() const {
        return _value;
    }
    observable<T>& as_observable() const {
        return _updater;
    }
    observer<T> observe(std::function<void (const T&)> callback) const {
        return _updater.observe(std::move(callback));
    }

    friend class updateable_value_base;
};

template <typename T>
updateable_value<T>::updateable_value(const updateable_value_source<T>& source)
        : updateable_value_base(source)
        , _value(source.get()) {
}

template <typename T>
updateable_value<T>::updateable_value(const updateable_value& v) : updateable_value_base(v), _value(v._value) {
}

template <typename T>
updateable_value<T>& updateable_value<T>::operator=(T value) {
    updateable_value_base::operator=(nullptr);
    _value = std::move(value);
    return *this;
}

template <typename T>
updateable_value<T>& updateable_value<T>::operator=(const updateable_value& v) {
    if (this != &v) {
        // Copy early to trigger exceptions, later move
        auto new_val = v._value;
        updateable_value_base::operator=(v);
        _value = std::move(new_val);
    }
    return *this;
}

template <typename T>
updateable_value<T>::updateable_value(updateable_value&& v) noexcept
        : updateable_value_base(v)
        , _value(std::move(v._value)) {
}

template <typename T>
updateable_value<T>& updateable_value<T>::operator=(updateable_value&& v) noexcept {
    if (this != &v) {
        updateable_value_base::operator=(std::move(v));
        _value = std::move(v._value);
    }
    return *this;
}

template <typename T>
inline
const updateable_value_source<T>*
updateable_value<T>::source() const {
    return static_cast<const updateable_value_source<T>*>(_source);
}

template <typename T>
observer<T>
updateable_value<T>::observe(std::function<void (const T&)> callback) const {
    auto* src = source();
    return src ? src->observe(std::move(callback)) : dummy_observer<T>();
}

// Automatically updates a value from a utils::updateable_value
// Where they can be of different types.
// An optional transfom function can provide an additional transformation
// when updating the value, like multiplying it by a factor for unit conversion,
// for example.
template <typename ValueType, typename UpdateableValueType>
class transforming_value_updater {
    ValueType& _value;
    utils::updateable_value<UpdateableValueType> _updateable_value;
    serialized_action _updater;
    utils::observer<UpdateableValueType> _observer;

public:
    transforming_value_updater(ValueType& value, utils::updateable_value<UpdateableValueType> updateable_value,
            std::function<ValueType (UpdateableValueType)> transform = [] (UpdateableValueType uv) { return static_cast<ValueType>(uv); })
        : _value(value)
        , _updateable_value(std::move(updateable_value))
        , _updater([this, transform = std::move(transform)] {
                _value = transform(_updateable_value());
                return make_ready_future<>();
          })
        , _observer(_updateable_value.observe(_updater.make_observer()))
    {}
};

}
#include <fmt/format.h>
#include <fmt/ostream.h>
#include <unordered_map>
#include <iosfwd>
#include <string_view>

#include <boost/program_options.hpp>

#include <seastar/core/sstring.hh>
#include <seastar/core/future.hh>
#include <seastar/util/log.hh>


namespace seastar { class file; }
namespace seastar::json { class json_return_type; }
namespace YAML { class Node; }

namespace utils {

namespace bpo = boost::program_options;

class config_type {
    std::string_view _name;
    std::function<json::json_return_type (const void*)> _to_json;
private:
    template <typename NativeType>
    std::function<json::json_return_type (const void*)> make_to_json(json::json_return_type (*func)(const NativeType&)) {
        return [func] (const void* value) {
            return func(*static_cast<const NativeType*>(value));
        };
    }
public:
    template <typename NativeType>
    config_type(std::string_view name, json::json_return_type (*to_json)(const NativeType&)) : _name(name), _to_json(make_to_json(to_json)) {}
    std::string_view name() const { return _name; }
    json::json_return_type to_json(const void* value) const;
};

template <typename T>
extern const config_type config_type_for;

class config_file {
    static thread_local unsigned s_shard_id;
    struct any_value {
        virtual ~any_value() = default;
        virtual std::unique_ptr<any_value> clone() const = 0;
        virtual void update_from(const any_value* source) = 0;
    };
    std::vector<std::vector<std::unique_ptr<any_value>>> _per_shard_values { 1 };
public:
    typedef std::unordered_map<sstring, sstring> string_map;
    typedef std::vector<sstring> string_list;

    enum class value_status {
        Used,
        Unused,
        Invalid,
    };

    enum class liveness {
        LiveUpdate,
        MustRestart,
    };

    enum class config_source : uint8_t {
        None,
        SettingsFile,
        CommandLine,
        CQL,
        Internal,
        API,
    };

    struct config_src {
        config_file* _cf;
        std::string_view _name, _alias, _desc;
        const config_type* _type;
        size_t _per_shard_values_offset;
    protected:
        virtual const void* current_value() const = 0;
    public:
        config_src(config_file* cf, std::string_view name, const config_type* type, std::string_view desc)
            : _cf(cf)
            , _name(name)
            , _desc(desc)
            , _type(type)
        {}
        config_src(config_file* cf, std::string_view name, std::string_view alias, const config_type* type, std::string_view desc)
            : _cf(cf)
            , _name(name)
            , _alias(alias)
            , _desc(desc)
            , _type(type)
        {}
        virtual ~config_src() {}

        const std::string_view & name() const {
            return _name;
        }
        std::string_view alias() const {
            return _alias;
        }
        const std::string_view & desc() const {
            return _desc;
        }
        std::string_view type_name() const {
            return _type->name();
        }
        config_file * get_config_file() const {
            return _cf;
        }
        bool matches(std::string_view name) const;
        virtual void add_command_line_option(bpo::options_description_easy_init&) = 0;
        virtual void set_value(const YAML::Node&) = 0;
        virtual bool set_value(sstring, config_source = config_source::Internal) = 0;
        virtual future<> set_value_on_all_shards(const YAML::Node&) = 0;
        virtual future<bool> set_value_on_all_shards(sstring, config_source = config_source::Internal) = 0;
        virtual value_status status() const noexcept = 0;
        virtual config_source source() const noexcept = 0;
        sstring source_name() const noexcept;
        json::json_return_type value_as_json() const;
    };

    template<typename T>
    struct named_value : public config_src {
    private:
        friend class config;
        config_source _source = config_source::None;
        value_status _value_status;
        struct the_value_type final : any_value {
            the_value_type(T value) : value(std::move(value)) {}
            utils::updateable_value_source<T> value;
            virtual std::unique_ptr<any_value> clone() const override {
                return std::make_unique<the_value_type>(value());
            }
            virtual void update_from(const any_value* source) override {
                auto typed_source = static_cast<const the_value_type*>(source);
                value.set(typed_source->value());
            }
        };
        liveness _liveness;
        std::vector<T> _allowed_values;
    protected:
        updateable_value_source<T>& the_value() {
            any_value* av = _cf->_per_shard_values[_cf->s_shard_id][_per_shard_values_offset].get();
            return static_cast<the_value_type*>(av)->value;
        }
        const updateable_value_source<T>& the_value() const {
            return const_cast<named_value*>(this)->the_value();
        }
        virtual const void* current_value() const override {
            return &the_value().get();
        }
    public:
        typedef T type;
        typedef named_value<T> MyType;

        named_value(config_file* file, std::string_view name, std::string_view alias, liveness liveness_, value_status vs, const T& t = T(), std::string_view desc = {},
                std::initializer_list<T> allowed_values = {})
            : config_src(file, name, alias, &config_type_for<T>, desc)
            , _value_status(vs)
            , _liveness(liveness_)
            , _allowed_values(std::move(allowed_values)) {
            file->add(*this, std::make_unique<the_value_type>(std::move(t)));
        }
        named_value(config_file* file, std::string_view name, liveness liveness_, value_status vs, const T& t = T(), std::string_view desc = {},
                std::initializer_list<T> allowed_values = {})
            : named_value(file, name, {}, liveness_, vs, t, desc) {
        }
        named_value(config_file* file, std::string_view name, std::string_view alias, value_status vs, const T& t = T(), std::string_view desc = {},
                std::initializer_list<T> allowed_values = {})
                : named_value(file, name, alias, liveness::MustRestart, vs, t, desc, allowed_values) {
        }
        named_value(config_file* file, std::string_view name, value_status vs, const T& t = T(), std::string_view desc = {},
                std::initializer_list<T> allowed_values = {})
                : named_value(file, name, {}, liveness::MustRestart, vs, t, desc, allowed_values) {
        }
        value_status status() const noexcept override {
            return _value_status;
        }
        config_source source() const noexcept override {
            return _source;
        }
        bool is_set() const {
            return _source > config_source::None;
        }
        MyType & operator()(const T& t, config_source src = config_source::Internal) {
            if (!_allowed_values.empty() && std::find(_allowed_values.begin(), _allowed_values.end(), t) == _allowed_values.end()) {
                throw std::invalid_argument(format("Invalid value for {}: got {} which is not inside the set of allowed values {}", name(), t, _allowed_values));
            }
            the_value().set(t);
            if (src > config_source::None) {
                _source = src;
            }
            return *this;
        }
        MyType & operator()(T&& t, config_source src = config_source::Internal) {
            if (!_allowed_values.empty() && std::find(_allowed_values.begin(), _allowed_values.end(), t) == _allowed_values.end()) {
                throw std::invalid_argument(format("Invalid value for {}: got {} which is not inside the set of allowed values {}", name(), t, _allowed_values));
            }
            the_value().set(std::move(t));
            if (src > config_source::None) {
                _source = src;
            }
            return *this;
        }
        void set(T&& t, config_source src = config_source::None) {
            operator()(std::move(t), src);
        }
        const T& operator()() const {
            return the_value().get();
        }

        operator updateable_value<T>() const & {
            return updateable_value<T>(the_value());
        }

        observer<T> observe(std::function<void (const T&)> callback) const {
            return the_value().observe(std::move(callback));
        }

        void add_command_line_option(bpo::options_description_easy_init&) override;
        void set_value(const YAML::Node&) override;
        bool set_value(sstring, config_source = config_source::Internal) override;
        // For setting a single value on all shards,
        // without having to call broadcast_to_all_shards
        // that broadcasts all values to all shards.
        future<> set_value_on_all_shards(const YAML::Node&) override;
        future<bool> set_value_on_all_shards(sstring, config_source = config_source::Internal) override;
    };

    typedef std::reference_wrapper<config_src> cfg_ref;

    config_file(std::initializer_list<cfg_ref> = {});
    config_file(const config_file&) = delete;

    void add(cfg_ref, std::unique_ptr<any_value> value);
    void add(std::initializer_list<cfg_ref>);
    void add(const std::vector<cfg_ref> &);

    boost::program_options::options_description get_options_description();
    boost::program_options::options_description get_options_description(boost::program_options::options_description);

    boost::program_options::options_description_easy_init&
    add_options(boost::program_options::options_description_easy_init&);

    /**
     * Default behaviour for yaml parser is to throw on
     * unknown stuff, invalid opts or conversion errors.
     *
     * Error handling function allows overriding this.
     *
     * error: <option name>, <message>, <optional value_status>
     *
     * The last arg, opt value_status will tell you the type of
     * error occurred. If not set, the option found does not exist.
     * If invalid, it is invalid. Otherwise, a parse error.
     *
     */
    using error_handler = std::function<void(const sstring&, const sstring&, std::optional<value_status>)>;

    void read_from_yaml(const sstring&, error_handler = {});
    void read_from_yaml(const char *, error_handler = {});
    future<> read_from_file(const sstring&, error_handler = {});
    future<> read_from_file(file, error_handler = {});

    using configs = std::vector<cfg_ref>;

    configs set_values() const;
    configs unset_values() const;
    const configs& values() const {
        return _cfgs;
    }
    future<> broadcast_to_all_shards();
private:
    configs
        _cfgs;
};

template <typename T>
requires requires (const config_file::named_value<T>& nv) {
    { nv().empty() } -> std::same_as<bool>;
}
const config_file::named_value<T>& operator||(const config_file::named_value<T>& a, const config_file::named_value<T>& b) {
    return !a().empty() ? a : b;
}

extern template struct config_file::named_value<seastar::log_level>;

}



#include <seastar/json/json_elements.hh>

namespace YAML {

/*
 * Add converters as needed here...
 *
 * TODO: Maybe we should just define all node conversionas as "lexical_cast".
 * However, vanilla yamp-cpp does some special treatment of scalar types,
 * mainly inf handling etc. Hm.
 */
template<>
struct convert<seastar::sstring> {
    static bool decode(const Node& node, sstring& rhs) {
        std::string tmp;
        if (!convert<std::string>::decode(node, tmp)) {
            return false;
        }
        rhs = tmp;
        return true;
    }
};

template<typename K, typename V, typename... Rest>
struct convert<std::unordered_map<K, V, Rest...>> {
    using map_type = std::unordered_map<K, V, Rest...>;

    static bool decode(const Node& node, map_type& rhs) {
        if (!node.IsMap()) {
            return false;
        }
        rhs.clear();
        for (auto& n : node) {
            rhs[n.first.as<K>()] = n.second.as<V>();
        }
        return true;
    }
};

}

namespace std {

template<typename K, typename V, typename... Args>
std::istream& operator>>(std::istream&, std::unordered_map<K, V, Args...>&);

template<>
std::istream& operator>>(std::istream&, std::unordered_map<seastar::sstring, seastar::sstring>&);

template<typename V, typename... Args>
std::istream& operator>>(std::istream&, std::vector<V, Args...>&);

template<>
std::istream& operator>>(std::istream&, std::vector<seastar::sstring>&);

extern template
std::istream& operator>>(std::istream&, std::vector<seastar::sstring>&);

template<typename K, typename V, typename... Args>
std::istream& operator>>(std::istream& is, std::unordered_map<K, V, Args...>& map) {
    std::unordered_map<sstring, sstring> tmp;
    is >> tmp;

    for (auto& p : tmp) {
        map[boost::lexical_cast<K>(p.first)] = boost::lexical_cast<V>(p.second);
    }
    return is;
}

template<typename V, typename... Args>
std::istream& operator>>(std::istream& is, std::vector<V, Args...>& dst) {
    std::vector<seastar::sstring> tmp;
    is >> tmp;
    for (auto& v : tmp) {
        dst.emplace_back(boost::lexical_cast<V>(v));
    }
    return is;
}

template<typename K, typename V, typename... Args>
void validate(boost::any& out, const std::vector<std::string>& in, std::unordered_map<K, V, Args...>*, int utf8) {
    using map_type = std::unordered_map<K, V, Args...>;

    if (out.empty()) {
        out = boost::any(map_type());
    }

    static const boost::regex key(R"foo((?:^|\:)([^=:]+)=)foo");

    auto* p = boost::any_cast<map_type>(&out);
    for (const auto& s : in) {
        boost::sregex_iterator i(s.begin(), s.end(), key), e;

        if (i == e) {
            throw boost::program_options::invalid_option_value(s);
        }

        while (i != e) {
            auto k = (*i)[1].str();
            auto vs = s.begin() + i->position() + i->length();
            auto ve = s.end();

            if (++i != e) {
                ve = s.begin() + i->position();
            }

            (*p)[boost::lexical_cast<K>(k)] = boost::lexical_cast<V>(sstring(vs, ve));
        }
    }
}

}

namespace utils {

namespace {

/*
 * Our own bpo::typed_valye.
 * Only difference is that we _don't_ apply defaults (they are already applied)
 * Needed to make aliases work properly.
 */
template<class T, class charT = char>
class typed_value_ex : public bpo::typed_value<T, charT> {
public:
    typedef bpo::typed_value<T, charT> _Super;

    typed_value_ex()
        : _Super(nullptr)
    {}
    bool apply_default(boost::any& value_store) const override {
        return false;
    }
};


template <typename T>
void maybe_multitoken(typed_value_ex<T>* r) {
}

template <typename T>
void maybe_multitoken(std::vector<typed_value_ex<T>>* r) {
    r->multitoken();
}

template<class T>
inline typed_value_ex<T>* value_ex() {
    typed_value_ex<T>* r = new typed_value_ex<T>();
    maybe_multitoken(r);
    return r;
}

}

sstring hyphenate(const std::string_view&);

}

template<typename T>
void utils::config_file::named_value<T>::add_command_line_option(boost::program_options::options_description_easy_init& init) {
    const auto hyphenated_name = hyphenate(name());
    // NOTE. We are not adding default values. We could, but must in that case manually (in some way) geenrate the textual
    // version, since the available ostream operators for things like pairs and collections don't match what we can deal with parser-wise.
    // See removed ostream operators above.
    init(hyphenated_name.data(), value_ex<T>()->notifier([this](T new_val) { set(std::move(new_val), config_source::CommandLine); }), desc().data());

    if (!alias().empty()) {
        const auto alias_desc = fmt::format("Alias for {}", hyphenated_name);
        init(hyphenate(alias()).data(), value_ex<T>()->notifier([this](T new_val) { set(std::move(new_val), config_source::CommandLine); }), alias_desc.data());
    }
}

template<typename T>
void utils::config_file::named_value<T>::set_value(const YAML::Node& node) {
    if (_source == config_source::SettingsFile && _liveness != liveness::LiveUpdate) {
        // FIXME: warn if different?
        return;
    }
    (*this)(node.as<T>());
    _source = config_source::SettingsFile;
}

template<typename T>
bool utils::config_file::named_value<T>::set_value(sstring value, config_source src) {
    if (_liveness != liveness::LiveUpdate) {
        return false;
    }

    (*this)(boost::lexical_cast<T>(value), src);
    return true;
}

template<typename T>
future<> utils::config_file::named_value<T>::set_value_on_all_shards(const YAML::Node& node) {
    if (_source == config_source::SettingsFile && _liveness != liveness::LiveUpdate) {
        // FIXME: warn if different?
        co_return;
    }
    co_await smp::invoke_on_all([this, value = node.as<T>()] () {
        (*this)(value);
    });
    _source = config_source::SettingsFile;
}

template<typename T>
future<bool> utils::config_file::named_value<T>::set_value_on_all_shards(sstring value, config_source src) {
    if (_liveness != liveness::LiveUpdate) {
        co_return false;
    }

    co_await smp::invoke_on_all([this, value = boost::lexical_cast<T>(value), src] () {
        (*this)(value, src);
    });
    co_return true;
}





#include <seastar/util/bool_class.hh>

class atomic_cell_view;
class collection_mutation_view;
class row_marker;
class row_tombstone;
class range_tombstone;
class tombstone;
class position_in_partition_view;

// When used on an entry, marks the range between this entry and the previous
// one as continuous or discontinuous, excluding the keys of both entries.
// This information doesn't apply to continuity of the entries themselves,
// that is specified by is_dummy flag.
// See class doc of mutation_partition.
using is_continuous = seastar::bool_class<class continuous_tag>;

// Dummy entry is an entry which is incomplete.
// Typically used for marking bounds of continuity range.
// See class doc of mutation_partition.
class dummy_tag {};
using is_dummy = seastar::bool_class<dummy_tag>;

// Guarantees:
//
// - any tombstones which affect cell's liveness are visited before that cell
//
// - rows are visited in ascending order with respect to their keys
//
// - row header (accept_row) is visited before that row's cells
//
// - row tombstones are visited in ascending order with respect to their key prefixes
//
// - cells in given row are visited in ascending order with respect to their column IDs
//
// - static row is visited before any clustered row
//
// - for each column in a row only one variant of accept_(static|row)_cell() is called, appropriate
//   for column's kind (atomic or collection).
//
class mutation_partition_visitor {
public:
    virtual void accept_partition_tombstone(tombstone) = 0;

    virtual void accept_static_cell(column_id, atomic_cell_view) = 0;

    virtual void accept_static_cell(column_id, collection_mutation_view) = 0;

    virtual void accept_row_tombstone(const range_tombstone&) = 0;

    virtual void accept_row(position_in_partition_view key, const row_tombstone& deleted_at, const row_marker& rm,
        is_dummy = is_dummy::no, is_continuous = is_continuous::yes) = 0;

    virtual void accept_row_cell(column_id id, atomic_cell_view) = 0;

    virtual void accept_row_cell(column_id id, collection_mutation_view) = 0;
};



class schema;
class row;
class mutation_partition;
class column_mapping;
class deletable_row;
class column_definition;
class abstract_type;
class atomic_cell_or_collection;

// Mutation partition visitor which applies visited data into
// existing mutation_partition. The visited data may be of a different schema.
// Data which is not representable in the new schema is dropped.
// Weak exception guarantees.
class converting_mutation_partition_applier : public mutation_partition_visitor {
    const schema& _p_schema;
    mutation_partition& _p;
    const column_mapping& _visited_column_mapping;
    deletable_row* _current_row;
private:
    static bool is_compatible(const column_definition& new_def, const abstract_type& old_type, column_kind kind);
    static atomic_cell upgrade_cell(const abstract_type& new_type, const abstract_type& old_type, atomic_cell_view cell,
                                    atomic_cell::collection_member cm = atomic_cell::collection_member::no);
    static void accept_cell(row& dst, column_kind kind, const column_definition& new_def, const abstract_type& old_type, atomic_cell_view cell);
    static void accept_cell(row& dst, column_kind kind, const column_definition& new_def, const abstract_type& old_type, collection_mutation_view cell);public:
    converting_mutation_partition_applier(
            const column_mapping& visited_column_mapping,
            const schema& target_schema,
            mutation_partition& target);
    virtual void accept_partition_tombstone(tombstone t) override;
    void accept_static_cell(column_id id, atomic_cell cell);
    virtual void accept_static_cell(column_id id, atomic_cell_view cell) override;
    virtual void accept_static_cell(column_id id, collection_mutation_view collection) override;
    virtual void accept_row_tombstone(const range_tombstone& rt) override;
    virtual void accept_row(position_in_partition_view key, const row_tombstone& deleted_at, const row_marker& rm, is_dummy dummy, is_continuous continuous) override;
    void accept_row_cell(column_id id, atomic_cell cell);
    virtual void accept_row_cell(column_id id, atomic_cell_view cell) override;
    virtual void accept_row_cell(column_id id, collection_mutation_view collection) override;

    // Appends the cell to dst upgrading it to the new schema.
    // Cells must have monotonic names.
    static void append_cell(row& dst, column_kind kind, const column_definition& new_def, const column_definition& old_def, const atomic_cell_or_collection& cell);
};


#include <vector>

#include <boost/algorithm/string/join.hpp>
#include <boost/range/adaptor/transformed.hpp>

#include <seastar/core/sstring.hh>



#include <algorithm>
#include <functional>
#include <iosfwd>

namespace cql3 {

class column_identifier_raw;

/**
 * Represents an identifer for a CQL column definition.
 * TODO : should support light-weight mode without text representation for when not interned
 */
class column_identifier final {
public:
    bytes bytes_;
private:
    sstring _text;
public:
    // less comparator sorting by text
    struct text_comparator {
        bool operator()(const column_identifier& c1, const column_identifier& c2) const;
    };

    column_identifier(sstring raw_text, bool keep_case);

    column_identifier(bytes bytes_, data_type type);

    column_identifier(bytes bytes_, sstring text);

    bool operator==(const column_identifier& other) const;

    const sstring& text() const { return _text; }

    const bytes& name() const;

    sstring to_string() const;

    sstring to_cql_string() const;

    friend std::ostream& operator<<(std::ostream& out, const column_identifier& i) {
        return out << i._text;
    }

#if 0
    public ColumnIdentifier clone(AbstractAllocator allocator)
    {
        return new ColumnIdentifier(allocator.clone(bytes), text);
    }
#endif

    using raw = column_identifier_raw;
};

/**
 * Because Thrift-created tables may have a non-text comparator, we cannot determine the proper 'key' until
 * we know the comparator. ColumnIdentifier.Raw is a placeholder that can be converted to a real ColumnIdentifier
 * once the comparator is known with prepare(). This should only be used with identifiers that are actual
 * column names. See CASSANDRA-8178 for more background.
 */
class column_identifier_raw final {
private:
    const sstring _raw_text;
    sstring _text;
public:
    column_identifier_raw(sstring raw_text, bool keep_case);

    // for selectable::with_expression::raw:
    ::shared_ptr<column_identifier> prepare(const schema& s) const;

    ::shared_ptr<column_identifier> prepare_column_identifier(const schema& s) const;

    // for selectable::with_expression::raw:
    bool processes_selection() const;

    bool operator==(const column_identifier_raw& other) const;

    virtual sstring to_string() const;
    sstring to_cql_string() const;

    friend std::hash<column_identifier_raw>;
    friend std::ostream& operator<<(std::ostream& out, const column_identifier_raw& id);
};

static inline
const column_definition* get_column_definition(const schema& schema, const column_identifier& id) {
    return schema.get_column_definition(id.bytes_);
}

static inline
::shared_ptr<column_identifier> to_identifier(const column_definition& def) {
    return def.column_specification->name;
}

}

namespace std {

template<>
struct hash<cql3::column_identifier> {
    size_t operator()(const cql3::column_identifier& i) const {
        return std::hash<bytes>()(i.bytes_);
    }
};

template<>
struct hash<cql3::column_identifier_raw> {
    size_t operator()(const cql3::column_identifier::raw& r) const {
        return std::hash<sstring>()(r._text);
    }
};

}



namespace cql3 {

class column_specification;
class prepare_context;

}

#include <optional>
#include <set>
#include <string_view>
#include <vector>
#include <seastar/core/shared_ptr.hh>

namespace replica {
class database; // For transition; remove
}

class schema;
using schema_ptr = lw_shared_ptr<const schema>;
class view_ptr;

namespace db {
class config;
class extensions;
}

namespace secondary_index {
class secondary_index_manager;
}

namespace gms {
class feature_service;
}

namespace locator {
class abstract_replication_strategy;
}

// Classes representing the overall schema, but without access to data.
// Useful on the coordinator side (where access to data is via storage_proxy).
//
// Everything here is type-erased to reduce dependencies. No references are
// kept, so lower-level objects like keyspaces and tables must not be held
// across continuations.
namespace data_dictionary {

// Used to forward all operations to the underlying backing store.
class impl;

class user_types_metadata;
class keyspace_metadata;

class no_such_keyspace : public std::runtime_error {
public:
    no_such_keyspace(std::string_view ks_name);
};

class no_such_column_family : public std::runtime_error {
public:
    no_such_column_family(const table_id& uuid);
    no_such_column_family(std::string_view ks_name, std::string_view cf_name);
    no_such_column_family(std::string_view ks_name, const table_id& uuid);
};

class table {
    const impl* _ops;
    const void* _table;
private:
    friend class impl;
    table(const impl* ops, const void* table);
public:
    schema_ptr schema() const;
    const std::vector<view_ptr>& views() const;
    const secondary_index::secondary_index_manager& get_index_manager() const;
};

class keyspace {
    const impl* _ops;
    const void* _keyspace;
private:
    friend class impl;
    keyspace(const impl* ops, const void* keyspace);
public:
    bool is_internal() const;
    lw_shared_ptr<keyspace_metadata> metadata() const;
    const user_types_metadata& user_types() const;
    const locator::abstract_replication_strategy& get_replication_strategy() const;
};

class database {
    const impl* _ops;
    const void* _database;
private:
    friend class impl;
    database(const impl* ops, const void* database);
public:
    keyspace find_keyspace(std::string_view name) const;
    std::optional<keyspace> try_find_keyspace(std::string_view name) const;
    bool has_keyspace(std::string_view name) const;  // throws no_keyspace
    std::vector<keyspace> get_keyspaces() const;
    std::vector<sstring> get_user_keyspaces() const;
    std::vector<sstring> get_all_keyspaces() const;
    std::vector<table> get_tables() const;
    table find_table(std::string_view ks, std::string_view table) const;  // throws no_such_column_family
    table find_column_family(table_id uuid) const;  // throws no_such_column_family
    schema_ptr find_schema(std::string_view ks, std::string_view table) const;  // throws no_such_column_family
    schema_ptr find_schema(table_id uuid) const;  // throws no_such_column_family
    table find_column_family(schema_ptr s) const;
    bool has_schema(std::string_view ks_name, std::string_view cf_name) const;
    std::optional<table> try_find_table(std::string_view ks, std::string_view table) const;
    std::optional<table> try_find_table(table_id id) const;
    const db::config& get_config() const;
    std::set<sstring> existing_index_names(std::string_view ks_name, std::string_view cf_to_exclude = sstring()) const;
    schema_ptr find_indexed_table(std::string_view ks_name, std::string_view index_name) const;
    sstring get_available_index_name(std::string_view ks_name, std::string_view table_name,
                                               std::optional<sstring> index_name_root) const;
    schema_ptr get_cdc_base_table(sstring_view ks_name, std::string_view table_name) const;
    schema_ptr get_cdc_base_table(const schema&) const;
    const db::extensions& extensions() const;
    const gms::feature_service& features() const;
    replica::database& real_database() const; // For transition; remove
};

}

#include <iosfwd>

namespace data_dictionary {
class database;
class user_types_metadata;
}
namespace auth {
class resource;
}

namespace cql3 {

class ut_name;

class cql3_type final {
    data_type _type;
public:
    cql3_type(data_type type) : _type(std::move(type)) {}
    bool is_collection() const { return _type->is_collection(); }
    bool is_counter() const { return _type->is_counter(); }
    bool is_native() const { return _type->is_native(); }
    bool is_user_type() const { return _type->is_user_type(); }
    data_type get_type() const { return _type; }
    const sstring& to_string() const { return _type->cql3_type_name(); }

    // For UserTypes, we need to know the current keyspace to resolve the
    // actual type used, so Raw is a "not yet prepared" CQL3Type.
    class raw {
        virtual sstring to_string() const = 0;
    protected:
        bool _frozen = false;
    public:
        virtual ~raw() {}
        virtual bool supports_freezing() const = 0;
        virtual bool is_collection() const;
        virtual bool is_counter() const;
        virtual bool is_duration() const;
        virtual bool is_user_type() const;
        bool is_frozen() const;
        virtual bool references_user_type(const sstring&) const;
        virtual std::optional<sstring> keyspace() const;
        virtual void freeze();
        virtual cql3_type prepare_internal(const sstring& keyspace, const data_dictionary::user_types_metadata&) = 0;
        virtual cql3_type prepare(data_dictionary::database db, const sstring& keyspace);
        static shared_ptr<raw> from(cql3_type type);
        static shared_ptr<raw> user_type(ut_name name);
        static shared_ptr<raw> map(shared_ptr<raw> t1, shared_ptr<raw> t2);
        static shared_ptr<raw> list(shared_ptr<raw> t);
        static shared_ptr<raw> set(shared_ptr<raw> t);
        static shared_ptr<raw> tuple(std::vector<shared_ptr<raw>> ts);
        static shared_ptr<raw> frozen(shared_ptr<raw> t);
        friend std::ostream& operator<<(std::ostream& os, const raw& r);
        friend class auth::resource;
    };

private:
    class raw_type;
    class raw_collection;
    class raw_ut;
    class raw_tuple;
    friend std::ostream& operator<<(std::ostream& os, const cql3_type& t) {
        return os << t.to_string();
    }

public:
    enum class kind : int8_t {
        ASCII, BIGINT, BLOB, BOOLEAN, COUNTER, DECIMAL, DOUBLE, EMPTY, FLOAT, INT, SMALLINT, TINYINT, INET, TEXT, TIMESTAMP, UUID, VARINT, TIMEUUID, DATE, TIME, DURATION
    };
    using kind_enum = super_enum<kind,
        kind::ASCII,
        kind::BIGINT,
        kind::BLOB,
        kind::BOOLEAN,
        kind::COUNTER,
        kind::DECIMAL,
        kind::DOUBLE,
        kind::EMPTY,
        kind::FLOAT,
        kind::INET,
        kind::INT,
        kind::SMALLINT,
        kind::TINYINT,
        kind::TEXT,
        kind::TIMESTAMP,
        kind::UUID,
        kind::VARINT,
        kind::TIMEUUID,
        kind::DATE,
        kind::TIME,
        kind::DURATION>;
    using kind_enum_set = enum_set<kind_enum>;

    static thread_local cql3_type ascii;
    static thread_local cql3_type bigint;
    static thread_local cql3_type blob;
    static thread_local cql3_type boolean;
    static thread_local cql3_type double_;
    static thread_local cql3_type empty;
    static thread_local cql3_type float_;
    static thread_local cql3_type int_;
    static thread_local cql3_type smallint;
    static thread_local cql3_type text;
    static thread_local cql3_type timestamp;
    static thread_local cql3_type tinyint;
    static thread_local cql3_type uuid;
    static thread_local cql3_type timeuuid;
    static thread_local cql3_type date;
    static thread_local cql3_type time;
    static thread_local cql3_type inet;
    static thread_local cql3_type varint;
    static thread_local cql3_type decimal;
    static thread_local cql3_type counter;
    static thread_local cql3_type duration;

    static const std::vector<cql3_type>& values();
public:
    kind_enum_set::prepared get_kind() const;
};

inline bool operator==(const cql3_type& a, const cql3_type& b) {
    return a.get_type() == b.get_type();
}

#if 0
    public static class Custom implements CQL3Type
    {
        private final AbstractType<?> type;

        public Custom(AbstractType<?> type)
        {
            this.type = type;
        }

        public Custom(String className) throws SyntaxException, ConfigurationException
        {
            this(TypeParser.parse(className));
        }

        public boolean isCollection()
        {
            return false;
        }

        public AbstractType<?> getType()
        {
            return type;
        }

        @Override
        public final boolean equals(Object o)
        {
            if(!(o instanceof Custom))
                return false;

            Custom that = (Custom)o;
            return type.equals(that.type);
        }

        @Override
        public final int hashCode()
        {
            return type.hashCode();
        }

        @Override
        public String toString()
        {
            return "'" + type + "'";
        }
    }

    public static class Collection implements CQL3Type
    {
        private final CollectionType type;

        public Collection(CollectionType type)
        {
            this.type = type;
        }

        public AbstractType<?> getType()
        {
            return type;
        }

        public boolean isCollection()
        {
            return true;
        }

        @Override
        public final boolean equals(Object o)
        {
            if(!(o instanceof Collection))
                return false;

            Collection that = (Collection)o;
            return type.equals(that.type);
        }

        @Override
        public final int hashCode()
        {
            return type.hashCode();
        }

        @Override
        public String toString()
        {
            boolean isFrozen = !this.type.isMultiCell();
            StringBuilder sb = new StringBuilder(isFrozen ? "frozen<" : "");
            switch (type.kind)
            {
                case LIST:
                    AbstractType<?> listType = ((ListType)type).getElementsType();
                    sb.append("list<").append(listType.asCQL3Type());
                    break;
                case SET:
                    AbstractType<?> setType = ((SetType)type).getElementsType();
                    sb.append("set<").append(setType.asCQL3Type());
                    break;
                case MAP:
                    AbstractType<?> keysType = ((MapType)type).getKeysType();
                    AbstractType<?> valuesType = ((MapType)type).getValuesType();
                    sb.append("map<").append(keysType.asCQL3Type()).append(", ").append(valuesType.asCQL3Type());
                    break;
                default:
                    throw new AssertionError();
            }
            sb.append(">");
            if (isFrozen)
                sb.append(">");
            return sb.toString();
        }
    }

    public static class UserDefined implements CQL3Type
    {
        // Keeping this separatly from type just to simplify toString()
        private final String name;
        private final UserType type;

        private UserDefined(String name, UserType type)
        {
            this.name = name;
            this.type = type;
        }

        public static UserDefined create(UserType type)
        {
            return new UserDefined(UTF8Type.instance.compose(type.name), type);
        }

        public boolean isCollection()
        {
            return false;
        }

        public AbstractType<?> getType()
        {
            return type;
        }

        @Override
        public final boolean equals(Object o)
        {
            if(!(o instanceof UserDefined))
                return false;

            UserDefined that = (UserDefined)o;
            return type.equals(that.type);
        }

        @Override
        public final int hashCode()
        {
            return type.hashCode();
        }

        @Override
        public String toString()
        {
            return name;
        }
    }

    public static class Tuple implements CQL3Type
    {
        private final TupleType type;

        private Tuple(TupleType type)
        {
            this.type = type;
        }

        public static Tuple create(TupleType type)
        {
            return new Tuple(type);
        }

        public boolean isCollection()
        {
            return false;
        }

        public AbstractType<?> getType()
        {
            return type;
        }

        @Override
        public final boolean equals(Object o)
        {
            if(!(o instanceof Tuple))
                return false;

            Tuple that = (Tuple)o;
            return type.equals(that.type);
        }

        @Override
        public final int hashCode()
        {
            return type.hashCode();
        }

        @Override
        public String toString()
        {
            StringBuilder sb = new StringBuilder();
            sb.append("tuple<");
            for (int i = 0; i < type.size(); i++)
            {
                if (i > 0)
                    sb.append(", ");
                sb.append(type.type(i).asCQL3Type());
            }
            sb.append(">");
            return sb.toString();
        }
    }
#endif

}


#include <iosfwd>
#include <optional>
#include <stdexcept>
#include <unordered_map>
#include <unordered_set>

#include <seastar/core/print.hh>
#include <seastar/core/sstring.hh>


namespace auth {

enum class authentication_option {
    password,
    options
};

using authentication_option_set = std::unordered_set<authentication_option>;

using custom_options = std::unordered_map<sstring, sstring>;

struct authentication_options final {
    std::optional<sstring> password;
    std::optional<custom_options> options;
};

inline bool any_authentication_options(const authentication_options& aos) noexcept {
    return aos.password || aos.options;
}

class unsupported_authentication_option : public std::invalid_argument {
public:
    explicit unsupported_authentication_option(authentication_option k)
            : std::invalid_argument(format("The {} option is not supported.", k)) {
    }
};

}

template <>
struct fmt::formatter<auth::authentication_option> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const auth::authentication_option a, FormatContext& ctx) const {
        using enum auth::authentication_option;
        switch (a) {
        case password:
            return formatter<std::string_view>::format("PASSWORD", ctx);
        case options:
            return formatter<std::string_view>::format("OPTIONS", ctx);
        }
        std::abort();
    }
};

#include <unordered_set>

#include <seastar/core/sstring.hh>


namespace auth {

enum class permission {
    //Deprecated
    READ,
    //Deprecated
    WRITE,

    // schema management
    CREATE, // required for CREATE KEYSPACE and CREATE TABLE.
    ALTER,  // required for ALTER KEYSPACE, ALTER TABLE, CREATE INDEX, DROP INDEX.
    DROP,   // required for DROP KEYSPACE and DROP TABLE.

    // data access
    SELECT, // required for SELECT.
    MODIFY, // required for INSERT, UPDATE, DELETE, TRUNCATE.

    // permission management
    AUTHORIZE, // required for GRANT and REVOKE.
    DESCRIBE, // required on the root-level role resource to list all roles.

    // function/aggregate/procedure calls
    EXECUTE,
};

typedef enum_set<
        super_enum<
                permission,
                permission::READ,
                permission::WRITE,
                permission::CREATE,
                permission::ALTER,
                permission::DROP,
                permission::SELECT,
                permission::MODIFY,
                permission::AUTHORIZE,
                permission::DESCRIBE,
                permission::EXECUTE>> permission_set;

bool operator<(const permission_set&, const permission_set&);

namespace permissions {

extern const permission_set ALL;
extern const permission_set NONE;

const sstring& to_string(permission);
permission from_string(const sstring&);

std::unordered_set<sstring> to_strings(const permission_set&);
permission_set from_strings(const std::unordered_set<sstring>&);

}

}

#ifndef UTILS_HASH_HH_
#define UTILS_HASH_HH_

#include <functional>

namespace utils {

// public for unit testing etc
inline size_t hash_combine(size_t left, size_t right) {
    return left + 0x9e3779b9 + (right << 6) + (right >> 2);
}

struct tuple_hash {
private:
    // CMH. Add specializations here to handle recursive tuples
    template<typename T>
    static size_t hash(const T& t) {
        return std::hash<T>()(t);
    }
    template<size_t index, typename...Types>
    struct hash_impl {
        size_t operator()(const std::tuple<Types...>& t, size_t a) const {
            return hash_impl<index-1, Types...>()(t, hash_combine(hash(std::get<index>(t)), a));
        }
        size_t operator()(const std::tuple<Types...>& t) const {
            return hash_impl<index-1, Types...>()(t, hash(std::get<index>(t)));
        }
    };
    template<class...Types>
    struct hash_impl<0, Types...> {
        size_t operator()(const std::tuple<Types...>& t, size_t a) const {
            return hash_combine(hash(std::get<0>(t)), a);
        }
        size_t operator()(const std::tuple<Types...>& t) const {
            return hash(std::get<0>(t));
        }
    };
public:
    // All the operator() implementations are templates, so this is transparent.
    using is_transparent = void;

    template<typename T1, typename T2>
    size_t operator()(const std::pair<T1, T2>& p) const {
        return hash_combine(hash(p.first), hash(p.second));
    }
    template<typename T1, typename T2>
    size_t operator()(const T1& t1, const T2& t2) const {
        return hash_combine(hash(t1), hash(t2));
    }
    template<typename... Args>
    size_t operator()(const std::tuple<Args...>& v) const;
};

template<typename... Args>
inline size_t tuple_hash::operator()(const std::tuple<Args...>& v) const {
    return hash_impl<std::tuple_size<std::tuple<Args...>>::value - 1, Args...>()(v);
}
template<>
inline size_t tuple_hash::operator()(const std::tuple<>& v) const {
    return 0;
}

}

#endif /* UTILS_HASH_HH_ */

#include <string_view>
#include <iostream>
#include <optional>
#include <stdexcept>
#include <tuple>
#include <vector>
#include <unordered_set>

#include <boost/range/adaptor/transformed.hpp>
#include <seastar/core/print.hh>
#include <seastar/core/sstring.hh>


namespace auth {

class invalid_resource_name : public std::invalid_argument {
public:
    explicit invalid_resource_name(std::string_view name)
            : std::invalid_argument(format("The resource name '{}' is invalid.", name)) {
    }
};

enum class resource_kind {
    data, role, service_level, functions
};

///
/// Type tag for constructing data resources.
///
struct data_resource_t final {};

///
/// Type tag for constructing role resources.
///
struct role_resource_t final {};

///
/// Type tag for constructing service_level resources.
///
struct service_level_resource_t final {};

///
/// Type tag for constructing function resources.
///
struct functions_resource_t final {};

///
/// Resources are entities that users can be granted permissions on.
///
/// There are data (keyspaces and tables), role and function resources. There may be other kinds of resources in the future.
///
/// When they are stored as system metadata, resources have the form `root/part_0/part_1/.../part_n`. Each kind of
/// resource has a specific root prefix, followed by a maximum of `n` parts (where `n` is distinct for each kind of
/// resource as well). In this code, this form is called the "name".
///
/// Since all resources have this same structure, all the different kinds are stored in instances of the same class:
/// \ref resource. When we wish to query a resource for kind-specific data (like the table of a "data" resource), we
/// create a kind-specific "view" of the resource.
///
class resource final {
    resource_kind _kind;

    utils::small_vector<sstring, 3> _parts;

public:
    ///
    /// A root resource of a particular kind.
    ///
    explicit resource(resource_kind);
    resource(data_resource_t, std::string_view keyspace);
    resource(data_resource_t, std::string_view keyspace, std::string_view table);
    resource(role_resource_t, std::string_view role);
    resource(service_level_resource_t);
    explicit resource(functions_resource_t);
    resource(functions_resource_t, std::string_view keyspace);
    resource(functions_resource_t, std::string_view keyspace, std::string_view function_signature);
    resource(functions_resource_t, std::string_view keyspace, std::string_view function_name,
            std::vector<::shared_ptr<cql3::cql3_type::raw>> function_args);

    resource_kind kind() const noexcept {
        return _kind;
    }

    ///
    /// A machine-friendly identifier unique to each resource.
    ///
    sstring name() const;

    std::optional<resource> parent() const;

    permission_set applicable_permissions() const;

private:
    resource(resource_kind, utils::small_vector<sstring, 3> parts);

    friend class std::hash<resource>;
    friend class data_resource_view;
    friend class role_resource_view;
    friend class service_level_resource_view;
    friend class functions_resource_view;

    friend bool operator<(const resource&, const resource&);
    friend bool operator==(const resource&, const resource&) = default;
    friend resource parse_resource(std::string_view);
};

bool operator<(const resource&, const resource&);

std::ostream& operator<<(std::ostream&, const resource&);

class resource_kind_mismatch : public std::invalid_argument {
public:
    explicit resource_kind_mismatch(resource_kind expected, resource_kind actual)
        : std::invalid_argument(
            format("This resource has kind '{}', but was expected to have kind '{}'.", actual, expected)) {
    }
};

/// A "data" view of \ref resource.
///
/// If neither `keyspace` nor `table` is present, this is the root resource.
class data_resource_view final {
    const resource& _resource;

public:
    ///
    /// \throws `resource_kind_mismatch` if the argument is not a `data` resource.
    ///
    explicit data_resource_view(const resource& r);

    std::optional<std::string_view> keyspace() const;

    std::optional<std::string_view> table() const;
};

std::ostream& operator<<(std::ostream&, const data_resource_view&);

///
/// A "role" view of \ref resource.
///
/// If `role` is not present, this is the root resource.
///
class role_resource_view final {
    const resource& _resource;

public:
    ///
    /// \throws \ref resource_kind_mismatch if the argument is not a "role" resource.
    ///
    explicit role_resource_view(const resource&);

    std::optional<std::string_view> role() const;
};

std::ostream& operator<<(std::ostream&, const role_resource_view&);

///
/// A "service_level" view of \ref resource.
///
class service_level_resource_view final {
public:
    ///
    /// \throws \ref resource_kind_mismatch if the argument is not a "service_level" resource.
    ///
    explicit service_level_resource_view(const resource&);

};

std::ostream& operator<<(std::ostream&, const service_level_resource_view&);

///
/// A "function" view of \ref resource.
///
class functions_resource_view final {
    const resource& _resource;
public:
    ///
    /// \throws \ref resource_kind_mismatch if the argument is not a "function" resource.
    ///
    explicit functions_resource_view(const resource&);

    std::optional<std::string_view> keyspace() const;
    std::optional<std::string_view> function_signature() const;
    std::optional<std::string_view> function_name() const;
    std::optional<std::vector<std::string_view>> function_args() const;
};

std::ostream& operator<<(std::ostream&, const functions_resource_view&);

///
/// Parse a resource from its name.
///
/// \throws \ref invalid_resource_name when the name is malformed.
///
resource parse_resource(std::string_view name);

const resource& root_data_resource();

inline resource make_data_resource(std::string_view keyspace) {
    return resource(data_resource_t{}, keyspace);
}
inline resource make_data_resource(std::string_view keyspace, std::string_view table) {
    return resource(data_resource_t{}, keyspace, table);
}

const resource& root_role_resource();

inline resource make_role_resource(std::string_view role) {
    return resource(role_resource_t{}, role);
}

const resource& root_service_level_resource();

inline resource make_service_level_resource() {
    return resource(service_level_resource_t{});
}

const resource& root_function_resource();

inline resource make_functions_resource() {
    return resource(functions_resource_t{});
}

inline resource make_functions_resource(std::string_view keyspace) {
    return resource(functions_resource_t{}, keyspace);
}

inline resource make_functions_resource(std::string_view keyspace, std::string_view function_signature) {
    return resource(functions_resource_t{}, keyspace, function_signature);
}

inline resource make_functions_resource(std::string_view keyspace, std::string_view function_name, std::vector<::shared_ptr<cql3::cql3_type::raw>> function_signature) {
    return resource(functions_resource_t{}, keyspace, function_name, function_signature);
}

sstring encode_signature(std::string_view name, std::vector<data_type> args);

std::pair<sstring, std::vector<data_type>> decode_signature(std::string_view encoded_signature);

}

template <>
struct fmt::formatter<auth::resource_kind> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const auth::resource_kind kind, FormatContext& ctx) const {
        using enum auth::resource_kind;
        switch (kind) {
        case data:
            return formatter<std::string_view>::format("data", ctx);
        case role:
            return formatter<std::string_view>::format("role", ctx);
        case service_level:
            return formatter<std::string_view>::format("service_level", ctx);
        case functions:
            return formatter<std::string_view>::format("functions", ctx);
        }
        std::abort();
    }
};

namespace std {

template <>
struct hash<auth::resource> {
    static size_t hash_data(const auth::data_resource_view& dv) {
        return utils::tuple_hash()(std::make_tuple(auth::resource_kind::data, dv.keyspace(), dv.table()));
    }

    static size_t hash_role(const auth::role_resource_view& rv) {
        return utils::tuple_hash()(std::make_tuple(auth::resource_kind::role, rv.role()));
    }

    static size_t hash_service_level(const auth::service_level_resource_view& rv) {
            return utils::tuple_hash()(std::make_tuple(auth::resource_kind::service_level));
    }

    static size_t hash_function(const auth::functions_resource_view& fv) {
        return utils::tuple_hash()(std::make_tuple(auth::resource_kind::functions, fv.keyspace(), fv.function_signature()));
    }

    size_t operator()(const auth::resource& r) const {
        std::size_t value;

        switch (r._kind) {
        case auth::resource_kind::data: value = hash_data(auth::data_resource_view(r)); break;
        case auth::resource_kind::role: value = hash_role(auth::role_resource_view(r)); break;
        case auth::resource_kind::service_level: value = hash_service_level(auth::service_level_resource_view(r)); break;
        case auth::resource_kind::functions: value = hash_function(auth::functions_resource_view(r)); break;
        }

        return value;
    }
};

}

namespace auth {

using resource_set = std::unordered_set<resource>;

//
// A resource and all of its parents.
//
resource_set expand_resource_family(const resource&);

}


namespace auth {

///
/// A type-safe wrapper for the name of a logged-in user, or a nameless (anonymous) user.
///
class authenticated_user final {
public:
    ///
    /// An anonymous user has no name.
    ///
    std::optional<sstring> name{};

    ///
    /// An anonymous user.
    ///
    authenticated_user() = default;
    explicit authenticated_user(std::string_view name);
    friend bool operator==(const authenticated_user&, const authenticated_user&) noexcept = default;
};

const authenticated_user& anonymous_user() noexcept;

inline bool is_anonymous(const authenticated_user& u) noexcept {
    return u == anonymous_user();
}

}

///
/// The user name, or "anonymous".
///
template <>
struct fmt::formatter<auth::authenticated_user> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const auth::authenticated_user& u, FormatContext& ctx) const {
        if (u.name) {
            return fmt::format_to(ctx.out(), "{}", *u.name);
        } else {
            return fmt::format_to(ctx.out(), "{}", "anonymous");
        }
    }
};

namespace std {

template <>
struct hash<auth::authenticated_user> final {
    size_t operator()(const auth::authenticated_user &u) const {
        return std::hash<std::optional<sstring>>()(u.name);
    }
};

}

#include <functional>
#include <optional>
#include <string_view>

#include <seastar/core/future.hh>
#include <seastar/core/sstring.hh>


namespace auth {

///
/// A stateful SASL challenge which supports many authentication schemes (depending on the implementation).
///
class sasl_challenge {
public:
    virtual ~sasl_challenge() = default;

    virtual bytes evaluate_response(bytes_view client_response) = 0;

    virtual bool is_complete() const = 0;

    virtual future<authenticated_user> get_authenticated_user() const = 0;
};

class plain_sasl_challenge : public sasl_challenge {
public:
    using completion_callback = std::function<future<authenticated_user>(std::string_view, std::string_view)>;

    explicit plain_sasl_challenge(completion_callback f) : _when_complete(std::move(f)) {
    }

    virtual bytes evaluate_response(bytes_view) override;

    virtual bool is_complete() const override;

    virtual future<authenticated_user> get_authenticated_user() const override;

private:
    std::optional<sstring> _username, _password;
    completion_callback _when_complete;
};

}


#include <string_view>
#include <memory>
#include <set>
#include <stdexcept>
#include <unordered_map>

#include <seastar/core/enum.hh>
#include <seastar/core/future.hh>
#include <seastar/core/sstring.hh>
#include <seastar/core/shared_ptr.hh>


namespace db {
    class config;
}

namespace auth {

class authenticated_user;

///
/// Abstract client for authenticating role identity.
///
/// All state necessary to authorize a role is stored externally to the client instance.
///
class authenticator {
public:
    using ptr_type = std::unique_ptr<authenticator>;

    ///
    /// The name of the key to be used for the user-name part of password authentication with \ref authenticate.
    ///
    static const sstring USERNAME_KEY;

    ///
    /// The name of the key to be used for the password part of password authentication with \ref authenticate.
    ///
    static const sstring PASSWORD_KEY;

    using credentials_map = std::unordered_map<sstring, sstring>;

    virtual ~authenticator() = default;

    virtual future<> start() = 0;

    virtual future<> stop() = 0;

    ///
    /// A fully-qualified (class with package) Java-like name for this implementation.
    ///
    virtual std::string_view qualified_java_name() const = 0;

    virtual bool require_authentication() const = 0;

    virtual authentication_option_set supported_options() const = 0;

    ///
    /// A subset of `supported_options()` that users are permitted to alter for themselves.
    ///
    virtual authentication_option_set alterable_options() const = 0;

    ///
    /// Authenticate a user given implementation-specific credentials.
    ///
    /// If this implementation does not require authentication (\ref require_authentication), an anonymous user may
    /// result.
    ///
    /// \returns an exceptional future with \ref exceptions::authentication_exception if given invalid credentials.
    ///
    virtual future<authenticated_user> authenticate(const credentials_map& credentials) const = 0;

    ///
    /// Create an authentication record for a new user. This is required before the user can log-in.
    ///
    /// The options provided must be a subset of `supported_options()`.
    ///
    virtual future<> create(std::string_view role_name, const authentication_options& options) const = 0;

    ///
    /// Alter the authentication record of an existing user.
    ///
    /// The options provided must be a subset of `supported_options()`.
    ///
    /// Callers must ensure that the specification of `alterable_options()` is adhered to.
    ///
    virtual future<> alter(std::string_view role_name, const authentication_options& options) const = 0;

    ///
    /// Delete the authentication record for a user. This will disallow the user from logging in.
    ///
    virtual future<> drop(std::string_view role_name) const = 0;

    ///
    /// Query for custom options (those corresponding to \ref authentication_options::options).
    ///
    /// If no options are set the result is an empty container.
    ///
    virtual future<custom_options> query_custom_options(std::string_view role_name) const = 0;

    ///
    /// System resources used internally as part of the implementation. These are made inaccessible to users.
    ///
    virtual const resource_set& protected_resources() const = 0;

    virtual ::shared_ptr<sasl_challenge> new_sasl_challenge() const = 0;
};

}



#include <string_view>
#include <functional>
#include <optional>
#include <stdexcept>
#include <tuple>
#include <vector>

#include <seastar/core/future.hh>
#include <seastar/core/shared_ptr.hh>

namespace auth {

class role_or_anonymous;

struct permission_details {
    sstring role_name;
    ::auth::resource resource;
    permission_set permissions;
};

inline bool operator==(const permission_details& pd1, const permission_details& pd2) {
    return std::forward_as_tuple(pd1.role_name, pd1.resource, pd1.permissions.mask())
            == std::forward_as_tuple(pd2.role_name, pd2.resource, pd2.permissions.mask());
}

inline bool operator<(const permission_details& pd1, const permission_details& pd2) {
    return std::forward_as_tuple(pd1.role_name, pd1.resource, pd1.permissions)
            < std::forward_as_tuple(pd2.role_name, pd2.resource, pd2.permissions);
}

class unsupported_authorization_operation : public std::invalid_argument {
public:
    using std::invalid_argument::invalid_argument;
};

///
/// Abstract client for authorizing roles to access resources.
///
/// All state necessary to authorize a role is stored externally to the client instance.
///
class authorizer {
public:
    using ptr_type = std::unique_ptr<authorizer>;

    virtual ~authorizer() = default;

    virtual future<> start() = 0;

    virtual future<> stop() = 0;

    ///
    /// A fully-qualified (class with package) Java-like name for this implementation.
    ///
    virtual std::string_view qualified_java_name() const = 0;

    ///
    /// Query for the permissions granted directly to a role for a particular \ref resource (and not any of its
    /// parents).
    ///
    /// The optional role name is empty when an anonymous user is authorized. Some implementations may still wish to
    /// grant default permissions in this case.
    ///
    virtual future<permission_set> authorize(const role_or_anonymous&, const resource&) const = 0;

    ///
    /// Grant a set of permissions to a role for a particular \ref resource.
    ///
    /// \throws \ref unsupported_authorization_operation if granting permissions is not supported.
    ///
    virtual future<> grant(std::string_view role_name, permission_set, const resource&) const = 0;

    ///
    /// Revoke a set of permissions from a role for a particular \ref resource.
    ///
    /// \throws \ref unsupported_authorization_operation if revoking permissions is not supported.
    ///
    virtual future<> revoke(std::string_view role_name, permission_set, const resource&) const = 0;

    ///
    /// Query for all directly granted permissions.
    ///
    /// \throws \ref unsupported_authorization_operation if listing permissions is not supported.
    ///
    virtual future<std::vector<permission_details>> list_all() const = 0;

    ///
    /// Revoke all permissions granted directly to a particular role.
    ///
    /// \throws \ref unsupported_authorization_operation if revoking permissions is not supported.
    ///
    virtual future<> revoke_all(std::string_view role_name) const = 0;

    ///
    /// Revoke all permissions granted to any role for a particular resource.
    ///
    /// \throws \ref unsupported_authorization_operation if revoking permissions is not supported.
    ///
    virtual future<> revoke_all(const resource&) const = 0;

    ///
    /// System resources used internally as part of the implementation. These are made inaccessible to users.
    ///
    virtual const resource_set& protected_resources() const = 0;
};

}


#include <string_view>
#include <functional>
#include <iosfwd>
#include <optional>

#include <seastar/core/sstring.hh>


namespace auth {

class role_or_anonymous final {
public:
    std::optional<sstring> name{};

    role_or_anonymous() = default;
    role_or_anonymous(std::string_view name) : name(name) {
    }
    friend bool operator==(const role_or_anonymous&, const role_or_anonymous&) noexcept = default;
};

std::ostream& operator<<(std::ostream&, const role_or_anonymous&);

bool is_anonymous(const role_or_anonymous&) noexcept;

}

namespace std {

template <>
struct hash<auth::role_or_anonymous> {
    size_t operator()(const auth::role_or_anonymous& mr) const {
        return hash<std::optional<sstring>>()(mr.name);
    }
};

}


#include <vector>
#include <memory>
#include <seastar/core/shared_future.hh>
#include <seastar/core/shared_ptr.hh>
#include <seastar/core/future.hh>
#include <seastar/core/bitops.hh>
#include <boost/intrusive/unordered_set.hpp>

namespace bi = boost::intrusive;

namespace utils {

struct do_nothing_loading_shared_values_stats {
    static void inc_hits() noexcept {} // Increase the number of times entry was found ready
    static void inc_misses() noexcept {} // Increase the number of times entry was not found
    static void inc_blocks() noexcept {} // Increase the number of times entry was not ready (>= misses)
    static void inc_evictions() noexcept {} // Increase the number of times entry was evicted
};

// Entries stay around as long as there is any live external reference (entry_ptr) to them.
// Supports asynchronous insertion, ensures that only one entry will be loaded.
// InitialBucketsCount is required to be greater than zero. Otherwise a constructor will throw an
// std::invalid_argument exception.
template<typename Key,
         typename Tp,
         typename Hash = std::hash<Key>,
         typename EqualPred = std::equal_to<Key>,
         typename Stats = do_nothing_loading_shared_values_stats,
         size_t InitialBucketsCount = 16>
requires requires () {
    Stats::inc_hits();
    Stats::inc_misses();
    Stats::inc_blocks();
    Stats::inc_evictions();
}
class loading_shared_values {
public:
    using key_type = Key;
    using value_type = Tp;
    static constexpr size_t initial_buckets_count = InitialBucketsCount;

private:
    class entry : public bi::unordered_set_base_hook<bi::store_hash<true>>, public enable_lw_shared_from_this<entry> {
    private:
        loading_shared_values& _parent;
        key_type _key;
        std::optional<value_type> _val;
        shared_promise<> _loaded;

    public:
        const key_type& key() const noexcept {
            return _key;
        }

        const value_type& value() const noexcept {
            return *_val;
        }

        value_type& value() noexcept {
            return *_val;
        }

        /// \brief "Release" the object from the contained value.
        /// After this call the state of the value kept inside this object is undefined and it may no longer be used.
        ///
        /// \return The r-value reference to the value kept inside this object.
        value_type&& release() {
            return *std::move(_val);
        }

        void set_value(value_type new_val) {
            _val.emplace(std::move(new_val));
        }

        bool orphaned() const {
            return !is_linked();
        }

        shared_promise<>& loaded() {
            return _loaded;
        }

        bool ready() const noexcept {
            return bool(_val);
        }

        entry(loading_shared_values& parent, key_type k)
                : _parent(parent), _key(std::move(k)) {}

        ~entry() {
            if (is_linked()) {
                _parent._set.erase(_parent._set.iterator_to(*this));
            }
            Stats::inc_evictions();
        }

        friend bool operator==(const entry& a, const entry& b){
            return EqualPred()(a.key(), b.key());
        }

        friend std::size_t hash_value(const entry& v) {
            return Hash()(v.key());
        }
    };

    template<typename KeyType, typename KeyEqual>
    struct key_eq {
        bool operator()(const KeyType& k, const entry& c) const {
           return KeyEqual()(k, c.key());
        }

        bool operator()(const entry& c, const KeyType& k) const {
           return KeyEqual()(c.key(), k);
        }
    };

    using set_type = bi::unordered_set<entry, bi::power_2_buckets<true>, bi::compare_hash<true>>;
    using bi_set_bucket_traits = typename set_type::bucket_traits;
    using set_iterator = typename set_type::iterator;
    enum class shrinking_is_allowed { no, yes };

public:
    // Pointer to entry value
    class entry_ptr {
        lw_shared_ptr<entry> _e;
    public:
        using element_type = value_type;
        entry_ptr() = default;
        entry_ptr(std::nullptr_t) noexcept : _e() {};
        explicit entry_ptr(lw_shared_ptr<entry> e) : _e(std::move(e)) {}
        entry_ptr& operator=(std::nullptr_t) noexcept {
            _e = nullptr;
            return *this;
        }
        explicit operator bool() const noexcept { return bool(_e); }
        bool operator==(const entry_ptr&) const = default;
        element_type& operator*() const noexcept { return _e->value(); }
        element_type* operator->() const noexcept { return &_e->value(); }

        /// \brief Get the wrapped value. Avoid the copy if this is the last reference to this value.
        /// If this is the last reference then the wrapped value is going to be std::move()ed. Otherwise it's going to
        /// be copied.
        /// \return The wrapped value.
        element_type release() {
            auto res = _e.owned() ? _e->release() : _e->value();
            _e = nullptr;
            return res;
        }

        // Returns the key this entry is associated with.
        // Valid if bool(*this).
        const key_type& key() const {
            return _e->key();
        }

        // Returns true iff the entry is not linked in the set.
        // Call only when bool(*this).
        bool orphaned() const {
            return _e->orphaned();
        }

        friend class loading_shared_values;
        friend std::ostream& operator<<(std::ostream& os, const entry_ptr& ep) {
            return os << ep._e.get();
        }
    };

private:
    std::vector<typename set_type::bucket_type> _buckets;
    set_type _set;

public:
    static const key_type& to_key(const entry_ptr& e_ptr) noexcept {
        return e_ptr._e->key();
    }

    /// \throw std::invalid_argument if InitialBucketsCount is zero
    loading_shared_values()
        : _buckets(InitialBucketsCount)
        , _set(bi_set_bucket_traits(_buckets.data(), _buckets.size()))
    {
        static_assert(noexcept(Stats::inc_evictions()), "Stats::inc_evictions must be non-throwing");
        static_assert(noexcept(Stats::inc_hits()), "Stats::inc_hits must be non-throwing");
        static_assert(noexcept(Stats::inc_misses()), "Stats::inc_misses must be non-throwing");
        static_assert(noexcept(Stats::inc_blocks()), "Stats::inc_blocks must be non-throwing");

        static_assert(InitialBucketsCount && ((InitialBucketsCount & (InitialBucketsCount - 1)) == 0), "Initial buckets count should be a power of two");
    }
    loading_shared_values(loading_shared_values&&) = default;
    loading_shared_values(const loading_shared_values&) = delete;
    ~loading_shared_values() {
         assert(!_set.size());
    }

    /// \brief
    /// Returns a future which resolves with a shared pointer to the entry for the given key.
    /// Always returns a valid pointer if succeeds.
    ///
    /// If entry is missing, the loader is invoked. If entry is already loading, this invocation
    /// will wait for prior loading to complete and use its result when it's done.
    ///
    /// The loader object does not survive deferring, so the caller must deal with its liveness.
    template<typename Loader>
    future<entry_ptr> get_or_load(const key_type& key, Loader&& loader) noexcept {
        static_assert(std::is_same<future<value_type>, typename futurize<std::result_of_t<Loader(const key_type&)>>::type>::value, "Bad Loader signature");
        try {
            auto i = _set.find(key, Hash(), key_eq<key_type, EqualPred>());
            lw_shared_ptr<entry> e;
            future<> f = make_ready_future<>();
            if (i != _set.end()) {
                e = i->shared_from_this();
                // take a short cut if the value is ready
                if (e->ready()) {
                    Stats::inc_hits();
                    return make_ready_future<entry_ptr>(entry_ptr(std::move(e)));
                }
                f = e->loaded().get_shared_future();
            } else {
                Stats::inc_misses();
                e = make_lw_shared<entry>(*this, key);
                rehash_before_insert();
                _set.insert(*e);
                // get_shared_future() may throw, so make sure to call it before invoking the loader(key)
                f = e->loaded().get_shared_future();
                // Future indirectly forwarded to `e`.
                (void)futurize_invoke([&] { return loader(key); }).then_wrapped([e](future<value_type>&& val_fut) mutable {
                    if (val_fut.failed()) {
                        e->loaded().set_exception(val_fut.get_exception());
                    } else {
                        e->set_value(val_fut.get0());
                        e->loaded().set_value();
                    }
                });
            }
            if (!f.available()) {
                Stats::inc_blocks();
                return f.then([e]() mutable {
                    return entry_ptr(std::move(e));
                });
            } else if (f.failed()) {
                return make_exception_future<entry_ptr>(std::move(f).get_exception());
            } else {
                Stats::inc_hits();
                return make_ready_future<entry_ptr>(entry_ptr(std::move(e)));
            }
        } catch (...) {
            return make_exception_future<entry_ptr>(std::current_exception());
        }
    }

    /// \brief Try to rehash the container so that the load factor is between 0.25 and 0.75.
    /// \throw May throw if allocation of a new buckets array throws.
    void rehash() {
        rehash<shrinking_is_allowed::yes>(_set.size());
    }

    size_t buckets_count() const {
        return _buckets.size();
    }

    size_t size() const {
        return _set.size();
    }

    template<typename KeyType, typename KeyHasher, typename KeyEqual>
    entry_ptr find(const KeyType& key, KeyHasher key_hasher_func, KeyEqual key_equal_func) noexcept {
        set_iterator it = _set.find(key, std::move(key_hasher_func), key_eq<KeyType, KeyEqual>());
        if (it == _set.end() || !it->ready()) {
            return entry_ptr();
        }
        return entry_ptr(it->shared_from_this());
    };

    // Removes a given key from this container.
    // If a given key is currently loading, the loading will succeed and will return entry_ptr
    // to the caller, but the value will not be present in the container. It will be removed
    // when the last entry_ptr dies, as usual.
    //
    // Post-condition: !find(key)
    template<typename KeyType, typename KeyHasher, typename KeyEqual>
    void remove(const KeyType& key, KeyHasher key_hasher_func, KeyEqual key_equal_func) {
        set_iterator it = _set.find(key, std::move(key_hasher_func), key_eq<KeyType, KeyEqual>());
        if (it != _set.end()) {
            _set.erase(it);
        }
    }

    // Removes a given key from this container.
    // If a given key is currently loading, the loading will succeed and will return entry_ptr
    // to the caller, but the value will not be present in the container. It will be removed
    // when the last entry_ptr dies, as usual.
    //
    // Post-condition: !find(key)
    template<typename KeyType>
    void remove(const KeyType& key) {
        remove(key, Hash(), EqualPred());
    }

    // Removes all values which match a given predicate or are currently loading.
    // Guarantees that no values which match the predicate and whose loading was initiated
    // before this call will be present after this call (or appear at any time later).
    // Same effects as if remove(e.key()) was called on each matching entry.
    template<typename Pred>
    requires std::is_invocable_r_v<bool, Pred, const Tp&>
    void remove_if(const Pred& pred) {
        auto it = _set.begin();
        while (it != _set.end()) {
            if (!it->ready() || pred(it->value())) {
                auto next = std::next(it);
                _set.erase(it);
                it = next;
            } else {
                ++it;
            }
        }
    }

    // keep the default non-templated overloads to ease on the compiler for specifications
    // that do not require the templated find().
    entry_ptr find(const key_type& key) noexcept {
        return find(key, Hash(), EqualPred());
    }

private:
    void rehash_before_insert() noexcept {
        try {
            rehash<shrinking_is_allowed::no>(_set.size() + 1);
        } catch (...) {
            // if rehashing fails - continue with the current buckets array
        }
    }

    template <shrinking_is_allowed ShrinkingIsAllowed>
    void rehash(size_t new_size) {
        size_t new_buckets_count = 0;

        // Try to keep the load factor between 0.25 (when shrinking is allowed) and 0.75.
        if (ShrinkingIsAllowed == shrinking_is_allowed::yes && new_size < buckets_count() / 4) {
            if (!new_size) {
                new_buckets_count = 1;
            } else {
                new_buckets_count = size_t(1) << log2floor(new_size * 4);
            }
        } else if (new_size > 3 * buckets_count() / 4) {
            new_buckets_count = buckets_count() * 2;
        }

        if (new_buckets_count < InitialBucketsCount) {
            return;
        }

        std::vector<typename set_type::bucket_type> new_buckets(new_buckets_count);
        _set.rehash(bi_set_bucket_traits(new_buckets.data(), new_buckets.size()));
        _buckets = std::move(new_buckets);
    }
};

}



#include <chrono>
#include <unordered_map>
#include <memory_resource>
#include <optional>

#include <boost/intrusive/list.hpp>
#include <boost/intrusive/unordered_set.hpp>
#include <boost/intrusive/parent_from_member.hpp>
#include <boost/range/adaptor/filtered.hpp>
#include <boost/range/adaptor/transformed.hpp>
#include <boost/range/join.hpp>

#include <seastar/core/seastar.hh>
#include <seastar/core/future-util.hh>
#include <seastar/core/timer.hh>
#include <seastar/core/gate.hh>


namespace bi = boost::intrusive;

namespace utils {

enum class loading_cache_reload_enabled { no, yes };

struct loading_cache_config final {
    size_t max_size = 0;
    seastar::lowres_clock::duration expiry;
    seastar::lowres_clock::duration refresh;
};

template <typename Tp>
struct simple_entry_size {
    size_t operator()(const Tp& val) {
        return 1;
    }
};

struct do_nothing_loading_cache_stats {
    // Accounts events when entries are evicted from the unprivileged cache section due to size restriction.
    // These events are interesting because they are an indication of a cache pollution event.
    static void inc_unprivileged_on_cache_size_eviction() noexcept {};
    // A metric complementary to the above one. Both combined allow to get the total number of cache evictions
    static void inc_privileged_on_cache_size_eviction() noexcept {};
};

/// \brief Loading cache is a cache that loads the value into the cache using the given asynchronous callback.
///
/// Each cached value if reloading is enabled (\tparam ReloadEnabled == loading_cache_reload_enabled::yes) is reloaded after
/// the "refresh" time period since it was loaded for the last time.
///
/// The values are going to be evicted from the cache if they are not accessed during the "expiration" period or haven't
/// been reloaded even once during the same period.
///
/// If "expiration" is set to zero - the caching is going to be disabled and get_XXX(...) is going to call the "loader" callback
/// every time in order to get the requested value.
///
/// \note In order to avoid the eviction of cached entries due to "aging" of the contained value the user has to choose
/// the "expiration" to be at least ("refresh" + "max load latency"). This way the value is going to stay in the cache and is going to be
/// read in a non-blocking way as long as it's frequently accessed. Note however that since reloading is an asynchronous
/// procedure it may get delayed by other running task. Therefore choosing the "expiration" too close to the ("refresh" + "max load latency")
/// value one risks to have his/her cache values evicted when the system is heavily loaded.
///
/// The cache is also limited in size and if adding the next value is going
/// to exceed the cache size limit the least recently used value(s) is(are) going to be evicted until the size of the cache
/// becomes such that adding the new value is not going to break the size limit. If the new entry's size is greater than
/// the cache size then the get_XXX(...) method is going to return a future with the loading_cache::entry_is_too_big exception.
///
/// The cache is comprised of 2 dynamic sections.
/// Total size of both sections should not exceed the maximum cache size.
/// New cache entry is always added to the unprivileged section.
/// After a cache entry is read more than SectionHitThreshold times it moves to the second (privileged) cache section.
/// Both sections' entries obey expiration and reload rules as explained above.
/// When cache entries need to be evicted due to a size restriction unprivileged section least recently used entries are evicted first.
/// If cache size is still too big event after there are no more entries in the unprivileged section the least recently used entries
/// from the privileged section are going to be evicted till the cache size restriction is met.
///
/// The size of the cache is defined as a sum of sizes of all cached entries.
/// The size of each entry is defined by the value returned by the \tparam EntrySize predicate applied on it.
///
/// The get(key) or get_ptr(key) methods ensures that the "loader" callback is called only once for each cached entry regardless of how many
/// callers are calling for the get_XXX(key) for the same "key" at the same time. Only after the value is evicted from the cache
/// it's going to be "loaded" in the context of get_XXX(key). As long as the value is cached get_XXX(key) is going to return the
/// cached value immediately and reload it in the background every "refresh" time period as described above.
///
/// \tparam Key type of the cache key
/// \tparam Tp type of the cached value
/// \tparam SectionHitThreshold number of hits after which a cache item is going to be moved to the privileged cache section.
/// \tparam ReloadEnabled if loading_cache_reload_enabled::yes allow reloading the values otherwise don't reload
/// \tparam EntrySize predicate to calculate the entry size
/// \tparam Hash hash function
/// \tparam EqualPred equality predicate
/// \tparam LoadingSharedValuesStats statistics incrementing class (see utils::loading_shared_values)
/// \tparam Alloc elements allocator
template<typename Key,
         typename Tp,
         int SectionHitThreshold = 0,
         loading_cache_reload_enabled ReloadEnabled = loading_cache_reload_enabled::no,
         typename EntrySize = simple_entry_size<Tp>,
         typename Hash = std::hash<Key>,
         typename EqualPred = std::equal_to<Key>,
         typename LoadingSharedValuesStats = utils::do_nothing_loading_shared_values_stats,
         typename LoadingCacheStats = utils::do_nothing_loading_cache_stats,
         typename Alloc = std::pmr::polymorphic_allocator<>>
class loading_cache {

    using loading_cache_clock_type = seastar::lowres_clock;
    using safe_link_list_hook = bi::list_base_hook<bi::link_mode<bi::safe_link>>;

    class timestamped_val {
    public:
        using value_type = Tp;
        using loading_values_type = typename utils::loading_shared_values<Key, timestamped_val, Hash, EqualPred, LoadingSharedValuesStats, 256>;
        class lru_entry;
        class value_ptr;

    private:
        value_type _value;
        loading_cache_clock_type::time_point _loaded;
        loading_cache_clock_type::time_point _last_read;
        lru_entry* _lru_entry_ptr = nullptr; /// MRU item is at the front, LRU - at the back
        size_t _size = 0;

    public:
        timestamped_val(value_type val)
            : _value(std::move(val))
            , _loaded(loading_cache_clock_type::now())
            , _last_read(_loaded)
            , _size(EntrySize()(_value))
        {}
        timestamped_val(timestamped_val&&) = default;

        timestamped_val& operator=(value_type new_val) {
            assert(_lru_entry_ptr);

            _value = std::move(new_val);
            _loaded = loading_cache_clock_type::now();
            _lru_entry_ptr->owning_section_size() -= _size;
            _size = EntrySize()(_value);
            _lru_entry_ptr->owning_section_size() += _size;
            return *this;
        }

        value_type& value() noexcept { return _value; }
        const value_type& value() const noexcept { return _value; }

        static const timestamped_val& container_of(const value_type& value) {
            return *bi::get_parent_from_member(&value, &timestamped_val::_value);
        }

        loading_cache_clock_type::time_point last_read() const noexcept {
            return _last_read;
        }

        loading_cache_clock_type::time_point loaded() const noexcept {
            return _loaded;
        }

        size_t size() const noexcept {
            return _size;
        }

        bool ready() const noexcept {
            return _lru_entry_ptr;
        }

        lru_entry* lru_entry_ptr() const noexcept {
            return _lru_entry_ptr;
        }

    private:
        void touch() noexcept {
            _last_read = loading_cache_clock_type::now();
            if (_lru_entry_ptr) {
                _lru_entry_ptr->touch();
            }
        }

        void set_anchor_back_reference(lru_entry* lru_entry_ptr) noexcept {
            _lru_entry_ptr = lru_entry_ptr;
        }
    };

private:
    using loading_values_type = typename timestamped_val::loading_values_type;
    using timestamped_val_ptr = typename loading_values_type::entry_ptr;
    using ts_value_lru_entry = typename timestamped_val::lru_entry;
    using lru_list_type = typename ts_value_lru_entry::lru_list_type;
    using list_iterator = typename lru_list_type::iterator;

public:
    using value_type = Tp;
    using key_type = Key;
    using value_ptr = typename timestamped_val::value_ptr;

    class entry_is_too_big : public std::exception {};

private:
    loading_cache(loading_cache_config cfg, logging::logger& logger)
        : _cfg(std::move(cfg))
        , _logger(logger)
        , _timer([this] { on_timer(); })
    {
        static_assert(noexcept(LoadingCacheStats::inc_unprivileged_on_cache_size_eviction()), "LoadingCacheStats::inc_unprivileged_on_cache_size_eviction must be non-throwing");
        static_assert(noexcept(LoadingCacheStats::inc_privileged_on_cache_size_eviction()), "LoadingCacheStats::inc_privileged_on_cache_size_eviction must be non-throwing");

        if (!validate_config(_cfg)) {
            throw exceptions::configuration_exception("loading_cache: caching is enabled but refresh period and/or max_size are zero");
        }
    }

    bool validate_config(const loading_cache_config& cfg) const noexcept {
        // Sanity check: if expiration period is given then non-zero refresh period and maximal size are required
        if (cfg.expiry != loading_cache_clock_type::duration(0) && (cfg.max_size == 0 || cfg.refresh == loading_cache_clock_type::duration(0))) {
            return false;
        }

        return true;
    }

public:
    template<typename Func>
    requires std::is_invocable_r_v<future<value_type>, Func, const key_type&>
    loading_cache(loading_cache_config cfg, logging::logger& logger, Func&& load)
        : loading_cache(std::move(cfg), logger)
    {
        static_assert(ReloadEnabled == loading_cache_reload_enabled::yes, "This constructor should only be invoked when ReloadEnabled == loading_cache_reload_enabled::yes");

        _load = std::forward<Func>(load);

        // If expiration period is zero - caching is disabled
        if (!caching_enabled()) {
            return;
        }

        _timer_period = std::min(_cfg.expiry, _cfg.refresh);
        _timer.arm(_timer_period);
    }

    loading_cache(size_t max_size, lowres_clock::duration expiry, logging::logger& logger)
        : loading_cache({max_size, expiry, loading_cache_clock_type::time_point::max().time_since_epoch()}, logger)
    {
        static_assert(ReloadEnabled == loading_cache_reload_enabled::no, "This constructor should only be invoked when ReloadEnabled == loading_cache_reload_enabled::no");

        // If expiration period is zero - caching is disabled
        if (!caching_enabled()) {
            return;
        }

        _timer_period = _cfg.expiry;
        _timer.arm(_timer_period);
    }

    ~loading_cache() {
        auto value_destroyer = [] (ts_value_lru_entry* ptr) { loading_cache::destroy_ts_value(ptr); };
        _unprivileged_lru_list.erase_and_dispose(_unprivileged_lru_list.begin(), _unprivileged_lru_list.end(), value_destroyer);
        _lru_list.erase_and_dispose(_lru_list.begin(), _lru_list.end(), value_destroyer);
    }

    void reset() noexcept {
        _logger.info("Resetting cache");

        remove_if([](const value_type&){ return true; });
    }

    bool update_config(utils::loading_cache_config cfg) {
        _logger.info("Updating loading cache; max_size: {}, expiry: {}ms, refresh: {}ms", cfg.max_size,
                     std::chrono::duration_cast<std::chrono::milliseconds>(cfg.expiry).count(),
                     std::chrono::duration_cast<std::chrono::milliseconds>(cfg.refresh).count());

        if (!validate_config(cfg)) {
            _logger.debug("loading_cache: caching is enabled but refresh period and/or max_size are zero");
            return false;
        }

        _updated_cfg.emplace(std::move(cfg));

        // * If the timer is already armed we need to rearm it so that the changes on config can take place.
        // * If timer is not armed and caching is enabled, it means that on_timer was executed but its continuation hasn't finished yet,
        //   so we don't need to rearm it here, since on_timer's continuation will take care of that
        // * If caching is disabled and it's being enabled here on update_config, we also need to arm the timer, so that the changes on config
        //   can take place
        if (_timer.armed() ||
            (!caching_enabled() && _updated_cfg->expiry != loading_cache_clock_type::duration(0))) {
            _timer.rearm(loading_cache_clock_type::now() + loading_cache_clock_type::duration(std::chrono::milliseconds(1)));
        }

        return true;
    }

    template <typename LoadFunc>
    requires std::is_invocable_r_v<future<value_type>, LoadFunc, const key_type&>
    future<value_ptr> get_ptr(const Key& k, LoadFunc&& load) {
        // We shouldn't be here if caching is disabled
        assert(caching_enabled());

        return _loading_values.get_or_load(k, [load = std::forward<LoadFunc>(load)] (const Key& k) mutable {
            return load(k).then([] (value_type val) {
                return timestamped_val(std::move(val));
            });
        }).then([this, k] (timestamped_val_ptr ts_val_ptr) {
            // check again since it could have already been inserted and initialized
            if (!ts_val_ptr->ready() && !ts_val_ptr.orphaned()) {
                _logger.trace("{}: storing the value for the first time", k);

                if (ts_val_ptr->size() > _cfg.max_size) {
                    return make_exception_future<value_ptr>(entry_is_too_big());
                }

                ts_value_lru_entry* new_lru_entry = Alloc().template allocate_object<ts_value_lru_entry>();

                // Remove the least recently used items if map is too big.
                shrink();

                new(new_lru_entry) ts_value_lru_entry(std::move(ts_val_ptr), *this);

                // This will "touch" the entry and add it to the LRU list - we must do this before the shrink() call.
                value_ptr vp(new_lru_entry->timestamped_value_ptr());

                return make_ready_future<value_ptr>(std::move(vp));
            }

            return make_ready_future<value_ptr>(std::move(ts_val_ptr));
        });
    }

    future<value_ptr> get_ptr(const Key& k) {
        static_assert(ReloadEnabled == loading_cache_reload_enabled::yes, "");
        return get_ptr(k, _load);
    }

    future<Tp> get(const Key& k) {
        static_assert(ReloadEnabled == loading_cache_reload_enabled::yes, "");

        // If caching is disabled - always load in the foreground
        if (!caching_enabled()) {
            return _load(k);
        }

        return get_ptr(k).then([] (value_ptr v_ptr) {
            return make_ready_future<Tp>(*v_ptr);
        });
    }

    future<> stop() {
        return _timer_reads_gate.close().finally([this] { _timer.cancel(); });
    }

    /// Find a value for a specific Key value and touch() it.
    /// \tparam KeyType Key type
    /// \tparam KeyHasher Hash functor type
    /// \tparam KeyEqual Equality functor type
    ///
    /// \param key Key value to look for
    /// \param key_hasher_func Hash functor
    /// \param key_equal_func Equality functor
    /// \return cache_value_ptr object pointing to the found value or nullptr otherwise.
    template<typename KeyType, typename KeyHasher, typename KeyEqual>
    value_ptr find(const KeyType& key, KeyHasher key_hasher_func, KeyEqual key_equal_func) noexcept {
        // cache_value_ptr constructor is going to update a "last read" timestamp of the corresponding object and move
        // the object to the front of the LRU
        return set_find(key, std::move(key_hasher_func), std::move(key_equal_func));
    };

    value_ptr find(const Key& k) noexcept {
        return set_find(k);
    }

    // Removes all values matching a given predicate and values which are currently loading.
    // Guarantees that no values which match the predicate and whose loading was initiated
    // before this call will be present after this call (or appear at any time later).
    // The predicate may be invoked multiple times on the same value.
    // It must return the same result for a given value (it must be a pure function).
    template <typename Pred>
    requires std::is_invocable_r_v<bool, Pred, const value_type&>
    void remove_if(Pred&& pred) {
        auto cond_pred = [&pred] (const ts_value_lru_entry& v) {
            return pred(v.timestamped_value().value());
        };
        auto value_destroyer = [] (ts_value_lru_entry* p) {
            loading_cache::destroy_ts_value(p);
        };

        _unprivileged_lru_list.remove_and_dispose_if(cond_pred, value_destroyer);
        _lru_list.remove_and_dispose_if(cond_pred, value_destroyer);
        _loading_values.remove_if([&pred] (const timestamped_val& v) {
            return pred(v.value());
        });
    }

    // Removes a given key from the cache.
    // The key is removed immediately.
    // After this, get_ptr() is guaranteed to reload the value before returning it.
    // As a consequence of the above, if there is a concurrent get_ptr() in progress with this,
    // its value will not populate the cache. It will still succeed.
    void remove(const Key& k) {
        remove_ts_value(set_find(k));
        // set_find() returns nullptr for a key which is currently loading, which we want to remove too.
        _loading_values.remove(k);
    }

    // Removes a given key from the cache.
    // Same guarantees as with remove(key).
    template<typename KeyType, typename KeyHasher, typename KeyEqual>
    void remove(const KeyType& key, KeyHasher key_hasher_func, KeyEqual key_equal_func) noexcept {
        remove_ts_value(set_find(key, key_hasher_func, key_equal_func));
        // set_find() returns nullptr for a key which is currently loading, which we want to remove too.
        _loading_values.remove(key, key_hasher_func, key_equal_func);
    }

    size_t size() const {
        return _lru_list.size() + _unprivileged_lru_list.size();
    }

    /// \brief returns the memory size the currently cached entries occupy according to the EntrySize predicate.
    size_t memory_footprint() const noexcept {
        return _unprivileged_section_size + _privileged_section_size;
    }

    /// \brief returns the memory size the currently cached entries occupy in the privileged section according to the EntrySize predicate.
    size_t privileged_section_memory_footprint() const noexcept {
        return _privileged_section_size;
    }

    /// \brief returns the memory size the currently cached entries occupy in the unprivileged section according to the EntrySize predicate.
    size_t unprivileged_section_memory_footprint() const noexcept {
        return _unprivileged_section_size;
    }
private:
    void remove_ts_value(timestamped_val_ptr ts_ptr) {
        if (!ts_ptr) {
            return;
        }
        ts_value_lru_entry* lru_entry_ptr = ts_ptr->lru_entry_ptr();
        lru_list_type& entry_list = container_list(*lru_entry_ptr);
        entry_list.erase_and_dispose(entry_list.iterator_to(*lru_entry_ptr), [] (ts_value_lru_entry* p) { loading_cache::destroy_ts_value(p); });
    }

    timestamped_val_ptr ready_entry_ptr(timestamped_val_ptr tv_ptr) {
        if (!tv_ptr || !tv_ptr->ready()) {
            return nullptr;
        }
        return std::move(tv_ptr);
    }

    lru_list_type& container_list(const ts_value_lru_entry& lru_entry_ptr) noexcept {
        return (lru_entry_ptr.touch_count() > SectionHitThreshold) ? _lru_list : _unprivileged_lru_list;
    }

    template<typename KeyType, typename KeyHasher, typename KeyEqual>
    timestamped_val_ptr set_find(const KeyType& key, KeyHasher key_hasher_func, KeyEqual key_equal_func) noexcept {
        return ready_entry_ptr(_loading_values.find(key, std::move(key_hasher_func), std::move(key_equal_func)));
    }

    // keep the default non-templated overloads to ease on the compiler for specifications
    // that do not require the templated find().
    timestamped_val_ptr set_find(const Key& key) noexcept {
        return ready_entry_ptr(_loading_values.find(key));
    }

    bool caching_enabled() const {
        return _cfg.expiry != lowres_clock::duration(0);
    }

    static void destroy_ts_value(ts_value_lru_entry* val) noexcept {
        Alloc().delete_object(val);
    }

    /// This is the core method in the 2 sections LRU implementation.
    /// Set the given item as the most recently used item at the corresponding cache section.
    /// The MRU item is going to be at the front of the list, the LRU item - at the back.
    /// The entry is initially entering the "unprivileged" section (represented by a _unprivileged_lru_list).
    /// After an entry is touched more than SectionHitThreshold times it moves to a "privileged" section
    /// (represented by an _lru_list).
    ///
    /// \param lru_entry Cache item that has been "touched"
    void touch_lru_entry_2_sections(ts_value_lru_entry& lru_entry) {
        if (lru_entry.is_linked()) {
            lru_list_type& lru_list = container_list(lru_entry);
            lru_list.erase(lru_list.iterator_to(lru_entry));
        }

        if (lru_entry.touch_count() < SectionHitThreshold) {
            _logger.trace("Putting key {} into the unprivileged section", lru_entry.key());
            _unprivileged_lru_list.push_front(lru_entry);
            lru_entry.inc_touch_count();
        } else {
            _logger.trace("Putting key {} into the privileged section", lru_entry.key());
            _lru_list.push_front(lru_entry);

            // Bump it up only once to avoid a wrap around
            if (lru_entry.touch_count() == SectionHitThreshold) {
                // This code will run only once, when a promotion
                // from unprivileged to privileged section happens.
                // Update section size bookkeeping.
                
                lru_entry.owning_section_size() -= lru_entry.timestamped_value().size();
                lru_entry.inc_touch_count();
                lru_entry.owning_section_size() += lru_entry.timestamped_value().size();
            }
        }
    }

    future<> reload(timestamped_val_ptr ts_value_ptr) {
        const Key& key = loading_values_type::to_key(ts_value_ptr);

        // Do nothing if the entry has been dropped before we got here (e.g. by the _load() call on another key that is
        // also being reloaded).
        if (!ts_value_ptr->lru_entry_ptr()) {
            _logger.trace("{}: entry was dropped before the reload", key);
            return make_ready_future<>();
        }

        return _load(key).then_wrapped([this, ts_value_ptr = std::move(ts_value_ptr), &key] (auto&& f) mutable {
            // if the entry has been evicted by now - simply end here
            if (!ts_value_ptr->lru_entry_ptr()) {
                _logger.trace("{}: entry was dropped during the reload", key);
                return make_ready_future<>();
            }

            // The exceptions are related to the load operation itself.
            // We should ignore them for the background reads - if
            // they persist the value will age and will be reloaded in
            // the forground. If the foreground READ fails the error
            // will be propagated up to the user and will fail the
            // corresponding query.
            try {
                *ts_value_ptr = f.get0();
            } catch (std::exception& e) {
                _logger.debug("{}: reload failed: {}", key, e.what());
            } catch (...) {
                _logger.debug("{}: reload failed: unknown error", key);
            }

            return make_ready_future<>();
        });
    }

    void drop_expired() {
        auto now = loading_cache_clock_type::now();
        auto expiration_cond = [now, this] (const ts_value_lru_entry& lru_entry) {
            using namespace std::chrono;
            // An entry should be discarded if it hasn't been reloaded for too long or nobody cares about it anymore
            const timestamped_val& v = lru_entry.timestamped_value();
            auto since_last_read = now - v.last_read();
            auto since_loaded = now - v.loaded();
            if (_cfg.expiry < since_last_read || (ReloadEnabled == loading_cache_reload_enabled::yes && _cfg.expiry < since_loaded)) {
                _logger.trace("drop_expired(): {}: dropping the entry: expiry {},  ms passed since: loaded {} last_read {}", lru_entry.key(), _cfg.expiry.count(), duration_cast<milliseconds>(since_loaded).count(), duration_cast<milliseconds>(since_last_read).count());
                return true;
            }
            return false;
        };
        auto value_destroyer = [] (ts_value_lru_entry* p) {
            loading_cache::destroy_ts_value(p);
        };

        _unprivileged_lru_list.remove_and_dispose_if(expiration_cond, value_destroyer);
        _lru_list.remove_and_dispose_if(expiration_cond, value_destroyer);
    }

    // Shrink the cache to the max_size discarding the least recently used items.
    // Get rid from the entries that were used exactly once first.
    void shrink() noexcept {
        using namespace std::chrono;

        auto drop_privileged_entry = [&] {
            ts_value_lru_entry& lru_entry = *_lru_list.rbegin();
            _logger.trace("shrink(): {}: dropping the entry: ms since last_read {}", lru_entry.key(), duration_cast<milliseconds>(loading_cache_clock_type::now() - lru_entry.timestamped_value().last_read()).count());
            loading_cache::destroy_ts_value(&lru_entry);
            LoadingCacheStats::inc_privileged_on_cache_size_eviction();
        };

        auto drop_unprivileged_entry = [&] {
            ts_value_lru_entry& lru_entry = *_unprivileged_lru_list.rbegin();
            _logger.trace("shrink(): {}: dropping the unprivileged entry: ms since last_read {}", lru_entry.key(), duration_cast<milliseconds>(loading_cache_clock_type::now() - lru_entry.timestamped_value().last_read()).count());
            loading_cache::destroy_ts_value(&lru_entry);
            LoadingCacheStats::inc_unprivileged_on_cache_size_eviction();
        };

        // When cache entries need to be evicted due to a size restriction,
        // unprivileged section entries are evicted first.
        //
        // However, we make sure that the unprivileged section does not get
        // too small, because this could lead to starving the unprivileged section.
        // For example if the cache could store at most 50 entries and there are 49 entries in
        // privileged section, after adding 5 entries (that would go to unprivileged
        // section) 4 of them would get evicted and only the 5th one would stay.
        // This caused problems with BATCH statements where all prepared statements
        // in the batch have to stay in cache at the same time for the batch to correctly
        // execute.
        auto minimum_unprivileged_section_size = _cfg.max_size / 2;
        while (memory_footprint() >= _cfg.max_size && _unprivileged_section_size > minimum_unprivileged_section_size) {
            drop_unprivileged_entry();
        }

        while (memory_footprint() >= _cfg.max_size && !_lru_list.empty()) {
            drop_privileged_entry();
        }

        // If dropping entries from privileged section did not help,
        // we have to drop entries from unprivileged section,
        // going below minimum_unprivileged_section_size.
        while (memory_footprint() >= _cfg.max_size) {
            drop_unprivileged_entry();
        }
    }

    // Try to bring the load factors of the _loading_values into a known range.
    void periodic_rehash() noexcept {
        try {
            _loading_values.rehash();
        } catch (...) {
            // if rehashing fails - continue with the current buckets array
        }
    }

    void on_timer() {
        _logger.trace("on_timer(): start");

        if (_updated_cfg) {
            _cfg = *_updated_cfg;
            _updated_cfg.reset();
            _timer_period = std::min(_cfg.expiry, _cfg.refresh);
        }

        // Caching might have been disabled during a config update
        if (!caching_enabled()) {
            reset();
            return;
        }

        // Clean up items that were not touched for the whole expiry period.
        drop_expired();

        // check if rehashing is needed and do it if it is.
        periodic_rehash();

        if constexpr (ReloadEnabled == loading_cache_reload_enabled::no) {
            _logger.trace("on_timer(): rearming");
            _timer.arm(loading_cache_clock_type::now() + _timer_period);
            return;
        }

        // Reload all those which value needs to be reloaded.
        // Future is waited on indirectly in `stop()` (via `_timer_reads_gate`).
        // FIXME: error handling
        (void)with_gate(_timer_reads_gate, [this] {
            auto to_reload = boost::copy_range<utils::chunked_vector<timestamped_val_ptr>>(boost::range::join(_unprivileged_lru_list, _lru_list)
                    | boost::adaptors::filtered([this] (ts_value_lru_entry& lru_entry) {
                        return lru_entry.timestamped_value().loaded() + _cfg.refresh < loading_cache_clock_type::now();
                    })
                    | boost::adaptors::transformed([] (ts_value_lru_entry& lru_entry) {
                        return lru_entry.timestamped_value_ptr();
                    }));

            return parallel_for_each(std::move(to_reload), [this] (timestamped_val_ptr ts_value_ptr) {
                _logger.trace("on_timer(): {}: reloading the value", loading_values_type::to_key(ts_value_ptr));
                return this->reload(std::move(ts_value_ptr));
            }).finally([this] {
                _logger.trace("on_timer(): rearming");

                // If the config was updated after on_timer and before this continuation finished
                // it's necessary to run on_timer again to make sure that everything will be reloaded correctly
                if (_updated_cfg) {
                    _timer.arm(loading_cache_clock_type::now() + loading_cache_clock_type::duration(std::chrono::milliseconds(1)));
                } else {
                    _timer.arm(loading_cache_clock_type::now() + _timer_period);
                }
            });
        });
    }

    loading_values_type _loading_values;
    lru_list_type _lru_list;              // list containing "privileged" section entries
    lru_list_type _unprivileged_lru_list; // list containing "unprivileged" section entries
    size_t _privileged_section_size = 0;
    size_t _unprivileged_section_size = 0;
    loading_cache_clock_type::duration _timer_period;
    loading_cache_config _cfg;
    std::optional<loading_cache_config> _updated_cfg;
    logging::logger& _logger;
    std::function<future<Tp>(const Key&)> _load;
    timer<loading_cache_clock_type> _timer;
    seastar::gate _timer_reads_gate;
};

template<typename Key, typename Tp, int SectionHitThreshold, loading_cache_reload_enabled ReloadEnabled, typename EntrySize, typename Hash, typename EqualPred, typename LoadingSharedValuesStats, typename LoadingCacheStats, typename Alloc>
class loading_cache<Key, Tp, SectionHitThreshold, ReloadEnabled, EntrySize, Hash, EqualPred, LoadingSharedValuesStats, LoadingCacheStats, Alloc>::timestamped_val::value_ptr {
private:
    using loading_values_type = typename timestamped_val::loading_values_type;

public:
    using timestamped_val_ptr = typename loading_values_type::entry_ptr;
    using value_type = Tp;

private:
    timestamped_val_ptr _ts_val_ptr;

public:
    value_ptr(timestamped_val_ptr ts_val_ptr) : _ts_val_ptr(std::move(ts_val_ptr)) {
        if (_ts_val_ptr) {
            _ts_val_ptr->touch();
        }
    }
    value_ptr(std::nullptr_t) noexcept : _ts_val_ptr() {}
    bool operator==(const value_ptr&) const = default;
    explicit operator bool() const noexcept { return bool(_ts_val_ptr); }
    value_type& operator*() const noexcept { return _ts_val_ptr->value(); }
    value_type* operator->() const noexcept { return &_ts_val_ptr->value(); }

    friend std::ostream& operator<<(std::ostream& os, const value_ptr& vp) {
        return os << vp._ts_val_ptr;
    }
};

/// \brief This is and LRU list entry which is also an anchor for a loading_cache value.
template<typename Key, typename Tp, int SectionHitThreshold, loading_cache_reload_enabled ReloadEnabled, typename EntrySize, typename Hash, typename EqualPred, typename LoadingSharedValuesStats, typename  LoadingCacheStats, typename Alloc>
class loading_cache<Key, Tp, SectionHitThreshold, ReloadEnabled, EntrySize, Hash, EqualPred, LoadingSharedValuesStats, LoadingCacheStats, Alloc>::timestamped_val::lru_entry : public safe_link_list_hook {
private:
    using loading_values_type = typename timestamped_val::loading_values_type;

public:
    using lru_list_type = bi::list<lru_entry>;
    using timestamped_val_ptr = typename loading_values_type::entry_ptr;

private:
    timestamped_val_ptr _ts_val_ptr;
    loading_cache& _parent;
    int _touch_count;

public:
    lru_entry(timestamped_val_ptr ts_val, loading_cache& owner_cache)
        : _ts_val_ptr(std::move(ts_val))
        , _parent(owner_cache)
        , _touch_count(0)
    {
        // We don't want to allow SectionHitThreshold to be greater than half the max value of _touch_count to avoid a wrap around
        static_assert(SectionHitThreshold <= std::numeric_limits<typeof(_touch_count)>::max() / 2, "SectionHitThreshold value is too big");

        _ts_val_ptr->set_anchor_back_reference(this);
        owning_section_size() += _ts_val_ptr->size();
    }

    void inc_touch_count() noexcept {
        ++_touch_count;
    }

    int touch_count() const noexcept {
        return _touch_count;
    }

    ~lru_entry() {
        if (safe_link_list_hook::is_linked()) {
            lru_list_type& lru_list = _parent.container_list(*this);
            lru_list.erase(lru_list.iterator_to(*this));
        }
        owning_section_size() -= _ts_val_ptr->size();
        _ts_val_ptr->set_anchor_back_reference(nullptr);
    }

    size_t& owning_section_size() noexcept {
        return _touch_count <= SectionHitThreshold ? _parent._unprivileged_section_size : _parent._privileged_section_size;
    }

    void touch() noexcept {
        _parent.touch_lru_entry_2_sections(*this);
    }

    const Key& key() const noexcept {
        return loading_values_type::to_key(_ts_val_ptr);
    }

    timestamped_val& timestamped_value() noexcept { return *_ts_val_ptr; }
    const timestamped_val& timestamped_value() const noexcept { return *_ts_val_ptr; }
    timestamped_val_ptr timestamped_value_ptr() noexcept { return _ts_val_ptr; }
};

}

#include <chrono>
#include <string_view>
#include <functional>
#include <iostream>
#include <optional>
#include <utility>

#include <seastar/core/future.hh>
#include <seastar/core/shared_ptr.hh>
#include <seastar/core/sstring.hh>


namespace std {

inline std::ostream& operator<<(std::ostream& os, const pair<auth::role_or_anonymous, auth::resource>& p) {
    os << "{role: " << p.first << ", resource: " << p.second << "}";
    return os;
}

}

namespace db {
class config;
}

namespace auth {

class service;

class permissions_cache final {
    using cache_type = utils::loading_cache<
            std::pair<role_or_anonymous, resource>,
            permission_set,
            1,
            utils::loading_cache_reload_enabled::yes,
            utils::simple_entry_size<permission_set>,
            utils::tuple_hash>;

    using key_type = typename cache_type::key_type;

    cache_type _cache;

public:
    explicit permissions_cache(const utils::loading_cache_config&, service&, logging::logger&);

    future <> stop() {
        return _cache.stop();
    }

    bool update_config(utils::loading_cache_config);
    void reset();
    future<permission_set> get(const role_or_anonymous&, const resource&);
};

}


#include <string_view>
#include <memory>
#include <optional>
#include <stdexcept>
#include <unordered_set>

#include <seastar/core/future.hh>
#include <seastar/core/print.hh>
#include <seastar/core/sstring.hh>

namespace auth {

struct role_config final {
    bool is_superuser{false};
    bool can_login{false};
};

///
/// Differential update for altering existing roles.
///
struct role_config_update final {
    std::optional<bool> is_superuser{};
    std::optional<bool> can_login{};
};

///
/// A logical argument error for a role-management operation.
///
class roles_argument_exception : public exceptions::invalid_request_exception {
public:
    using exceptions::invalid_request_exception::invalid_request_exception;
};

class role_already_exists : public roles_argument_exception {
public:
    explicit role_already_exists(std::string_view role_name)
            : roles_argument_exception(format("Role {} already exists.", role_name)) {
    }
};

class nonexistant_role : public roles_argument_exception {
public:
    explicit nonexistant_role(std::string_view role_name)
            : roles_argument_exception(format("Role {} doesn't exist.", role_name)) {
    }
};

class role_already_included : public roles_argument_exception {
public:
    role_already_included(std::string_view grantee_name, std::string_view role_name)
            : roles_argument_exception(
                      format("{} already includes role {}.", grantee_name, role_name)) {
    }
};

class revoke_ungranted_role : public roles_argument_exception {
public:
    revoke_ungranted_role(std::string_view revokee_name, std::string_view role_name)
            : roles_argument_exception(
                      format("{} was not granted role {}, so it cannot be revoked.", revokee_name, role_name)) {
    }
};

using role_set = std::unordered_set<sstring>;

enum class recursive_role_query { yes, no };

///
/// Abstract client for managing roles.
///
/// All state necessary for managing roles is stored externally to the client instance.
///
/// All implementations should throw role-related exceptions as documented. Authorization is not addressed here, and
/// access-control should never be enforced in implementations.
///
class role_manager {
public:
    // this type represents a mapping between a role and some attribute value.
    // i.e: given attribute name  'a' this map holds role name and it's assigned
    // value of 'a'.
    using attribute_vals = std::unordered_map<sstring, sstring>;
    using ptr_type = std::unique_ptr<role_manager>;
public:
    virtual ~role_manager() = default;

    virtual std::string_view qualified_java_name() const noexcept = 0;

    virtual const resource_set& protected_resources() const = 0;

    virtual future<> start() = 0;

    virtual future<> stop() = 0;

    ///
    /// \returns an exceptional future with \ref role_already_exists for a role that has previously been created.
    ///
    virtual future<> create(std::string_view role_name, const role_config&) = 0;

    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    virtual future<> drop(std::string_view role_name) = 0;

    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    virtual future<> alter(std::string_view role_name, const role_config_update&) = 0;

    ///
    /// Grant `role_name` to `grantee_name`.
    ///
    /// \returns an exceptional future with \ref nonexistant_role if either the role or the grantee do not exist.
    ///
    /// \returns an exceptional future with \ref role_already_included if granting the role would be redundant, or
    /// create a cycle.
    ///
    virtual future<> grant(std::string_view grantee_name, std::string_view role_name) = 0;

    ///
    /// Revoke `role_name` from `revokee_name`.
    ///
    /// \returns an exceptional future with \ref nonexistant_role if either the role or the revokee do not exist.
    ///
    /// \returns an exceptional future with \ref revoke_ungranted_role if the role was not granted.
    ///
    virtual future<> revoke(std::string_view revokee_name, std::string_view role_name) = 0;

    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    virtual future<role_set> query_granted(std::string_view grantee, recursive_role_query) = 0;

    virtual future<role_set> query_all() = 0;

    virtual future<bool> exists(std::string_view role_name) = 0;

    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    virtual future<bool> is_superuser(std::string_view role_name) = 0;

    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    virtual future<bool> can_login(std::string_view role_name) = 0;

    ///
    /// \returns the value of the named attribute, if one is set.
    ///
    virtual future<std::optional<sstring>> get_attribute(std::string_view role_name, std::string_view attribute_name) = 0;

    ///
    /// \returns a mapping of each role's value for the named attribute, if one is set for the role.
    ///
    virtual future<attribute_vals> query_attribute_for_all(std::string_view attribute_name) = 0;

    /// Sets `attribute_name` with `attribute_value` for `role_name`.
    /// \returns an exceptional future with nonexistant_role if the role does not exist.
    ///
    virtual future<> set_attribute(std::string_view role_name, std::string_view attribute_name, std::string_view attribute_value) = 0;

    /// Removes `attribute_name` for `role_name`.
    /// \returns an exceptional future with nonexistant_role if the role does not exist.
    /// \note: This is a no-op if the role does not have the named attribute set.
    ///
    virtual future<> remove_attribute(std::string_view role_name, std::string_view attribute_name) = 0;
};
}

#include <string_view>
#include <memory>
#include <optional>

#include <seastar/core/future.hh>
#include <seastar/core/sstring.hh>
#include <seastar/util/bool_class.hh>
#include <seastar/core/sharded.hh>


namespace cql3 {
class query_processor;
}

namespace service {
class migration_manager;
class migration_notifier;
class migration_listener;
}

namespace auth {

class role_or_anonymous;

struct service_config final {
    sstring authorizer_java_name;
    sstring authenticator_java_name;
    sstring role_manager_java_name;
};

///
/// Due to poor (in this author's opinion) decisions of Apache Cassandra, certain choices of one role-manager,
/// authenticator, or authorizer imply restrictions on the rest.
///
/// This exception is thrown when an invalid combination of modules is selected, with a message explaining the
/// incompatibility.
///
class incompatible_module_combination : public std::invalid_argument {
public:
    using std::invalid_argument::invalid_argument;
};

///
/// Client for access-control in the system.
///
/// Access control encompasses user/role management, authentication, and authorization. This client provides access to
/// the dynamically-loaded implementations of these modules (through the `underlying_*` member functions), but also
/// builds on their functionality with caching and abstractions for common operations.
///
/// All state associated with access-control is stored externally to any particular instance of this class.
///
/// peering_sharded_service inheritance is needed to be able to access shard local authentication service
/// given an object from another shard. Used for bouncing lwt requests to correct shard.
class service final : public seastar::peering_sharded_service<service> {
    utils::loading_cache_config _loading_cache_config;
    std::unique_ptr<permissions_cache> _permissions_cache;

    cql3::query_processor& _qp;

    ::service::migration_notifier& _mnotifier;

    authorizer::ptr_type _authorizer;

    authenticator::ptr_type _authenticator;

    role_manager::ptr_type _role_manager;

    // Only one of these should be registered, so we end up with some unused instances. Not the end of the world.
    std::unique_ptr<::service::migration_listener> _migration_listener;

    std::function<void(uint32_t)> _permissions_cache_cfg_cb;
    serialized_action _permissions_cache_config_action;

    utils::observer<uint32_t> _permissions_cache_max_entries_observer;
    utils::observer<uint32_t> _permissions_cache_update_interval_in_ms_observer;
    utils::observer<uint32_t> _permissions_cache_validity_in_ms_observer;

public:
    service(
            utils::loading_cache_config,
            cql3::query_processor&,
            ::service::migration_notifier&,
            std::unique_ptr<authorizer>,
            std::unique_ptr<authenticator>,
            std::unique_ptr<role_manager>);

    ///
    /// This constructor is intended to be used when the class is sharded via \ref seastar::sharded. In that case, the
    /// arguments must be copyable, which is why we delay construction with instance-construction instructions instead
    /// of the instances themselves.
    ///
    service(
            utils::loading_cache_config,
            cql3::query_processor&,
            ::service::migration_notifier&,
            ::service::migration_manager&,
            const service_config&);

    future<> start(::service::migration_manager&);

    future<> stop();

    void update_cache_config();

    void reset_authorization_cache();

    ///
    /// \returns an exceptional future with \ref nonexistant_role if the named role does not exist.
    ///
    future<permission_set> get_permissions(const role_or_anonymous&, const resource&) const;

    ///
    /// Like \ref get_permissions, but never returns cached permissions.
    ///
    future<permission_set> get_uncached_permissions(const role_or_anonymous&, const resource&) const;

    ///
    /// Query whether the named role has been granted a role that is a superuser.
    ///
    /// A role is always granted to itself. Therefore, a role that "is" a superuser also "has" superuser.
    ///
    /// \returns an exceptional future with \ref nonexistant_role if the role does not exist.
    ///
    future<bool> has_superuser(std::string_view role_name) const;

    ///
    /// Return the set of all roles granted to the given role, including itself and roles granted through other roles.
    ///
    /// \returns an exceptional future with \ref nonexistent_role if the role does not exist.
    future<role_set> get_roles(std::string_view role_name) const;

    future<bool> exists(const resource&) const;

    const authenticator& underlying_authenticator() const {
        return *_authenticator;
    }

    const authorizer& underlying_authorizer() const {
        return *_authorizer;
    }

    role_manager& underlying_role_manager() const {
        return *_role_manager;
    }

private:
    future<bool> has_existing_legacy_users() const;

    future<> create_keyspace_if_missing(::service::migration_manager& mm) const;
};

future<bool> has_superuser(const service&, const authenticated_user&);

future<role_set> get_roles(const service&, const authenticated_user&);

future<permission_set> get_permissions(const service&, const authenticated_user&, const resource&);

///
/// Access-control is "enforcing" when either the authenticator or the authorizer are not their "allow-all" variants.
///
/// Put differently, when access control is not enforcing, all operations on resources will be allowed and users do not
/// need to authenticate themselves.
///
bool is_enforcing(const service&);

/// A description of a CQL command from which auth::service can tell whether or not this command could endanger
/// internal data on which auth::service depends.
struct command_desc {
    auth::permission permission; ///< Nature of the command's alteration.
    const ::auth::resource& resource; ///< Resource impacted by this command.
    enum class type {
        ALTER_WITH_OPTS, ///< Command is ALTER ... WITH ...
        OTHER
    } type_ = type::OTHER;
};

///
/// Protected resources cannot be modified even if the performer has permissions to do so.
///
bool is_protected(const service&, command_desc) noexcept;

///
/// Create a role with optional authentication information.
///
/// \returns an exceptional future with \ref role_already_exists if the user or role exists.
///
/// \returns an exceptional future with \ref unsupported_authentication_option if an unsupported option is included.
///
future<> create_role(
        const service&,
        std::string_view name,
        const role_config&,
        const authentication_options&);

///
/// Alter an existing role and its authentication information.
///
/// \returns an exceptional future with \ref nonexistant_role if the named role does not exist.
///
/// \returns an exceptional future with \ref unsupported_authentication_option if an unsupported option is included.
///
future<> alter_role(
        const service&,
        std::string_view name,
        const role_config_update&,
        const authentication_options&);

///
/// Drop a role from the system, including all permissions and authentication information.
///
/// \returns an exceptional future with \ref nonexistant_role if the named role does not exist.
///
future<> drop_role(const service&, std::string_view name);

///
/// Check if `grantee` has been granted the named role.
///
/// \returns an exceptional future with \ref nonexistent_role if `grantee` or `name` do not exist.
///
future<bool> has_role(const service&, std::string_view grantee, std::string_view name);
///
/// Check if the authenticated user has been granted the named role.
///
/// \returns an exceptional future with \ref nonexistent_role if the user or `name` do not exist.
///
future<bool> has_role(const service&, const authenticated_user&, std::string_view name);

///
/// \returns an exceptional future with \ref nonexistent_role if the named role does not exist.
///
/// \returns an exceptional future with \ref unsupported_authorization_operation if granting permissions is not
/// supported.
///
future<> grant_permissions(
        const service&,
        std::string_view role_name,
        permission_set,
        const resource&);

///
/// Like \ref grant_permissions, but grants all applicable permissions on the resource.
///
/// \returns an exceptional future with \ref nonexistent_role if the named role does not exist.
///
/// \returns an exceptional future with \ref unsupported_authorization_operation if granting permissions is not
/// supported.
///
future<> grant_applicable_permissions(const service&, std::string_view role_name, const resource&);
future<> grant_applicable_permissions(const service&, const authenticated_user&, const resource&);

///
/// \returns an exceptional future with \ref nonexistent_role if the named role does not exist.
///
/// \returns an exceptional future with \ref unsupported_authorization_operation if revoking permissions is not
/// supported.
///
future<> revoke_permissions(
        const service&,
        std::string_view role_name,
        permission_set,
        const resource&);

using recursive_permissions = bool_class<struct recursive_permissions_tag>;

///
/// Query for all granted permissions according to filtering criteria.
///
/// Only permissions included in the provided set are included.
///
/// If a role name is provided, only permissions granted (directly or recursively) to the role are included.
///
/// If a resource filter is provided, only permissions granted on the resource are included. When \ref
/// recursive_permissions is `true`, permissions on a parent resource are included.
///
/// \returns an exceptional future with \ref nonexistent_role if a role name is included which refers to a role that
/// does not exist.
///
/// \returns an exceptional future with \ref unsupported_authorization_operation if listing permissions is not
/// supported.
///
future<std::vector<permission_details>> list_filtered_permissions(
        const service&,
        permission_set,
        std::optional<std::string_view> role_name,
        const std::optional<std::pair<resource, recursive_permissions>>& resource_filter);

}


#include <iosfwd>
#include <seastar/core/print.hh>
#include <seastar/core/sstring.hh>
#include <seastar/core/enum.hh>

namespace unimplemented {

enum class cause {
    API,
    INDEXES,
    LWT,
    PAGING,
    AUTH,
    PERMISSIONS,
    TRIGGERS,
    COUNTERS,
    METRICS,
    MIGRATIONS,
    GOSSIP,
    TOKEN_RESTRICTION,
    LEGACY_COMPOSITE_KEYS,
    COLLECTION_RANGE_TOMBSTONES,
    RANGE_DELETES,
    THRIFT,
    VALIDATION,
    REVERSED,
    COMPRESSION,
    NONATOMIC,
    CONSISTENCY,
    HINT,
    SUPER,
    WRAP_AROUND, // Support for handling wrap around ranges in queries on database level and below
    STORAGE_SERVICE,
    SCHEMA_CHANGE,
    MIXED_CF,
    SSTABLE_FORMAT_M,
};

[[noreturn]] void fail(cause what);
void warn(cause what);

}

namespace std {

template <>
struct hash<unimplemented::cause> : seastar::enum_hash<unimplemented::cause> {};

}

#include <seastar/core/lowres_clock.hh>
#include <seastar/core/semaphore.hh>
#include <chrono>

namespace db {
using timeout_clock = seastar::lowres_clock;
using timeout_semaphore = seastar::basic_semaphore<seastar::default_timeout_exception_factory, timeout_clock>;
using timeout_semaphore_units = seastar::semaphore_units<seastar::default_timeout_exception_factory, timeout_clock>;
static constexpr timeout_clock::time_point no_timeout = timeout_clock::time_point::max();
}

#include <chrono>

namespace db { class config; }

class updateable_timeout_config;

/// timeout_config represents a snapshot of the options stored in it when
/// an instance of this class is created. so far this class is only used by
/// client_state and thrift_handler. so either these classes are obliged to
/// update it by themselves, or they are fine with using the maybe-updated
/// options in the lifecycle of a client / connection even if some of these
/// options are changed whtn the client / connection is still alive.
struct timeout_config {
    using duration_t = db::timeout_clock::duration;

    duration_t read_timeout;
    duration_t write_timeout;
    duration_t range_read_timeout;
    duration_t counter_write_timeout;
    duration_t truncate_timeout;
    duration_t cas_timeout;
    duration_t other_timeout;
};

struct updateable_timeout_config {
    using timeout_option_t = utils::updateable_value<uint32_t>;

    timeout_option_t read_timeout_in_ms;
    timeout_option_t write_timeout_in_ms;
    timeout_option_t range_read_timeout_in_ms;
    timeout_option_t counter_write_timeout_in_ms;
    timeout_option_t truncate_timeout_in_ms;
    timeout_option_t cas_timeout_in_ms;
    timeout_option_t other_timeout_in_ms;

    explicit updateable_timeout_config(const db::config& cfg);

    timeout_config current_values() const;
};


using timeout_config_selector = db::timeout_clock::duration (timeout_config::*);

extern const timeout_config infinite_timeout_config;



using inet_address_vector_replica_set = utils::small_vector<gms::inet_address, 3>;

using inet_address_vector_topology_change = utils::small_vector<gms::inet_address, 1>;

#include <deque>
#include <unordered_set>
#include <seastar/util/lazy.hh>
#include <seastar/core/weak_ptr.hh>
#include <seastar/core/checked_ptr.hh>

namespace cql3{
class query_options;
struct raw_value_view;
struct raw_value_view_vector_with_unset;

namespace statements {
class prepared_statement;
}
}

namespace tracing {

using prepared_checked_weak_ptr = seastar::checked_ptr<seastar::weak_ptr<cql3::statements::prepared_statement>>;

class trace_state final {
public:
    // A primary session may be in 3 states:
    //   - "inactive": between the creation and a begin() call.
    //   - "foreground": after a begin() call and before a
    //     stop_foreground_and_write() call.
    //   - "background": after a stop_foreground_and_write() call and till the
    //     state object is destroyed.
    //
    // - Traces are not allowed while state is in an "inactive" state.
    // - The time the primary session was in a "foreground" state is the time
    //   reported as a session's "duration".
    // - Traces that have arrived during the "background" state will be recorded
    //   as usual but their "elapsed" time will be greater or equal to the
    //   session's "duration".
    //
    // Secondary sessions may only be in an "inactive" or in a "foreground"
    // states.
    enum class state {
        inactive,
        foreground,
        background
    };

private:
    shared_ptr<tracing> _local_tracing_ptr;
    trace_state_props_set _state_props;
    lw_shared_ptr<one_session_records> _records;
    // Used for calculation of time passed since the beginning of a tracing
    // session till each tracing event. Secondary slow-query-logging sessions inherit `_start` from parents.
    elapsed_clock::time_point _start;
    std::optional<uint64_t> _supplied_start_ts_us; // Parent's `_start`, as microseconds from POSIX epoch.
    std::chrono::microseconds _slow_query_threshold;
    state _state = state::inactive;

    struct params_values;
    struct params_values_deleter {
        void operator()(params_values* pv) {}
    };

    class params_ptr {
    private:
        std::unique_ptr<params_values, params_values_deleter> _vals;
        params_values* get_ptr_safe();

    public:
        explicit operator bool() const {
            return (bool)_vals;
        }

        params_values* operator->() {
            return get_ptr_safe();
        }

        params_values& operator*() {
            return *get_ptr_safe();
        }
    } _params_ptr;

    static trace_state_props_set make_primary(trace_state_props_set props) {
        if (!props.contains(trace_state_props::full_tracing) && !props.contains(trace_state_props::log_slow_query)) {
            throw std::logic_error("A primary session has to be created for either full tracing or a slow query logging");
        }
        props.set(trace_state_props::primary);
        return props;
    }

    static trace_state_props_set make_secondary(trace_state_props_set props) noexcept {
        props.remove(trace_state_props::primary);
        // Default a secondary session to a full tracing.
        // We may get both zeroes for a full_tracing and a log_slow_query if a
        // primary session is created with an older server version.
        props.set_if<trace_state_props::full_tracing>(!props.contains(trace_state_props::full_tracing) && !props.contains(trace_state_props::log_slow_query));
        return props;
    }

public:
    trace_state(trace_type type, trace_state_props_set props)
        : _local_tracing_ptr(tracing::get_local_tracing_instance().shared_from_this())
        , _state_props(make_primary(props))
        , _records(make_lw_shared<one_session_records>(type, ttl_by_type(type, _local_tracing_ptr->slow_query_record_ttl()), _local_tracing_ptr->slow_query_record_ttl()))
        , _slow_query_threshold(_local_tracing_ptr->slow_query_threshold())
    {
    }

    trace_state(const trace_info& info)
        : _local_tracing_ptr(tracing::get_local_tracing_instance().shared_from_this())
        , _state_props(make_secondary(info.state_props))
        // inherit the slow query threshold and ttl from the coordinator
        , _records(make_lw_shared<one_session_records>(info.type, ttl_by_type(info.type, std::chrono::seconds(info.slow_query_ttl_sec)), std::chrono::seconds(info.slow_query_ttl_sec), info.session_id, info.parent_id))
        , _slow_query_threshold(info.slow_query_threshold_us)
    {
        if (info.state_props.contains<trace_state_props::log_slow_query>() && info.start_ts_us > 0u) {
            _supplied_start_ts_us = info.start_ts_us;
        }

    }

    ~trace_state() {}

    const utils::UUID& session_id() const {
        return _records->session_id;
    }

    bool is_in_state(state s) const {
        return _state == s;
    }

    void set_state(state s) {
        _state = s;
    }

    trace_type type() const {
        return _records->session_rec.command;
    }

    bool is_primary() const {
        return _state_props.contains(trace_state_props::primary);
    }

    bool write_on_close() const {
        return _state_props.contains(trace_state_props::write_on_close);
    }

    bool full_tracing() const {
        return _state_props.contains(trace_state_props::full_tracing);
    }

    bool log_slow_query() const {
        return _state_props.contains(trace_state_props::log_slow_query);
    }

    bool ignore_events() const {
        return _state_props.contains(trace_state_props::ignore_events);
    }

    trace_state_props_set raw_props() const {
        return _state_props;
    }

    /**
     * @return the moment `begin()` was called, in microseconds from POSIX epoch.
     */
    uint64_t start_ts_us() const {
        // `elapsed_clock` has undefined epoch, so we use the POSIX TS to expose times outside
        const std::chrono::system_clock::time_point start_system_time_point = std::chrono::system_clock::now()
                + (_start - elapsed_clock::now());
        return std::chrono::duration_cast<std::chrono::microseconds>(start_system_time_point.time_since_epoch()).count();
    }

    /**
     * @return a slow query threshold value in microseconds.
     */
    uint32_t slow_query_threshold_us() const {
        return _slow_query_threshold.count();
    }

    /**
     * @return a slow query entry TTL value in seconds
     */
    uint32_t slow_query_ttl_sec() const {
        return _records->session_rec.slow_query_record_ttl.count();
    }

    /**
     * @return a span ID
     */
    span_id my_span_id() const {
        return _records->my_span_id;
    }

    uint64_t events_size() const {
        return _records->events_recs.size();
    }

private:
    /**
     * Stop a foreground state and write pending records to I/O.
     *
     * @note The tracing session's "duration" is the time it was in the "foreground" state.
     */
    void stop_foreground_and_write() noexcept;

    bool should_log_slow_query(elapsed_clock::duration e) const {
        return log_slow_query() && e > _slow_query_threshold;
    }

    std::chrono::seconds ttl_by_type(trace_type type, std::chrono::seconds slow_query_ttl) noexcept {
        if (full_tracing()) {
            if (!log_slow_query()) {
                return ::tracing::ttl_by_type(type);
            } else {
                return std::max(::tracing::ttl_by_type(type), slow_query_ttl);
            }
        } else {
            return slow_query_ttl;
        }
    }

    bool should_write_records() const {
        return full_tracing() || _records->do_log_slow_query;
    }

    /**
     * Returns the amount of time passed since the beginning of this tracing session.
     *
     * @return the amount of time passed since the beginning of this session
     */
    elapsed_clock::duration elapsed();

    /**
     * Initiates a tracing session.
     *
     * Starts the tracing session time measurments.
     * This overload is meant for secondary sessions.
     */
    void begin() {
        std::atomic_signal_fence(std::memory_order_seq_cst);
        if (_supplied_start_ts_us) {
            // Shorten `_slow_query_threshold` by the time spent since starting the parent span.
            _slow_query_threshold -= std::chrono::duration_cast<std::chrono::microseconds>(
                    std::chrono::system_clock::now().time_since_epoch() - std::chrono::microseconds(*_supplied_start_ts_us));
            // And do not let it be negative
            _slow_query_threshold = std::max(std::chrono::microseconds::zero(), _slow_query_threshold);
        }
        _start = elapsed_clock::now();
        std::atomic_signal_fence(std::memory_order_seq_cst);
        set_state(state::foreground);
    }

    /**
     * Initiates a tracing session.
     *
     * Starts the tracing session time measurments.
     * This overload is meant for primary sessions.
     *
     * @param request description of a request being traces
     * @param client address of a client the traced request came from
     */
    void begin(sstring request, gms::inet_address client) {
        begin();
        _records->session_rec.client = client;
        _records->session_rec.request = std::move(request);
        _records->session_rec.started_at = std::chrono::system_clock::now();
    }

    template <typename Func>
    requires std::is_invocable_r_v<sstring, Func>
    void begin(const seastar::lazy_eval<Func>& lf, gms::inet_address client) {
        begin(lf(), client);
    }

    /**
     * Stores a batchlog endpoints.
     *
     * This value will eventually be stored in a params<string, string> map of a tracing session
     * with a 'batchlog_endpoints' key.
     *
     * @param val the set of batchlog endpoints
     */
    void set_batchlog_endpoints(const inet_address_vector_replica_set& val);

    /**
     * Stores a consistency level of a query being traced.
     *
     * This value will eventually be stored in a params<string, string> map of a tracing session
     * with a 'consistency_level' key.
     *
     * @param val the consistency level
     */
    void set_consistency_level(db::consistency_level val);

    /**
     * Stores an optional serial consistency level of a query being traced.
     *
     * This value will eventually be stored in a params<string, string> map of a tracing session
     * with a 'serial_consistency_level' key.
     *
     * @param val the optional value with a serial consistency level
     */
    void set_optional_serial_consistency_level(const std::optional<db::consistency_level>& val);

    /**
     * Returns the string with the representation of the given raw value.
     * If the value is NULL or unset the 'null' or 'unset value' strings are returned correspondingly.
     *
     * @param v view of the given raw value
     * @param t type object corresponding to the given raw value.
     * @return the string with the representation of the given raw value.
     */
    sstring raw_value_to_sstring(const cql3::raw_value_view& v, bool is_unset, const data_type& t);

    /**
     * Stores a page size of a query being traced.
     *
     * This value will eventually be stored in a params<string, string> map of a tracing session
     * with a 'page_size' key.
     *
     * @param val the PAGE size
     */
    void set_page_size(int32_t val);

    /**
     * Set a size of the request being traces.
     *
     * @param s a request size
     */
    void set_request_size(size_t s) noexcept;

    /**
     * Set a size of the response of the query being traces.
     *
     * @param s a response size
     */
    void set_response_size(size_t s) noexcept;

    /**
     * Store a query string.
     *
     * This value will eventually be stored in a params<string, string> map of a tracing session
     * with a 'query' key.
     *
     * @param val the query string
     */
    void add_query(sstring_view val);

    /**
     * Store a custom session parameter.
     * 
     * Thus value will be stored in the params<string, string> map of a tracing session
     * 
     * @param key the parameter key
     * @param val the parameter value
     */
    void add_session_param(sstring_view key, sstring_view val);

    /**
     * Store a user provided timestamp.
     *
     * This value will eventually be stored in a params<string, string> map of a tracing session
     * with a 'user_timestamp' key.
     *
     * @param val the timestamp
     */
    void set_user_timestamp(api::timestamp_type val);

    /**
     * Store a pointer to a prepared statement that is being traced.
     *
     * There may be more than one prepared statement that is traced in case of a BATCH command.
     *
     * @param prepared a checked weak pointer to a prepared statement
     */
    void add_prepared_statement(prepared_checked_weak_ptr& prepared);

    void set_username(const std::optional<auth::authenticated_user>& user) {
        if (user) {
            _records->session_rec.username = format("{}", *user);
        }
    }

    void add_table_name(sstring full_table_name) {
        _records->session_rec.tables.emplace(std::move(full_table_name));
    }

    /**
     * Fill the map in a session's record with the values set so far.
     *
     */
    void build_parameters_map();

    /**
     * Store prepared statement parameters for traced query
     *
     * @param prepared_options_ptr parameters of the prepared statement
     */
    void add_prepared_query_options(const cql3::query_options& prepared_options_ptr);

    /**
     * Fill the map in a session's record with the parameters' values of a single prepared statement.
     *
     * Parameters values will be stored with a key '@ref param_name_prefix[X]' where X is an index of the corresponding
     * parameter.
     *
     * @param prepared prepared statement handle
     * @param names_opt CQL cell names used in the current invocation of the prepared statement
     * @param values CQL value used in the current invocation of the prepared statement
     * @param param_name_prefix prefix of the parameter key in the map, e.g. "param" or "param[1]"
     */
    void build_parameters_map_for_one_prepared(const prepared_checked_weak_ptr& prepared_ptr,
            std::optional<std::vector<sstring_view>>& names_opt,
            cql3::raw_value_view_vector_with_unset& values, const sstring& param_name_prefix);

    /**
     * The actual trace message storing method.
     *
     * @note This method is allowed to throw.
     * @param msg the trace message to store
     */
    void trace_internal(sstring msg);

    /**
     * Add a single trace entry - a special case for a simple string.
     *
     * @param msg trace message
     */
    void trace(sstring msg) noexcept {
        try {
            trace_internal(std::move(msg));
        } catch (...) {
            // Bump up an error counter and ignore
            ++_local_tracing_ptr->stats.trace_errors;
        }
    }
    void trace(const char* msg) noexcept {
        try {
            trace_internal(sstring(msg));
        } catch (...) {
            // Bump up an error counter and ignore
            ++_local_tracing_ptr->stats.trace_errors;
        }
    }

    /**
     * Add a single trace entry - printf-like version
     *
     * Add a single trace entry with a message given in a printf-like way:
     * format string with positional parameters.
     *
     * @note Both format string and positional parameters are going to be copied
     * and the final string is going to built later. A caller has to take this
     * into an account and make sure that positional parameters are both
     * copiable and that their copying is not expensive.
     *
     * @tparam A
     * @param fmt format string
     * @param a positional parameters
     */
    template <typename... A>
    void trace(const char* fmt, A&&... a) noexcept;

    template <typename... A>
    friend void begin(const trace_state_ptr& p, A&&... a);

    template <typename... A>
    friend void trace(const trace_state_ptr& p, A&&... a) noexcept;

    friend void set_page_size(const trace_state_ptr& p, int32_t val);
    friend void set_request_size(const trace_state_ptr& p, size_t s) noexcept;
    friend void set_response_size(const trace_state_ptr& p, size_t s) noexcept;
    friend void set_batchlog_endpoints(const trace_state_ptr& p, const inet_address_vector_replica_set& val);
    friend void set_consistency_level(const trace_state_ptr& p, db::consistency_level val);
    friend void set_optional_serial_consistency_level(const trace_state_ptr& p, const std::optional<db::consistency_level>&val);
    friend void add_query(const trace_state_ptr& p, sstring_view val);
    friend void add_session_param(const trace_state_ptr& p, sstring_view key, sstring_view val);
    friend void set_user_timestamp(const trace_state_ptr& p, api::timestamp_type val);
    friend void add_prepared_statement(const trace_state_ptr& p, prepared_checked_weak_ptr& prepared);
    friend void set_username(const trace_state_ptr& p, const std::optional<auth::authenticated_user>& user);
    friend void add_table_name(const trace_state_ptr& p, const sstring& ks_name, const sstring& cf_name);
    friend void add_prepared_query_options(const trace_state_ptr& state, const cql3::query_options& prepared_options_ptr);
    friend void stop_foreground(const trace_state_ptr& state) noexcept;
};

class trace_state_ptr final {
private:
    lw_shared_ptr<trace_state> _state_ptr;

public:
    trace_state_ptr() = default;
    trace_state_ptr(lw_shared_ptr<trace_state> state_ptr)
        : _state_ptr(std::move(state_ptr))
    {}
    trace_state_ptr(std::nullptr_t)
        : _state_ptr(nullptr)
    {}

    explicit operator bool() const noexcept {
        return __builtin_expect(bool(_state_ptr), false);
    }

    trace_state* operator->() const noexcept {
        return _state_ptr.get();
    }

    trace_state& operator*() const noexcept {
        return *_state_ptr;
    }
};

inline void trace_state::trace_internal(sstring message) {
    if (is_in_state(state::inactive)) {
        throw std::logic_error("trying to use a trace() before begin() for \"" + message + "\" tracepoint");
    }

    // We don't want the total amount of pending, active and flushing records to
    // bypass two times the maximum number of pending records.
    //
    // If either records are being created too fast or a backend doesn't
    // keep up we want to start dropping records.
    // In any case, this should be rare, therefore we don't try to optimize this
    // flow.
    if (!_local_tracing_ptr->have_records_budget()) {

        return;
    }

    try {
        auto e = elapsed();
        _records->events_recs.emplace_back(std::move(message), e, i_tracing_backend_helper::wall_clock::now());
        _records->consume_from_budget();

        // If we have aggregated enough records - schedule them for write already.
        //
        // We prefer the traces to be written after the session is over. However
        // if there is a session that creates a lot of traces - we want to write
        // them before we start to drop new records.
        //
        // We don't want to write records of a tracing session if we trace only
        // slow queries and the elapsed time is still below the slow query
        // logging threshold.
        if (_records->events_recs.size() >= tracing::exp_trace_events_per_session && (full_tracing() || should_log_slow_query(e))) {
            _local_tracing_ptr->schedule_for_write(_records);
            _local_tracing_ptr->write_maybe();
        }
    } catch (...) {
        // Bump up an error counter and ignore
        ++_local_tracing_ptr->stats.trace_errors;
    }
}

template <typename... A>
void trace_state::trace(const char* fmt, A&&... a) noexcept {
    try {
        trace_internal(seastar::format(fmt, std::forward<A>(a)...));
    } catch (...) {
        // Bump up an error counter and ignore
        ++_local_tracing_ptr->stats.trace_errors;
    }
}

inline elapsed_clock::duration trace_state::elapsed() {
    using namespace std::chrono;
    std::atomic_signal_fence(std::memory_order_seq_cst);
    elapsed_clock::duration elapsed = elapsed_clock::now() - _start;
    std::atomic_signal_fence(std::memory_order_seq_cst);

    return elapsed;
}

inline void set_page_size(const trace_state_ptr& p, int32_t val) {
    if (p) {
        p->set_page_size(val);
    }
}

inline void set_request_size(const trace_state_ptr& p, size_t s) noexcept {
    if (p) {
        p->set_request_size(s);
    }
}

inline void set_response_size(const trace_state_ptr& p, size_t s) noexcept {
    if (p) {
        p->set_response_size(s);
    }
}

inline void set_batchlog_endpoints(const trace_state_ptr& p, const inet_address_vector_replica_set& val) {
    if (p) {
        p->set_batchlog_endpoints(val);
    }
}

inline void set_consistency_level(const trace_state_ptr& p, db::consistency_level val) {
    if (p) {
        p->set_consistency_level(val);
    }
}

inline void set_optional_serial_consistency_level(const trace_state_ptr& p, const std::optional<db::consistency_level>& val) {
    if (p) {
        p->set_optional_serial_consistency_level(val);
    }
}

inline void add_query(const trace_state_ptr& p, sstring_view val) {
    if (p) {
        p->add_query(std::move(val));
    }
}

inline void add_session_param(const trace_state_ptr& p, sstring_view key, sstring_view val) {
    if (p) {
        p->add_session_param(std::move(key), std::move(val));
    }
}

inline void set_user_timestamp(const trace_state_ptr& p, api::timestamp_type val) {
    if (p) {
        p->set_user_timestamp(val);
    }
}

inline void add_prepared_statement(const trace_state_ptr& p, prepared_checked_weak_ptr& prepared) {
    if (p) {
        p->add_prepared_statement(prepared);
    }
}

inline void set_username(const trace_state_ptr& p, const std::optional<auth::authenticated_user>& user) {
    if (p) {
        p->set_username(user);
    }
}

inline void add_table_name(const trace_state_ptr& p, const sstring& ks_name, const sstring& cf_name) {
    if (p) {
        p->add_table_name(ks_name + "." + cf_name);
    }
}

inline bool should_return_id_in_response(const trace_state_ptr& p) {
    if (p) {
        return p->write_on_close();
    }
    return false;
}

/**
 * A helper for conditional invoking trace_state::begin() functions.
 *
 * If trace state is initialized the operation takes place immediatelly,
 * otherwise nothing happens.
 *
 * @tparam A
 * @param p trace state handle
 * @param a optional parameters for trace_state::begin()
 */
template <typename... A>
inline void begin(const trace_state_ptr& p, A&&... a) {
    if (p) {
        p->begin(std::forward<A>(a)...);
    }
}

/**
 * A helper for conditional invoking trace_state::trace() function.
 *
 * Create a trace entry if a given trace state @param p is initialized.
 * Otherwise, it @param p is not initialized - do nothing.
 * Trace message may be passed as a printf-like format string with the
 * corresponding positional parameters.
 *
 * If @param p is initialized both trace message string and positional
 * parameters are going to be copied and the final string is going to be build
 * later. Therefore a caller has to take this into an account and make sure
 * that positional parameters are both copiable and that the copy is not
 * expensive.
 *
 * @param A
 * @param p trace state handle
 * @param a trace message format string with optional parameters
 */
template <typename... A>
inline void trace(const trace_state_ptr& p, A&&... a) noexcept {
    if (p && !p->ignore_events()) {
        p->trace(std::forward<A>(a)...);
    }
}

inline std::optional<trace_info> make_trace_info(const trace_state_ptr& state) {
    // We want to trace the remote replicas' operations only when a full tracing
    // is requested or when a slow query logging is enabled and the session is
    // still active and only if the session events tracing is not explicitly disabled.
    //
    // When only a slow query logging is enabled we don't really care what
    // happens on a remote replica after a Client has received a response for
    // his/her query.
    if (state && !state->ignore_events() && (state->full_tracing() || (state->log_slow_query() && !state->is_in_state(trace_state::state::background)))) {
        // When slow query logging is requested, secondary session will continue
        // calculating time *since the start of the primary session*
        const auto start_ts_us = state->log_slow_query() ? state->start_ts_us() : 0u;
        return trace_info{state->session_id(), state->type(), state->write_on_close(), state->raw_props(),
                state->slow_query_threshold_us(), state->slow_query_ttl_sec(), state->my_span_id(), start_ts_us};
    }

    return std::nullopt;
}

inline void stop_foreground(const trace_state_ptr& state) noexcept {
    if (state) {
        state->stop_foreground_and_write();
    }
}

inline void add_prepared_query_options(const trace_state_ptr& state, const cql3::query_options& prepared_options_ptr) {
    if (state) {
        state->add_prepared_query_options(prepared_options_ptr);
    }
}

// global_trace_state_ptr is a helper class that may be used for creating spans
// of an existing tracing session on other shards. When a tracing span on a
// different shard is needed global_trace_state_ptr would create a secondary
// tracing session on that shard similarly to what we do when we create tracing
// spans on remote Nodes.
//
// The usage is straight forward:
// 1. Create a global_trace_state_ptr from the existing trace_state_ptr object.
// 2. Pass it to the execution unit that (possibly) runs on a different shard
//    and pass the global_trace_state_ptr object instead of a trace_state_ptr
//    object.
class global_trace_state_ptr {
    unsigned _cpu_of_origin;
    trace_state_ptr _ptr;
public:
    // Note: the trace_state_ptr must come from the current shard
    global_trace_state_ptr(trace_state_ptr t)
            : _cpu_of_origin(this_shard_id())
            , _ptr(std::move(t))
    { }

    // May be invoked across shards.
    global_trace_state_ptr(const global_trace_state_ptr& other)
            : global_trace_state_ptr(other.get())
    { }

    // May be invoked across shards.
    global_trace_state_ptr(global_trace_state_ptr&& other)
            : global_trace_state_ptr(other.get())
    { }

    global_trace_state_ptr& operator=(const global_trace_state_ptr&) = delete;

    // May be invoked across shards.
    trace_state_ptr get() const {
        return nullptr;
    }

    // May be invoked across shards.
    operator trace_state_ptr() const { return get(); }
};
}

#include <seastar/core/sstring.hh>

#include <map>

namespace cql_transport {

/**
 * CQL Protocol extensions. They can be viewed as an opportunity to provide
 * some vendor-specific extensions to the CQL protocol without changing
 * the version of the protocol itself (i.e. when the changes introduced by
 * extensions are binary-compatible with the current version of the protocol).
 * 
 * Extensions are meant to be passed between client and server in terms of
 * SUPPORTED/STARTUP messages in order to negotiate compatible set of features
 * to be used in a connection.
 * 
 * The negotiation procedure and extensions themselves are documented in the
 * `docs/dev/protocol-extensions.md`. 
 */
enum class cql_protocol_extension {
    LWT_ADD_METADATA_MARK,
    RATE_LIMIT_ERROR
};

using cql_protocol_extension_enum = super_enum<cql_protocol_extension,
    cql_protocol_extension::LWT_ADD_METADATA_MARK,
    cql_protocol_extension::RATE_LIMIT_ERROR>;

using cql_protocol_extension_enum_set = enum_set<cql_protocol_extension_enum>;

cql_protocol_extension_enum_set supported_cql_protocol_extensions();

/**
 * Returns the name of extension to be used in SUPPORTED/STARTUP feature negotiation.
 */
const seastar::sstring& protocol_extension_name(cql_protocol_extension ext);

/**
 * Returns a list of additional key-value pairs (in the form of "ARG=VALUE" string)
 * that belong to a particular extension and provide some additional capabilities
 * to be used by the client driver in order to support this extension.
 */
std::vector<seastar::sstring> additional_options_for_proto_ext(cql_protocol_extension ext);

} // namespace cql_transport


#include <seastar/core/sstring.hh>
#include <seastar/core/print.hh>
#include <map>
#include <stdexcept>
#include <variant>
#include <seastar/core/lowres_clock.hh>

namespace qos {

/**
 *  a structure that holds the configuration for
 *  a service level.
 */
struct service_level_options {
    struct unset_marker {
        bool operator==(const unset_marker&) const { return true; };
    };
    struct delete_marker {
        bool operator==(const delete_marker&) const { return true; };
    };

    enum class workload_type {
        unspecified, batch, interactive, delete_marker
    };

    using timeout_type = std::variant<unset_marker, delete_marker, lowres_clock::duration>;
    timeout_type timeout = unset_marker{};
    workload_type workload = workload_type::unspecified;

    service_level_options replace_defaults(const service_level_options& other) const;
    // Merges the values of two service level options. The semantics depends
    // on the type of the parameter - e.g. for timeouts, a min value is preferred.
    service_level_options merge_with(const service_level_options& other) const;

    bool operator==(const service_level_options& other) const = default;

    static std::string_view to_string(const workload_type& wt);
    static std::optional<workload_type> parse_workload_type(std::string_view sv);
};

std::ostream& operator<<(std::ostream& os, const service_level_options::workload_type&);

using service_levels_info = std::map<sstring, service_level_options>;

///
/// A logical argument error for a service_level statement operation.
///
class service_level_argument_exception : public std::invalid_argument {
public:
    using std::invalid_argument::invalid_argument;
};

///
/// An exception to indicate that the service level given as parameter doesn't exist.
///
class nonexistant_service_level_exception : public service_level_argument_exception {
public:
    nonexistant_service_level_exception(sstring service_level_name)
            : service_level_argument_exception(format("Service Level {} doesn't exists.", service_level_name)) {
    }
};

}

#include <seastar/core/rwlock.hh>
#include <seastar/util/defer.hh>
#include <seastar/util/noncopyable_function.hh>
#include <seastar/core/coroutine.hh>

#include <vector>

// This class supports atomic removes (by using a lock and returning a
// future) and non atomic insert and iteration (by using indexes).
template <typename T>
class atomic_vector {
    std::vector<T> _vec;
    seastar::rwlock _vec_lock;

public:
    void add(const T& value) {
        _vec.push_back(value);
    }
    seastar::future<> remove(const T& value) {
        return with_lock(_vec_lock.for_write(), [this, value] {
            _vec.erase(std::remove(_vec.begin(), _vec.end(), value), _vec.end());
        });
    }

    // This must be called on a thread. The callback function must not
    // call remove.
    //
    // We would take callbacks that take a T&, but we had bugs in the
    // past with some of those callbacks holding that reference past a
    // preemption.
    void thread_for_each(seastar::noncopyable_function<void(T)> func) {
        _vec_lock.for_read().lock().get();
        auto unlock = seastar::defer([this] {
            _vec_lock.for_read().unlock();
        });
        // We grab a lock in remove(), but not in add(), so we
        // iterate using indexes to guard against the vector being
        // reallocated.
        for (size_t i = 0, n = _vec.size(); i < n; ++i) {
            func(_vec[i]);
        }
    }

    // The callback function must not call remove.
    //
    // We would take callbacks that take a T&, but we had bugs in the
    // past with some of those callbacks holding that reference past a
    // preemption.
    seastar::future<> for_each(seastar::noncopyable_function<seastar::future<>(T)> func) {
        auto holder = co_await _vec_lock.hold_read_lock();
        // We grab a lock in remove(), but not in add(), so we
        // iterate using indexes to guard against the vector being
        // reallocated.
        for (size_t i = 0, n = _vec.size(); i < n; ++i) {
            co_await func(_vec[i]);
        }
    }
};



namespace service {

/**
 * Interface on which interested parties can be notified of high level endpoint
 * state changes.
 *
 * Note that while IEndpointStateChangeSubscriber notify about gossip related
 * changes (IEndpointStateChangeSubscriber.onJoin() is called when a node join
 * gossip), this interface allows to be notified about higher level events.
 */
class endpoint_lifecycle_subscriber {
public:
    virtual ~endpoint_lifecycle_subscriber()
    { }

    /**
     * Called when a new node joins the cluster, i.e. either has just been
     * bootstrapped or "instajoins".
     *
     * @param endpoint the newly added endpoint.
     */
    virtual void on_join_cluster(const gms::inet_address& endpoint) = 0;

    /**
     * Called when a new node leave the cluster (decommission or removeToken).
     *
     * @param endpoint the endpoint that is leaving.
     */
    virtual void on_leave_cluster(const gms::inet_address& endpoint) = 0;

    /**
     * Called when a node is marked UP.
     *
     * @param endpoint the endpoint marked UP.
     */
    virtual void on_up(const gms::inet_address& endpoint) = 0;

    /**
     * Called when a node is marked DOWN.
     *
     * @param endpoint the endpoint marked DOWN.
     */
    virtual void on_down(const gms::inet_address& endpoint) = 0;
};

class endpoint_lifecycle_notifier {
    atomic_vector<endpoint_lifecycle_subscriber*> _subscribers;

public:
    void register_subscriber(endpoint_lifecycle_subscriber* subscriber);
    future<> unregister_subscriber(endpoint_lifecycle_subscriber* subscriber) noexcept;

    future<> notify_down(gms::inet_address endpoint);
    future<> notify_left(gms::inet_address endpoint);
    future<> notify_up(gms::inet_address endpoint);
    future<> notify_joined(gms::inet_address endpoint);
};

}



namespace qos {

    struct service_level_info {
        sstring name;
    };
    class qos_configuration_change_subscriber {
    public:
        /** This callback is going to be called just before the service level is available **/
        virtual future<> on_before_service_level_add(service_level_options slo, service_level_info sl_info) = 0;
        /** This callback is going to be called just after the service level is removed **/
        virtual future<> on_after_service_level_remove(service_level_info sl_info) = 0;
        /** This callback is going to be called just before the service level is changed **/
        virtual future<> on_before_service_level_change(service_level_options slo_before, service_level_options slo_after, service_level_info sl_info) = 0;

        virtual ~qos_configuration_change_subscriber() {};
    };
}

#include <seastar/core/sstring.hh>
#include <seastar/core/distributed.hh>
#include <seastar/core/abort_source.hh>
#include <map>
#include <unordered_set>

namespace db {
    class system_distributed_keyspace;
}
namespace qos {
/**
 *  a structure to hold a service level
 *  data and configuration.
 */
struct service_level {
     service_level_options slo;
     bool marked_for_deletion;
     bool is_static;
};

/**
 *  The service_level_controller class is an implementation of the service level
 *  controller design.
 *  It is logically divided into 2 parts:
 *      1. Global controller which is responsible for all of the data and plumbing
 *      manipulation.
 *      2. Local controllers that act upon the data and facilitates execution in
 *      the service level context
 */
class service_level_controller : public peering_sharded_service<service_level_controller>, public service::endpoint_lifecycle_subscriber {
public:
    class service_level_distributed_data_accessor {
    public:
        virtual future<qos::service_levels_info> get_service_levels() const = 0;
        virtual future<qos::service_levels_info> get_service_level(sstring service_level_name) const = 0;
        virtual future<> set_service_level(sstring service_level_name, qos::service_level_options slo) const = 0;
        virtual future<> drop_service_level(sstring service_level_name) const = 0;
    };
    using service_level_distributed_data_accessor_ptr = ::shared_ptr<service_level_distributed_data_accessor>;

private:
    struct global_controller_data {
        service_levels_info  static_configurations{};
        int schedg_group_cnt = 0;
        int io_priority_cnt = 0;
        service_level_options default_service_level_config;
        // The below future is used to serialize work so no reordering can occur.
        // This is needed so for example: delete(x), add(x) will not reverse yielding
        // a completely different result than the one intended.
        future<> work_future = make_ready_future();
        semaphore notifications_serializer = semaphore(1);
        future<> distributed_data_update = make_ready_future();
        abort_source dist_data_update_aborter;
    };

    std::unique_ptr<global_controller_data> _global_controller_db;

    static constexpr shard_id global_controller = 0;

    std::map<sstring, service_level> _service_levels_db;
    std::unordered_map<sstring, sstring> _role_to_service_level;
    service_level _default_service_level;
    service_level_distributed_data_accessor_ptr _sl_data_accessor;
    sharded<auth::service>& _auth_service;
    std::chrono::time_point<seastar::lowres_clock> _last_successful_config_update;
    unsigned _logged_intervals;
    atomic_vector<qos_configuration_change_subscriber*> _subscribers;
public:
    service_level_controller(sharded<auth::service>& auth_service, service_level_options default_service_level_config);

    /**
     * this function must be called *once* from any shard before any other functions are called.
     * No other function should be called before the future returned by the function is resolved.
     * @return a future that resolves when the initialization is over.
     */
    future<> start();

    void set_distributed_data_accessor(service_level_distributed_data_accessor_ptr sl_data_accessor);

    /**
     *  Adds a service level configuration if it doesn't exists, and updates
     *  an the existing one if it does exist.
     *  Handles both, static and non static service level configurations.
     * @param name - the service level name.
     * @param slo - the service level configuration
     * @param is_static - is this configuration static or not
     */
    future<> add_service_level(sstring name, service_level_options slo, bool is_static = false);

    /**
     *  Removes a service level configuration if it exists.
     *  Handles both, static and non static service level configurations.
     * @param name - the service level name.
     * @param remove_static - indicates if it is a removal of a static configuration
     * or not.
     */
    future<> remove_service_level(sstring name, bool remove_static);

    /**
     * stops the distributed updater
     * @return a future that is resolved when the updates stopped
     */
    future<> drain();

    /**
     * stops all ongoing operations if exists
     * @return a future that is resolved when all operations has stopped
     */
    future<> stop();

    /**
     * Chack the distributed data for changes in a constant interval and updates
     * the service_levels configuration in accordance (adds, removes, or updates
     * service levels as necessairy).
     * @param interval - the interval is seconds to check the distributed data.
     * @return a future that is resolved when the update loop stops.
     */
    void update_from_distributed_data(std::chrono::duration<float> interval);

    /**
     * Updates the service level data from the distributed data store.
     * @return a future that is resolved when the update is done
     */
    future<> update_service_levels_from_distributed_data();


    future<> add_distributed_service_level(sstring name, service_level_options slo, bool if_not_exsists);
    future<> alter_distributed_service_level(sstring name, service_level_options slo);
    future<> drop_distributed_service_level(sstring name, bool if_exists);
    future<service_levels_info> get_distributed_service_levels();
    future<service_levels_info> get_distributed_service_level(sstring service_level_name);

    /**
     * Returns the service level options **in effect** for a user having the given
     * collection of roles.
     * @param roles - the collection of roles to consider
     * @return the effective service level options - they may in particular be a combination
     *         of options from multiple service levels
     */
    future<std::optional<service_level_options>> find_service_level(auth::role_set roles);

    /**
     * Gets the service level data by name.
     * @param service_level_name - the name of the requested service level
     * @return the service level data if it exists (in the local controller) or
     * get_service_level("default") otherwise.
     */
    service_level& get_service_level(sstring service_level_name) {
        auto sl_it = _service_levels_db.find(service_level_name);
        if (sl_it == _service_levels_db.end() || sl_it->second.marked_for_deletion) {
            sl_it = _service_levels_db.find(default_service_level_name);
        }
        return sl_it->second;
    }

private:
    /**
     *  Adds a service level configuration if it doesn't exists, and updates
     *  an the existing one if it does exist.
     *  Handles both, static and non static service level configurations.
     * @param name - the service level name.
     * @param slo - the service level configuration
     * @param is_static - is this configuration static or not
     */
    future<> do_add_service_level(sstring name, service_level_options slo, bool is_static = false);

    /**
     *  Removes a service level configuration if it exists.
     *  Handles both, static and non static service level configurations.
     * @param name - the service level name.
     * @param remove_static - indicates if it is a removal of a static configuration
     * or not.
     */
    future<> do_remove_service_level(sstring name, bool remove_static);

    /**
     * The notify functions are used by the global service level controller
     * to propagate configuration changes to the local controllers.
     * the returned future is resolved when the local controller is done acting
     * on the notification. updates are done in sequence so their meaning will not
     * change due to execution reordering.
     */
    future<> notify_service_level_added(sstring name, service_level sl_data);
    future<> notify_service_level_updated(sstring name, service_level_options slo);
    future<> notify_service_level_removed(sstring name);

    enum class  set_service_level_op_type {
        add_if_not_exists,
        add,
        alter
    };

    future<> set_distributed_service_level(sstring name, service_level_options slo, set_service_level_op_type op_type);
public:

    /**
     *  Register a subscriber for service level configuration changes
     *  notifications
     * @param subscriber - a pointer to the subscriber.
     *
     * Note: the caller is responsible to keep the pointed to object alive until performing
     * a call to service_level_controller::unregister_subscriber()).
     *
     * Note 2: the subscription is per shard.
     */
    void register_subscriber(qos_configuration_change_subscriber* subscriber);
    future<> unregister_subscriber(qos_configuration_change_subscriber* subscriber);

    static sstring default_service_level_name;

    virtual void on_join_cluster(const gms::inet_address& endpoint) override;
    virtual void on_leave_cluster(const gms::inet_address& endpoint) override;
    virtual void on_up(const gms::inet_address& endpoint) override;
    virtual void on_down(const gms::inet_address& endpoint) override;
};
}



namespace auth {
class resource;
}

namespace data_dictionary {
class database;
}

namespace service {

/**
 * State related to a client connection.
 */
class client_state {
public:
    enum class auth_state : uint8_t {
        UNINITIALIZED, AUTHENTICATION, READY
    };
    using workload_type = qos::service_level_options::workload_type;

    // This class is used to move client_state between shards
    // It is created on a shard that owns client_state than passed
    // to a target shard where client_state_for_another_shard::get()
    // can be called to obtain a shard local copy.
    class client_state_for_another_shard {
    private:
        const client_state* _cs;
        seastar::sharded<auth::service>* _auth_service;
        client_state_for_another_shard(const client_state* cs, seastar::sharded<auth::service>* auth_service) : _cs(cs), _auth_service(auth_service) {}
        friend client_state;
    public:
        client_state get() const {
            return client_state(_cs, _auth_service);
        }
    };
private:
    client_state(const client_state* cs, seastar::sharded<auth::service>* auth_service)
            : _keyspace(cs->_keyspace)
            , _user(cs->_user)
            , _auth_state(cs->_auth_state)
            , _is_internal(cs->_is_internal)
            , _is_thrift(cs->_is_thrift)
            , _remote_address(cs->_remote_address)
            , _auth_service(auth_service ? &auth_service->local() : nullptr)
            , _default_timeout_config(cs->_default_timeout_config)
            , _timeout_config(cs->_timeout_config)
            , _enabled_protocol_extensions(cs->_enabled_protocol_extensions)
    {}
    friend client_state_for_another_shard;
private:
    sstring _keyspace;
#if 0
    private static final Logger logger = LoggerFactory.getLogger(ClientState.class);
    public static final SemanticVersion DEFAULT_CQL_VERSION = org.apache.cassandra.cql3.QueryProcessor.CQL_VERSION;

    private static final Set<IResource> READABLE_SYSTEM_RESOURCES = new HashSet<>();
    private static final Set<IResource> PROTECTED_AUTH_RESOURCES = new HashSet<>();

    static
    {
        // We want these system cfs to be always readable to authenticated users since many tools rely on them
        // (nodetool, cqlsh, bulkloader, etc.)
        for (String cf : Iterables.concat(Arrays.asList(SystemKeyspace.LOCAL, SystemKeyspace.PEERS), LegacySchemaTables.ALL))
            READABLE_SYSTEM_RESOURCES.add(DataResource.columnFamily(SystemKeyspace.NAME, cf));

        PROTECTED_AUTH_RESOURCES.addAll(DatabaseDescriptor.getAuthenticator().protectedResources());
        PROTECTED_AUTH_RESOURCES.addAll(DatabaseDescriptor.getAuthorizer().protectedResources());
    }

    // Current user for the session
    private volatile AuthenticatedUser user;
    private volatile String keyspace;
#endif
    std::optional<auth::authenticated_user> _user;
    std::optional<sstring> _driver_name, _driver_version;

    auth_state _auth_state = auth_state::UNINITIALIZED;

    // isInternal is used to mark ClientState as used by some internal component
    // that should have an ability to modify system keyspace.
    bool _is_internal;
    bool _is_thrift;

    // The biggest timestamp that was returned by getTimestamp/assigned to a query
    static thread_local api::timestamp_type _last_timestamp_micros;

    // Address of a client
    socket_address _remote_address;

    // Only populated for external client state.
    auth::service* _auth_service{nullptr};
    qos::service_level_controller* _sl_controller{nullptr};

    // For restoring default values in the timeout config
    timeout_config _default_timeout_config;
    timeout_config _timeout_config;

    workload_type _workload_type = workload_type::unspecified;

public:
    struct internal_tag {};
    struct external_tag {};

    workload_type get_workload_type() const noexcept {
        return _workload_type;
    }

    auth_state get_auth_state() const noexcept {
        return _auth_state;
    }

    void set_auth_state(auth_state new_state) noexcept {
        _auth_state = new_state;
    }

    std::optional<sstring> get_driver_name() const {
        return _driver_name;
    }
    void set_driver_name(sstring driver_name) {
        _driver_name = std::move(driver_name);
    }

    std::optional<sstring> get_driver_version() const {
        return _driver_version;
    }
    void set_driver_version(sstring driver_version) {
        _driver_version = std::move(driver_version);
    }

    client_state(external_tag,
                 auth::service& auth_service,
                 qos::service_level_controller* sl_controller,
                 timeout_config timeout_config,
                 const socket_address& remote_address = socket_address(),
                 bool thrift = false)
            : _is_internal(false)
            , _is_thrift(thrift)
            , _remote_address(remote_address)
            , _auth_service(&auth_service)
            , _sl_controller(sl_controller)
            , _default_timeout_config(timeout_config)
            , _timeout_config(timeout_config) {
        if (!auth_service.underlying_authenticator().require_authentication()) {
            _user = auth::authenticated_user();
        }
    }

    gms::inet_address get_client_address() const {
        return gms::inet_address(_remote_address);
    }
    
    ::in_port_t get_client_port() const {
        return _remote_address.port();
    }

    const socket_address& get_remote_address() const {
        return _remote_address;
    }

    const timeout_config& get_timeout_config() const {
        return _timeout_config;
    }

    qos::service_level_controller& get_service_level_controller() const {
        return *_sl_controller;
    }

    client_state(internal_tag) : client_state(internal_tag{}, infinite_timeout_config)
    {}

    client_state(internal_tag, const timeout_config& config)
            : _keyspace("system")
            , _is_internal(true)
            , _is_thrift(false)
            , _default_timeout_config(config)
            , _timeout_config(config)
    {}

    client_state(internal_tag, auth::service& auth_service, qos::service_level_controller& sl_controller, sstring username)
        : _user(auth::authenticated_user(username))
        , _auth_state(auth_state::READY)
        , _is_internal(true)
        , _is_thrift(false)
        , _auth_service(&auth_service)
        , _sl_controller(&sl_controller)
    {}

    client_state(const client_state&) = delete;
    client_state(client_state&&) = default;

    ///
    /// `nullptr` for internal instances.
    ///
    const auth::service* get_auth_service() const {
        return _auth_service;
    }

    bool is_thrift() const {
        return _is_thrift;
    }

    bool is_internal() const {
        return _is_internal;
    }

    /**
     * @return a ClientState object for internal C* calls (not limited by any kind of auth).
     */
    static client_state& for_internal_calls() {
        static thread_local client_state s(internal_tag{});
        return s;
    }

    /**
     * This clock guarantees that updates for the same ClientState will be ordered
     * in the sequence seen, even if multiple updates happen in the same millisecond.
     */
    api::timestamp_type get_timestamp() {
        auto current = api::new_timestamp();
        auto last = _last_timestamp_micros;
        auto result = last >= current ? last + 1 : current;
        _last_timestamp_micros = result;
        return result;
    }

    /**
     * Returns a timestamp suitable for paxos given the timestamp of the last known commit (or in progress update).
     *
     * Paxos ensures that the timestamp it uses for commits respects the serial order of those commits. It does so
     * by having each replica reject any proposal whose timestamp is not strictly greater than the last proposal it
     * accepted. So in practice, which timestamp we use for a given proposal doesn't affect correctness but it does
     * affect the chance of making progress (if we pick a timestamp lower than what has been proposed before, our
     * new proposal will just get rejected).
     *
     * As during the prepared phase replica send us the last propose they accepted, a first option would be to take
     * the maximum of those last accepted proposal timestamp plus 1 (and use a default value, say 0, if it's the
     * first known proposal for the partition). This would mostly work (giving commits the timestamp 0, 1, 2, ...
     * in the order they are commited) but with 2 important caveats:
     *   1) it would give a very poor experience when Paxos and non-Paxos updates are mixed in the same partition,
     *      since paxos operations wouldn't be using microseconds timestamps. And while you shouldn't theoretically
     *      mix the 2 kind of operations, this would still be pretty nonintuitive. And what if you started writing
     *      normal updates and realize later you should switch to Paxos to enforce a property you want?
     *   2) this wouldn't actually be safe due to the expiration set on the Paxos state table.
     *
     * So instead, we initially chose to use the current time in microseconds as for normal update. Which works in
     * general but mean that clock skew creates unavailability periods for Paxos updates (either a node has his clock
     * in the past and he may no be able to get commit accepted until its clock catch up, or a node has his clock in
     * the future and then once one of its commit his accepted, other nodes ones won't be until they catch up). This
     * is ok for small clock skew (few ms) but can be pretty bad for large one.
     *
     * Hence our current solution: we mix both approaches. That is, we compare the timestamp of the last known
     * accepted proposal and the local time. If the local time is greater, we use it, thus keeping paxos timestamps
     * locked to the current time in general (making mixing Paxos and non-Paxos more friendly, and behaving correctly
     * when the paxos state expire (as long as your maximum clock skew is lower than the Paxos state expiration
     * time)). Otherwise (the local time is lower than the last proposal, meaning that this last proposal was done
     * with a clock in the future compared to the local one), we use the last proposal timestamp plus 1, ensuring
     * progress.
     *
     * @param min_timestamp_to_use the max timestamp of the last proposal accepted by replica having responded
     * to the prepare phase of the paxos round this is for. In practice, that's the minimum timestamp this method
     * may return.
     * @return a timestamp suitable for a Paxos proposal (using the reasoning described above). Note that
     * contrary to the get_timestamp() method, the return value is not guaranteed to be unique (nor
     * monotonic) across calls since it can return it's argument (so if the same argument is passed multiple times,
     * it may be returned multiple times). Note that we still ensure Paxos "ballot" are unique (for different
     * proposal) by (securely) randomizing the non-timestamp part of the UUID.
     */
    api::timestamp_type get_timestamp_for_paxos(api::timestamp_type min_timestamp_to_use) {
        api::timestamp_type current = std::max(api::new_timestamp(), min_timestamp_to_use);
        _last_timestamp_micros = _last_timestamp_micros >= current ? _last_timestamp_micros + 1 : current;
        return _last_timestamp_micros;
    }

#if 0
    public SocketAddress getRemoteAddress()
    {
        return remoteAddress;
    }
#endif

    const sstring& get_raw_keyspace() const noexcept {
        return _keyspace;
    }

    sstring& get_raw_keyspace() noexcept {
        return _keyspace;
    }

public:
    void set_keyspace(replica::database& db, std::string_view keyspace);

    void set_raw_keyspace(sstring new_keyspace) noexcept {
        _keyspace = std::move(new_keyspace);
    }

    const sstring& get_keyspace() const {
        if (_keyspace.empty()) {
            throw exceptions::invalid_request_exception("No keyspace has been specified. USE a keyspace, or explicitly specify keyspace.tablename");
        }
        return _keyspace;
    }

    /**
     * Sets active user. Does _not_ validate anything
     */
    void set_login(auth::authenticated_user);

    /// \brief A user can login if it's anonymous, or if it exists and the `LOGIN` option for the user is `true`.
    future<> check_user_can_login();

    future<> has_all_keyspaces_access(auth::permission) const;
    future<> has_keyspace_access(data_dictionary::database db, const sstring&, auth::permission) const;
    future<> has_column_family_access(data_dictionary::database db, const sstring&, const sstring&, auth::permission,
                                      auth::command_desc::type = auth::command_desc::type::OTHER) const;
    future<> has_schema_access(data_dictionary::database db, const schema& s, auth::permission p) const;
    future<> has_schema_access(data_dictionary::database db, const sstring&, const sstring&, auth::permission p) const;

    future<> has_functions_access(data_dictionary::database db, auth::permission p) const;
    future<> has_functions_access(data_dictionary::database db, const sstring& ks, auth::permission p) const;
    future<> has_function_access(data_dictionary::database db, const sstring& ks, const sstring& function_signature, auth::permission p) const;
private:
    future<> has_access(data_dictionary::database db, const sstring& keyspace, auth::command_desc) const;

public:
    future<bool> check_has_permission(auth::command_desc) const;
    future<> ensure_has_permission(auth::command_desc) const;
    future<> maybe_update_per_service_level_params();

    /**
     * Returns an exceptional future with \ref exceptions::invalid_request_exception if the resource does not exist.
     */
    future<> ensure_exists(const auth::resource&) const;

    void validate_login() const;
    void ensure_not_anonymous() const; // unauthorized_exception on error

#if 0
    public void ensureIsSuper(String message) throws UnauthorizedException
    {
        if (DatabaseDescriptor.getAuthenticator().requireAuthentication() && (user == null || !user.isSuper()))
            throw new UnauthorizedException(message);
    }

    private static void validateKeyspace(String keyspace) throws InvalidRequestException
    {
        if (keyspace == null)
            throw new InvalidRequestException("You have not set a keyspace for this session");
    }
#endif

    const std::optional<auth::authenticated_user>& user() const {
        return _user;
    }

    client_state_for_another_shard move_to_other_shard() {
        return client_state_for_another_shard(this, _auth_service ? &_auth_service->container() : nullptr);
    }

#if 0
    public static SemanticVersion[] getCQLSupportedVersion()
    {
        return new SemanticVersion[]{ QueryProcessor.CQL_VERSION };
    }

    private Set<Permission> authorize(IResource resource)
    {
        // AllowAllAuthorizer or manually disabled caching.
        if (Auth.permissionsCache == null)
            return DatabaseDescriptor.getAuthorizer().authorize(user, resource);

        try
        {
            return Auth.permissionsCache.get(Pair.create(user, resource));
        }
        catch (ExecutionException e)
        {
            throw new RuntimeException(e);
        }
    }
#endif

private:

    cql_transport::cql_protocol_extension_enum_set _enabled_protocol_extensions;

public:

    bool is_protocol_extension_set(cql_transport::cql_protocol_extension ext) const {
        return _enabled_protocol_extensions.contains(ext);
    }

    void set_protocol_extensions(cql_transport::cql_protocol_extension_enum_set exts) {
        _enabled_protocol_extensions = std::move(exts);
    }
};

}

#include <seastar/core/semaphore.hh>
#include <seastar/core/shared_ptr.hh>

class service_permit {
    seastar::lw_shared_ptr<seastar::semaphore_units<>> _permit;
    service_permit(seastar::semaphore_units<>&& u) : _permit(seastar::make_lw_shared<seastar::semaphore_units<>>(std::move(u))) {}
    friend service_permit make_service_permit(seastar::semaphore_units<>&& permit);
    friend service_permit empty_service_permit();
public:
    size_t count() const { return _permit ? _permit->count() : 0; };
};

inline service_permit make_service_permit(seastar::semaphore_units<>&& permit) {
    return service_permit(std::move(permit));
}

inline service_permit empty_service_permit() {
    return make_service_permit(seastar::semaphore_units<>());
}

#ifndef SERVICE_QUERY_STATE_HH
#define SERVICE_QUERY_STATE_HH


namespace qos {
class service_level_controller;
}
namespace service {

class query_state final {
private:
    client_state& _client_state;
    tracing::trace_state_ptr _trace_state_ptr;
    service_permit _permit;

public:
    query_state(client_state& client_state, service_permit permit)
            : _client_state(client_state)
            , _trace_state_ptr(tracing::trace_state_ptr())
            , _permit(std::move(permit))
    {}

    query_state(client_state& client_state, tracing::trace_state_ptr trace_state_ptr, service_permit permit)
        : _client_state(client_state)
        , _trace_state_ptr(std::move(trace_state_ptr))
        , _permit(std::move(permit))
    { }

    const tracing::trace_state_ptr& get_trace_state() const {
        return _trace_state_ptr;
    }

    tracing::trace_state_ptr& get_trace_state() {
        return _trace_state_ptr;
    }

    client_state& get_client_state() {
        return _client_state;
    }

    const client_state& get_client_state() const {
        return _client_state;
    }
    api::timestamp_type get_timestamp() {
        return _client_state.get_timestamp();
    }

    service_permit get_permit() const& {
        return _permit;
    }

    service_permit&& get_permit() && {
        return std::move(_permit);
    }

    qos::service_level_controller& get_service_level_controller() const {
        return _client_state.get_service_level_controller();
    }

};

}

#endif

#include <iostream>

namespace db {

enum class read_repair_decision {
  NONE,
  GLOBAL,
  DC_LOCAL
};

inline std::ostream&  operator<<(std::ostream& out, db::read_repair_decision d) {
    switch (d) {
    case db::read_repair_decision::NONE: out << "NONE"; break;
    case db::read_repair_decision::GLOBAL: out << "GLOBAL"; break;
    case db::read_repair_decision::DC_LOCAL: out << "DC_LOCAL"; break;
    default: out << "ERR"; break;
    }
    return out;
}

}

#include <optional>

#include <functional>

/**
 * Represents the kind of bound in a range tombstone.
 */
enum class bound_kind : uint8_t {
    excl_end = 0,
    incl_start = 1,
    // values 2 to 5 are reserved for forward Origin compatibility
    incl_end = 6,
    excl_start = 7,
};

std::ostream& operator<<(std::ostream& out, const bound_kind k);

// Swaps start <-> end && incl <-> excl
bound_kind invert_kind(bound_kind k);
// Swaps start <-> end
bound_kind reverse_kind(bound_kind k);
int32_t weight(bound_kind k);

class bound_view {
    const static thread_local clustering_key _empty_prefix;
    std::reference_wrapper<const clustering_key_prefix> _prefix;
    bound_kind _kind;
public:
    bound_view(const clustering_key_prefix& prefix, bound_kind kind)
        : _prefix(prefix)
        , _kind(kind)
    { }
    bound_view(const bound_view& other) noexcept = default;
    bound_view& operator=(const bound_view& other) noexcept = default;

    bound_kind kind() const { return _kind; }
    const clustering_key_prefix& prefix() const { return _prefix; }

    struct tri_compare {
        // To make it assignable and to avoid taking a schema_ptr, we
        // wrap the schema reference.
        std::reference_wrapper<const schema> _s;
        tri_compare(const schema& s) : _s(s)
        { }
        std::strong_ordering operator()(const clustering_key_prefix& p1, int32_t w1, const clustering_key_prefix& p2, int32_t w2) const {
            auto type = _s.get().clustering_key_prefix_type();
            auto res = prefix_equality_tri_compare(type->types().begin(),
                type->begin(p1.representation()), type->end(p1.representation()),
                type->begin(p2.representation()), type->end(p2.representation()),
                ::tri_compare);
            if (res != 0) {
                return res;
            }
            auto d1 = p1.size(_s);
            auto d2 = p2.size(_s);

            /*
             * The logic below is
             *
             * if d1 == d2 { return w1 <=> w2 }
             * if d1 < d2  { if w1 <= 0 return less else return greater
             * if d1 > d2  { if w2 <= 0 return greater else return less
             *
             * Those w vs 0 checks are effectively { w <=> 0.5 } which is the same
             * as { 2*w <=> 1 }. Next, w1 <=> w2 is equivalent to 2*w1 <=> 2*w2, so
             * all three branches can be collapsed into a single <=>.
             *
             * This looks like veiling the above ifs for no reason, but it really
             * helps compiler generate jump-less assembly.
             */

            return ((d1 <= d2) ? w1 << 1 : 1) <=> ((d2 <= d1) ? w2 << 1 : 1);
        }
        std::strong_ordering operator()(const bound_view b, const clustering_key_prefix& p) const {
            return operator()(b._prefix, weight(b._kind), p, 0);
        }
        std::strong_ordering operator()(const clustering_key_prefix& p, const bound_view b) const {
            return operator()(p, 0, b._prefix, weight(b._kind));
        }
        std::strong_ordering operator()(const bound_view b1, const bound_view b2) const {
            return operator()(b1._prefix, weight(b1._kind), b2._prefix, weight(b2._kind));
        }
    };
    struct compare {
        // To make it assignable and to avoid taking a schema_ptr, we
        // wrap the schema reference.
        tri_compare _cmp;
        compare(const schema& s) : _cmp(s)
        { }
        bool operator()(const clustering_key_prefix& p1, int32_t w1, const clustering_key_prefix& p2, int32_t w2) const {
            return _cmp(p1, w1, p2, w2) < 0;
        }
        bool operator()(const bound_view b, const clustering_key_prefix& p) const {
            return operator()(b._prefix, weight(b._kind), p, 0);
        }
        bool operator()(const clustering_key_prefix& p, const bound_view b) const {
            return operator()(p, 0, b._prefix, weight(b._kind));
        }
        bool operator()(const bound_view b1, const bound_view b2) const {
            return operator()(b1._prefix, weight(b1._kind), b2._prefix, weight(b2._kind));
        }
    };
    bool equal(const schema& s, const bound_view other) const {
        return _kind == other._kind && _prefix.get().equal(s, other._prefix.get());
    }
    bool adjacent(const schema& s, const bound_view other) const {
        return invert_kind(other._kind) == _kind && _prefix.get().equal(s, other._prefix.get());
    }
    static bound_view bottom() {
        return {_empty_prefix, bound_kind::incl_start};
    }
    static bound_view top() {
        return {_empty_prefix, bound_kind::incl_end};
    }
    template<template<typename> typename R>
    requires Range<R, clustering_key_prefix_view>
    static bound_view from_range_start(const R<clustering_key_prefix>& range) {
        return range.start()
               ? bound_view(range.start()->value(), range.start()->is_inclusive() ? bound_kind::incl_start : bound_kind::excl_start)
               : bottom();
    }
    template<template<typename> typename R>
    requires Range<R, clustering_key_prefix>
    static bound_view from_range_end(const R<clustering_key_prefix>& range) {
        return range.end()
               ? bound_view(range.end()->value(), range.end()->is_inclusive() ? bound_kind::incl_end : bound_kind::excl_end)
               : top();
    }
    template<template<typename> typename R>
    requires Range<R, clustering_key_prefix>
    static std::pair<bound_view, bound_view> from_range(const R<clustering_key_prefix>& range) {
        return {from_range_start(range), from_range_end(range)};
    }
    template<template<typename> typename R>
    requires Range<R, clustering_key_prefix_view>
    static std::optional<typename R<clustering_key_prefix_view>::bound> to_range_bound(const bound_view& bv) {
        if (&bv._prefix.get() == &_empty_prefix) {
            return {};
        }
        bool inclusive = bv._kind != bound_kind::excl_end && bv._kind != bound_kind::excl_start;
        return {typename R<clustering_key_prefix_view>::bound(bv._prefix.get().view(), inclusive)};
    }
    friend std::ostream& operator<<(std::ostream& out, const bound_view& b) {
        return out << "{bound: prefix=" << b._prefix.get() << ", kind=" << b._kind << "}";
    }
};


#include <optional>
#include <cstdlib>

inline
lexicographical_relation relation_for_lower_bound(composite_view v) {
    switch (v.last_eoc()) {
        case composite::eoc::start:
        case composite::eoc::none:
            return lexicographical_relation::before_all_prefixed;
        case composite::eoc::end:
            return lexicographical_relation::after_all_prefixed;
    }
    abort();
}

inline
lexicographical_relation relation_for_upper_bound(composite_view v) {
    switch (v.last_eoc()) {
        case composite::eoc::start:
            return lexicographical_relation::before_all_prefixed;
        case composite::eoc::none:
            return lexicographical_relation::before_all_strictly_prefixed;
        case composite::eoc::end:
            return lexicographical_relation::after_all_prefixed;
    }
    abort();
}

enum class bound_weight : int8_t {
    before_all_prefixed = -1,
    equal = 0,
    after_all_prefixed = 1,
};

inline
bound_weight reversed(bound_weight w) {
    switch (w) {
        case bound_weight::equal:
            return w;
        case bound_weight::before_all_prefixed:
            return bound_weight::after_all_prefixed;
        case bound_weight::after_all_prefixed:
            return bound_weight::before_all_prefixed;
    }
    std::abort();
}

inline
bound_weight position_weight(bound_kind k) {
    switch (k) {
    case bound_kind::excl_end:
    case bound_kind::incl_start:
        return bound_weight::before_all_prefixed;
    case bound_kind::incl_end:
    case bound_kind::excl_start:
        return bound_weight::after_all_prefixed;
    }
    abort();
}

enum class partition_region : uint8_t {
    partition_start,
    static_row,
    clustered,
    partition_end,
};

struct view_and_holder;

template <>
struct fmt::formatter<partition_region> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const ::partition_region& r, FormatContext& ctx) const {
        switch (r) {
            case partition_region::partition_start:
                return formatter<std::string_view>::format("partition_start", ctx);
            case partition_region::static_row:
                return formatter<std::string_view>::format("static_row", ctx);
            case partition_region::clustered:
                return formatter<std::string_view>::format("clustered", ctx);
            case partition_region::partition_end:
                return formatter<std::string_view>::format("partition_end", ctx);
        }
        std::abort(); // compiler will error before we reach here
    }
};

partition_region parse_partition_region(std::string_view);

class position_in_partition_view {
    friend class position_in_partition;

    partition_region _type;
    bound_weight _bound_weight = bound_weight::equal;
    const clustering_key_prefix* _ck; // nullptr when _type != clustered
public:
    position_in_partition_view(partition_region type, bound_weight weight, const clustering_key_prefix* ck)
        : _type(type)
        , _bound_weight(weight)
        , _ck(ck)
    { }
    bool is_before_key() const {
        return _bound_weight == bound_weight::before_all_prefixed;
    }
    bool is_after_key() const {
        return _bound_weight == bound_weight::after_all_prefixed;
    }
private:
    // Returns placement of this position_in_partition relative to *_ck,
    // or lexicographical_relation::at_prefix if !_ck.
    lexicographical_relation relation() const {
        // FIXME: Currently position_range cannot represent a range end bound which
        // includes just the prefix key or a range start which excludes just a prefix key.
        // In both cases we should return lexicographical_relation::before_all_strictly_prefixed here.
        // Refs #1446.
        if (_bound_weight == bound_weight::after_all_prefixed) {
            return lexicographical_relation::after_all_prefixed;
        } else {
            return lexicographical_relation::before_all_prefixed;
        }
    }
public:
    struct partition_start_tag_t { };
    struct end_of_partition_tag_t { };
    struct static_row_tag_t { };
    struct clustering_row_tag_t { };
    struct range_tag_t { };
    using range_tombstone_tag_t = range_tag_t;

    explicit position_in_partition_view(partition_start_tag_t) : _type(partition_region::partition_start), _ck(nullptr) { }
    explicit position_in_partition_view(end_of_partition_tag_t) : _type(partition_region::partition_end), _ck(nullptr) { }
    explicit position_in_partition_view(static_row_tag_t) : _type(partition_region::static_row), _ck(nullptr) { }
    position_in_partition_view(clustering_row_tag_t, const clustering_key_prefix& ck)
        : _type(partition_region::clustered), _ck(&ck) { }
    position_in_partition_view(const clustering_key_prefix& ck)
        : _type(partition_region::clustered), _ck(&ck) { }
    position_in_partition_view(range_tag_t, bound_view bv)
        : _type(partition_region::clustered), _bound_weight(position_weight(bv.kind())), _ck(&bv.prefix()) { }
    position_in_partition_view(const clustering_key_prefix& ck, bound_weight w)
        : _type(partition_region::clustered), _bound_weight(w), _ck(&ck) { }

    static position_in_partition_view for_range_start(const query::clustering_range& r) {
        return {position_in_partition_view::range_tag_t(), bound_view::from_range_start(r)};
    }

    static position_in_partition_view for_range_end(const query::clustering_range& r) {
        return {position_in_partition_view::range_tag_t(), bound_view::from_range_end(r)};
    }

    static position_in_partition_view before_all_clustered_rows() {
        return {range_tag_t(), bound_view::bottom()};
    }

    static position_in_partition_view after_all_clustered_rows() {
        return {position_in_partition_view::range_tag_t(), bound_view::top()};
    }

    static position_in_partition_view for_partition_start() {
        return position_in_partition_view(partition_start_tag_t());
    }

    static position_in_partition_view for_partition_end() {
        return position_in_partition_view(end_of_partition_tag_t());
    }

    static position_in_partition_view for_static_row() {
        return position_in_partition_view(static_row_tag_t());
    }

    static position_in_partition_view for_key(const clustering_key& ck) {
        return {clustering_row_tag_t(), ck};
    }

    // Returns a view, as the first element of the returned pair, to before_key(pos._ck)
    // if pos.is_clustering_row() else returns pos as-is.
    // The second element of the pair needs to be kept alive as long as the first element is used.
    // The returned view is valid as long as the view passed to this method is valid.
    static view_and_holder after_key(const schema&, position_in_partition_view);

    static position_in_partition_view after_all_prefixed(const clustering_key& ck) {
        return {partition_region::clustered, bound_weight::after_all_prefixed, &ck};
    }

    // Returns a view to after_all_prefixed(pos._ck) if pos.is_clustering_row() else returns pos as-is.
    static position_in_partition_view after_all_prefixed(position_in_partition_view pos) {
        return {partition_region::clustered, pos._bound_weight == bound_weight::equal ? bound_weight::after_all_prefixed : pos._bound_weight, pos._ck};
    }

    static position_in_partition_view before_key(const clustering_key& ck) {
        return {partition_region::clustered, bound_weight::before_all_prefixed, &ck};
    }

    // Returns a view to before_key(pos._ck) if pos.is_clustering_row() else returns pos as-is.
    static position_in_partition_view before_key(position_in_partition_view pos) {
        return {partition_region::clustered, pos._bound_weight == bound_weight::equal ? bound_weight::before_all_prefixed : pos._bound_weight, pos._ck};
    }

    partition_region region() const { return _type; }
    bound_weight get_bound_weight() const { return _bound_weight; }
    bool is_partition_start() const { return _type == partition_region::partition_start; }
    bool is_partition_end() const { return _type == partition_region::partition_end; }
    bool is_static_row() const { return _type == partition_region::static_row; }
    bool is_clustering_row() const { return has_clustering_key() && _bound_weight == bound_weight::equal; }
    bool has_clustering_key() const { return _type == partition_region::clustered; }

    // Returns true if all fragments that can be seen for given schema have
    // positions >= than this. partition_start is ignored.
    bool is_before_all_fragments(const schema& s) const {
        return _type == partition_region::partition_start || _type == partition_region::static_row
               || (_type == partition_region::clustered && !s.has_static_columns() && _bound_weight == bound_weight::before_all_prefixed && key().is_empty(s));
    }

    bool is_after_all_clustered_rows(const schema& s) const {
        return is_partition_end() || (_ck && _ck->is_empty(s) && _bound_weight == bound_weight::after_all_prefixed);
    }

    bool has_key() const { return bool(_ck); }

    // Valid when has_key() == true
    const clustering_key_prefix& key() const {
        return *_ck;
    }

    // Can be called only when !is_static_row && !is_clustering_row().
    bound_view as_start_bound_view() const {
        assert(_bound_weight != bound_weight::equal);
        return bound_view(*_ck, _bound_weight == bound_weight::before_all_prefixed ? bound_kind::incl_start : bound_kind::excl_start);
    }

    bound_view as_end_bound_view() const {
        assert(_bound_weight != bound_weight::equal);
        return bound_view(*_ck, _bound_weight == bound_weight::before_all_prefixed ? bound_kind::excl_end : bound_kind::incl_end);
    }

    class printer {
        const schema& _schema;
        const position_in_partition_view& _pipv;
    public:
        printer(const schema& schema, const position_in_partition_view& pipv) : _schema(schema), _pipv(pipv) {}
        friend fmt::formatter<printer>;
    };

    // Create a position which is the same as this one but governed by a schema with reversed clustering key order.
    position_in_partition_view reversed() const {
        return position_in_partition_view(_type, ::reversed(_bound_weight), _ck);
    }

    friend fmt::formatter<printer>;
    friend fmt::formatter<position_in_partition_view>;
    friend bool no_clustering_row_between(const schema&, position_in_partition_view, position_in_partition_view);
};

template <>
struct fmt::formatter<position_in_partition_view> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const ::position_in_partition_view& pos, FormatContext& ctx) const {
        fmt::format_to(ctx.out(), "{{position: {}, ", pos._type);
        if (pos._ck) {
            fmt::format_to(ctx.out(), "{}, ", *pos._ck);
        } else {
            fmt::format_to(ctx.out(), "null, ");
        }
        return fmt::format_to(ctx.out(), "{}}}", int32_t(pos._bound_weight));
    }
};

template <>
struct fmt::formatter<position_in_partition_view::printer> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const ::position_in_partition_view::printer& p, FormatContext& ctx) const {
        auto& pos = p._pipv;
        fmt::format_to(ctx.out(), "{{position: {},", pos._type);
        if (pos._ck) {
            fmt::format_to(ctx.out(), "{}", clustering_key_prefix::with_schema_wrapper(p._schema, *pos._ck));
        } else {
            fmt::format_to(ctx.out(), "null");
        }
        return fmt::format_to(ctx.out(), ", {}}}", int32_t(pos._bound_weight));
    }
};

class position_in_partition {
    partition_region _type;
    bound_weight _bound_weight = bound_weight::equal;
    std::optional<clustering_key_prefix> _ck;
public:
    friend class clustering_interval_set;
    struct partition_start_tag_t { };
    struct end_of_partition_tag_t { };
    struct static_row_tag_t { };
    struct after_static_row_tag_t { };
    struct clustering_row_tag_t { };
    struct after_clustering_row_tag_t { };
    struct before_clustering_row_tag_t { };
    struct range_tag_t { };
    using range_tombstone_tag_t = range_tag_t;
    partition_region get_type() const { return _type; }
    bound_weight get_bound_weight() const { return _bound_weight; }
    const std::optional<clustering_key_prefix>& get_clustering_key_prefix() const { return _ck; }
    position_in_partition(partition_region type, bound_weight weight, std::optional<clustering_key_prefix> ck)
        : _type(type), _bound_weight(weight), _ck(std::move(ck)) { }
    explicit position_in_partition(partition_start_tag_t) : _type(partition_region::partition_start) { }
    explicit position_in_partition(end_of_partition_tag_t) : _type(partition_region::partition_end) { }
    explicit position_in_partition(static_row_tag_t) : _type(partition_region::static_row) { }
    position_in_partition(clustering_row_tag_t, clustering_key_prefix ck)
        : _type(partition_region::clustered), _ck(std::move(ck)) { }
    position_in_partition(after_clustering_row_tag_t, const schema& s, clustering_key_prefix ck)
        : _type(partition_region::clustered)
        , _bound_weight(bound_weight::after_all_prefixed)
        , _ck(std::move(ck))
    {
        if (clustering_key::make_full(s, *_ck)) { // Refs #1446
            _bound_weight = bound_weight::before_all_prefixed;
        }
    }
    position_in_partition(after_clustering_row_tag_t, const schema& s, position_in_partition_view pos)
        : position_in_partition(after_clustering_row_tag_t(), s, position_in_partition(pos))
    { }
    position_in_partition(after_clustering_row_tag_t, const schema& s, position_in_partition&& pos)
        : _type(partition_region::clustered)
        , _bound_weight(pos._bound_weight != bound_weight::equal ? pos._bound_weight : bound_weight::after_all_prefixed)
        , _ck(std::move(pos._ck))
    {
        if (pos._bound_weight == bound_weight::equal && _ck && clustering_key::make_full(s, *_ck)) { // Refs #1446
            _bound_weight = bound_weight::before_all_prefixed;
        }
    }
    position_in_partition(before_clustering_row_tag_t, clustering_key_prefix ck)
        : _type(partition_region::clustered), _bound_weight(bound_weight::before_all_prefixed), _ck(std::move(ck)) { }
    position_in_partition(before_clustering_row_tag_t, position_in_partition_view pos)
        : _type(partition_region::clustered)
        , _bound_weight(pos._bound_weight != bound_weight::equal ? pos._bound_weight : bound_weight::before_all_prefixed)
        , _ck(*pos._ck) { }
    position_in_partition(range_tag_t, bound_view bv)
        : _type(partition_region::clustered), _bound_weight(position_weight(bv.kind())), _ck(bv.prefix()) { }
    position_in_partition(range_tag_t, bound_kind kind, clustering_key_prefix&& prefix)
        : _type(partition_region::clustered), _bound_weight(position_weight(kind)), _ck(std::move(prefix)) { }
    position_in_partition(after_static_row_tag_t) :
        position_in_partition(range_tag_t(), bound_view::bottom()) { }
    explicit position_in_partition(position_in_partition_view view)
        : _type(view._type), _bound_weight(view._bound_weight)
        {
            if (view._ck) {
                _ck = *view._ck;
            }
        }

    position_in_partition& operator=(position_in_partition_view view) {
        _type = view._type;
        _bound_weight = view._bound_weight;
        if (view._ck) {
            _ck = *view._ck;
        } else {
            _ck.reset();
        }
        return *this;
    }

    static position_in_partition before_all_clustered_rows() {
        return {position_in_partition::range_tag_t(), bound_view::bottom()};
    }

    static position_in_partition after_all_clustered_rows() {
        return {position_in_partition::range_tag_t(), bound_view::top()};
    }

    // If given position is a clustering row position, returns a position
    // right before it. Otherwise, returns it unchanged.
    // The position "pos" must be a clustering position.
    static position_in_partition before_key(position_in_partition_view pos) {
        return {before_clustering_row_tag_t(), pos};
    }

    static position_in_partition before_key(clustering_key ck) {
        return {before_clustering_row_tag_t(), std::move(ck)};
    }

    static position_in_partition after_key(const schema& s, clustering_key ck) {
        return {after_clustering_row_tag_t(), s, std::move(ck)};
    }

    // If given position is a clustering row position, returns a position
    // right after it. Otherwise returns it unchanged.
    // The position "pos" must be a clustering position.
    static position_in_partition after_key(const schema& s, position_in_partition_view pos) {
        return {after_clustering_row_tag_t(), s, pos};
    }

    static position_in_partition after_key(const schema& s, position_in_partition&& pos) noexcept {
        return {after_clustering_row_tag_t(), s, std::move(pos)};
    }

    static position_in_partition for_key(clustering_key ck) {
        return {clustering_row_tag_t(), std::move(ck)};
    }

    static position_in_partition for_partition_start() {
        return position_in_partition{partition_start_tag_t()};
    }

    static position_in_partition for_partition_end() {
        return position_in_partition(end_of_partition_tag_t());
    }

    static position_in_partition for_static_row() {
        return position_in_partition{static_row_tag_t()};
    }

    static position_in_partition min() {
        return for_static_row();
    }

    static position_in_partition for_range_start(const query::clustering_range&);
    static position_in_partition for_range_end(const query::clustering_range&);

    partition_region region() const { return _type; }
    bool is_partition_start() const { return _type == partition_region::partition_start; }
    bool is_partition_end() const { return _type == partition_region::partition_end; }
    bool is_static_row() const { return _type == partition_region::static_row; }
    bool is_clustering_row() const { return has_clustering_key() && _bound_weight == bound_weight::equal; }
    bool has_clustering_key() const { return _type == partition_region::clustered; }

    bool is_after_all_clustered_rows(const schema& s) const {
        return is_partition_end() || (_ck && _ck->is_empty(s) && _bound_weight == bound_weight::after_all_prefixed);
    }

    bool is_before_all_clustered_rows(const schema& s) const {
        return _type < partition_region::clustered
               || (_type == partition_region::clustered && _ck->is_empty(s) && _bound_weight == bound_weight::before_all_prefixed);
    }

    template<typename Hasher>
    void feed_hash(Hasher& hasher, const schema& s) const {
        ::feed_hash(hasher, _bound_weight);
        if (_ck) {
            ::feed_hash(hasher, true);
            ::feed_hash(hasher, *_ck, s);
        } else {
            ::feed_hash(hasher, false);
        }
    }

    bool has_key() const { return bool(_ck); }

    const clustering_key_prefix& key() const {
        return *_ck;
    }
    operator position_in_partition_view() const {
        return { _type, _bound_weight, _ck ? &*_ck : nullptr };
    }

    size_t external_memory_usage() const {
        return _ck ? _ck->external_memory_usage() : 0;
    }

    // Defines total order on the union of position_and_partition and composite objects.
    //
    // The ordering is compatible with position_range (r). The following is satisfied for
    // all cells with name c included by the range:
    //
    //   r.start() <= c < r.end()
    //
    // The ordering on composites given by this is compatible with but weaker than the cell name order.
    //
    // The ordering on position_in_partition given by this is compatible but weaker than the ordering
    // given by position_in_partition::tri_compare.
    //
    class composite_tri_compare {
        const schema& _s;
    public:
        static int rank(partition_region t) {
            return static_cast<int>(t);
        }

        composite_tri_compare(const schema& s) : _s(s) {}

        std::strong_ordering operator()(position_in_partition_view a, position_in_partition_view b) const {
            if (a._type != b._type) {
                return rank(a._type) <=> rank(b._type);
            }
            if (!a._ck) {
                return std::strong_ordering::equal;
            }
            auto&& types = _s.clustering_key_type()->types();
            auto cmp = [&] (const data_type& t, managed_bytes_view c1, managed_bytes_view c2) { return t->compare(c1, c2); };
            return lexicographical_tri_compare(types.begin(), types.end(),
                a._ck->begin(_s), a._ck->end(_s),
                b._ck->begin(_s), b._ck->end(_s),
                cmp, a.relation(), b.relation());
        }

        std::strong_ordering operator()(position_in_partition_view a, composite_view b) const {
            if (b.empty()) {
                return std::strong_ordering::greater; // a cannot be empty.
            }
            partition_region b_type = b.is_static() ? partition_region::static_row : partition_region::clustered;
            if (a._type != b_type) {
                return rank(a._type) <=> rank(b_type);
            }
            if (!a._ck) {
                return std::strong_ordering::equal;
            }
            auto&& types = _s.clustering_key_type()->types();
            auto b_values = b.values();
            auto cmp = [&] (const data_type& t, managed_bytes_view c1, managed_bytes_view c2) { return t->compare(c1, c2); };
            return lexicographical_tri_compare(types.begin(), types.end(),
                a._ck->begin(_s), a._ck->end(_s),
                b_values.begin(), b_values.end(),
                cmp, a.relation(), relation_for_lower_bound(b));
        }

        std::strong_ordering operator()(composite_view a, position_in_partition_view b) const {
            return 0 <=> (*this)(b, a);
        }

        std::strong_ordering operator()(composite_view a, composite_view b) const {
            if (a.is_static() != b.is_static()) {
                return a.is_static() ? std::strong_ordering::less : std::strong_ordering::greater;
            }
            auto&& types = _s.clustering_key_type()->types();
            auto a_values = a.values();
            auto b_values = b.values();
            auto cmp = [&] (const data_type& t, bytes_view c1, bytes_view c2) { return t->compare(c1, c2); };
            return lexicographical_tri_compare(types.begin(), types.end(),
                a_values.begin(), a_values.end(),
                b_values.begin(), b_values.end(),
                cmp,
                relation_for_lower_bound(a),
                relation_for_lower_bound(b));
        }
    };

    // Less comparator giving the same order as composite_tri_compare.
    class composite_less_compare {
        composite_tri_compare _cmp;
    public:
        composite_less_compare(const schema& s) : _cmp(s) {}

        template<typename T, typename U>
        bool operator()(const T& a, const U& b) const {
            return _cmp(a, b) < 0;
        }
    };

    class tri_compare {
        bound_view::tri_compare _cmp;
    private:
        template<typename T, typename U>
        std::strong_ordering compare(const T& a, const U& b) const {
            if (a._type != b._type) {
                return composite_tri_compare::rank(a._type) <=> composite_tri_compare::rank(b._type);
            }
            if (!a._ck) {
                return std::strong_ordering::equal;
            }
            return _cmp(*a._ck, int8_t(a._bound_weight), *b._ck, int8_t(b._bound_weight));
        }
    public:
        tri_compare(const schema& s) : _cmp(s) { }
        std::strong_ordering operator()(const position_in_partition& a, const position_in_partition& b) const {
            return compare(a, b);
        }
        std::strong_ordering operator()(const position_in_partition_view& a, const position_in_partition_view& b) const {
            return compare(a, b);
        }
        std::strong_ordering operator()(const position_in_partition& a, const position_in_partition_view& b) const {
            return compare(a, b);
        }
        std::strong_ordering operator()(const position_in_partition_view& a, const position_in_partition& b) const {
            return compare(a, b);
        }
    };
    class less_compare {
        tri_compare _cmp;
    public:
        less_compare(const schema& s) : _cmp(s) { }
        bool operator()(const position_in_partition& a, const position_in_partition& b) const {
            return _cmp(a, b) < 0;
        }
        bool operator()(const position_in_partition_view& a, const position_in_partition_view& b) const {
            return _cmp(a, b) < 0;
        }
        bool operator()(const position_in_partition& a, const position_in_partition_view& b) const {
            return _cmp(a, b) < 0;
        }
        bool operator()(const position_in_partition_view& a, const position_in_partition& b) const {
            return _cmp(a, b) < 0;
        }
    };
    class equal_compare {
        clustering_key_prefix::equality _equal;
        template<typename T, typename U>
        bool compare(const T& a, const U& b) const {
            if (a._type != b._type) {
                return false;
            }
            bool a_rt_weight = bool(a._ck);
            bool b_rt_weight = bool(b._ck);
            return a_rt_weight == b_rt_weight
                   && (!a_rt_weight || (a._bound_weight == b._bound_weight && _equal(*a._ck, *b._ck)));
        }
    public:
        equal_compare(const schema& s) : _equal(s) { }
        bool operator()(const position_in_partition& a, const position_in_partition& b) const {
            return compare(a, b);
        }
        bool operator()(const position_in_partition_view& a, const position_in_partition_view& b) const {
            return compare(a, b);
        }
        bool operator()(const position_in_partition_view& a, const position_in_partition& b) const {
            return compare(a, b);
        }
        bool operator()(const position_in_partition& a, const position_in_partition_view& b) const {
            return compare(a, b);
        }
    };
    friend std::ostream& operator<<(std::ostream&, const position_in_partition&);

    // Create a position which is the same as this one but governed by a schema with reversed clustering key order.
    position_in_partition reversed() const& {
        return position_in_partition(_type, ::reversed(_bound_weight), _ck);
    }

    // Create a position which is the same as this one but governed by a schema with reversed clustering key order.
    position_in_partition reversed() && {
        return position_in_partition(_type, ::reversed(_bound_weight), std::move(_ck));
    }
};

template <>
struct fmt::formatter<position_in_partition> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const ::position_in_partition& pos, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", position_in_partition_view(pos));
    }
};

struct view_and_holder {
    std::optional<position_in_partition> holder;
    position_in_partition_view view;

    view_and_holder(position_in_partition pos)
        : holder(std::move(pos))
        , view(*holder)
    { }

    explicit view_and_holder(position_in_partition_view pos)
        : view(pos)
    { }

    view_and_holder(view_and_holder&& other) noexcept
        : holder(std::move(other.holder))
        , view(holder ? *holder : other.view)
    { }

    view_and_holder& operator=(view_and_holder&& other) noexcept {
        holder = std::move(other.holder);
        view = holder ? *holder: other.view;
        return *this;
    }
};

inline
view_and_holder position_in_partition_view::after_key(const schema& s, position_in_partition_view pos) {
    if (!pos.is_clustering_row()) {
        return view_and_holder(pos);
    }
    if (pos.key().is_full(s)) {
        return view_and_holder(position_in_partition_view::after_all_prefixed(pos.key()));
    }
    // FIXME: This wouldn't be needed if we had a bound weight to represent this.
    return view_and_holder(position_in_partition::after_key(s, clustering_key(pos.key())));
}

inline
position_in_partition position_in_partition::for_range_start(const query::clustering_range& r) {
    return {position_in_partition::range_tag_t(), bound_view::from_range_start(r)};
}

inline
position_in_partition position_in_partition::for_range_end(const query::clustering_range& r) {
    return {position_in_partition::range_tag_t(), bound_view::from_range_end(r)};
}

// Returns true if and only if there can't be any clustering_row with position > a and < b.
// It is assumed that a <= b.
inline
bool no_clustering_row_between(const schema& s, position_in_partition_view a, position_in_partition_view b) {
    clustering_key_prefix::equality eq(s);
    if (a._ck && b._ck) {
        return eq(*a._ck, *b._ck) && (a._bound_weight != bound_weight::before_all_prefixed || b._bound_weight != bound_weight::after_all_prefixed);
    } else {
        return !a._ck && !b._ck;
    }
}

// Returns true if and only if there can't be any clustering_row with position >= a and < b.
// It is assumed that a <= b.
inline
bool no_clustering_row_between_weak(const schema& s, position_in_partition_view a, position_in_partition_view b) {
    clustering_key_prefix::equality eq(s);
    if (a.has_key() && b.has_key()) {
        return eq(a.key(), b.key())
               && (a.get_bound_weight() == bound_weight::after_all_prefixed
                   || b.get_bound_weight() != bound_weight::after_all_prefixed);
    } else {
        return !a.has_key() && !b.has_key();
    }
}

// Includes all position_in_partition objects "p" for which: start <= p < end
// And only those.
class position_range {
private:
    position_in_partition _start;
    position_in_partition _end;
public:
    static position_range from_range(const query::clustering_range&);

    static position_range for_static_row() {
        return {
            position_in_partition(position_in_partition::static_row_tag_t()),
            position_in_partition(position_in_partition::after_static_row_tag_t())
        };
    }

    static position_range full() {
        return {
            position_in_partition(position_in_partition::static_row_tag_t()),
            position_in_partition::after_all_clustered_rows()
        };
    }

    static position_range all_clustered_rows() {
        return {
            position_in_partition::before_all_clustered_rows(),
            position_in_partition::after_all_clustered_rows()
        };
    }

    position_range(position_range&&) = default;
    position_range& operator=(position_range&&) = default;
    position_range(const position_range&) = default;
    position_range& operator=(const position_range&) = default;

    // Constructs position_range which covers the same rows as given clustering_range.
    // position_range includes a fragment if it includes position of that fragment.
    position_range(const query::clustering_range&);
    position_range(query::clustering_range&&);

    position_range(position_in_partition start, position_in_partition end)
        : _start(std::move(start))
        , _end(std::move(end))
    { }

    void set_start(position_in_partition pos) { _start = std::move(pos); }
    void set_end(position_in_partition pos) { _end = std::move(pos); }
    const position_in_partition& start() const& { return _start; }
    position_in_partition&& start() && { return std::move(_start); }
    const position_in_partition& end() const& { return _end; }
    position_in_partition&& end() && { return std::move(_end); }
    bool contains(const schema& s, position_in_partition_view pos) const;
    bool overlaps(const schema& s, position_in_partition_view start, position_in_partition_view end) const;
    // Returns true iff this range contains all keys contained by position_range(start, end).
    bool contains(const schema& s, position_in_partition_view start, position_in_partition_view end) const;
    bool is_all_clustered_rows(const schema&) const;

    friend std::ostream& operator<<(std::ostream&, const position_range&);
};

class clustering_interval_set;

inline
bool position_range::contains(const schema& s, position_in_partition_view pos) const {
    position_in_partition::less_compare less(s);
    return !less(pos, _start) && less(pos, _end);
}

inline
bool position_range::contains(const schema& s, position_in_partition_view start, position_in_partition_view end) const {
    position_in_partition::less_compare less(s);
    return !less(start, _start) && !less(_end, end);
}

inline
bool position_range::overlaps(const schema& s, position_in_partition_view start, position_in_partition_view end) const {
    position_in_partition::less_compare less(s);
    return !less(end, _start) && less(start, _end);
}

inline
bool position_range::is_all_clustered_rows(const schema& s) const {
    return _start.is_before_all_clustered_rows(s) && _end.is_after_all_clustered_rows(s);
}

// Assumes that the bounds of `r` are of 'clustered' type
// and that `r` is non-empty (the left bound is smaller than the right bound).
//
// If `r` does not contain any keys, returns nullopt.
std::optional<query::clustering_range> position_range_to_clustering_range(const position_range& r, const schema&);


namespace locator {

using host_id = utils::tagged_uuid<struct host_id_tag>;

}


namespace service {

namespace pager {

class paging_state final {
public:
    using replicas_per_token_range = std::unordered_map<dht::token_range, std::vector<locator::host_id>>;

private:
    partition_key _partition_key;
    std::optional<clustering_key> _clustering_key;
    uint32_t _remaining_low_bits;
    query_id _query_uuid;
    replicas_per_token_range _last_replicas;
    std::optional<db::read_repair_decision> _query_read_repair_decision;
    uint32_t _rows_fetched_for_last_partition_low_bits;
    uint32_t _remaining_high_bits;
    uint32_t _rows_fetched_for_last_partition_high_bits;
    bound_weight _ck_weight = bound_weight::equal;
    partition_region _region = partition_region::partition_start;

public:
    // IDL ctor
    paging_state(partition_key pk,
            std::optional<clustering_key> ck,
            uint32_t rem,
            query_id reader_recall_uuid,
            replicas_per_token_range last_replicas,
            std::optional<db::read_repair_decision> query_read_repair_decision,
            uint32_t rows_fetched_for_last_partition,
            uint32_t remaining_ext,
            uint32_t rows_fetched_for_last_partition_high_bits,
            bound_weight ck_weight,
            partition_region region);

    paging_state(partition_key pk,
            position_in_partition_view pos,
            uint64_t rem,
            query_id reader_recall_uuid,
            replicas_per_token_range last_replicas,
            std::optional<db::read_repair_decision> query_read_repair_decision,
            uint64_t rows_fetched_for_last_partition);

    void set_partition_key(partition_key pk) {
        _partition_key = std::move(pk);
    }

    // sets position to at the given clustering key
    void set_clustering_key(clustering_key ck) {
        _clustering_key = std::move(ck);
        _ck_weight = bound_weight::equal;
        _region = partition_region::clustered;
    }

    void set_remaining(uint64_t remaining) {
        _remaining_low_bits = static_cast<uint32_t>(remaining);
        _remaining_high_bits = static_cast<uint32_t>(remaining >> 32);
    }

    /**
     * Last processed key, i.e. where to start from in next paging round
     */
    const partition_key& get_partition_key() const {
        return _partition_key;
    }
    /**
     * Clustering key in last partition. I.e. first, next, row
     *
     * Use \ref get_position_in_partition() instead.
     */
    const std::optional<clustering_key>& get_clustering_key() const {
        return _clustering_key;
    }
    /**
     * Weight of last processed position, see \ref get_position_in_partition()
     */
    bound_weight get_clustering_key_weight() const {
        return _ck_weight;
    }
    /**
     * Partition region of last processed position, see \ref get_position_in_partition()
     */
    partition_region get_partition_region() const {
        return _region;
    }
    position_in_partition_view get_position_in_partition() const {
        return position_in_partition_view(_region, _ck_weight, _clustering_key ? &*_clustering_key : nullptr);
    }
    /**
     * Max remaining rows to fetch in total.
     * I.e. initial row_limit - #rows returned so far.
     */
    uint32_t get_remaining_low_bits() const {
        return _remaining_low_bits;
    }

    uint32_t get_rows_fetched_for_last_partition_low_bits() const {
        return _rows_fetched_for_last_partition_low_bits;
    }
    uint32_t get_remaining_high_bits() const {
        return _remaining_high_bits;
    }

    uint32_t get_rows_fetched_for_last_partition_high_bits() const {
        return _rows_fetched_for_last_partition_high_bits;
    }

    uint64_t get_remaining() const {
        return (static_cast<uint64_t>(_remaining_high_bits) << 32) | _remaining_low_bits;
    }

    uint64_t get_rows_fetched_for_last_partition() const {
        return (static_cast<uint64_t>(_rows_fetched_for_last_partition_high_bits) << 32) | _rows_fetched_for_last_partition_low_bits;
    }

    /**
     * query_uuid is a unique key under which the replicas saved the
     * readers used to serve the last page. These saved readers may be
     * resumed (if not already purged from the cache) instead of opening new
     * ones - as a performance optimization.
     * If the uuid is the invalid default-constructed UUID(), it means that
     * the client got this paging_state from a coordinator running an older
     * version of Scylla.
     */
    query_id get_query_uuid() const {
        return _query_uuid;
    }

    /**
     * The replicas used to serve the last page.
     *
     * Helps paged queries consistently hit the same replicas for each
     * subsequent page. Replicas that already served a page will keep
     * the readers used for filling it around in a cache. Subsequent
     * page request hitting the same replicas can reuse these readers
     * to fill the pages avoiding the work of creating these readers
     * from scratch on every page.
     * In a mixed cluster older coordinators will ignore this value.
     * Replicas are stored per token-range where the token-range
     * is some subrange of the query range that doesn't cross node
     * boundaries.
     */
    replicas_per_token_range get_last_replicas() const {
        return _last_replicas;
    }

    /**
     * The read-repair decision made for this query.
     *
     * The read-repair decision is made on the first page and is applied to
     * all subsequent pages. This way we can ensure that we consistently use
     * the same replicas on each page throughout the query. This helps
     * reader-reuse on the replicas as readers can only be reused if they're
     * used for all pages, if the replica is skipped for one or more pages the
     * saved reader has to be dropped.
     */
    std::optional<db::read_repair_decision> get_query_read_repair_decision() const {
        return _query_read_repair_decision;
    }

    static lw_shared_ptr<paging_state> deserialize(bytes_opt bytes);
    bytes_opt serialize() const;
};

}

}

/**
 * A helper class to encompass multiple lambdas accepting different input types
 * into a single object.
 */

template<typename... Ts> struct overloaded_functor : Ts... { using Ts::operator()...; };
template<typename... Ts> overloaded_functor(Ts...) -> overloaded_functor<Ts...>;



#include <optional>
#include <variant>

#include <seastar/util/variant_utils.hh>


namespace cql3 {

struct null_value {
    friend bool operator==(const null_value&, const null_value) { return true; }
};

class raw_value;
/// \brief View to a raw CQL protocol value.
///
/// \see raw_value
class raw_value_view {
    std::variant<fragmented_temporary_buffer::view, managed_bytes_view, null_value> _data;
    // Temporary storage is only useful if a raw_value_view needs to be instantiated
    // with a value which lifetime is bounded only to the view itself.
    // This hack is introduced in order to avoid storing temporary storage
    // in an external container, which may cause memory leaking problems.
    // This pointer is disengaged for regular raw_value_view instances.
    // Data is stored in a shared pointer for two reasons:
    // - pointers are cheap to copy
    // - it makes the view keep its semantics - it's safe to copy a view multiple times
    //   and all copies still refer to the same underlying data.
    lw_shared_ptr<managed_bytes> _temporary_storage = nullptr;

    raw_value_view(null_value data)
        : _data{std::move(data)}
    {}
    raw_value_view(fragmented_temporary_buffer::view data)
        : _data{data}
    {}
    raw_value_view(managed_bytes_view data)
        : _data{data}
    {}
    // This constructor is only used by make_temporary() and it acquires ownership
    // of the given buffer. The view created that way refers to its own temporary storage.
    explicit raw_value_view(managed_bytes&& temporary_storage);
public:
    static raw_value_view make_null() {
        return raw_value_view{null_value{}};
    }
    static raw_value_view make_value(fragmented_temporary_buffer::view view) {
        return raw_value_view{view};
    }
    static raw_value_view make_value(managed_bytes_view view) {
        return raw_value_view{view};
    }
    static raw_value_view make_value(bytes_view view) {
        return raw_value_view{managed_bytes_view(view)};
    }
    static raw_value_view make_temporary(raw_value&& value);
    bool is_null() const {
        return std::holds_alternative<null_value>(_data);
    }
    // An empty value is not null, but it has 0 bytes of data.
    // An empty int value can be created in CQL using blobasint(0x).
    bool is_empty_value() const {
        if (is_null()) {
            return false;
        }
        return size_bytes() == 0;
    }
    bool is_value() const {
        return _data.index() <= 1;
    }
    explicit operator bool() const {
        return is_value();
    }

    template <typename Func>
    requires std::invocable<Func, const managed_bytes_view&> && std::invocable<Func, const fragmented_temporary_buffer::view&>
    decltype(auto) with_value(Func f) const {
        switch (_data.index()) {
        case 0: return f(std::get<fragmented_temporary_buffer::view>(_data));
        default: return f(std::get<managed_bytes_view>(_data));
        }
    }

    template <typename Func>
    requires std::invocable<Func, bytes_view>
    decltype(auto) with_linearized(Func f) const {
        return with_value([&] (const FragmentedView auto& v) {
            return ::with_linearized(v, std::forward<Func>(f));
        });
    }

    size_t size_bytes() const {
        return with_value([&] (const FragmentedView auto& v) {
            return v.size_bytes();
        });
    }

    template <typename ValueType>
    ValueType deserialize(const abstract_type& t) const {
        return value_cast<ValueType>(with_value([&] (const FragmentedView auto& v) { return t.deserialize(v); }));
    }

    template <typename ValueType>
    ValueType deserialize(const collection_type_impl& t) const {
        return value_cast<ValueType>(with_value([&] (const FragmentedView auto& v) { return t.deserialize(v); }));
    }

    void validate(const abstract_type& t) const {
        return with_value([&] (const FragmentedView auto& v) { return t.validate(v); });
    }

    template <typename ValueType>
    ValueType validate_and_deserialize(const collection_type_impl& t) const {
        return with_value([&] (const FragmentedView auto& v) {
            t.validate(v);
            return value_cast<ValueType>(t.deserialize(v));
        });
    }

    template <typename ValueType>
    ValueType validate_and_deserialize(const abstract_type& t) const {
        return with_value([&] (const FragmentedView auto& v) {
            t.validate(v);
            return value_cast<ValueType>(t.deserialize(v));
        });
    }

    friend managed_bytes_opt to_managed_bytes_opt(const cql3::raw_value_view& view) {
        if (view.is_value()) {
            return view.with_value([] (const FragmentedView auto& v) { return managed_bytes(v); });
        }
        return managed_bytes_opt();
    }

    friend managed_bytes_opt to_managed_bytes_opt(cql3::raw_value_view&& view) {
        if (view._temporary_storage) {
            return std::move(*view._temporary_storage);
        }
        return to_managed_bytes_opt(view);
    }

    friend std::ostream& operator<<(std::ostream& os, const raw_value_view& value);
    friend class raw_value;
};

/// \brief Raw CQL protocol value.
///
/// The `raw_value` type represents an uninterpreted value from the CQL wire
/// protocol. A raw value can hold either a null value, or a byte
/// blob that represents the value.
class raw_value {
    std::variant<bytes, managed_bytes, null_value> _data;

    raw_value(null_value&& data)
        : _data{std::move(data)}
    {}
    raw_value(bytes&& data)
        : _data{std::move(data)}
    {}
    raw_value(const bytes& data)
        : _data{data}
    {}
    raw_value(managed_bytes&& data)
        : _data{std::move(data)}
    {}
    raw_value(const managed_bytes& data)
        : _data{data}
    {}
public:
    static raw_value make_null() {
        return raw_value{null_value{}};
    }
    static raw_value make_value(const raw_value_view& view);
    static raw_value make_value(managed_bytes&& mb) {
        return raw_value{std::move(mb)};
    }
    static raw_value make_value(managed_bytes_opt&& mbo) {
        return mbo ? make_value(std::move(*mbo)) : make_null();
    }
    static raw_value make_value(const managed_bytes& mb) {
        return raw_value{mb};
    }
    static raw_value make_value(const managed_bytes_opt& mbo) {
        if (mbo) {
            return make_value(*mbo);
        }
        return make_null();
    }
    static raw_value make_value(bytes&& bytes) {
        return raw_value{std::move(bytes)};
    }
    static raw_value make_value(const bytes& bytes) {
        return raw_value{bytes};
    }
    static raw_value make_value(const bytes_opt& bytes) {
        if (bytes) {
            return make_value(*bytes);
        }
        return make_null();
    }
    bool is_null() const {
        return std::holds_alternative<null_value>(_data);
    }
    // An empty value is not null, but it has 0 bytes of data.
    // An empty int value can be created in CQL using blobasint(0x).
    bool is_empty_value() const {
        if (is_null()) {
            return false;
        }
        return view().size_bytes() == 0;
    }
    bool is_value() const {
        return _data.index() <= 1;
    }
    explicit operator bool() const {
        return is_value();
    }
    bytes to_bytes() && {
        return std::visit(overloaded_functor{
            [](bytes&& bytes_val) { return std::move(bytes_val); },
            [](managed_bytes&& managed_bytes_val) { return ::to_bytes(managed_bytes_val); },
            [](null_value&&) -> bytes {
                throw std::runtime_error("to_bytes() called on raw value that is null");
            },
        }, std::move(_data));
    }
    bytes_opt to_bytes_opt() && {
        return std::visit(overloaded_functor{
            [](bytes&& bytes_val) { return bytes_opt(bytes_val); },
            [](managed_bytes&& managed_bytes_val) { return bytes_opt(::to_bytes(managed_bytes_val)); },
            [](null_value&&) -> bytes_opt {
                return std::nullopt;
            },
        }, std::move(_data));
    }
    managed_bytes to_managed_bytes() && {
        return std::visit(overloaded_functor{
            [](bytes&& bytes_val) { return managed_bytes(bytes_val); },
            [](managed_bytes&& managed_bytes_val) { return std::move(managed_bytes_val); },
            [](null_value&&) -> managed_bytes {
                throw std::runtime_error("to_managed_bytes() called on raw value that is null");
            },
        }, std::move(_data));
    }
    managed_bytes_opt to_managed_bytes_opt() && {
        return std::visit(overloaded_functor{
            [](bytes&& bytes_val) { return managed_bytes_opt(bytes_val); },
            [](managed_bytes&& managed_bytes_val) { return managed_bytes_opt(std::move(managed_bytes_val)); },
            [](null_value&&) -> managed_bytes_opt {
                return std::nullopt;
            },
        }, std::move(_data));
    }
    raw_value_view view() const;
    friend class raw_value_view;

    friend bool operator==(const raw_value& v1, const raw_value& v2);
    friend std::ostream& operator<<(std::ostream& os, const raw_value& value);
};

}

inline bytes to_bytes(const cql3::raw_value_view& view)
{
    return view.with_value([] (const FragmentedView auto& v) {
        return linearized(v);
    });
}

inline bytes_opt to_bytes_opt(const cql3::raw_value_view& view) {
    if (view.is_value()) {
        return to_bytes(view);
    }
    return bytes_opt();
}

inline bytes_opt to_bytes_opt(const cql3::raw_value& value) {
    return to_bytes_opt(value.view());
}


#include <concepts>
#include <initializer_list>

namespace cql3 {

class cql_config;
extern const cql_config default_cql_config;

class column_specification;

using computed_function_values = std::unordered_map<uint8_t, bytes_opt>;

using unset_bind_variable_vector = utils::small_vector<bool, 16>;

// Matches a raw_value_view with an unset vector to support CQL binary protocol
// "unset" values.
struct raw_value_view_vector_with_unset {
    std::vector<raw_value_view> values;
    unset_bind_variable_vector unset;

    raw_value_view_vector_with_unset(std::vector<raw_value_view> values_, unset_bind_variable_vector unset_) : values(std::move(values_)), unset(std::move(unset_)) {}
    // Constructor with no unset support, for tests and internal queries
    raw_value_view_vector_with_unset(std::vector<raw_value_view> values_) : values(std::move(values_)) {
        unset.resize(values.size());
    }
    raw_value_view_vector_with_unset() = default;
};

// Matches a raw_value with an unset vector to support CQL binary protocol
// "unset" values.
struct raw_value_vector_with_unset {
    std::vector<raw_value> values;
    unset_bind_variable_vector unset;

    raw_value_vector_with_unset(std::vector<raw_value> values_, unset_bind_variable_vector unset_) : values(std::move(values_)), unset(std::move(unset_)) {}
    // Constructor with no unset support, for tests and internal queries
    raw_value_vector_with_unset(std::vector<raw_value> values_) : values(std::move(values_)) {
        unset.resize(values.size());
    }
    // Mostly for testing.
    raw_value_vector_with_unset(std::initializer_list<raw_value> values_) : raw_value_vector_with_unset(std::vector(values_)) {}
    raw_value_vector_with_unset() = default;
};

/**
 * Options for a query.
 */
class query_options {
public:
    // Options that are likely to not be present in most queries
    struct specific_options final {
        static thread_local const specific_options DEFAULT;

        const int32_t page_size;
        const lw_shared_ptr<service::pager::paging_state> state;
        const std::optional<db::consistency_level> serial_consistency;
        const api::timestamp_type timestamp;
    };
private:
    const cql_config& _cql_config;
    const db::consistency_level _consistency;
    const std::optional<std::vector<sstring_view>> _names;
    std::vector<cql3::raw_value> _values;
    std::vector<cql3::raw_value_view> _value_views;
    unset_bind_variable_vector _unset;
    const bool _skip_metadata;
    const specific_options _options;
    std::optional<std::vector<query_options>> _batch_options;
    // We must use the same microsecond-precision timestamp for
    // all cells created by an LWT statement or when a statement
    // has a user-provided timestamp. In case the statement or
    // a BATCH appends many values to a list, each value should
    // get a unique and monotonic timeuuid. This sequence is
    // used to make all time-based UUIDs:
    // 1) share the same microsecond,
    // 2) monotonic
    // 3) unique.
    mutable int _list_append_seq = 0;

    // Cached `function_call` evaluation results. `function_call` AST nodes
    // are created for each function with side effects in a CQL query, i.e.
    // non-deterministic functions (`uuid()`, `now()` and some others
    // timeuuid-related).
    //
    // These nodes are evaluated either when a query itself is executed
    // or query restrictions are computed (e.g. partition/clustering
    // key ranges for LWT requests).
    //
    // We need to cache the calls since otherwise when handling a
    // `bounce_to_shard` request for an LWT query, we can possibly enter an
    // infinite bouncing loop (in case a function is used to calculate
    // partition key ranges for a query), since the results can be different
    // each time. Furthermore, we don't support bouncing more than one time.
    // Refs: #8604 (https://github.com/scylladb/scylla/issues/8604)
    //
    // Using mutable because `query_state` is not available at
    // evaluation sites and we only have a const reference to `query_options`.
    mutable computed_function_values _cached_pk_fn_calls;
private:
    // Batch constructor.
    template <typename Values>
    requires std::same_as<Values, raw_value_vector_with_unset> || std::same_as<Values, raw_value_view_vector_with_unset>
    explicit query_options(query_options&& o, std::vector<Values> values_ranges);

public:
    query_options(query_options&&) = default;
    explicit query_options(const query_options&) = default;

    explicit query_options(const cql_config& cfg,
                           db::consistency_level consistency,
                           std::optional<std::vector<sstring_view>> names,
                           raw_value_vector_with_unset values,
                           bool skip_metadata,
                           specific_options options
                           );
    explicit query_options(const cql_config& cfg,
                           db::consistency_level consistency,
                           std::optional<std::vector<sstring_view>> names,
                           std::vector<cql3::raw_value> values,
                           std::vector<cql3::raw_value_view> value_views,
                           unset_bind_variable_vector unset,
                           bool skip_metadata,
                           specific_options options
                           );
    explicit query_options(const cql_config& cfg,
                           db::consistency_level consistency,
                           std::optional<std::vector<sstring_view>> names,
                           raw_value_view_vector_with_unset value_views,
                           bool skip_metadata,
                           specific_options options
                           );

    template <typename Values>
    requires std::same_as<Values, raw_value_vector_with_unset> || std::same_as<Values, raw_value_view_vector_with_unset>
    static query_options make_batch_options(query_options&& o, std::vector<Values> values_ranges) {
        return query_options(std::move(o), std::move(values_ranges));
    }

    // It can't be const because of prepare()
    static thread_local query_options DEFAULT;

    // forInternalUse
    explicit query_options(raw_value_vector_with_unset values);
    explicit query_options(db::consistency_level, raw_value_vector_with_unset values, specific_options options = specific_options::DEFAULT);
    explicit query_options(std::unique_ptr<query_options>, lw_shared_ptr<service::pager::paging_state> paging_state);
    explicit query_options(std::unique_ptr<query_options>, lw_shared_ptr<service::pager::paging_state> paging_state, int32_t page_size);

    db::consistency_level get_consistency() const {
        return _consistency;
    }

    cql3::raw_value_view get_value_at(size_t idx) const {
        if (_unset.at(idx)) {
            throw exceptions::invalid_request_exception(fmt::format("Unexpected unset value for bind variable {}", idx));
        }
        return _value_views[idx];
    }

    bool is_unset(size_t idx) const {
        return _unset.at(idx);
    }

    size_t get_values_count() const {
        return _value_views.size();
    }

    bool skip_metadata() const {
        return _skip_metadata;
    }

    int32_t get_page_size() const {
        return get_specific_options().page_size;
    }

    /** The paging state for this query, or null if not relevant. */
    lw_shared_ptr<service::pager::paging_state> get_paging_state() const {
        return get_specific_options().state;
    }

    /** Serial consistency for conditional updates. */
    std::optional<db::consistency_level> get_serial_consistency() const {
        return get_specific_options().serial_consistency;
    }

    /**  Return serial consistency for conditional updates. Throws if the consistency is not set. */
    db::consistency_level check_serial_consistency() const;

    api::timestamp_type get_timestamp(service::query_state& state) const {
        auto tstamp = get_specific_options().timestamp;
        return tstamp != api::missing_timestamp ? tstamp : state.get_timestamp();
    }

    const query_options::specific_options& get_specific_options() const {
        return _options;
    }

    // Mainly for the sake of BatchQueryOptions
    const query_options& for_statement(size_t i) const {
        if (!_batch_options) {
            // No per-statement options supplied, so use the "global" options
            return *this;
        }
        return _batch_options->at(i);
    }


    const std::optional<std::vector<sstring_view>>& get_names() const noexcept {
        return _names;
    }

    const std::vector<cql3::raw_value_view>& get_values() const noexcept {
        return _value_views;
    }

    const cql_config& get_cql_config() const {
        return _cql_config;
    }

    // Generate a next unique list sequence for list append, e.g.
    // a = a + [val1, val2, ...]
    int next_list_append_seq() const {
        return _list_append_seq++;
    }

    // To preserve prepend monotonicity within a batch, each next
    // value must get a timestamp that's smaller than the previous one:
    // BEGIN BATCH
    //      UPDATE t SET l = [1, 2] + l WHERE pk = 0;
    //      UPDATE t SET l = [3] + l WHERE pk = 0;
    //      UPDATE t SET l = [4] + l WHERE pk = 0;
    // APPLY BATCH
    // SELECT l FROM t WHERE pk = 0;
    //  l
    // ------------
    // [4, 3, 1, 2]
    //
    // This function reserves the given number of prepend entries
    // and returns an id for the first prepended entry (it
    // got to be the smallest one, to preserve the order of
    // a multi-value append).
    //
    // @retval sequence number of the first entry of a multi-value
    // append. To get the next value, add 1.
    int next_list_prepend_seq(int num_entries, int max_entries) const {
        if (_list_append_seq + num_entries < max_entries) {
            _list_append_seq += num_entries;
            return max_entries - _list_append_seq;
        }
        return max_entries;
    }

    void prepare(const std::vector<lw_shared_ptr<column_specification>>& specs);

    void cache_pk_function_call(computed_function_values::key_type id, computed_function_values::mapped_type value) const;
    const computed_function_values& cached_pk_function_calls() const;
    computed_function_values&& take_cached_pk_function_calls();
    void set_cached_pk_function_calls(computed_function_values vals);
    computed_function_values::mapped_type* find_cached_pk_function_call(computed_function_values::key_type id) const;

private:
    void fill_value_views();
};

template <typename Values>
requires std::same_as<Values, raw_value_vector_with_unset> || std::same_as<Values, raw_value_view_vector_with_unset>
query_options::query_options(query_options&& o, std::vector<Values> values_ranges)
    : query_options(std::move(o))
{
    std::vector<query_options> tmp;
    tmp.reserve(values_ranges.size());
    std::transform(values_ranges.begin(), values_ranges.end(), std::back_inserter(tmp), [this](auto& values_range) {
        return query_options(_cql_config, _consistency, {}, std::move(values_range), _skip_metadata, _options);
    });
    _batch_options = std::move(tmp);
}

}

#include <cstdint>

namespace query {

enum class digest_algorithm : uint8_t {
    none = 0,  // digest not required
    MD5 = 1,
    legacy_xxHash_without_null_digest = 2,
    xxHash = 3, // default algorithm
};

}

struct full_position;

struct full_position_view {
    const partition_key_view partition;
    const position_in_partition_view position;

    full_position_view(const full_position&);
    full_position_view(const partition_key&, const position_in_partition_view);
};

struct full_position {
    partition_key partition;
    position_in_partition position;

    full_position(full_position_view);
    full_position(partition_key, position_in_partition);

    operator full_position_view() {
        return full_position_view(partition, position);
    }

    static std::strong_ordering cmp(const schema& s, const full_position& a, const full_position& b) {
        partition_key::tri_compare pk_cmp(s);
        if (auto res = pk_cmp(a.partition, b.partition); res != 0) {
            return res;
        }
        position_in_partition::tri_compare pos_cmp(s);
        return pos_cmp(a.position, b.position);
    }
};

inline full_position_view::full_position_view(const full_position& fp) : partition(fp.partition), position(fp.position) { }
inline full_position_view::full_position_view(const partition_key& pk, const position_in_partition_view pos) : partition(pk), position(pos) { }

inline full_position::full_position(full_position_view fpv) : partition(fpv.partition), position(fpv.position) { }
inline full_position::full_position(partition_key pk, position_in_partition pos) : partition(std::move(pk)), position(pos) { }


#include <optional>
#include <seastar/util/bool_class.hh>

namespace query {

struct short_read_tag { };
using short_read = bool_class<short_read_tag>;

// result_memory_limiter, result_memory_accounter and result_memory_tracker
// form an infrastructure for limiting size of query results.
//
// result_memory_limiter is a shard-local object which ensures that all results
// combined do not use more than 10% of the shard memory.
//
// result_memory_accounter is used by result producers, updates the shard-local
// limits as well as keeps track of the individual maximum result size limit
// which is 1 MB.
//
// result_memory_tracker is just an object that makes sure the
// result_memory_limiter is notified when memory is released (but not sooner).

class result_memory_accounter;

class result_memory_limiter {
    const size_t _maximum_total_result_memory;
    semaphore _memory_limiter;
public:
    static constexpr size_t minimum_result_size = 4 * 1024;
    static constexpr size_t maximum_result_size = 1 * 1024 * 1024;
    static constexpr size_t unlimited_result_size = std::numeric_limits<size_t>::max();
public:
    explicit result_memory_limiter(size_t maximum_total_result_memory)
        : _maximum_total_result_memory(maximum_total_result_memory)
        , _memory_limiter(_maximum_total_result_memory)
    { }

    result_memory_limiter(const result_memory_limiter&) = delete;
    result_memory_limiter(result_memory_limiter&&) = delete;

    ssize_t total_used_memory() const {
        return _maximum_total_result_memory - _memory_limiter.available_units();
    }

    // Reserves minimum_result_size and creates new memory accounter for
    // mutation query. Uses the specified maximum result size and may be
    // stopped before reaching it due to memory pressure on shard.
    future<result_memory_accounter> new_mutation_read(query::max_result_size max_result_size, short_read short_read_allowed);

    // Reserves minimum_result_size and creates new memory accounter for
    // data query. Uses the specified maximum result size, result will *not*
    // be stopped due to on shard memory pressure in order to avoid digest
    // mismatches.
    future<result_memory_accounter> new_data_read(query::max_result_size max_result_size, short_read short_read_allowed);

    // Creates a memory accounter for digest reads. Such accounter doesn't
    // contribute to the shard memory usage, but still stops producing the
    // result after individual limit has been reached.
    future<result_memory_accounter> new_digest_read(query::max_result_size max_result_size, short_read short_read_allowed);

    // Checks whether the result can grow any more, takes into account only
    // the per shard limit.
    stop_iteration check() const {
        return stop_iteration(_memory_limiter.current() <= 0);
    }

    // Consumes n bytes from memory limiter and checks whether the result
    // can grow any more (considering just the per-shard limit).
    stop_iteration update_and_check(size_t n) {
        _memory_limiter.consume(n);
        return check();
    }

    void release(size_t n) noexcept {
        _memory_limiter.signal(n);
    }

    semaphore& sem() noexcept { return _memory_limiter; }
};


class result_memory_tracker {
    semaphore_units<> _units;
    size_t _used_memory;
private:
    static thread_local semaphore _dummy;
public:
    result_memory_tracker() noexcept : _units(_dummy, 0), _used_memory(0) { }
    result_memory_tracker(semaphore& sem, size_t blocked, size_t used) noexcept
        : _units(sem, blocked), _used_memory(used) { }
    size_t used_memory() const { return _used_memory; }
};

class result_memory_accounter {
    result_memory_limiter* _limiter = nullptr;
    size_t _blocked_bytes = 0;
    size_t _used_memory = 0;
    size_t _total_used_memory = 0;
    query::max_result_size _maximum_result_size;
    stop_iteration _stop_on_global_limit;
    short_read _short_read_allowed;
    mutable bool _below_soft_limit = true;
private:
    // Mutation query accounter. Uses provided individual result size limit and
    // will stop when shard memory pressure grows too high.
    struct mutation_query_tag { };
    explicit result_memory_accounter(mutation_query_tag, result_memory_limiter& limiter, query::max_result_size max_size, short_read short_read_allowed) noexcept
        : _limiter(&limiter)
        , _blocked_bytes(result_memory_limiter::minimum_result_size)
        , _maximum_result_size(max_size)
        , _stop_on_global_limit(true)
        , _short_read_allowed(short_read_allowed)
    { }

    // Data query accounter. Uses provided individual result size limit and
    // will *not* stop even though shard memory pressure grows too high.
    struct data_query_tag { };
    explicit result_memory_accounter(data_query_tag, result_memory_limiter& limiter, query::max_result_size max_size, short_read short_read_allowed) noexcept
        : _limiter(&limiter)
        , _blocked_bytes(result_memory_limiter::minimum_result_size)
        , _maximum_result_size(max_size)
        , _short_read_allowed(short_read_allowed)
    { }

    // Digest query accounter. Uses provided individual result size limit and
    // will *not* stop even though shard memory pressure grows too high. This
    // accounter does not contribute to the shard memory limits.
    struct digest_query_tag { };
    explicit result_memory_accounter(digest_query_tag, result_memory_limiter&, query::max_result_size max_size, short_read short_read_allowed) noexcept
        : _blocked_bytes(0)
        , _maximum_result_size(max_size)
        , _short_read_allowed(short_read_allowed)
    { }

    stop_iteration check_local_limit() const;

    friend class result_memory_limiter;
public:
    explicit result_memory_accounter(size_t max_size) noexcept
        : _blocked_bytes(0)
        , _maximum_result_size(max_size) {
    }

    result_memory_accounter(result_memory_accounter&& other) noexcept
        : _limiter(std::exchange(other._limiter, nullptr))
        , _blocked_bytes(other._blocked_bytes)
        , _used_memory(other._used_memory)
        , _total_used_memory(other._total_used_memory)
        , _maximum_result_size(other._maximum_result_size)
        , _stop_on_global_limit(other._stop_on_global_limit)
        , _short_read_allowed(other._short_read_allowed)
        , _below_soft_limit(other._below_soft_limit)
    { }

    result_memory_accounter& operator=(result_memory_accounter&& other) noexcept {
        if (this != &other) {
            this->~result_memory_accounter();
            new (this) result_memory_accounter(std::move(other));
        }
        return *this;
    }

    ~result_memory_accounter() {
        if (_limiter) {
            _limiter->release(_blocked_bytes);
        }
    }

    size_t used_memory() const { return _used_memory; }

    // Consume n more bytes for the result. Returns stop_iteration::yes if
    // the result cannot grow any more (taking into account both individual
    // and per-shard limits).
    stop_iteration update_and_check(size_t n) {
        _used_memory += n;
        _total_used_memory += n;
        auto stop = check_local_limit();
        if (_limiter && _used_memory > _blocked_bytes) {
            auto to_block = std::min(_used_memory - _blocked_bytes, n);
            _blocked_bytes += to_block;
            stop = (_limiter->update_and_check(to_block) && _stop_on_global_limit) || stop;
            if (stop && !_short_read_allowed) {
                // If we are here we stopped because of the global limit.
                throw std::runtime_error("Maximum amount of memory for building query results is exhausted, unpaged query cannot be finished");
            }
        }
        return stop;
    }

    // Checks whether the result can grow any more.
    stop_iteration check() const {
        auto stop = check_local_limit();
        if (!stop && _used_memory >= _blocked_bytes && _limiter) {
            return _limiter->check() && _stop_on_global_limit;
        }
        return stop;
    }

    // Consume n more bytes for the result.
    void update(size_t n) {
        update_and_check(n);
    }

    result_memory_tracker done() && {
        if (!_limiter) {
            return { };
        }
        auto& sem = std::exchange(_limiter, nullptr)->sem();
        return result_memory_tracker(sem, _blocked_bytes, _used_memory);
    }
};

inline future<result_memory_accounter> result_memory_limiter::new_mutation_read(query::max_result_size max_size, short_read short_read_allowed) {
    return _memory_limiter.wait(minimum_result_size).then([this, max_size, short_read_allowed] {
        return result_memory_accounter(result_memory_accounter::mutation_query_tag(), *this, max_size, short_read_allowed);
    });
}

inline future<result_memory_accounter> result_memory_limiter::new_data_read(query::max_result_size max_size, short_read short_read_allowed) {
    return _memory_limiter.wait(minimum_result_size).then([this, max_size, short_read_allowed] {
        return result_memory_accounter(result_memory_accounter::data_query_tag(), *this, max_size, short_read_allowed);
    });
}

inline future<result_memory_accounter> result_memory_limiter::new_digest_read(query::max_result_size max_size, short_read short_read_allowed) {
    return make_ready_future<result_memory_accounter>(result_memory_accounter(result_memory_accounter::digest_query_tag(), *this, max_size, short_read_allowed));
}

enum class result_request {
    only_result,
    only_digest,
    result_and_digest,
};

struct result_options {
    result_request request = result_request::only_result;
    digest_algorithm digest_algo = query::digest_algorithm::none;

    static result_options only_result() {
        return result_options{};
    }

    static result_options only_digest(digest_algorithm da) {
        return {result_request::only_digest, da};
    }
};

class result_digest {
public:
    using type = std::array<uint8_t, 16>;
private:
    type _digest;
public:
    result_digest() = default;
    result_digest(type&& digest) : _digest(std::move(digest)) {}
    const type& get() const { return _digest; }
    bool operator==(const result_digest& rh) const = default;
};

//
// The query results are stored in a serialized form. This is in order to
// address the following problems, which a structured format has:
//
//   - high level of indirection (vector of vectors of vectors of blobs), which
//     is not CPU cache friendly
//
//   - high allocation rate due to fine-grained object structure
//
// On replica side, the query results are probably going to be serialized in
// the transport layer anyway, so serializing the results up-front doesn't add
// net work. There is no processing of the query results on replica other than
// concatenation in case of range queries and checksum calculation. If query
// results are collected in serialized form from different cores, we can
// concatenate them without copying by simply appending the fragments into the
// packet.
//
// On coordinator side, the query results would have to be parsed from the
// transport layer buffers anyway, so the fact that iterators parse it also
// doesn't add net work, but again saves allocations and copying. The CQL
// server doesn't need complex data structures to process the results, it just
// goes over it linearly consuming it.
//
// The coordinator side could be optimized even further for CQL queries which
// do not need processing (eg. select * from cf where ...). We could make the
// replica send the query results in the format which is expected by the CQL
// binary protocol client. So in the typical case the coordinator would just
// pass the data using zero-copy to the client, prepending a header.
//
// Users which need more complex structure of query results can convert this
// to query::result_set.
//
// Related headers:
//  - query-result-reader.hh
//  - query-result-writer.hh

class result {
    bytes_ostream _w;
    std::optional<result_digest> _digest;
    std::optional<uint32_t> _row_count_low_bits;
    api::timestamp_type _last_modified = api::missing_timestamp;
    short_read _short_read;
    query::result_memory_tracker _memory_tracker;
    std::optional<uint32_t> _partition_count;
    std::optional<uint32_t> _row_count_high_bits;
    std::optional<full_position> _last_position;
public:
    class builder;
    class partition_writer;
    friend class result_merger;

    result();
    result(bytes_ostream&& w, short_read sr, std::optional<uint32_t> c_low_bits, std::optional<uint32_t> pc,
           std::optional<uint32_t> c_high_bits, std::optional<full_position> last_position, result_memory_tracker memory_tracker = { })
        : _w(std::move(w))
        , _row_count_low_bits(c_low_bits)
        , _short_read(sr)
        , _memory_tracker(std::move(memory_tracker))
        , _partition_count(pc)
        , _row_count_high_bits(c_high_bits)
        , _last_position(std::move(last_position))
    {
        w.reduce_chunk_count();
    }
    result(bytes_ostream&& w, std::optional<result_digest> d, api::timestamp_type last_modified,
           short_read sr, std::optional<uint32_t> c_low_bits, std::optional<uint32_t> pc, std::optional<uint32_t> c_high_bits,
           std::optional<full_position> last_position, result_memory_tracker memory_tracker = { })
        : _w(std::move(w))
        , _digest(d)
        , _row_count_low_bits(c_low_bits)
        , _last_modified(last_modified)
        , _short_read(sr)
        , _memory_tracker(std::move(memory_tracker))
        , _partition_count(pc)
        , _row_count_high_bits(c_high_bits)
        , _last_position(std::move(last_position))
    {
        w.reduce_chunk_count();
    }
    result(bytes_ostream&& w, short_read sr, uint64_t c, std::optional<uint32_t> pc,
           std::optional<full_position> last_position, result_memory_tracker memory_tracker = { })
        : _w(std::move(w))
        , _row_count_low_bits(static_cast<uint32_t>(c))
        , _short_read(sr)
        , _memory_tracker(std::move(memory_tracker))
        , _partition_count(pc)
        , _row_count_high_bits(static_cast<uint32_t>(c >> 32))
        , _last_position(std::move(last_position))
    {
        w.reduce_chunk_count();
    }
    result(bytes_ostream&& w, std::optional<result_digest> d, api::timestamp_type last_modified,
           short_read sr, uint64_t c, std::optional<uint32_t> pc, std::optional<full_position> last_position, result_memory_tracker memory_tracker = { })
        : _w(std::move(w))
        , _digest(d)
        , _row_count_low_bits(static_cast<uint32_t>(c))
        , _last_modified(last_modified)
        , _short_read(sr)
        , _memory_tracker(std::move(memory_tracker))
        , _partition_count(pc)
        , _row_count_high_bits(static_cast<uint32_t>(c >> 32))
        , _last_position(std::move(last_position))
    {
        w.reduce_chunk_count();
    }
    result(result&&) = default;
    result& operator=(result&&) = default;

    const bytes_ostream& buf() const {
        return _w;
    }

    const std::optional<result_digest>& digest() const {
        return _digest;
    }

    const std::optional<uint32_t> row_count_low_bits() const {
        return _row_count_low_bits;
    }

    const std::optional<uint32_t> row_count_high_bits() const {
        return _row_count_high_bits;
    }

    const std::optional<uint64_t> row_count() const {
        if (!_row_count_low_bits) {
            return _row_count_low_bits;
        }
        return (static_cast<uint64_t>(_row_count_high_bits.value_or(0)) << 32) | _row_count_low_bits.value();
    }

    void set_row_count(std::optional<uint64_t> row_count) {
        if (!row_count) {
            _row_count_low_bits = std::nullopt;
            _row_count_high_bits = std::nullopt;
        } else {
            _row_count_low_bits = std::make_optional(static_cast<uint32_t>(row_count.value()));
            _row_count_high_bits = std::make_optional(static_cast<uint32_t>(row_count.value() >> 32));
        }
    }

    const api::timestamp_type last_modified() const {
        return _last_modified;
    }

    short_read is_short_read() const {
        return _short_read;
    }

    const std::optional<uint32_t>& partition_count() const {
        return _partition_count;
    }

    void ensure_counts();

    const std::optional<full_position>& last_position() const {
        return _last_position;
    }

    void set_last_position(std::optional<full_position> last_position) {
        _last_position = std::move(last_position);
    }

    // Return _last_position if replica filled it, otherwise calculate it based
    // on the content (by looking up the last row in the last partition).
    full_position get_or_calculate_last_position() const;

    struct printer {
        schema_ptr s;
        const query::partition_slice& slice;
        const query::result& res;
    };

    sstring pretty_print(schema_ptr, const query::partition_slice&) const;
    printer pretty_printer(schema_ptr, const query::partition_slice&) const;
};

std::ostream& operator<<(std::ostream& os, const query::result::printer&);
}

 /*
  * The generate code should be included in a header file after
  * The object definition
  */
    

namespace ser {

template <>
struct serializer<clustering_key_prefix> {
  template <typename Output>
  static void write(Output& buf, const clustering_key_prefix& v);

  template <typename Input>
  static clustering_key_prefix read(Input& buf);

  template <typename Input>
  static void skip(Input& buf);
};


template <>
struct serializer<const clustering_key_prefix> : public serializer<clustering_key_prefix>
{};


template <>
struct serializer<partition_key> {
  template <typename Output>
  static void write(Output& buf, const partition_key& v);

  template <typename Input>
  static partition_key read(Input& buf);

  template <typename Input>
  static void skip(Input& buf);
};


template <>
struct serializer<const partition_key> : public serializer<partition_key>
{};

} // ser


 

 /*
  * The generate code should be included in a header file after
  * The object definition
  */
    

namespace ser {

template <>
struct serializer<query::digest_algorithm> {
  template <typename Output>
  static void write(Output& buf, const query::digest_algorithm& v);

  template <typename Input>
  static query::digest_algorithm read(Input& buf);

  template <typename Input>
  static void skip(Input& buf);
};


template <>
struct serializer<const query::digest_algorithm> : public serializer<query::digest_algorithm>
{};

} // ser


namespace ser {

// frame represents a place holder for object size which will be known later

template<typename Output>
struct place_holder { };

template<typename Output>
struct frame { };

template<>
struct place_holder<bytes_ostream> {
    bytes_ostream::place_holder<size_type> ph;

    place_holder(bytes_ostream::place_holder<size_type> ph) : ph(ph) { }

    void set(bytes_ostream& out, size_type v) {
        auto stream = ph.get_stream();
        serialize(stream, v);
    }
};

template<>
struct frame<bytes_ostream> : public place_holder<bytes_ostream> {
    bytes_ostream::size_type offset;

    frame(bytes_ostream::place_holder<size_type> ph, bytes_ostream::size_type offset)
        : place_holder(ph), offset(offset) { }

    void end(bytes_ostream& out) {
        set(out, out.size() - offset);
    }
};

struct vector_position {
    bytes_ostream::position pos;
    size_type count;
};

//empty frame, behave like a place holder, but is used when no place holder is needed
template<typename Output>
struct empty_frame {
    void end(Output&) {}
    empty_frame() = default;
    empty_frame(const frame<Output>&){}
};

inline place_holder<bytes_ostream> start_place_holder(bytes_ostream& out) {
    auto size_ph = out.write_place_holder<size_type>();
    return { size_ph};
}

inline frame<bytes_ostream> start_frame(bytes_ostream& out) {
    auto offset = out.size();
    auto size_ph = out.write_place_holder<size_type>();
    {
        auto out = size_ph.get_stream();
        serialize(out, (size_type)0);
    }
    return frame<bytes_ostream> { size_ph, offset };
}

template<typename Input>
size_type read_frame_size(Input& in) {
    auto sz = deserialize(in, boost::type<size_type>());
    if (sz < sizeof(size_type)) {
        throw std::runtime_error(fmt::format("IDL frame truncated: expected to have at least {} bytes, got {}", sizeof(size_type), sz));
    }
    return sz - sizeof(size_type);
}


template<>
struct place_holder<seastar::measuring_output_stream> {
    void set(seastar::measuring_output_stream&, size_type) { }
};

template<>
struct frame<seastar::measuring_output_stream> : public place_holder<seastar::measuring_output_stream> {
    void end(seastar::measuring_output_stream& out) { }
};

inline place_holder<seastar::measuring_output_stream> start_place_holder(seastar::measuring_output_stream& out) {
    serialize(out, size_type());
    return { };
}

inline frame<seastar::measuring_output_stream> start_frame(seastar::measuring_output_stream& out) {
    serialize(out, size_type());
    return { };
}

template<>
class place_holder<seastar::simple_output_stream> {
    seastar::simple_output_stream _substream;
public:
    place_holder(seastar::simple_output_stream substream)
        : _substream(substream) { }

    void set(seastar::simple_output_stream& out, size_type v) {
        serialize(_substream, v);
    }
};

template<>
class frame<seastar::simple_output_stream> : public place_holder<seastar::simple_output_stream> {
    char* _start;
public:
    frame(seastar::simple_output_stream ph, char* start)
        : place_holder(ph), _start(start) { }

    void end(seastar::simple_output_stream& out) {
        set(out, out.begin() - _start);
    }
};

inline place_holder<seastar::simple_output_stream> start_place_holder(seastar::simple_output_stream& out) {
    return { out.write_substream(sizeof(size_type)) };
}

inline frame<seastar::simple_output_stream> start_frame(seastar::simple_output_stream& out) {
    auto start = out.begin();
    auto substream = out.write_substream(sizeof(size_type));
    {
        auto sstr = substream;
        serialize(sstr, size_type(0));
    }
    return frame<seastar::simple_output_stream>(substream, start);
}

template<typename Iterator>
class place_holder<seastar::memory_output_stream<Iterator>> {
    seastar::memory_output_stream<Iterator> _substream;
public:
    place_holder(seastar::memory_output_stream<Iterator> substream)
        : _substream(substream) { }

    void set(seastar::memory_output_stream<Iterator>& out, size_type v) {
        serialize(_substream, v);
    }
};

template<typename Iterator>
class frame<seastar::memory_output_stream<Iterator>> : public place_holder<seastar::memory_output_stream<Iterator>> {
    size_t _start_left;
public:
    frame(seastar::memory_output_stream<Iterator> ph, size_t start_left)
        : place_holder<seastar::memory_output_stream<Iterator>>(ph), _start_left(start_left) { }

    void end(seastar::memory_output_stream<Iterator>& out) {
        this->set(out, _start_left - out.size());
    }
};

template<typename Iterator>
inline place_holder<seastar::memory_output_stream<Iterator>> start_place_holder(seastar::memory_output_stream<Iterator>& out) {
    return { out.write_substream(sizeof(size_type)) };
}

template<typename Iterator>
inline frame<seastar::memory_output_stream<Iterator>> start_frame(seastar::memory_output_stream<Iterator>& out) {
    auto start_left = out.size();
    auto substream = out.write_substream(sizeof(size_type));
    {
        auto sstr = substream;
        serialize(sstr, size_type(0));
    }
    return frame<seastar::memory_output_stream<Iterator>>(substream, start_left);
}

}

namespace ser {


template <typename Output>
void serializer<clustering_key_prefix>::write(Output& buf, const clustering_key_prefix& obj) {
  set_size(buf, obj);
  static_assert(is_equivalent<decltype(obj.explode()), std::vector<bytes>>::value, "member value has a wrong type");
  serialize(buf, obj.explode());
}


template <typename Input>
clustering_key_prefix serializer<clustering_key_prefix>::read(Input& buf) {
 return seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  auto in = buf.read_substream(size - sizeof(size_type));
  auto __local_0 = deserialize(in, boost::type<std::vector<bytes>>());

  clustering_key_prefix res {std::move(__local_0)};
  return res;
 });
}


template <typename Input>
void serializer<clustering_key_prefix>::skip(Input& buf) {
 seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  buf.skip(size - sizeof(size_type));
 });
}


template <typename Output>
void serializer<partition_key>::write(Output& buf, const partition_key& obj) {
  set_size(buf, obj);
  static_assert(is_equivalent<decltype(obj.explode()), std::vector<bytes>>::value, "member value has a wrong type");
  serialize(buf, obj.explode());
}


template <typename Input>
partition_key serializer<partition_key>::read(Input& buf) {
 return seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  auto in = buf.read_substream(size - sizeof(size_type));
  auto __local_0 = deserialize(in, boost::type<std::vector<bytes>>());

  partition_key res {std::move(__local_0)};
  return res;
 });
}


template <typename Input>
void serializer<partition_key>::skip(Input& buf) {
 seastar::with_serialized_stream(buf, [] (auto& buf) {
  size_type size = deserialize(buf, boost::type<size_type>());
  buf.skip(size - sizeof(size_type));
 });
}
} // ser


namespace ser {


template <typename Output>
void serializer<query::digest_algorithm>::write(Output& buf, const query::digest_algorithm& v) {
  serialize(buf, static_cast<uint8_t>(v));
}


template<typename Input>
query::digest_algorithm serializer<query::digest_algorithm>::read(Input& buf) {
  return static_cast<query::digest_algorithm>(deserialize(buf, boost::type<uint8_t>()));
}
struct qr_cell_view {
    utils::input_stream v;
    

    auto timestamp() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<std::optional<api::timestamp_type>>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       return deserialize(in, boost::type<std::optional<api::timestamp_type>>());
      });
    }


    auto expiry() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<std::optional<gc_clock::time_point>>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       ser::skip(in, boost::type<std::optional<api::timestamp_type>>());
       return deserialize(in, boost::type<std::optional<gc_clock::time_point>>());
      });
    }


    auto value() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<bytes>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       ser::skip(in, boost::type<std::optional<api::timestamp_type>>());
       ser::skip(in, boost::type<std::optional<gc_clock::time_point>>());
       return deserialize(in, boost::type<bytes>());
      });
    }


    auto ttl() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<std::optional<gc_clock::duration>>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       ser::skip(in, boost::type<std::optional<api::timestamp_type>>());
       ser::skip(in, boost::type<std::optional<gc_clock::time_point>>());
       ser::skip(in, boost::type<bytes>());
       return (in.size()>0) ? deserialize(in, boost::type<std::optional<gc_clock::duration>>()) : std::optional<gc_clock::duration>();
      });
    }

};

template<>
struct serializer<qr_cell_view> {
    template<typename Input>
    static qr_cell_view read(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        auto v_start = v;
        auto start_size = v.size();
        skip(v);
        return qr_cell_view{v_start.read_substream(start_size - v.size())};
      });
    }
    template<typename Output>
    static void write(Output& out, qr_cell_view v) {
        v.v.copy_to(out);
    }
    template<typename Input>
    static void skip(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        v.skip(read_frame_size(v));
      });
    }
};

struct qr_row_view {
    utils::input_stream v;
    

    auto cells() const {
      return seastar::with_serialized_stream(v, [] (auto& v) {
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       return vector_deserializer<std::optional<qr_cell_view>>(in);
      });
    }

};

template<>
struct serializer<qr_row_view> {
    template<typename Input>
    static qr_row_view read(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        auto v_start = v;
        auto start_size = v.size();
        skip(v);
        return qr_row_view{v_start.read_substream(start_size - v.size())};
      });
    }
    template<typename Output>
    static void write(Output& out, qr_row_view v) {
        v.v.copy_to(out);
    }
    template<typename Input>
    static void skip(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        v.skip(read_frame_size(v));
      });
    }
};

struct qr_clustered_row_view {
    utils::input_stream v;
    

    auto key() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<std::optional<clustering_key>>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       return deserialize(in, boost::type<std::optional<clustering_key>>());
      });
    }


    auto cells() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<qr_row_view>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       ser::skip(in, boost::type<std::optional<clustering_key>>());
       return deserialize(in, boost::type<qr_row_view>());
      });
    }

};

template<>
struct serializer<qr_clustered_row_view> {
    template<typename Input>
    static qr_clustered_row_view read(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        auto v_start = v;
        auto start_size = v.size();
        skip(v);
        return qr_clustered_row_view{v_start.read_substream(start_size - v.size())};
      });
    }
    template<typename Output>
    static void write(Output& out, qr_clustered_row_view v) {
        v.v.copy_to(out);
    }
    template<typename Input>
    static void skip(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        v.skip(read_frame_size(v));
      });
    }
};

struct qr_partition_view {
    utils::input_stream v;
    

    auto key() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<std::optional<partition_key>>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       return deserialize(in, boost::type<std::optional<partition_key>>());
      });
    }


    auto static_row() const {
      return seastar::with_serialized_stream(v, [this] (auto& v) -> decltype(deserialize(std::declval<utils::input_stream&>(), boost::type<qr_row_view>())) {
       std::ignore = this;
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       ser::skip(in, boost::type<std::optional<partition_key>>());
       return deserialize(in, boost::type<qr_row_view>());
      });
    }


    auto rows() const {
      return seastar::with_serialized_stream(v, [] (auto& v) {
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       ser::skip(in, boost::type<std::optional<partition_key>>());
       ser::skip(in, boost::type<qr_row_view>());
       return vector_deserializer<qr_clustered_row_view>(in);
      });
    }

};

template<>
struct serializer<qr_partition_view> {
    template<typename Input>
    static qr_partition_view read(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        auto v_start = v;
        auto start_size = v.size();
        skip(v);
        return qr_partition_view{v_start.read_substream(start_size - v.size())};
      });
    }
    template<typename Output>
    static void write(Output& out, qr_partition_view v) {
        v.v.copy_to(out);
    }
    template<typename Input>
    static void skip(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        v.skip(read_frame_size(v));
      });
    }
};

struct query_result_view {
    utils::input_stream v;
    

    auto partitions() const {
      return seastar::with_serialized_stream(v, [] (auto& v) {
       auto in = v;
       ser::skip(in, boost::type<size_type>());
       return vector_deserializer<qr_partition_view>(in);
      });
    }

};

template<>
struct serializer<query_result_view> {
    template<typename Input>
    static query_result_view read(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        auto v_start = v;
        auto start_size = v.size();
        skip(v);
        return query_result_view{v_start.read_substream(start_size - v.size())};
      });
    }
    template<typename Output>
    static void write(Output& out, query_result_view v) {
        v.v.copy_to(out);
    }
    template<typename Input>
    static void skip(Input& v) {
      return seastar::with_serialized_stream(v, [] (auto& v) {
        v.skip(read_frame_size(v));
      });
    }
};


////// State holders

template<typename Output>
struct state_of_qr_cell {
    frame<Output> f;
};

template<typename Output>
struct state_of_qr_row {
    frame<Output> f;
};

template<typename Output>
struct state_of_qr_clustered_row {
    frame<Output> f;
};

template<typename Output>
struct state_of_qr_clustered_row__cells {
    frame<Output> f;
    state_of_qr_clustered_row<Output> _parent;
};

template<typename Output>
struct state_of_qr_partition {
    frame<Output> f;
};

template<typename Output>
struct state_of_qr_partition__static_row {
    frame<Output> f;
    state_of_qr_partition<Output> _parent;
};

template<typename Output>
struct state_of_query_result {
    frame<Output> f;
};

////// Nodes

template<typename Output>
struct after_qr_cell__ttl {
    Output& _out;
    state_of_qr_cell<Output> _state;
    
    
    
    void  end_qr_cell() {
        _state.f.end(_out);
    }

};

template<typename Output>
struct after_qr_cell__value {
    Output& _out;
    state_of_qr_cell<Output> _state;
    
    
    
    after_qr_cell__ttl<Output> skip_ttl() && {
        serialize(_out, false);
        return { _out, std::move(_state) };
    }
    after_qr_cell__ttl<Output> write_ttl(const gc_clock::duration& t) && {
        
        serialize(_out, true);

        serialize(_out, t);
        
        return { _out, std::move(_state) };
    }
};

template<typename Output>
struct after_qr_cell__expiry {
    Output& _out;
    state_of_qr_cell<Output> _state;
    
    
    
    after_qr_cell__value<Output> write_value(bytes_view t) && {
        
        
        serialize(_out, t);
        
        return { _out, std::move(_state) };
    }
    template<typename FragmentedBuffer>
    requires FragmentRange<FragmentedBuffer>
    after_qr_cell__value<Output> write_fragmented_value(FragmentedBuffer&& fragments) && {
        
        serialize_fragmented(_out, std::forward<FragmentedBuffer>(fragments));
        
        return { _out, std::move(_state) };
    }
};

template<typename Output>
struct after_qr_cell__timestamp {
    Output& _out;
    state_of_qr_cell<Output> _state;
    
    
    
    after_qr_cell__expiry<Output> skip_expiry() && {
        serialize(_out, false);
        return { _out, std::move(_state) };
    }
    after_qr_cell__expiry<Output> write_expiry(const gc_clock::time_point& t) && {
        
        serialize(_out, true);

        serialize(_out, t);
        
        return { _out, std::move(_state) };
    }
};

template<typename Output>
struct writer_of_qr_cell {
    Output& _out;
    state_of_qr_cell<Output> _state;
    
    writer_of_qr_cell(Output& out)
            : _out(out)
            , _state{start_frame(out)}
            {}
    
    after_qr_cell__timestamp<Output> skip_timestamp() && {
        serialize(_out, false);
        return { _out, std::move(_state) };
    }
    after_qr_cell__timestamp<Output> write_timestamp(const api::timestamp_type& t) && {
        
        serialize(_out, true);

        serialize(_out, t);
        
        return { _out, std::move(_state) };
    }
};

template<typename Output>
struct after_qr_row__cells {
    Output& _out;
    state_of_qr_row<Output> _state;
    
    
    
    void  end_qr_row() {
        _state.f.end(_out);
    }

};

template<typename Output>
struct writer_of_std__optional__qr_cell {
    Output& _out;

    void skip()  {
        serialize(_out, false);
    }
    void write(const qr_cell_view& obj) {
        serialize(_out, true);
        serialize(_out, obj);
    }
    writer_of_qr_cell<Output> write() {
        serialize(_out, true);
        return {_out};
    }
};

template<typename Output>
struct qr_row__cells {
    Output& _out;
    state_of_qr_row<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
    qr_row__cells(Output& out, state_of_qr_row<Output> state)
            : _out(out)
            , _state(state)
            , _size(start_place_holder(out))
            {}
    
  writer_of_std__optional__qr_cell<Output> add() {
        _count++;
        return {_out};
  }
  void add(std::optional<qr_cell_view> v) {
        serialize(_out, v);
        _count++;
  }
  after_qr_row__cells<Output> end_cells() && {
        _size.set(_out, _count);
        return { _out, std::move(_state) };
  }

  vector_position pos() const {
        return vector_position{_out.pos(), _count};
  }

  void rollback(const vector_position& vp) {
        _out.retract(vp.pos);
        _count = vp.count;
  }
};

template<typename Output>
struct writer_of_qr_row {
    Output& _out;
    state_of_qr_row<Output> _state;
    
    writer_of_qr_row(Output& out)
            : _out(out)
            , _state{start_frame(out)}
            {}
    
    qr_row__cells<Output> start_cells() && {
        return { _out, std::move(_state) };
    }

    after_qr_row__cells<Output> skip_cells() && {
        serialize(_out, size_type(0));
        return { _out, std::move(_state) };
    }

};

template<typename Output>
struct after_qr_clustered_row__cells {
    Output& _out;
    state_of_qr_clustered_row<Output> _state;
    
    
    
    void  end_qr_clustered_row() {
        _state.f.end(_out);
    }

};

template<typename Output>
struct after_qr_clustered_row__cells__cells {
    Output& _out;
    state_of_qr_clustered_row__cells<Output> _state;
    
    
    
    after_qr_clustered_row__cells<Output>  end_cells() && {
        _state.f.end(_out);
        return { _out, std::move(_state._parent) };
    }

};

template<typename Output>
struct qr_clustered_row__cells__cells {
    Output& _out;
    state_of_qr_clustered_row__cells<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
    qr_clustered_row__cells__cells(Output& out, state_of_qr_clustered_row__cells<Output> state)
            : _out(out)
            , _state(state)
            , _size(start_place_holder(out))
            {}
    
  writer_of_std__optional__qr_cell<Output> add() {
        _count++;
        return {_out};
  }
  void add(std::optional<qr_cell_view> v) {
        serialize(_out, v);
        _count++;
  }
  after_qr_clustered_row__cells__cells<Output> end_cells() && {
        _size.set(_out, _count);
        return { _out, std::move(_state) };
  }

  vector_position pos() const {
        return vector_position{_out.pos(), _count};
  }

  void rollback(const vector_position& vp) {
        _out.retract(vp.pos);
        _count = vp.count;
  }
};

template<typename Output>
struct qr_clustered_row__cells {
    Output& _out;
    state_of_qr_clustered_row__cells<Output> _state;
    
    qr_clustered_row__cells(Output& out, state_of_qr_clustered_row<Output> state)
            : _out(out)
            , _state{start_frame(out), std::move(state)}
            {}
    
    qr_clustered_row__cells__cells<Output> start_cells() && {
        return { _out, std::move(_state) };
    }

    after_qr_clustered_row__cells__cells<Output> skip_cells() && {
        serialize(_out, size_type(0));
        return { _out, std::move(_state) };
    }

};

template<typename Output>
struct after_qr_clustered_row__key {
    Output& _out;
    state_of_qr_clustered_row<Output> _state;
    
    
    
    qr_clustered_row__cells<Output> start_cells() && {
        
        
        return { _out, std::move(_state) };
    }

    template<typename Serializer>
    after_qr_clustered_row__cells<Output> cells(Serializer&& f) && {
        
        f(writer_of_qr_row<Output>(_out));
        
        return { _out, std::move(_state) };
    }
};

template<typename Output>
struct writer_of_qr_clustered_row {
    Output& _out;
    state_of_qr_clustered_row<Output> _state;
    
    writer_of_qr_clustered_row(Output& out)
            : _out(out)
            , _state{start_frame(out)}
            {}
    
    after_qr_clustered_row__key<Output> skip_key() && {
        serialize(_out, false);
        return { _out, std::move(_state) };
    }
    after_qr_clustered_row__key<Output> write_key(const clustering_key& t) && {
        
        serialize(_out, true);

        serialize(_out, t);
        
        return { _out, std::move(_state) };
    }
};

template<typename Output>
struct after_qr_partition__rows {
    Output& _out;
    state_of_qr_partition<Output> _state;
    
    
    
    void  end_qr_partition() {
        _state.f.end(_out);
    }

};

template<typename Output>
struct qr_partition__rows {
    Output& _out;
    state_of_qr_partition<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
    qr_partition__rows(Output& out, state_of_qr_partition<Output> state)
            : _out(out)
            , _state(state)
            , _size(start_place_holder(out))
            {}
    
  writer_of_qr_clustered_row<Output> add() {
        _count++;
        return {_out};
  }
  void add(qr_clustered_row_view v) {
        serialize(_out, v);
        _count++;
  }
  after_qr_partition__rows<Output> end_rows() && {
        _size.set(_out, _count);
        return { _out, std::move(_state) };
  }

  vector_position pos() const {
        return vector_position{_out.pos(), _count};
  }

  void rollback(const vector_position& vp) {
        _out.retract(vp.pos);
        _count = vp.count;
  }
};

template<typename Output>
struct after_qr_partition__static_row {
    Output& _out;
    state_of_qr_partition<Output> _state;
    
    
    
    qr_partition__rows<Output> start_rows() && {
        return { _out, std::move(_state) };
    }

    after_qr_partition__rows<Output> skip_rows() && {
        serialize(_out, size_type(0));
        return { _out, std::move(_state) };
    }

};

template<typename Output>
struct after_qr_partition__static_row__cells {
    Output& _out;
    state_of_qr_partition__static_row<Output> _state;
    
    
    
    after_qr_partition__static_row<Output>  end_static_row() && {
        _state.f.end(_out);
        return { _out, std::move(_state._parent) };
    }

};

template<typename Output>
struct qr_partition__static_row__cells {
    Output& _out;
    state_of_qr_partition__static_row<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
    qr_partition__static_row__cells(Output& out, state_of_qr_partition__static_row<Output> state)
            : _out(out)
            , _state(state)
            , _size(start_place_holder(out))
            {}
    
  writer_of_std__optional__qr_cell<Output> add() {
        _count++;
        return {_out};
  }
  void add(std::optional<qr_cell_view> v) {
        serialize(_out, v);
        _count++;
  }
  after_qr_partition__static_row__cells<Output> end_cells() && {
        _size.set(_out, _count);
        return { _out, std::move(_state) };
  }

  vector_position pos() const {
        return vector_position{_out.pos(), _count};
  }

  void rollback(const vector_position& vp) {
        _out.retract(vp.pos);
        _count = vp.count;
  }
};

template<typename Output>
struct qr_partition__static_row {
    Output& _out;
    state_of_qr_partition__static_row<Output> _state;
    
    qr_partition__static_row(Output& out, state_of_qr_partition<Output> state)
            : _out(out)
            , _state{start_frame(out), std::move(state)}
            {}
    
    qr_partition__static_row__cells<Output> start_cells() && {
        return { _out, std::move(_state) };
    }

    after_qr_partition__static_row__cells<Output> skip_cells() && {
        serialize(_out, size_type(0));
        return { _out, std::move(_state) };
    }

};

template<typename Output>
struct after_qr_partition__key {
    Output& _out;
    state_of_qr_partition<Output> _state;
    
    
    
    qr_partition__static_row<Output> start_static_row() && {
        
        
        return { _out, std::move(_state) };
    }

    template<typename Serializer>
    after_qr_partition__static_row<Output> static_row(Serializer&& f) && {
        
        f(writer_of_qr_row<Output>(_out));
        
        return { _out, std::move(_state) };
    }
};

template<typename Output>
struct writer_of_qr_partition {
    Output& _out;
    state_of_qr_partition<Output> _state;
    
    writer_of_qr_partition(Output& out)
            : _out(out)
            , _state{start_frame(out)}
            {}
    
    after_qr_partition__key<Output> skip_key() && {
        serialize(_out, false);
        return { _out, std::move(_state) };
    }
    after_qr_partition__key<Output> write_key(const partition_key& t) && {
        
        serialize(_out, true);

        serialize(_out, t);
        
        return { _out, std::move(_state) };
    }
};

template<typename Output>
struct after_query_result__partitions {
    Output& _out;
    state_of_query_result<Output> _state;
    
    
    
    void  end_query_result() {
        _state.f.end(_out);
    }

};

template<typename Output>
struct query_result__partitions {
    Output& _out;
    state_of_query_result<Output> _state;
        place_holder<Output> _size;
    size_type _count = 0;
    query_result__partitions(Output& out, state_of_query_result<Output> state)
            : _out(out)
            , _state(state)
            , _size(start_place_holder(out))
            {}
    
  writer_of_qr_partition<Output> add() {
        _count++;
        return {_out};
  }
  void add(qr_partition_view v) {
        serialize(_out, v);
        _count++;
  }
  after_query_result__partitions<Output> end_partitions() && {
        _size.set(_out, _count);
        return { _out, std::move(_state) };
  }

  vector_position pos() const {
        return vector_position{_out.pos(), _count};
  }

  void rollback(const vector_position& vp) {
        _out.retract(vp.pos);
        _count = vp.count;
  }
};

template<typename Output>
struct writer_of_query_result {
    Output& _out;
    state_of_query_result<Output> _state;
    
    writer_of_query_result(Output& out)
            : _out(out)
            , _state{start_frame(out)}
            {}
    
    query_result__partitions<Output> start_partitions() && {
        return { _out, std::move(_state) };
    }

    after_query_result__partitions<Output> skip_partitions() && {
        serialize(_out, size_type(0));
        return { _out, std::move(_state) };
    }

};
} // ser


#include <boost/range/adaptor/transformed.hpp>
#include <boost/range/numeric.hpp>



namespace query {

using result_bytes_view = ser::buffer_view<bytes_ostream::fragment_iterator>;

class result_atomic_cell_view {
    ser::qr_cell_view _view;
public:
    result_atomic_cell_view(ser::qr_cell_view view)
        : _view(view) { }

    api::timestamp_type timestamp() const {
        return _view.timestamp().value_or(api::missing_timestamp);
    }

    expiry_opt expiry() const {
        return _view.expiry();
    }

    ttl_opt ttl() const {
        return _view.ttl();
    }

    result_bytes_view value() const {
        return _view.value().view();
    }
};

// Contains cells in the same order as requested by partition_slice.
// Contains only live cells.
class result_row_view {
    ser::qr_row_view _v;
public:
    result_row_view(ser::qr_row_view v) : _v(v) {}

    class iterator_type {
        using cells_deserializer = ser::vector_deserializer<std::optional<ser::qr_cell_view>>;
        cells_deserializer _cells;
        cells_deserializer::iterator _i;
    public:
        iterator_type(const ser::qr_row_view& v)
            : _cells(v.cells())
            , _i(_cells.begin())
        { }
        std::optional<result_atomic_cell_view> next_atomic_cell() {
            auto cell_opt = *_i++;
            if (!cell_opt) {
                return {};
            }
            return {result_atomic_cell_view(*cell_opt)};
        }
        std::optional<result_bytes_view> next_collection_cell() {
            auto cell_opt = *_i++;
            if (!cell_opt) {
                return {};
            }
            ser::qr_cell_view v = *cell_opt;
            return {v.value().view()};
        };
        void skip(const column_definition& def) {
            ++_i;
        }
    };

    iterator_type iterator() const {
        return iterator_type(_v);
    }
};

// Describes expectations about the ResultVisitor concept.
//
// Interaction flow:
//   -> accept_new_partition()
//   -> accept_new_row()
//   -> accept_new_row()
//   -> accept_partition_end()
//   -> accept_new_partition()
//   -> accept_new_row()
//   -> accept_new_row()
//   -> accept_new_row()
//   -> accept_partition_end()
//   ...
//
struct result_visitor {
    void accept_new_partition(
        const partition_key& key, // FIXME: use view for the key
        uint64_t row_count) {}

    void accept_new_partition(uint64_t row_count) {}

    void accept_new_row(
        const clustering_key& key, // FIXME: use view for the key
        const result_row_view& static_row,
        const result_row_view& row) {}

    void accept_new_row(const result_row_view& static_row, const result_row_view& row) {}

    void accept_partition_end(const result_row_view& static_row) {}
};

template<typename Visitor>
concept ResultVisitor = requires(Visitor visitor, const partition_key& pkey,
                                      uint64_t row_count, const clustering_key& ckey,
                                      const result_row_view& static_row, const result_row_view& row)
{
    visitor.accept_new_partition(pkey, row_count);
    visitor.accept_new_partition(row_count);
    visitor.accept_new_row(ckey, static_row, row);
    visitor.accept_new_row(static_row, row);
    visitor.accept_partition_end(static_row);
};

class result_view {
    ser::query_result_view _v;
    friend class result_merger;
public:
    result_view(const bytes_ostream& v) : _v(ser::query_result_view{ser::as_input_stream(v)}) {}
    result_view(ser::query_result_view v) : _v(v) {}
    explicit result_view(const query::result& res) : result_view(res.buf()) { }

    template <typename Func>
    static auto do_with(const query::result& res, Func&& func) {
        result_view view(res.buf());
        return func(view);
    }

    template <typename ResultVisitor>
    static void consume(const query::result& res, const partition_slice& slice, ResultVisitor&& visitor) {
        result_view(res).consume(slice, visitor);
    }

    template <typename Visitor>
    requires ResultVisitor<Visitor>
    void consume(const partition_slice& slice, Visitor&& visitor) const {
        for (auto&& p : _v.partitions()) {
            auto rows = p.rows();
            auto row_count = rows.size();
            if (slice.options.contains<partition_slice::option::send_partition_key>()) {
                auto key = *p.key();
                visitor.accept_new_partition(key, row_count);
            } else {
                visitor.accept_new_partition(row_count);
            }

            result_row_view static_row(p.static_row());

            for (auto&& row : rows) {
                result_row_view view(row.cells());
                if (slice.options.contains<partition_slice::option::send_clustering_key>()) {
                    visitor.accept_new_row(*row.key(), static_row, view);
                } else {
                    visitor.accept_new_row(static_row, view);
                }
            }

            visitor.accept_partition_end(static_row);
        }
    }

    std::tuple<uint32_t, uint64_t> count_partitions_and_rows() const {
        auto ps = _v.partitions();
        uint64_t rows = 0;
        for (auto p : ps) {
            rows += std::max(p.rows().size(), size_t(1));
        }
        return std::make_tuple(ps.size(), rows);
    }

    full_position calculate_last_position() const {
        auto ps = _v.partitions();
        assert(!ps.empty());
        auto pit = ps.begin();
        auto pnext = pit;
        while (++pnext != ps.end()) {
            pit = pnext;
        }
        auto p = *pit;
        auto rs = p.rows();
        auto pos = position_in_partition::for_partition_start();
        if (!rs.empty()) {
            auto rit = rs.begin();
            auto rnext = rit;
            while (++rnext != rs.end()) {
                rit = rnext;
            }
            const auto& key_opt = (*rit).key();
            if (key_opt) {
                pos = position_in_partition::for_key(*key_opt);
            }
        }
        return { p.key().value(), std::move(pos) };
    }
};

}


#include <memory>
#include <vector>

namespace cql3 {

class assignment_testable {
public:
    virtual ~assignment_testable() {}

    enum class test_result {
        EXACT_MATCH,
        WEAKLY_ASSIGNABLE,
        NOT_ASSIGNABLE,
    };

    static bool is_assignable(test_result tr) {
        return tr != test_result::NOT_ASSIGNABLE;
    }

    static bool is_exact_match(test_result tr) {
        return tr != test_result::EXACT_MATCH;
    }

    /**
     * @return whether this object can be assigned to the provided receiver. We distinguish
     * between 3 values: 
     *   - EXACT_MATCH if this object is exactly of the type expected by the receiver
     *   - WEAKLY_ASSIGNABLE if this object is not exactly the expected type but is assignable nonetheless
     *   - NOT_ASSIGNABLE if it's not assignable
     * Most caller should just call the isAssignable() method on the result, though functions have a use for
     * testing "strong" equality to decide the most precise overload to pick when multiple could match.
     */
    virtual test_result test_assignment(data_dictionary::database db, const sstring& keyspace, const column_specification& receiver) const = 0;

    // for error reporting
    virtual sstring assignment_testable_source_context() const = 0;
};

inline bool is_assignable(assignment_testable::test_result tr) {
    return assignment_testable::is_assignable(tr);
}

inline bool is_exact_match(assignment_testable::test_result tr) {
    return assignment_testable::is_exact_match(tr);
}

inline
std::ostream&
operator<<(std::ostream& os, const assignment_testable& at) {
    return os << at.assignment_testable_source_context();
}

}


#include <boost/range/iterator_range.hpp>
#include <boost/range/algorithm/find_if.hpp>
#include <boost/range/numeric.hpp>


class mutation;
class atomic_cell_or_collection;

using counter_id = utils::tagged_uuid<struct counter_id_tag>;

template<mutable_view is_mutable>
class basic_counter_shard_view {
    enum class offset : unsigned {
        id = 0u,
        value = unsigned(id) + sizeof(counter_id),
        logical_clock = unsigned(value) + sizeof(int64_t),
        total_size = unsigned(logical_clock) + sizeof(int64_t),
    };
private:
    managed_bytes_basic_view<is_mutable> _base;
private:
    template<typename T>
    T read(offset off) const {
        auto v = _base;
        v.remove_prefix(size_t(off));
        return read_simple_native<T>(v);
    }
public:
    static constexpr auto size = size_t(offset::total_size);
public:
    basic_counter_shard_view() = default;
    explicit basic_counter_shard_view(managed_bytes_basic_view<is_mutable> v) noexcept
        : _base(v) { }

    counter_id id() const { return read<counter_id>(offset::id); }
    int64_t value() const { return read<int64_t>(offset::value); }
    int64_t logical_clock() const { return read<int64_t>(offset::logical_clock); }

    void swap_value_and_clock(basic_counter_shard_view& other) noexcept {
        static constexpr size_t off = size_t(offset::value);
        static constexpr size_t size = size_t(offset::total_size) - off;

        signed char tmp[size];
        auto tmp_view = single_fragmented_mutable_view(bytes_mutable_view(std::data(tmp), std::size(tmp)));

        managed_bytes_mutable_view this_view = _base.substr(off, size);
        managed_bytes_mutable_view other_view = other._base.substr(off, size);

        copy_fragmented_view(tmp_view, this_view);
        copy_fragmented_view(this_view, other_view);
        copy_fragmented_view(other_view, tmp_view);
    }

    void set_value_and_clock(const basic_counter_shard_view& other) noexcept {
        static constexpr size_t off = size_t(offset::value);
        static constexpr size_t size = size_t(offset::total_size) - off;

        managed_bytes_mutable_view this_view = _base.substr(off, size);
        managed_bytes_mutable_view other_view = other._base.substr(off, size);

        copy_fragmented_view(this_view, other_view);
    }

    bool operator==(const basic_counter_shard_view& other) const {
        return id() == other.id() && value() == other.value()
               && logical_clock() == other.logical_clock();
    }

    struct less_compare_by_id {
        bool operator()(const basic_counter_shard_view& x, const basic_counter_shard_view& y) const {
            return x.id() < y.id();
        }
    };
};

using counter_shard_view = basic_counter_shard_view<mutable_view::no>;

std::ostream& operator<<(std::ostream& os, counter_shard_view csv);

class counter_shard {
    counter_id _id;
    int64_t _value;
    int64_t _logical_clock;
private:
    // Shared logic for applying counter_shards and counter_shard_views.
    // T is either counter_shard or basic_counter_shard_view<U>.
    template<typename T>
    requires requires(T shard) {
        { shard.value() } -> std::same_as<int64_t>;
        { shard.logical_clock() } -> std::same_as<int64_t>;
    }
    counter_shard& do_apply(T&& other) noexcept {
        auto other_clock = other.logical_clock();
        if (_logical_clock < other_clock) {
            _logical_clock = other_clock;
            _value = other.value();
        }
        return *this;
    }
public:
    counter_shard(counter_id id, int64_t value, int64_t logical_clock) noexcept
        : _id(id)
        , _value(value)
        , _logical_clock(logical_clock)
    { }

    explicit counter_shard(counter_shard_view csv) noexcept
        : _id(csv.id())
        , _value(csv.value())
        , _logical_clock(csv.logical_clock())
    { }

    counter_id id() const { return _id; }
    int64_t value() const { return _value; }
    int64_t logical_clock() const { return _logical_clock; }

    counter_shard& update(int64_t value_delta, int64_t clock_increment) noexcept {
        _value = uint64_t(_value) + uint64_t(value_delta); // signed int overflow is undefined hence the cast
        _logical_clock += clock_increment;
        return *this;
    }

    counter_shard& apply(counter_shard_view other) noexcept {
        return do_apply(other);
    }

    counter_shard& apply(const counter_shard& other) noexcept {
        return do_apply(other);
    }

    static constexpr size_t serialized_size() {
        return counter_shard_view::size;
    }
    void serialize(atomic_cell_value_mutable_view& out) const {
        write_native<counter_id>(out, _id);
        write_native<int64_t>(out, _value);
        write_native<int64_t>(out, _logical_clock);
    }
};

class counter_cell_builder {
    std::vector<counter_shard> _shards;
    bool _sorted = true;
private:
    void do_sort_and_remove_duplicates();
public:
    counter_cell_builder() = default;
    counter_cell_builder(size_t shard_count) {
        _shards.reserve(shard_count);
    }

    void add_shard(const counter_shard& cs) {
        _shards.emplace_back(cs);
    }

    void add_maybe_unsorted_shard(const counter_shard& cs) {
        add_shard(cs);
        if (_sorted && _shards.size() > 1) {
            auto current = _shards.rbegin();
            auto previous = std::next(current);
            _sorted = current->id() > previous->id();
        }
    }

    void sort_and_remove_duplicates() {
        if (!_sorted) {
            do_sort_and_remove_duplicates();
        }
    }

    size_t serialized_size() const {
        return _shards.size() * counter_shard::serialized_size();
    }
    void serialize(atomic_cell_value_mutable_view& out) const {
        for (auto&& cs : _shards) {
            cs.serialize(out);
        }
    }

    bool empty() const {
        return _shards.empty();
    }

    atomic_cell build(api::timestamp_type timestamp) const {
        auto ac = atomic_cell::make_live_uninitialized(*counter_type, timestamp, serialized_size());

        auto dst = ac.value();
        for (auto&& cs : _shards) {
            cs.serialize(dst);
        }
        return ac;
    }

    static atomic_cell from_single_shard(api::timestamp_type timestamp, const counter_shard& cs) {
        auto ac = atomic_cell::make_live_uninitialized(*counter_type, timestamp, counter_shard::serialized_size());
        auto dst = ac.value();
        cs.serialize(dst);
        return ac;
    }

    class inserter_iterator {
    public:
        using iterator_category = std::output_iterator_tag;
        using value_type = counter_shard;
        using difference_type = std::ptrdiff_t;
        using pointer = counter_shard*;
        using reference = counter_shard&;
    private:
        counter_cell_builder* _builder;
    public:
        explicit inserter_iterator(counter_cell_builder& b) : _builder(&b) { }
        inserter_iterator& operator=(const counter_shard& cs) {
            _builder->add_shard(cs);
            return *this;
        }
        inserter_iterator& operator=(const counter_shard_view& csv) {
            return this->operator=(counter_shard(csv));
        }
        inserter_iterator& operator++() { return *this; }
        inserter_iterator& operator++(int) { return *this; }
        inserter_iterator& operator*() { return *this; };
    };

    inserter_iterator inserter() {
        return inserter_iterator(*this);
    }
};

// <counter_id>   := <int64_t><int64_t>
// <shard>        := <counter_id><int64_t:value><int64_t:logical_clock>
// <counter_cell> := <shard>*
template<mutable_view is_mutable>
class basic_counter_cell_view {
protected:
    basic_atomic_cell_view<is_mutable> _cell;
private:
    class shard_iterator {
    public:
        using iterator_category = std::input_iterator_tag;
        using value_type = basic_counter_shard_view<is_mutable>;
        using difference_type = std::ptrdiff_t;
        using pointer = basic_counter_shard_view<is_mutable>*;
        using reference = basic_counter_shard_view<is_mutable>&;
    private:
        managed_bytes_basic_view<is_mutable> _current;
        basic_counter_shard_view<is_mutable> _current_view;
        size_t _pos = 0;
    public:
        shard_iterator(managed_bytes_basic_view<is_mutable> v, size_t offset) noexcept
            : _current(v), _current_view(_current), _pos(offset) { }

        basic_counter_shard_view<is_mutable>& operator*() noexcept {
            return _current_view;
        }
        basic_counter_shard_view<is_mutable>* operator->() noexcept {
            return &_current_view;
        }
        shard_iterator& operator++() noexcept {
            _pos += counter_shard_view::size;
            _current_view = basic_counter_shard_view<is_mutable>(_current.substr(_pos, counter_shard_view::size));
            return *this;
        }
        shard_iterator operator++(int) noexcept {
            auto it = *this;
            operator++();
            return it;
        }
        shard_iterator& operator--() noexcept {
            _pos -= counter_shard_view::size;
            _current_view = basic_counter_shard_view<is_mutable>(_current.substr(_pos, counter_shard_view::size));
            return *this;
        }
        shard_iterator operator--(int) noexcept {
            auto it = *this;
            operator--();
            return it;
        }
        bool operator==(const shard_iterator& other) const noexcept {
            return _pos == other._pos;
        }
    };
public:
    boost::iterator_range<shard_iterator> shards() const {
        auto value = _cell.value();
        auto begin = shard_iterator(value, 0);
        auto end = shard_iterator(value, value.size());
        return boost::make_iterator_range(begin, end);
    }

    size_t shard_count() const {
        return _cell.value().size() / counter_shard_view::size;
    }
public:
    // ac must be a live counter cell
    explicit basic_counter_cell_view(basic_atomic_cell_view<is_mutable> ac) noexcept
        : _cell(ac)
    {
        assert(_cell.is_live());
        assert(!_cell.is_counter_update());
    }

    api::timestamp_type timestamp() const { return _cell.timestamp(); }

    static data_type total_value_type() { return long_type; }

    int64_t total_value() const {
        return boost::accumulate(shards(), int64_t(0), [] (int64_t v, counter_shard_view cs) {
            return v + cs.value();
        });
    }

    std::optional<counter_shard_view> get_shard(const counter_id& id) const {
        auto it = boost::range::find_if(shards(), [&id] (counter_shard_view csv) {
            return csv.id() == id;
        });
        if (it == shards().end()) {
            return { };
        }
        return *it;
    }

    bool operator==(const basic_counter_cell_view& other) const {
        return timestamp() == other.timestamp() && boost::equal(shards(), other.shards());
    }
};

struct counter_cell_view : basic_counter_cell_view<mutable_view::no> {
    using basic_counter_cell_view::basic_counter_cell_view;

    // Reversibly applies two counter cells, at least one of them must be live.
    static void apply(const column_definition& cdef, atomic_cell_or_collection& dst, atomic_cell_or_collection& src);

    // Computes a counter cell containing minimal amount of data which, when
    // applied to 'b' returns the same cell as 'a' and 'b' applied together.
    static std::optional<atomic_cell> difference(atomic_cell_view a, atomic_cell_view b);

    friend std::ostream& operator<<(std::ostream& os, counter_cell_view ccv);
};

struct counter_cell_mutable_view : basic_counter_cell_view<mutable_view::yes> {
    using basic_counter_cell_view::basic_counter_cell_view;

    explicit counter_cell_mutable_view(atomic_cell_mutable_view ac) noexcept
        : basic_counter_cell_view<mutable_view::yes>(ac)
    {
    }

    void set_timestamp(api::timestamp_type ts) { _cell.set_timestamp(ts); }
};

// Transforms mutation dst from counter updates to counter shards using state
// stored in current_state.
// If current_state is present it has to be in the same schema as dst.
void transform_counter_updates_to_shards(mutation& dst, const mutation* current_state, uint64_t clock_offset, locator::host_id local_id);

template<>
struct appending_hash<counter_shard_view> {
    template<typename Hasher>
    void operator()(Hasher& h, const counter_shard_view& cshard) const {
        ::feed_hash(h, cshard.id());
        ::feed_hash(h, cshard.value());
        ::feed_hash(h, cshard.logical_clock());
    }
};

template<>
struct appending_hash<counter_cell_view> {
    template<typename Hasher>
    void operator()(Hasher& h, const counter_cell_view& cell) const {
        ::feed_hash(h, true); // is_live
        ::feed_hash(h, cell.timestamp());
        for (auto&& csv : cell.shards()) {
            ::feed_hash(h, csv);
        }
    }
};



#include <vector>

namespace cql3 {

namespace selection {

class result_set_builder;

/**
 * A <code>selector</code> is used to convert the data returned by the storage engine into the data requested by the
 * user. They correspond to the &lt;selector&gt; elements from the select clause.
 * <p>Since the introduction of aggregation, <code>selector</code>s cannot be called anymore by multiple threads
 * as they have an internal state.</p>
 */
class selector : public assignment_testable {
public:
    class factory;

    virtual ~selector() {}

    /**
     * Add the current value from the specified <code>result_set_builder</code>.
     *
     * @param rs the <code>result_set_builder</code>
     * @throws InvalidRequestException if a problem occurs while add the input value
     */
    virtual void add_input(result_set_builder& rs) = 0;

    /**
     * Returns the selector output.
     *
     * @return the selector output
     * @throws InvalidRequestException if a problem occurs while computing the output value
     */
    virtual managed_bytes_opt get_output() = 0;

    /**
     * Returns the <code>selector</code> output type.
     *
     * @return the <code>selector</code> output type.
     */
    virtual data_type get_type() const = 0;

    virtual bool requires_thread() const;

    /**
     * Checks if this <code>selector</code> is creating aggregates.
     *
     * @return <code>true</code> if this <code>selector</code> is creating aggregates <code>false</code>
     * otherwise.
     */
    virtual bool is_aggregate() const {
        return false;
    }

    /**
     * Reset the internal state of this <code>selector</code>.
     */
    virtual void reset() = 0;

    virtual assignment_testable::test_result test_assignment(data_dictionary::database db, const sstring& keyspace, const column_specification& receiver) const override {
        auto t1 = receiver.type->underlying_type();
        auto t2 = get_type()->underlying_type();
        // We want columns of `counter_type' to be served by underlying type's overloads
        // (here: `counter_cell_view::total_value_type()') with an `EXACT_MATCH'.
        // Weak assignability between the two would lead to ambiguity because
        // `WEAKLY_ASSIGNABLE' counter->blob conversion exists and would compete.
        if (t1 == t2 || (t1 == counter_cell_view::total_value_type() && t2->is_counter())) {
            return assignment_testable::test_result::EXACT_MATCH;
        } else if (t1->is_value_compatible_with(*t2)) {
            return assignment_testable::test_result::WEAKLY_ASSIGNABLE;
        } else {
            return assignment_testable::test_result::NOT_ASSIGNABLE;
        }
    }
};

/**
 * A factory for <code>selector</code> instances.
 */
class selector::factory {
public:
    virtual ~factory() {}

    /**
     * Returns the column specification corresponding to the output value of the selector instances created by
     * this factory.
     *
     * @param schema the column family schema
     * @return a column specification
     */
    lw_shared_ptr<column_specification> get_column_specification(const schema& schema) const;

    /**
     * Creates a new <code>selector</code> instance.
     *
     * @return a new <code>selector</code> instance
     */
    virtual ::shared_ptr<selector> new_instance() const = 0;

    /**
     * Checks if this factory creates simple selectors instances.
     *
     * @return <code>true</code> if this factory creates simple selectors instances,
     * <code>false</code> otherwise
     */
    virtual bool is_simple_selector_factory() const {
        return false;
    }

    /**
     * Checks if arguments for this factory contains only simple slectors.
     *
     * @return <code>true</code> if this factory contains 
     * <code>false</code> otherwise, or if it isn't function selector factory
     */
    virtual bool contains_only_simple_arguments() const {
        return false;
    }

    /**
     * Checks if this factory creates selectors instances that creates aggregates.
     *
     * @return <code>true</code> if this factory creates selectors instances that creates aggregates,
     * <code>false</code> otherwise
     */
    virtual bool is_aggregate_selector_factory() const {
        return false;
    }

    virtual bool is_count_selector_factory() const {
        return false;
    }

    virtual bool is_reducible_selector_factory() const {
        return false;
    }

    virtual std::optional<std::pair<query::forward_request::reduction_type, query::forward_request::aggregation_info>> 
    get_reduction() const {return std::nullopt;}

    /**
     * Checks if this factory creates <code>writetime</code> selectors instances.
     *
     * @return <code>true</code> if this factory creates <code>writetime</code> selectors instances,
     * <code>false</code> otherwise
     */
    virtual bool is_write_time_selector_factory() const {
        return false;
    }

    /**
     * Checks if this factory creates <code>TTL</code> selectors instances.
     *
     * @return <code>true</code> if this factory creates <code>TTL</code> selectors instances,
     * <code>false</code> otherwise
     */
    virtual bool is_ttl_selector_factory() const {
        return false;
    }

    /**
     * Returns the name of the column corresponding to the output value of the selector instances created by
     * this factory.
     *
     * @return a column name
     */
    virtual sstring column_name() const = 0;

    /**
     * Returns the type of the values returned by the selector instances created by this factory.
     *
     * @return the selector output type
     */
    virtual data_type get_return_type() const = 0;
};

}

}


namespace cql3 {
namespace functions {

using function = db::functions::function;

}
}


namespace cql3 {
namespace functions {

using function_name = db::functions::function_name;

}
}



#include <seastar/core/shared_ptr.hh>

namespace cql3 {

namespace selection {

class selectable;

class selectable {
public:
    virtual ~selectable() {}
    virtual ::shared_ptr<selector::factory> new_selector_factory(data_dictionary::database db, schema_ptr schema, std::vector<const column_definition*>& defs) = 0;
    virtual sstring to_string() const = 0;
protected:
    static size_t add_and_get_index(const column_definition& def, std::vector<const column_definition*>& defs) {
        auto i = std::find(defs.begin(), defs.end(), &def);
        if (i != defs.end()) {
            return std::distance(defs.begin(), i);
        }
        defs.push_back(&def);
        return defs.size() - 1;
    }
public:
    class writetime_or_ttl;

    class with_function;
    class with_anonymous_function;

    class with_field_selection;

    class with_cast;
};

std::ostream & operator<<(std::ostream &os, const selectable& s);

class selectable::with_function : public selectable {
    functions::function_name _function_name;
    std::vector<shared_ptr<selectable>> _args;
public:
    with_function(functions::function_name fname, std::vector<shared_ptr<selectable>> args)
        : _function_name(std::move(fname)), _args(std::move(args)) {
    }

    virtual sstring to_string() const override;

    virtual shared_ptr<selector::factory> new_selector_factory(data_dictionary::database db, schema_ptr s, std::vector<const column_definition*>& defs) override;
};

class selectable::with_anonymous_function : public selectable {
    shared_ptr<functions::function> _function;
    std::vector<shared_ptr<selectable>> _args;
public:
    with_anonymous_function(::shared_ptr<functions::function> f, std::vector<shared_ptr<selectable>> args)
        : _function(f), _args(std::move(args)) {
    }

    virtual sstring to_string() const override;

    virtual shared_ptr<selector::factory> new_selector_factory(data_dictionary::database db, schema_ptr s, std::vector<const column_definition*>& defs) override;
};

class selectable::with_cast : public selectable {
    ::shared_ptr<selectable> _arg;
    data_type _type;
public:
    with_cast(::shared_ptr<selectable> arg, data_type type)
        : _arg(std::move(arg)), _type(std::move(type)) {
    }

    virtual sstring to_string() const override;

    virtual shared_ptr<selector::factory> new_selector_factory(data_dictionary::database db, schema_ptr s, std::vector<const column_definition*>& defs) override;
};

}

}


#include <seastar/core/thread.hh>

namespace cql3 {

class result_set;
class result_set_builder;
class metadata;
class query_options;

namespace restrictions {
class statement_restrictions;
}

namespace selection {

class raw_selector;
class selector_factories;

class selectors {
public:
    virtual ~selectors() {}

    virtual bool requires_thread() const = 0;

    virtual bool is_aggregate() const = 0;

    /**
    * Adds the current row of the specified <code>ResultSetBuilder</code>.
    *
    * @param rs the <code>ResultSetBuilder</code>
    * @throws InvalidRequestException
    */
    virtual void add_input_row(result_set_builder& rs) = 0;

    virtual std::vector<managed_bytes_opt> get_output_row() = 0;

    virtual void reset() = 0;
};

class selection {
private:
    schema_ptr _schema;
    std::vector<const column_definition*> _columns;
    ::shared_ptr<metadata> _metadata;
    const bool _collect_timestamps;
    const bool _collect_TTLs;
    const bool _contains_static_columns;
    bool _is_trivial;
protected:
    using trivial = bool_class<class trivial_tag>;

    selection(schema_ptr schema,
        std::vector<const column_definition*> columns,
        std::vector<lw_shared_ptr<column_specification>> metadata_,
        bool collect_timestamps,
        bool collect_TTLs, trivial is_trivial = trivial::no);

    virtual ~selection() {}
public:
    // Overriden by SimpleSelection when appropriate.
    virtual bool is_wildcard() const {
        return false;
    }

    /**
     * Checks if this selection contains static columns.
     * @return <code>true</code> if this selection contains static columns, <code>false</code> otherwise;
     */
    bool contains_static_columns() const {
        return _contains_static_columns;
    }

    /**
     * Checks if this selection contains only static columns.
     * @return <code>true</code> if this selection contains only static columns, <code>false</code> otherwise;
     */
    bool contains_only_static_columns() const;

    /**
     * Returns the index of the specified column.
     *
     * @param def the column definition
     * @return the index of the specified column
     */
    int32_t index_of(const column_definition& def) const;

    bool has_column(const column_definition& def) const;

    ::shared_ptr<const metadata> get_result_metadata() const {
        return _metadata;
    }

    ::shared_ptr<metadata> get_result_metadata() {
        return _metadata;
    }

    static ::shared_ptr<selection> wildcard(schema_ptr schema);
    static ::shared_ptr<selection> for_columns(schema_ptr schema, std::vector<const column_definition*> columns);

    virtual uint32_t add_column_for_post_processing(const column_definition& c);

    virtual std::vector<shared_ptr<functions::function>> used_functions() const { return {}; }

    query::partition_slice::option_set get_query_options();
private:
    static bool processes_selection(const std::vector<::shared_ptr<raw_selector>>& raw_selectors);

    static std::vector<lw_shared_ptr<column_specification>> collect_metadata(const schema& schema,
        const std::vector<::shared_ptr<raw_selector>>& raw_selectors, const selector_factories& factories);
public:
    static ::shared_ptr<selection> from_selectors(data_dictionary::database db, schema_ptr schema, const std::vector<::shared_ptr<raw_selector>>& raw_selectors);

    virtual std::unique_ptr<selectors> new_selectors() const = 0;

    /**
     * Returns a range of CQL3 columns this selection needs.
     */
    auto const& get_columns() const {
        return _columns;
    }

    uint32_t get_column_count() const {
        return _columns.size();
    }

    virtual bool is_aggregate() const = 0;

    virtual bool is_count() const {return false;}

    virtual bool is_reducible() const {return false;}

    virtual query::forward_request::reductions_info get_reductions() const {return {{}, {}};}

    /**
     * Checks that selectors are either all aggregates or that none of them is.
     *
     * @param selectors the selectors to test.
     * @param messageTemplate the error message template
     * @param messageArgs the error message arguments
     * @throws InvalidRequestException if some of the selectors are aggregate but not all of them
     */
    template<typename... Args>
    static void validate_selectors(const std::vector<::shared_ptr<selector>>& selectors, const sstring& msg, Args&&... args) {
        int32_t aggregates = 0;
        for (auto&& s : selectors) {
            if (s->is_aggregate()) {
                ++aggregates;
            }
        }

        if (aggregates != 0 && aggregates != selectors.size()) {
            throw exceptions::invalid_request_exception(fmt::format(msg, std::forward<Args>(args)...));
        }
    }

    /**
     * Returns true if the selection is trivial, i.e. there are no function
     * selectors (including casts or aggregates).
     */
    bool is_trivial() const { return _is_trivial; }

    friend class result_set_builder;
};

shared_ptr<selection> selection_from_partition_slice(schema_ptr schema, const query::partition_slice& slice);

class result_set_builder {
private:
    std::unique_ptr<result_set> _result_set;
    std::unique_ptr<selectors> _selectors;
    const std::vector<size_t> _group_by_cell_indices; ///< Indices in \c current of cells holding GROUP BY values.
    std::vector<managed_bytes_opt> _last_group; ///< Previous row's group: all of GROUP BY column values.
    bool _group_began; ///< Whether a group began being formed.
public:
    std::optional<std::vector<managed_bytes_opt>> current;
private:
    std::vector<api::timestamp_type> _timestamps;
    std::vector<int32_t> _ttls;
    const gc_clock::time_point _now;
public:
    template<typename Func>
    auto with_thread_if_needed(Func&& func) {
        if (_selectors->requires_thread()) {
            return async(std::move(func));
        } else {
            return futurize_invoke(std::move(func));
        }
    }

    class nop_filter {
    public:
        inline bool operator()(const selection&, const std::vector<bytes>&, const std::vector<bytes>&, const query::result_row_view&, const query::result_row_view*) const {
            return true;
        }
        void reset(const partition_key* = nullptr) {
        }
        uint64_t get_rows_dropped() const {
            return 0;
        }
    };
    class restrictions_filter {
        const ::shared_ptr<const restrictions::statement_restrictions> _restrictions;
        const query_options& _options;
        const bool _skip_pk_restrictions;
        const bool _skip_ck_restrictions;
        mutable bool _current_partition_key_does_not_match = false;
        mutable bool _current_static_row_does_not_match = false;
        mutable uint64_t _rows_dropped = 0;
        mutable uint64_t _remaining;
        schema_ptr _schema;
        mutable uint64_t _per_partition_limit;
        mutable uint64_t _per_partition_remaining;
        mutable uint64_t _rows_fetched_for_last_partition;
        mutable std::optional<partition_key> _last_pkey;
        mutable bool _is_first_partition_on_page = true;
    public:
        explicit restrictions_filter(::shared_ptr<const restrictions::statement_restrictions> restrictions,
                const query_options& options,
                uint64_t remaining,
                schema_ptr schema,
                uint64_t per_partition_limit,
                std::optional<partition_key> last_pkey = {},
                uint64_t rows_fetched_for_last_partition = 0);
        bool operator()(const selection& selection, const std::vector<bytes>& pk, const std::vector<bytes>& ck, const query::result_row_view& static_row, const query::result_row_view* row) const;
        void reset(const partition_key* key = nullptr);
        uint64_t get_rows_dropped() const {
            return _rows_dropped;
        }
    private:
        bool do_filter(const selection& selection, const std::vector<bytes>& pk, const std::vector<bytes>& ck, const query::result_row_view& static_row, const query::result_row_view* row) const;
    };

    result_set_builder(const selection& s, gc_clock::time_point now,
                       std::vector<size_t> group_by_cell_indices = {});
    void add_empty();
    void add(bytes_opt value);
    void add(const column_definition& def, const query::result_atomic_cell_view& c);
    void add_collection(const column_definition& def, bytes_view c);
    void new_row();
    std::unique_ptr<result_set> build();
    api::timestamp_type timestamp_of(size_t idx);
    int32_t ttl_of(size_t idx);

    // Implements ResultVisitor concept from query.hh
    template<typename Filter = nop_filter>
    class visitor {
    protected:
        result_set_builder& _builder;
        const schema& _schema;
        const selection& _selection;
        uint64_t _row_count;
        std::vector<bytes> _partition_key;
        std::vector<bytes> _clustering_key;
        Filter _filter;
    public:
        visitor(cql3::selection::result_set_builder& builder, const schema& s,
                const selection& selection, Filter filter = Filter())
            : _builder(builder)
            , _schema(s)
            , _selection(selection)
            , _row_count(0)
            , _filter(filter)
        {}
        visitor(visitor&&) = default;

        void add_value(const column_definition& def, query::result_row_view::iterator_type& i) {
            if (def.type->is_multi_cell()) {
                auto cell = i.next_collection_cell();
                if (!cell) {
                    _builder.add_empty();
                    return;
                }
                _builder.add_collection(def, cell->linearize());
            } else {
                auto cell = i.next_atomic_cell();
                if (!cell) {
                    _builder.add_empty();
                    return;
                }
                _builder.add(def, *cell);
            }
        }

        void accept_new_partition(const partition_key& key, uint64_t row_count) {
            _partition_key = key.explode(_schema);
            _row_count = row_count;
            _filter.reset(&key);
        }

        void accept_new_partition(uint64_t row_count) {
            _row_count = row_count;
            _filter.reset();
        }

        void accept_new_row(const clustering_key& key, const query::result_row_view& static_row, const query::result_row_view& row) {
            _clustering_key = key.explode(_schema);
            accept_new_row(static_row, row);
        }

        void accept_new_row(const query::result_row_view& static_row, const query::result_row_view& row) {
            auto static_row_iterator = static_row.iterator();
            auto row_iterator = row.iterator();
            if (!_filter(_selection, _partition_key, _clustering_key, static_row, &row)) {
                return;
            }
            _builder.new_row();
            for (auto&& def : _selection.get_columns()) {
                switch (def->kind) {
                case column_kind::partition_key:
                    _builder.add(_partition_key[def->component_index()]);
                    break;
                case column_kind::clustering_key:
                    if (_clustering_key.size() > def->component_index()) {
                        _builder.add(_clustering_key[def->component_index()]);
                    } else {
                        _builder.add({});
                    }
                    break;
                case column_kind::regular_column:
                    add_value(*def, row_iterator);
                    break;
                case column_kind::static_column:
                    add_value(*def, static_row_iterator);
                    break;
                default:
                    assert(0);
                }
            }
        }

        uint64_t accept_partition_end(const query::result_row_view& static_row) {
            if (_row_count == 0) {
                if (!_filter(_selection, _partition_key, _clustering_key, static_row, nullptr)) {
                    return _filter.get_rows_dropped();
                }
                _builder.new_row();
                auto static_row_iterator = static_row.iterator();
                for (auto&& def : _selection.get_columns()) {
                    if (def->is_partition_key()) {
                        _builder.add(_partition_key[def->component_index()]);
                    } else if (def->is_static()) {
                        add_value(*def, static_row_iterator);
                    } else {
                        _builder.add_empty();
                    }
                }
            }
            return _filter.get_rows_dropped();
        }
    };

private:
    bytes_opt get_value(data_type t, query::result_atomic_cell_view c);

    /// True iff the \c current row ends a previously started group, either according to
    /// _group_by_cell_indices or aggregation.
    bool last_group_ended() const;

    /// If there is a valid row in this->current, process it; if \p more_rows_coming, get ready to
    /// receive another.
    void process_current_row(bool more_rows_coming);

    /// Gets output row from _selectors and resets them.
    void flush_selectors();

    /// Updates _last_group from the \c current row.
    void update_last_group();
};

}

}




namespace cql3 {

/**
 * A simple container that simplify passing parameters for collections methods.
 */
class update_parameters final {
public:
    // Option set for partition_slice to be used when fetching prefetch_data
    static constexpr query::partition_slice::option_set options = query::partition_slice::option_set::of<
        query::partition_slice::option::send_partition_key,
        query::partition_slice::option::send_clustering_key,
        query::partition_slice::option::collections_as_maps>();

    // Holder for data for
    // 1) CQL list updates which depend on current state of the list
    // 2) cells needed to check conditions of a CAS statement,
    // 3) rows of CAS result set.
    struct prefetch_data {
        using key = std::pair<partition_key, clustering_key>;
        using key_view = std::pair<const partition_key&, const clustering_key&>;
        struct key_less {
            partition_key::tri_compare pk_cmp;
            clustering_key::tri_compare ck_cmp;

            key_less(const schema& s)
                : pk_cmp(s)
                , ck_cmp(s)
            { }
            std::strong_ordering tri_compare(const partition_key& pk1, const clustering_key& ck1,
                    const partition_key& pk2, const clustering_key& ck2) const {

                std::strong_ordering rc = pk_cmp(pk1, pk2);
                return rc != 0 ? rc : ck_cmp(ck1, ck2);
            }
            // Allow mixing std::pair<partition_key, clustering_key> and
            // std::pair<const partition_key&, const clustering_key&> during lookup
            template <typename Pair1, typename Pair2>
            bool operator()(const Pair1& k1, const Pair2& k2) const {
                return  tri_compare(k1.first, k1.second, k2.first, k2.second) < 0;
            }
        };
    public:
        struct row {
            bool has_static;
            // indexes are determined by prefetch_data::selection
            std::vector<managed_bytes_opt> cells;
            // Return true if this row has at least one static column set.
            bool has_static_columns(const schema& schema) const {
                return has_static;
            }
        };
        // Use an ordered map since CAS result set must be naturally ordered
        // when returned to the client.
        std::map<key, row, key_less> rows;
        schema_ptr schema;
        shared_ptr<cql3::selection::selection> selection;
    public:
        prefetch_data(schema_ptr schema);
        // Find a row object for either static or regular subset of cells, depending
        // on whether clustering key is empty or not.
        // A particular cell within the row can then be found using a column id.
        const row* find_row(const partition_key& pkey, const clustering_key& ckey) const;
    };
    // Note: value (mutation) only required to contain the rows we are interested in
private:
    const std::optional<gc_clock::duration> _ttl;
    // For operations that require a read-before-write, stores prefetched cell values.
    // For CAS statements, stores values of conditioned columns.
    // Is a reference to an outside prefetch_data container since a CAS BATCH statement
    // prefetches all rows at once, for all its nested modification statements.
    const prefetch_data& _prefetched;
public:
    const api::timestamp_type _timestamp;
    const gc_clock::time_point _local_deletion_time;
    const schema_ptr _schema;
    const query_options& _options;

    update_parameters(const schema_ptr schema_, const query_options& options,
            api::timestamp_type timestamp, std::optional<gc_clock::duration> ttl, const prefetch_data& prefetched)
        : _ttl(ttl)
        , _prefetched(prefetched)
        , _timestamp(timestamp)
        , _local_deletion_time(gc_clock::now())
        , _schema(std::move(schema_))
        , _options(options)
    {
        // We use MIN_VALUE internally to mean the absence of of timestamp (in Selection, in sstable stats, ...), so exclude
        // it to avoid potential confusion.
        if (timestamp < api::min_timestamp || timestamp > api::max_timestamp) {
            throw exceptions::invalid_request_exception(format("Out of bound timestamp, must be in [{:d}, {:d}]",
                    api::min_timestamp, api::max_timestamp));
        }
    }

    atomic_cell make_dead_cell() const {
        return atomic_cell::make_dead(_timestamp, _local_deletion_time);
    }

    atomic_cell make_cell(const abstract_type& type, const raw_value_view& value, atomic_cell::collection_member cm = atomic_cell::collection_member::no) const {
        auto ttl = this->ttl();

        return value.with_value([&] (const FragmentedView auto& v) {
            if (ttl.count() > 0) {
                return atomic_cell::make_live(type, _timestamp, v, _local_deletion_time + ttl, ttl, cm);
            } else {
                return atomic_cell::make_live(type, _timestamp, v, cm);
            }
        });
    };

    atomic_cell make_cell(const abstract_type& type, const managed_bytes_view& value, atomic_cell::collection_member cm = atomic_cell::collection_member::no) const {
        auto ttl = this->ttl();

        if (ttl.count() > 0) {
            return atomic_cell::make_live(type, _timestamp, value, _local_deletion_time + ttl, ttl, cm);
        } else {
            return atomic_cell::make_live(type, _timestamp, value, cm);
        }
    };

    atomic_cell make_counter_update_cell(int64_t delta) const {
        return atomic_cell::make_live_counter_update(_timestamp, delta);
    }

    tombstone make_tombstone() const {
        return {_timestamp, _local_deletion_time};
    }

    tombstone make_tombstone_just_before() const {
        return {_timestamp - 1, _local_deletion_time};
    }

    gc_clock::duration ttl() const {
        return _ttl.value_or(_schema->default_time_to_live());
    }

    gc_clock::time_point expiry() const {
        return ttl() + _local_deletion_time;
    }

    api::timestamp_type timestamp() const {
        return _timestamp;
    }

    std::optional<std::vector<std::pair<data_value, data_value>>>
    get_prefetched_list(const partition_key& pkey, const clustering_key& ckey, const column_definition& column) const;

    static prefetch_data build_prefetch_data(schema_ptr schema, const query::result& query_result,
            const query::partition_slice& slice);
};

}


namespace cql3 {

namespace statements {

enum class bound : int32_t { START = 0, END };

static inline
int32_t get_idx(bound b) {
    return (int32_t)b;
}

static inline
bound reverse(bound b) {
    return bound((int32_t)b ^ 1);
}

static inline
bool is_start(bound b) {
    return b == bound::START;
}

static inline
bool is_end(bound b) {
    return b == bound::END;
}

}

}

#include <variant>
#include <type_traits>

namespace utils {

// Given type T and an std::variant Variant, return std::true_type if T is a variant element
template <class T, class Variant>
struct is_variant_element;

template <class T, class... Elements>
struct is_variant_element<T, std::variant<Elements...>> : std::bool_constant<(std::is_same_v<T, Elements> || ...)> {
};

// Givent type T and std::variant, true if T is one of the variant elements.
template <typename T, typename Variant>
concept VariantElement = is_variant_element<T, Variant>::value;

}


#include <fmt/core.h>
#include <ostream>
#include <seastar/core/shared_ptr.hh>
#include <variant>
#include <concepts>
#include <numeric>


class row;

namespace db {
namespace functions {
    class function;
}
}

namespace secondary_index {
class index;
class secondary_index_manager;
} // namespace secondary_index

namespace query {
    class result_row_view;
} // namespace query

namespace cql3 {
struct prepare_context;

class column_identifier_raw;
class query_options;

namespace selection {
    class selection;
} // namespace selection

namespace restrictions {
    class restriction;
}

namespace expr {

struct allow_local_index_tag {};
using allow_local_index = bool_class<allow_local_index_tag>;

struct binary_operator;
struct conjunction;
struct column_value;
struct subscript;
struct unresolved_identifier;
struct column_mutation_attribute;
struct function_call;
struct cast;
struct field_selection;
struct bind_variable;
struct untyped_constant;
struct constant;
struct tuple_constructor;
struct collection_constructor;
struct usertype_constructor;

template <typename T>
concept ExpressionElement
        = std::same_as<T, conjunction>
        || std::same_as<T, binary_operator>
        || std::same_as<T, column_value>
        || std::same_as<T, subscript>
        || std::same_as<T, unresolved_identifier>
        || std::same_as<T, column_mutation_attribute>
        || std::same_as<T, function_call>
        || std::same_as<T, cast>
        || std::same_as<T, field_selection>
        || std::same_as<T, bind_variable>
        || std::same_as<T, untyped_constant>
        || std::same_as<T, constant>
        || std::same_as<T, tuple_constructor>
        || std::same_as<T, collection_constructor>
        || std::same_as<T, usertype_constructor>
        ;

template <typename Func>
concept invocable_on_expression
        = std::invocable<Func, conjunction>
        && std::invocable<Func, binary_operator>
        && std::invocable<Func, column_value>
        && std::invocable<Func, subscript>
        && std::invocable<Func, unresolved_identifier>
        && std::invocable<Func, column_mutation_attribute>
        && std::invocable<Func, function_call>
        && std::invocable<Func, cast>
        && std::invocable<Func, field_selection>
        && std::invocable<Func, bind_variable>
        && std::invocable<Func, untyped_constant>
        && std::invocable<Func, constant>
        && std::invocable<Func, tuple_constructor>
        && std::invocable<Func, collection_constructor>
        && std::invocable<Func, usertype_constructor>
        ;

template <typename Func>
concept invocable_on_expression_ref
        = std::invocable<Func, conjunction&>
        && std::invocable<Func, binary_operator&>
        && std::invocable<Func, column_value&>
        && std::invocable<Func, subscript&>
        && std::invocable<Func, unresolved_identifier&>
        && std::invocable<Func, column_mutation_attribute&>
        && std::invocable<Func, function_call&>
        && std::invocable<Func, cast&>
        && std::invocable<Func, field_selection&>
        && std::invocable<Func, bind_variable&>
        && std::invocable<Func, untyped_constant&>
        && std::invocable<Func, constant&>
        && std::invocable<Func, tuple_constructor&>
        && std::invocable<Func, collection_constructor&>
        && std::invocable<Func, usertype_constructor&>
        ;

/// A CQL expression -- union of all possible expression types.
class expression final {
    // 'impl' holds a variant of all expression types, but since 
    // variants of incomplete types are not allowed, we forward declare it
    // here and fully define it later.
    struct impl;                 
    std::unique_ptr<impl> _v;
public:
    expression(); // FIXME: remove
    expression(ExpressionElement auto e);

    expression(const expression&);
    expression(expression&&) noexcept = default;
    expression& operator=(const expression&);
    expression& operator=(expression&&) noexcept = default;

    template <invocable_on_expression Visitor>
    friend decltype(auto) visit(Visitor&& visitor, const expression& e);

    template <invocable_on_expression_ref Visitor>
    friend decltype(auto) visit(Visitor&& visitor, expression& e);

    template <ExpressionElement E>
    friend bool is(const expression& e);

    template <ExpressionElement E>
    friend const E& as(const expression& e);

    template <ExpressionElement E>
    friend const E* as_if(const expression* e);

    template <ExpressionElement E>
    friend E* as_if(expression* e);

    // Prints given expression using additional options
    struct printer {
        const expression& expr_to_print;
        bool debug_mode = true;
    };

    friend bool operator==(const expression& e1, const expression& e2);
};

/// Checks if two expressions are equal. If they are, they definitely
/// perform the same computation. If they are unequal, they may perform
/// the same computation or different computations.
bool operator==(const expression& e1, const expression& e2);

// An expression that doesn't contain subexpressions
template <typename E>
concept LeafExpression
        = std::same_as<unresolved_identifier, E>
        || std::same_as<bind_variable, E> 
        || std::same_as<untyped_constant, E> 
        || std::same_as<constant, E>
        || std::same_as<column_value, E>
        ;

/// A column, usually encountered on the left side of a restriction.
/// An expression like `mycol < 5` would be expressed as a binary_operator
/// with column_value on the left hand side.
struct column_value {
    const column_definition* col;

    column_value(const column_definition* col) : col(col) {}

    friend bool operator==(const column_value&, const column_value&) = default;
};

/// A subscripted value, eg list_colum[2], val[sub]
struct subscript {
    expression val;
    expression sub;
    data_type type; // may be null before prepare

    friend bool operator==(const subscript&, const subscript&) = default;
};

/// Gets the subscripted column_value out of the subscript.
/// Only columns can be subscripted in CQL, so we can expect that the subscripted expression is a column_value.
const column_value& get_subscripted_column(const subscript&);

/// Gets the column_definition* out of expression that can be a column_value or subscript
/// Only columns can be subscripted in CQL, so we can expect that the subscripted expression is a column_value.
const column_value& get_subscripted_column(const expression&);

enum class oper_t { EQ, NEQ, LT, LTE, GTE, GT, IN, CONTAINS, CONTAINS_KEY, IS_NOT, LIKE };

/// Describes the nature of clustering-key comparisons.  Useful for implementing SCYLLA_CLUSTERING_BOUND.
enum class comparison_order : char {
    cql, ///< CQL order. (a,b)>(1,1) is equivalent to a>1 OR (a=1 AND b>1).
    clustering, ///< Table's clustering order. (a,b)>(1,1) means any row past (1,1) in storage.
};

enum class null_handling_style {
    sql,           // evaluate(NULL = NULL) -> NULL, evaluate(NULL < x) -> NULL
    lwt_nulls,     // evaluate(NULL = NULL) -> TRUE, evaluate(NULL < x) -> exception
};

/// Operator restriction: LHS op RHS.
struct binary_operator {
    expression lhs;
    oper_t op;
    expression rhs;
    comparison_order order;
    null_handling_style null_handling = null_handling_style::sql;

    binary_operator(expression lhs, oper_t op, expression rhs, comparison_order order = comparison_order::cql);

    friend bool operator==(const binary_operator&, const binary_operator&) = default;
};

/// A conjunction of restrictions.
struct conjunction {
    std::vector<expression> children;

    friend bool operator==(const conjunction&, const conjunction&) = default;
};

// Gets resolved eventually into a column_value.
struct unresolved_identifier {
    ::shared_ptr<column_identifier_raw> ident;

    ~unresolved_identifier();

    friend bool operator==(const unresolved_identifier&, const unresolved_identifier&) = default;
};

// An attribute attached to a column mutation: writetime or ttl
struct column_mutation_attribute {
    enum class attribute_kind { writetime, ttl };

    attribute_kind kind;
    // note: only unresolved_identifier is legal here now. One day, when prepare()
    // on expressions yields expressions, column_value will also be legal here.
    expression column;

    friend bool operator==(const column_mutation_attribute&, const column_mutation_attribute&) = default;
};

struct function_call {
    std::variant<functions::function_name, shared_ptr<db::functions::function>> func;
    std::vector<expression> args;

    // 0-based index of the function call within a CQL statement.
    // Used to populate the cache of execution results while passing to
    // another shard (handling `bounce_to_shard` messages) in LWT statements.
    //
    // The id is set only for the function calls that are a part of LWT
    // statement restrictions for the partition key. Otherwise, the id is not
    // set and the call is not considered when using or populating the cache.
    //
    // For example in a query like:
    // INSERT INTO t (pk) VALUES (uuid()) IF NOT EXISTS
    // The query should be executed on a shard that has the pk partition,
    // but it changes with each uuid() call.
    // uuid() call result is cached and sent to the proper shard.
    //
    // Cache id is kept in shared_ptr because of how prepare_context works.
    // During fill_prepare_context all function cache ids are collected
    // inside prepare_context.
    // Later when some condition occurs we might decide to clear
    // cache ids of all function calls found in prepare_context.
    // However by this time these function calls could have been
    // copied multiple times. Prepare_context keeps a shared_ptr
    // to function_call ids, and then clearing the shared id
    // clears it in all possible copies.
    // This logic was introduced back when everything was shared_ptr<term>,
    // now a better solution might exist.
    //
    // This field can be nullptr, it means that there is no cache id set.
    ::shared_ptr<std::optional<uint8_t>> lwt_cache_id;

    friend bool operator==(const function_call&, const function_call&) = default;
};

struct cast {
    expression arg;
    std::variant<data_type, shared_ptr<cql3_type::raw>> type;

    friend bool operator==(const cast&, const cast&) = default;
};

struct field_selection {
    expression structure;
    shared_ptr<column_identifier_raw> field;
    data_type type; // may be null before prepare

    friend bool operator==(const field_selection&, const field_selection&) = default;
};

struct bind_variable {
    int32_t bind_index;

    // Describes where this bound value will be assigned.
    // Contains value type and other useful information.
    ::lw_shared_ptr<column_specification> receiver;

    friend bool operator==(const bind_variable&, const bind_variable&) = default;
};

// A constant which does not yet have a date type. It is partially typed
// (we know if it's floating or int) but not sized.
struct untyped_constant {
    enum type_class { integer, floating_point, string, boolean, duration, uuid, hex, null };
    type_class partial_type;
    sstring raw_text;

    friend bool operator==(const untyped_constant&, const untyped_constant&) = default;
};

untyped_constant make_untyped_null();

// Represents a constant value with known value and type
// For null and unset the type can sometimes be set to empty_type
struct constant {
    cql3::raw_value value;

    // Never nullptr, for NULL and UNSET might be empty_type
    data_type type;

    constant(cql3::raw_value value, data_type type);
    static constant make_null(data_type val_type = empty_type);
    static constant make_bool(bool bool_val);

    bool is_null() const;
    bool is_unset_value() const;
    bool is_null_or_unset() const;
    bool has_empty_value_bytes() const;

    cql3::raw_value_view view() const;

    friend bool operator==(const constant&, const constant&) = default;
};

// Denotes construction of a tuple from its elements, e.g.  ('a', ?, some_column) in CQL.
struct tuple_constructor {
    std::vector<expression> elements;

    // Might be nullptr before prepare.
    // After prepare always holds a valid type, although it might be reversed_type(tuple_type).
    data_type type;

    friend bool operator==(const tuple_constructor&, const tuple_constructor&) = default;
};

// Constructs a collection of same-typed elements
struct collection_constructor {
    enum class style_type { list, set, map };
    style_type style;
    std::vector<expression> elements;

    // Might be nullptr before prepare.
    // After prepare always holds a valid type, although it might be reversed_type(collection_type).
    data_type type;

    friend bool operator==(const collection_constructor&, const collection_constructor&) = default;
};

// Constructs an object of a user-defined type
struct usertype_constructor {
    using elements_map_type = std::unordered_map<column_identifier, expression>;
    elements_map_type elements;

    // Might be nullptr before prepare.
    // After prepare always holds a valid type, although it might be reversed_type(user_type).
    data_type type;

    friend bool operator==(const usertype_constructor&, const usertype_constructor&) = default;
};

// now that all expression types are fully defined, we can define expression::impl
struct expression::impl final {
    using variant_type = std::variant<
            conjunction, binary_operator, column_value, unresolved_identifier,
            column_mutation_attribute, function_call, cast, field_selection,
            bind_variable, untyped_constant, constant, tuple_constructor, collection_constructor,
            usertype_constructor, subscript>;
    variant_type v;
    impl(variant_type v) : v(std::move(v)) {}
};

expression::expression(ExpressionElement auto e)
        : _v(std::make_unique<impl>(std::move(e))) {
}

inline expression::expression()
        : expression(conjunction{}) {
}

template <invocable_on_expression Visitor>
decltype(auto) visit(Visitor&& visitor, const expression& e) {
    return std::visit(std::forward<Visitor>(visitor), e._v->v);
}

template <invocable_on_expression_ref Visitor>
decltype(auto) visit(Visitor&& visitor, expression& e) {
    return std::visit(std::forward<Visitor>(visitor), e._v->v);
}

template <ExpressionElement E>
bool is(const expression& e) {
    return std::holds_alternative<E>(e._v->v);
}

template <ExpressionElement E>
const E& as(const expression& e) {
    return std::get<E>(e._v->v);
}

template <ExpressionElement E>
const E* as_if(const expression* e) {
    return std::get_if<E>(&e->_v->v);
}

template <ExpressionElement E>
E* as_if(expression* e) {
    return std::get_if<E>(&e->_v->v);
}

/// Creates a conjunction of a and b.  If either a or b is itself a conjunction, its children are inserted
/// directly into the resulting conjunction's children, flattening the expression tree.
extern expression make_conjunction(expression a, expression b);

extern std::ostream& operator<<(std::ostream&, oper_t);

// Input data needed to evaluate an expression. Individual members can be
// null if not applicable (e.g. evaluating outside a row context)
struct evaluation_inputs {
    const std::vector<bytes>* partition_key = nullptr;
    const std::vector<bytes>* clustering_key = nullptr;
    const std::vector<managed_bytes_opt>* static_and_regular_columns = nullptr; // indexes match `selection` member
    const cql3::selection::selection* selection = nullptr;
    const query_options* options = nullptr;
};

/// Helper for generating evaluation_inputs::static_and_regular_columns
std::vector<managed_bytes_opt> get_non_pk_values(const cql3::selection::selection& selection, const query::result_row_view& static_row,
                                         const query::result_row_view* row);

/// Helper for accessing a column value from evaluation_inputs
managed_bytes_opt extract_column_value(const column_definition* cdef, const evaluation_inputs& inputs);

/// True iff restr evaluates to true, given these inputs
extern bool is_satisfied_by(
        const expression& restr, const evaluation_inputs& inputs);


/// A set of discrete values.
using value_list = std::vector<managed_bytes>; // Sorted and deduped using value comparator.

/// General set of values.  Empty set and single-element sets are always value_list.  nonwrapping_range is
/// never singular and never has start > end.  Universal set is a nonwrapping_range with both bounds null.
using value_set = std::variant<value_list, nonwrapping_range<managed_bytes>>;

/// A set of all column values that would satisfy an expression. The _token_values variant finds
/// matching values for the partition token function call instead of the column.
///
/// An expression restricts possible values of a column or token:
/// - `A>5` restricts A from below
/// - `A>5 AND A>6 AND B<10 AND A=12 AND B>0` restricts A to 12 and B to between 0 and 10
/// - `A IN (1, 3, 5)` restricts A to 1, 3, or 5
/// - `A IN (1, 3, 5) AND A>3` restricts A to just 5
/// - `A=1 AND A<=0` restricts A to an empty list; no value is able to satisfy the expression
/// - `A>=NULL` also restricts A to an empty list; all comparisons to NULL are false
/// - an expression without A "restricts" A to unbounded range
extern value_set possible_column_values(const column_definition*, const expression&, const query_options&);
extern value_set possible_partition_token_values(const expression&, const query_options&, const schema& table_schema);

/// Turns value_set into a range, unless it's a multi-valued list (in which case this throws).
extern nonwrapping_range<managed_bytes> to_range(const value_set&);

/// A range of all X such that X op val.
nonwrapping_range<clustering_key_prefix> to_range(oper_t op, const clustering_key_prefix& val);

/// True iff the index can support the entire expression.
extern bool is_supported_by(const expression&, const secondary_index::index&);

/// True iff any of the indices from the manager can support the entire expression.  If allow_local, use all
/// indices; otherwise, use only global indices.
extern bool has_supporting_index(
        const expression&, const secondary_index::secondary_index_manager&, allow_local_index allow_local);

// Looks at each column indivudually and checks whether some index can support restrictions on this single column.
// Expression has to consist only of single column restrictions.
extern bool index_supports_some_column(
    const expression&,
    const secondary_index::secondary_index_manager&,
    allow_local_index allow_local);

extern sstring to_string(const expression&);

extern std::ostream& operator<<(std::ostream&, const column_value&);

extern std::ostream& operator<<(std::ostream&, const expression&);

extern std::ostream& operator<<(std::ostream&, const expression::printer&);

extern bool recurse_until(const expression& e, const noncopyable_function<bool (const expression&)>& predicate_fun);

// Looks into the expression and finds the given expression variant
// for which the predicate function returns true.
// If nothing is found returns nullptr.
// For example:
// find_in_expression<binary_operator>(e, [](const binary_operator&) {return true;})
// Will return the first binary operator found in the expression
template<ExpressionElement ExprElem, class Fn>
requires std::invocable<Fn, const ExprElem&>
      && std::same_as<std::invoke_result_t<Fn, const ExprElem&>, bool>
const ExprElem* find_in_expression(const expression& e, Fn predicate_fun) {
    const ExprElem* ret = nullptr;
    recurse_until(e, [&] (const expression& e) {
        if (auto expr_elem = as_if<ExprElem>(&e)) {
            if (predicate_fun(*expr_elem)) {
                ret = expr_elem;
                return true;
            }
        }
        return false;
    });
    return ret;
}

/// If there is a binary_operator atom b for which f(b) is true, returns it.  Otherwise returns null.
template<class Fn>
requires std::invocable<Fn, const binary_operator&>
      && std::same_as<std::invoke_result_t<Fn, const binary_operator&>, bool>
const binary_operator* find_binop(const expression& e, Fn predicate_fun) {
    return find_in_expression<binary_operator>(e, predicate_fun);
}

// Goes over each expression of the specified type and calls for_each_func for each of them.
// For example:
// for_each_expression<column_vaue>(e, [](const column_value& cval) {std::cout << cval << '\n';});
// Will print all column values in an expression
template<ExpressionElement ExprElem, class Fn>
requires std::invocable<Fn, const ExprElem&>
void for_each_expression(const expression& e, Fn for_each_func) {
    recurse_until(e, [&] (const expression& cur_expr) -> bool {
        if (auto expr_elem = as_if<ExprElem>(&cur_expr)) {
            for_each_func(*expr_elem);
        }
        return false;
    });
}

/// Counts binary_operator atoms b for which f(b) is true.
size_t count_if(const expression& e, const noncopyable_function<bool (const binary_operator&)>& f);

inline const binary_operator* find(const expression& e, oper_t op) {
    return find_binop(e, [&] (const binary_operator& o) { return o.op == op; });
}

inline bool needs_filtering(oper_t op) {
    return (op == oper_t::CONTAINS) || (op == oper_t::CONTAINS_KEY) || (op == oper_t::LIKE) ||
           (op == oper_t::IS_NOT) || (op == oper_t::NEQ) ;
}

inline auto find_needs_filtering(const expression& e) {
    return find_binop(e, [] (const binary_operator& bo) { return needs_filtering(bo.op); });
}

inline bool is_slice(oper_t op) {
    return (op == oper_t::LT) || (op == oper_t::LTE) || (op == oper_t::GT) || (op == oper_t::GTE);
}

inline bool has_slice(const expression& e) {
    return find_binop(e, [] (const binary_operator& bo) { return is_slice(bo.op); });
}

inline bool is_compare(oper_t op) {
    switch (op) {
    case oper_t::EQ:
    case oper_t::LT:
    case oper_t::LTE:
    case oper_t::GT:
    case oper_t::GTE:
    case oper_t::NEQ:
        return true;
    default:
        return false;
    }
}

inline bool is_multi_column(const binary_operator& op) {
    return expr::is<tuple_constructor>(op.lhs);
}

// Check whether the given expression represents
// a call to the token() function.
bool is_token_function(const function_call&);
bool is_token_function(const expression&);

bool is_partition_token_for_schema(const function_call&, const schema&);
bool is_partition_token_for_schema(const expression&, const schema&);

/// Check whether the expression contains a binary_operator whose LHS is a call to the token
/// function representing a partition key token.
/// Examples:
/// For expression: "token(p1, p2, p3) < 123 AND c = 2" returns true
/// For expression: "p1 = token(1, 2, 3) AND c = 2" return false
inline bool has_partition_token(const expression& e, const schema& table_schema) {
    return find_binop(e, [&] (const binary_operator& o) { return is_partition_token_for_schema(o.lhs, table_schema); });
}

inline bool has_slice_or_needs_filtering(const expression& e) {
    return find_binop(e, [] (const binary_operator& o) { return is_slice(o.op) || needs_filtering(o.op); });
}

inline bool is_clustering_order(const binary_operator& op) {
    return op.order == comparison_order::clustering;
}

inline auto find_clustering_order(const expression& e) {
    return find_binop(e, is_clustering_order);
}

/// Given a Boolean expression, compute its factors such as e=f1 AND f2 AND f3 ...
/// If the expression is TRUE, may return no factors (happens today for an
/// empty conjunction).
std::vector<expression> boolean_factors(expression e);

/// Run the given function for each element in the top level conjunction.
void for_each_boolean_factor(const expression& e, const noncopyable_function<void (const expression&)>& for_each_func);

/// True iff binary_operator involves a collection.
extern bool is_on_collection(const binary_operator&);

// Checks whether the given column occurs in the expression.
// Uses column_defintion::operator== for comparison, columns with the same name but different schema will not be equal.
bool contains_column(const column_definition& column, const expression& e);

// Checks whether this expression contains a nonpure function.
// The expression must be prepared, so that function names are converted to function pointers.
bool contains_nonpure_function(const expression&);

// Checks whether the given column has an EQ restriction in the expression.
// EQ restriction is `col = ...` or `(col, col2) = ...`
// IN restriction is NOT an EQ restriction, this function will not look for IN restrictions.
// Uses column_defintion::operator== for comparison, columns with the same name but different schema will not be equal.
bool has_eq_restriction_on_column(const column_definition& column, const expression& e);

/// Replaces every column_definition in an expression with this one.  Throws if any LHS is not a single
/// column_value.
extern expression replace_column_def(const expression&, const column_definition*);

// Replaces all occurences of token(p1, p2) on the left hand side with the given colum.
// For example this changes token(p1, p2) < token(1, 2) to my_column_name < token(1, 2).
// Schema is needed to find out which calls to token() describe the partition token.
extern expression replace_partition_token(const expression&, const column_definition*, const schema&);

// Recursively copies e and returns it. Calls replace_candidate() on all nodes. If it returns nullopt,
// continue with the copying. If it returns an expression, that expression replaces the current node.
extern expression search_and_replace(const expression& e,
        const noncopyable_function<std::optional<expression> (const expression& candidate)>& replace_candidate);

// Adjust an expression for rows that were fetched using query::partition_slice::options::collections_as_maps
expression adjust_for_collection_as_maps(const expression& e);

extern expression prepare_expression(const expression& expr, data_dictionary::database db, const sstring& keyspace, const schema* schema_opt, lw_shared_ptr<column_specification> receiver);
std::optional<expression> try_prepare_expression(const expression& expr, data_dictionary::database db, const sstring& keyspace, const schema* schema_opt, lw_shared_ptr<column_specification> receiver);

// Prepares a binary operator received from the parser.
// Does some basic type checks but no advanced validation.
extern binary_operator prepare_binary_operator(binary_operator binop, data_dictionary::database db, const schema& table_schema);

// Pre-compile any constant LIKE patterns and return equivalent expression
expression optimize_like(const expression& e);


/**
 * @return whether this object can be assigned to the provided receiver. We distinguish
 * between 3 values: 
 *   - EXACT_MATCH if this object is exactly of the type expected by the receiver
 *   - WEAKLY_ASSIGNABLE if this object is not exactly the expected type but is assignable nonetheless
 *   - NOT_ASSIGNABLE if it's not assignable
 * Most caller should just call the is_assignable() method on the result, though functions have a use for
 * testing "strong" equality to decide the most precise overload to pick when multiple could match.
 */
extern assignment_testable::test_result test_assignment(const expression& expr, data_dictionary::database db, const sstring& keyspace, const column_specification& receiver);

// Test all elements of exprs for assignment. If all are exact match, return exact match. If any is not assignable,
// return not assignable. Otherwise, return weakly assignable.
extern assignment_testable::test_result test_assignment_all(const std::vector<expression>& exprs, data_dictionary::database db, const sstring& keyspace, const column_specification& receiver);

extern shared_ptr<assignment_testable> as_assignment_testable(expression e);

inline oper_t pick_operator(statements::bound b, bool inclusive) {
    return is_start(b) ?
            (inclusive ? oper_t::GTE : oper_t::GT) :
            (inclusive ? oper_t::LTE : oper_t::LT);
}

// Extracts all binary operators which have the given column on their left hand side.
// Extracts only single-column restrictions.
// Does not include multi-column restrictions.
// Does not include token() restrictions.
// Does not include boolean constant restrictions.
// For example "WHERE c = 1 AND (a, c) = (2, 1) AND token(p) < 2 AND FALSE" will return {"c = 1"}.
std::vector<expression> extract_single_column_restrictions_for_column(const expression&, const column_definition&);

std::optional<bool> get_bool_value(const constant&);

data_type type_of(const expression& e);

// Takes a prepared expression and calculates its value.
// Evaluates bound values, calls functions and returns just the bytes and type.
cql3::raw_value evaluate(const expression& e, const evaluation_inputs&);

cql3::raw_value evaluate(const expression& e, const query_options&);

utils::chunked_vector<managed_bytes_opt> get_list_elements(const cql3::raw_value&);
utils::chunked_vector<managed_bytes_opt> get_set_elements(const cql3::raw_value&);
std::vector<managed_bytes_opt> get_tuple_elements(const cql3::raw_value&, const abstract_type& type);
std::vector<managed_bytes_opt> get_user_type_elements(const cql3::raw_value&, const abstract_type& type);
std::vector<std::pair<managed_bytes, managed_bytes>> get_map_elements(const cql3::raw_value&);

// Gets the elements of a constant which can be a list, set, tuple or user type
std::vector<managed_bytes_opt> get_elements(const cql3::raw_value&, const abstract_type& type);

// Get elements of list<tuple<>> as vector<vector<managed_bytes_opt>
// It is useful with IN restrictions like (a, b) IN [(1, 2), (3, 4)].
// `type` parameter refers to the list<tuple<>> type.
utils::chunked_vector<std::vector<managed_bytes_opt>> get_list_of_tuples_elements(const cql3::raw_value&, const abstract_type& type);

// Retrieves information needed in prepare_context.
// Collects the column specification for the bind variables in this expression.
// Sets lwt_cache_id field in function_calls.
void fill_prepare_context(expression&, cql3::prepare_context&);

// Checks whether there is a bind_variable inside this expression
// It's important to note, that even when there are no bind markers,
// there can be other things that prevent immediate evaluation of an expression.
// For example an expression can contain calls to nonpure functions.
bool contains_bind_marker(const expression& e);

// Checks whether this expression contains restrictions on one single column.
// There might be more than one restriction, but exactly one column.
// The expression must be prepared.
bool is_single_column_restriction(const expression&);

// Gets the only column from a single_column_restriction expression.
const column_value& get_the_only_column(const expression&);

// A comparator that orders columns by their position in the schema
// For primary key columns the `id` field is used to determine their position.
// Other columns are assumed to have position std::numeric_limits<uint32_t>::max().
// In case the position is the same they are compared by their name.
// This comparator has been used in the original restricitons code to keep
// restrictions for each column sorted by their place in the schema.
// It's not recommended to use this comparator with columns of different kind
// (partition/clustering/nonprimary) because the id field is unique
// for (kind, schema). So a partition and clustering column might
// have the same id within one schema.
struct schema_pos_column_definition_comparator {
    bool operator()(const column_definition* def1, const column_definition* def2) const;
};

// Extracts column_defs from the expression and sorts them using schema_pos_column_definition_comparator.
std::vector<const column_definition*> get_sorted_column_defs(const expression&);

// Extracts column_defs and returns the last one according to schema_pos_column_definition_comparator.
const column_definition* get_last_column_def(const expression&);

// A map of single column restrictions for each column
using single_column_restrictions_map = std::map<const column_definition*, expression, schema_pos_column_definition_comparator>;

// Extracts map of single column restrictions for each column from expression
single_column_restrictions_map get_single_column_restrictions_map(const expression&);

// Checks whether this expression is empty - doesn't restrict anything
bool is_empty_restriction(const expression&);

// Finds common columns between both expressions and prints them to a string.
// Uses schema_pos_column_definition_comparator for comparison.
sstring get_columns_in_commons(const expression& a, const expression& b);

// Finds the value of the given column in the expression
// In case of multpiple possible values calls on_internal_error
bytes_opt value_for(const column_definition&, const expression&, const query_options&);

bool contains_multi_column_restriction(const expression&);

bool has_only_eq_binops(const expression&);
} // namespace expr

} // namespace cql3

/// Custom formatter for an expression. Use {:user} for user-oriented
/// output, {:debug} for debug-oriented output. Debug is the default.
///
/// Required for fmt::join() to work on expression.
template <>
class fmt::formatter<cql3::expr::expression> {
    bool _debug = true;
private:
    constexpr static bool try_match_and_advance(format_parse_context& ctx, std::string_view s) {
        auto [ctx_end, s_end] = std::ranges::mismatch(ctx, s);
        if (s_end == s.end()) {
            ctx.advance_to(ctx_end);
            return true;
        }
        return false;
    }
public:
    constexpr auto parse(format_parse_context& ctx) {
        using namespace std::string_view_literals;
        if (try_match_and_advance(ctx, "debug"sv)) {
            _debug = true;
        } else if (try_match_and_advance(ctx, "user"sv)) {
            _debug = false;
        }
        return ctx.begin();
    }

    template <typename FormatContext>
    auto format(const cql3::expr::expression& expr, FormatContext& ctx) const {
        std::ostringstream os;
        os << cql3::expr::expression::printer{.expr_to_print = expr, .debug_mode = _debug};
        return fmt::format_to(ctx.out(), "{}", os.str());
    }
};

/// Required for fmt::join() to work on expression::printer.
template <>
struct fmt::formatter<cql3::expr::expression::printer> {
    constexpr auto parse(format_parse_context& ctx) {
        return ctx.end();
    }

    template <typename FormatContext>
    auto format(const cql3::expr::expression::printer& pr, FormatContext& ctx) const {
        std::ostringstream os;
        os << pr;
        return fmt::format_to(ctx.out(), "{}", os.str());
    }
};

/// Required for fmt::join() to work on ExpressionElement, and for {:user}/{:debug} to work on ExpressionElement.
template <cql3::expr::ExpressionElement E>
struct fmt::formatter<E> : public fmt::formatter<cql3::expr::expression> {
};



namespace cql3 {

class query_options;

}

namespace cql3::expr {

// Some expression users can behave differently if the expression is a bind variable
// and if that bind variable is unset. unset_bind_variable_guard encapsulates the two
// conditions.
class unset_bind_variable_guard {
    // Disengaged if the operand is not exactly a single bind variable.
    std::optional<bind_variable> _var;
public:
    explicit unset_bind_variable_guard(const expr::expression& operand);
    explicit unset_bind_variable_guard(std::nullopt_t) {}
    explicit unset_bind_variable_guard(const std::optional<expr::expression>& operand);
    bool is_unset(const query_options& qo) const;
};

}



#include <seastar/core/shared_ptr.hh>

#include <optional>

namespace cql3 {

namespace statements::broadcast_tables {
    struct prepared_update;
}

class update_parameters;

/**
 * An UPDATE or DELETE operation.
 *
 * For UPDATE this includes:
 *   - setting a constant
 *   - counter operations
 *   - collections operations
 * and for DELETE:
 *   - deleting a column
 *   - deleting an element of collection column
 *
 * Fine grained operation are obtained from their raw counterpart (Operation.Raw, which
 * correspond to a parsed, non-checked operation) by provided the receiver for the operation.
 */
class operation {
public:
    // the column the operation applies to
    // We can hold a reference because all operations have life bound to their statements and
    // statements pin the schema.
    const column_definition& column;

protected:
    // Value involved in the operation. In theory this should not be here since some operation
    // may require none of more than one expression, but most need 1 so it simplify things a bit.
    std::optional<expr::expression> _e;

    // A guard to check if the operation should be skipped due to unset operand.
    expr::unset_bind_variable_guard _unset_guard;
public:
    operation(const column_definition& column_, std::optional<expr::expression> e, expr::unset_bind_variable_guard ubvg)
        : column{column_}
        , _e(std::move(e))
        , _unset_guard(std::move(ubvg))
    { }

    virtual ~operation() {}

    virtual bool is_raw_counter_shard_write() const {
        return false;
    }

    /**
    * @return whether the operation requires a read of the previous value to be executed
    * (only lists setterByIdx, discard and discardByIdx requires that).
    */
    virtual bool requires_read() const {
        return false;
    }

    /**
     * Collects the column specification for the bind variables of this operation.
     *
     * @param meta the list of column specification where to collect the
     * bind variables of this term in.
     */
    virtual void fill_prepare_context(prepare_context& ctx) {
        if (_e.has_value()) {
            expr::fill_prepare_context(*_e, ctx);
        }
    }

    /**
     * Execute the operation. Check should_skip_operation() first.
     */
    virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) = 0;

    bool should_skip_operation(const query_options& qo) const {
        return _unset_guard.is_unset(qo);
    }

    virtual void prepare_for_broadcast_tables(statements::broadcast_tables::prepared_update&) const;

    /**
     * A parsed raw UPDATE operation.
     *
     * This can be one of:
     *   - Setting a value: c = v
     *   - Setting an element of a collection: c[x] = v
     *   - Setting a field of a user-defined type: c.x = v
     *   - An addition/subtraction to a variable: c = c +/- v (where v can be a collection literal)
     *   - An prepend operation: c = v + c
     */
    class raw_update {
    public:
        virtual ~raw_update() {}

        /**
         * This method validates the operation (i.e. validate it is well typed)
         * based on the specification of the receiver of the operation.
         *
         * It returns an Operation which can be though as post-preparation well-typed
         * Operation.
         *
         * @param receiver the "column" this operation applies to. Note that
         * contrarly to the method of same name in raw expression, the receiver should always
         * be a true column.
         * @return the prepared update operation.
         */
        virtual ::shared_ptr<operation> prepare(data_dictionary::database db, const sstring& keyspace, const column_definition& receiver) const = 0;

        /**
         * @return whether this operation can be applied alongside the {@code
         * other} update (in the same UPDATE statement for the same column).
         */
        virtual bool is_compatible_with(const std::unique_ptr<raw_update>& other) const = 0;
    };

    /**
     * A parsed raw DELETE operation.
     *
     * This can be one of:
     *   - Deleting a column
     *   - Deleting an element of a collection
     *   - Deleting a field of a user-defined type
     */
    class raw_deletion {
    public:
        virtual ~raw_deletion() = default;

        /**
         * The name of the column affected by this delete operation.
         */
        virtual const column_identifier::raw& affected_column() const = 0;

        /**
         * This method validates the operation (i.e. validate it is well typed)
         * based on the specification of the column affected by the operation (i.e the
         * one returned by affectedColumn()).
         *
         * It returns an Operation which can be though as post-preparation well-typed
         * Operation.
         *
         * @param receiver the "column" this operation applies to.
         * @return the prepared delete operation.
         */
        virtual ::shared_ptr<operation> prepare(data_dictionary::database db, const sstring& keyspace, const column_definition& receiver) const = 0;
    };

    class set_value;
    class set_counter_value_from_tuple_list;

    class set_element : public raw_update {
        const expr::expression _selector;
        const expr::expression _value;
        const bool _by_uuid;
    private:
        sstring to_string(const column_definition& receiver) const;
    public:
        set_element(expr::expression selector, expr::expression value, bool by_uuid = false)
            : _selector(std::move(selector)), _value(std::move(value)), _by_uuid(by_uuid) {
        }

        virtual shared_ptr<operation> prepare(data_dictionary::database db, const sstring& keyspace, const column_definition& receiver) const override;

        virtual bool is_compatible_with(const std::unique_ptr<raw_update>& other) const override;
    };

    // Set a single field inside a user-defined type.
    class set_field : public raw_update {
        const shared_ptr<column_identifier> _field;
        const expr::expression _value;
    private:
        sstring to_string(const column_definition& receiver) const;
    public:
        set_field(shared_ptr<column_identifier> field, expr::expression value)
            : _field(std::move(field)), _value(std::move(value)) {
        }

        virtual shared_ptr<operation> prepare(data_dictionary::database db, const sstring& keyspace, const column_definition& receiver) const override;

        virtual bool is_compatible_with(const std::unique_ptr<raw_update>& other) const override;
    };

    // Delete a single field inside a user-defined type.
    // Equivalent to setting the field to null.
    class field_deletion : public raw_deletion {
        const shared_ptr<column_identifier::raw> _id;
        const shared_ptr<column_identifier> _field;
    public:
        field_deletion(shared_ptr<column_identifier::raw> id, shared_ptr<column_identifier> field)
                : _id(std::move(id)), _field(std::move(field)) {
        }

        virtual const column_identifier::raw& affected_column() const override;

        virtual shared_ptr<operation> prepare(data_dictionary::database db, const sstring& keyspace, const column_definition& receiver) const override;
    };

    class addition : public raw_update {
        const expr::expression _value;
    private:
        sstring to_string(const column_definition& receiver) const;
    public:
        addition(expr::expression value)
                : _value(std::move(value)) {
        }

        virtual shared_ptr<operation> prepare(data_dictionary::database db, const sstring& keyspace, const column_definition& receiver) const override;

        virtual bool is_compatible_with(const std::unique_ptr<raw_update>& other) const override;
    };

    class subtraction : public raw_update {
        const expr::expression _value;
    private:
        sstring to_string(const column_definition& receiver) const;
    public:
        subtraction(expr::expression value)
                : _value(std::move(value)) {
        }

        virtual shared_ptr<operation> prepare(data_dictionary::database db, const sstring& keyspace, const column_definition& receiver) const override;

        virtual bool is_compatible_with(const std::unique_ptr<raw_update>& other) const override;
    };

    class prepend : public raw_update {
        expr::expression _value;
    private:
        sstring to_string(const column_definition& receiver) const;
    public:
        prepend(expr::expression value)
                : _value(std::move(value)) {
        }

        virtual shared_ptr<operation> prepare(data_dictionary::database db, const sstring& keyspace, const column_definition& receiver) const override;

        virtual bool is_compatible_with(const std::unique_ptr<raw_update>& other) const override;
    };

    class column_deletion;

    class element_deletion : public raw_deletion {
        shared_ptr<column_identifier::raw> _id;
        expr::expression _element;
    public:
        element_deletion(shared_ptr<column_identifier::raw> id, expr::expression element)
                : _id(std::move(id)), _element(std::move(element)) {
        }
        virtual const column_identifier::raw& affected_column() const override;
        virtual shared_ptr<operation> prepare(data_dictionary::database db, const sstring& keyspace, const column_definition& receiver) const override;
    };
};

class operation_skip_if_unset : public operation {
public:
    operation_skip_if_unset(const column_definition& column, expr::expression e)
            : operation(column, e, expr::unset_bind_variable_guard(e)) {
    }
};

class operation_no_unset_support : public operation {
public:
    operation_no_unset_support(const column_definition& column, std::optional<expr::expression> e)
            : operation(column, std::move(e), expr::unset_bind_variable_guard(std::nullopt)) {
    }
};

}


namespace cql3 {

/**
 * Static helper methods and classes for lists.
 */
class lists {
    lists() = delete;
public:
    static lw_shared_ptr<column_specification> index_spec_of(const column_specification&);
    static lw_shared_ptr<column_specification> value_spec_of(const column_specification&);
    static lw_shared_ptr<column_specification> uuid_index_spec_of(const column_specification&);
public:
    class setter : public operation_skip_if_unset {
    public:
        setter(const column_definition& column, expr::expression e)
                : operation_skip_if_unset(column, std::move(e)) {
        }
        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override;
        static void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params, const column_definition& column, const cql3::raw_value& value);
    };

    class setter_by_index : public operation_skip_if_unset {
    protected:
        expr::expression _idx;
    public:
        setter_by_index(const column_definition& column, expr::expression idx, expr::expression e)
            : operation_skip_if_unset(column, std::move(e)), _idx(std::move(idx)) {
        }
        virtual bool requires_read() const override;
        virtual void fill_prepare_context(prepare_context& ctx) override;
        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override;
    };

    class setter_by_uuid : public setter_by_index {
    public:
        setter_by_uuid(const column_definition& column, expr::expression idx, expr::expression e)
            : setter_by_index(column, std::move(idx), std::move(e)) {
        }
        virtual bool requires_read() const override;
        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override;
    };

    class appender : public operation_skip_if_unset {
    public:
        using operation_skip_if_unset::operation_skip_if_unset;
        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override;
    };

    static void do_append(const cql3::raw_value& list_value,
            mutation& m,
            const clustering_key_prefix& prefix,
            const column_definition& column,
            const update_parameters& params);

    class prepender : public operation_skip_if_unset {
    public:
        using operation_skip_if_unset::operation_skip_if_unset;
        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override;
    };

    class discarder : public operation_skip_if_unset {
    public:
        discarder(const column_definition& column, expr::expression e)
                : operation_skip_if_unset(column, std::move(e)) {
        }
        virtual bool requires_read() const override;
        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override;
    };

    class discarder_by_index : public operation_skip_if_unset {
    public:
        discarder_by_index(const column_definition& column, expr::expression idx)
                : operation_skip_if_unset(column, std::move(idx)) {
        }
        virtual bool requires_read() const override;
        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override;
    };
};

}



// A variant type that can hold either an atomic_cell, or a serialized collection.
// Which type is stored is determined by the schema.
// Has an "empty" state.
// Objects moved-from are left in an empty state.
class atomic_cell_or_collection final {
    managed_bytes _data;
private:
    atomic_cell_or_collection(managed_bytes&& data) : _data(std::move(data)) {}
public:
    atomic_cell_or_collection() = default;
    atomic_cell_or_collection(atomic_cell_or_collection&&) = default;
    atomic_cell_or_collection(const atomic_cell_or_collection&) = delete;
    atomic_cell_or_collection& operator=(atomic_cell_or_collection&&) = default;
    atomic_cell_or_collection& operator=(const atomic_cell_or_collection&) = delete;
    atomic_cell_or_collection(atomic_cell ac) : _data(std::move(ac._data)) {}
    atomic_cell_or_collection(const abstract_type& at, atomic_cell_view acv);
    static atomic_cell_or_collection from_atomic_cell(atomic_cell data) { return { std::move(data._data) }; }
    atomic_cell_view as_atomic_cell(const column_definition& cdef) const { return atomic_cell_view::from_bytes(*cdef.type, _data); }
    atomic_cell_mutable_view as_mutable_atomic_cell(const column_definition& cdef) { return atomic_cell_mutable_view::from_bytes(*cdef.type, _data); }
    atomic_cell_or_collection(collection_mutation cm) : _data(std::move(cm._data)) { }
    atomic_cell_or_collection copy(const abstract_type&) const;
    explicit operator bool() const {
        return !_data.empty();
    }
    static constexpr bool can_use_mutable_view() {
        return true;
    }
    static atomic_cell_or_collection from_collection_mutation(collection_mutation data) { return std::move(data._data); }
    collection_mutation_view as_collection_mutation() const;
    bytes_view serialize() const;
    bool equals(const abstract_type& type, const atomic_cell_or_collection& other) const;
    size_t external_memory_usage(const abstract_type&) const;

    class printer {
        const column_definition& _cdef;
        const atomic_cell_or_collection& _cell;
    public:
        printer(const column_definition& cdef, const atomic_cell_or_collection& cell)
            : _cdef(cdef), _cell(cell) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;

        friend std::ostream& operator<<(std::ostream&, const printer&);
    };
    friend std::ostream& operator<<(std::ostream&, const printer&);
};



// Not part of atomic_cell.hh to avoid cyclic dependency between types.hh and atomic_cell.hh

template<>
struct appending_hash<collection_mutation_view> {
    template<typename Hasher>
    void operator()(Hasher& h, collection_mutation_view cell, const column_definition& cdef) const {
        cell.with_deserialized(*cdef.type, [&] (collection_mutation_view_description m_view) {
            ::feed_hash(h, m_view.tomb);
            for (auto&& key_and_value : m_view.cells) {
                ::feed_hash(h, key_and_value.first);
                ::feed_hash(h, key_and_value.second, cdef);
            }
      });
    }
};

template<>
struct appending_hash<atomic_cell_view> {
    template<typename Hasher>
    void operator()(Hasher& h, atomic_cell_view cell, const column_definition& cdef) const {
        feed_hash(h, cell.is_live());
        feed_hash(h, cell.timestamp());
        if (cell.is_live()) {
            if (cdef.is_counter()) {
                ::feed_hash(h, counter_cell_view(cell));
                return;
            }
            if (cell.is_live_and_has_ttl()) {
                feed_hash(h, cell.expiry());
                feed_hash(h, cell.ttl());
            }
            feed_hash(h, cell.value());
        } else {
            feed_hash(h, cell.deletion_time());
        }
    }
};

template<>
struct appending_hash<atomic_cell> {
    template<typename Hasher>
    void operator()(Hasher& h, const atomic_cell& cell, const column_definition& cdef) const {
        feed_hash(h, static_cast<atomic_cell_view>(cell), cdef);
    }
};

template<>
struct appending_hash<collection_mutation> {
    template<typename Hasher>
    void operator()(Hasher& h, const collection_mutation& cm, const column_definition& cdef) const {
        feed_hash(h, static_cast<collection_mutation_view>(cm), cdef);
    }
};

template<>
struct appending_hash<atomic_cell_or_collection> {
    template<typename Hasher>
    void operator()(Hasher& h, const atomic_cell_or_collection& c, const column_definition& cdef) const {
        if (cdef.is_atomic()) {
            feed_hash(h, c.as_atomic_cell(cdef), cdef);
        } else {
            feed_hash(h, c.as_collection_mutation(), cdef);
        }
    }
};



// Calculates a hash of a mutation_partition which is consistent with
// mutation equality. For any equal mutations, no matter which schema
// version they were generated under, the hash fed will be the same for both of them.
template<typename Hasher>
class hashing_partition_visitor : public mutation_partition_visitor {
    Hasher& _h;
    const schema& _s;
public:
    hashing_partition_visitor(Hasher& h, const schema& s)
        : _h(h)
        , _s(s)
    { }

    virtual void accept_partition_tombstone(tombstone t) override {
        feed_hash(_h, t);
    }

    virtual void accept_static_cell(column_id id, atomic_cell_view cell) override {
        auto&& col = _s.static_column_at(id);
        feed_hash(_h, col.name());
        feed_hash(_h, col.type->name());
        feed_hash(_h, cell, col);
    }

    virtual void accept_static_cell(column_id id, collection_mutation_view cell) override {
        auto&& col = _s.static_column_at(id);
        feed_hash(_h, col.name());
        feed_hash(_h, col.type->name());
        feed_hash(_h, cell, col);
    }

    virtual void accept_row_tombstone(const range_tombstone& rt) override {
        feed_hash(_h, rt, _s);
    }

    virtual void accept_row(position_in_partition_view pos, const row_tombstone& deleted_at, const row_marker& rm, is_dummy dummy, is_continuous continuous) override {
        if (dummy) {
            return;
        }
        feed_hash(_h, pos.key(), _s);
        feed_hash(_h, deleted_at);
        feed_hash(_h, rm);
    }

    virtual void accept_row_cell(column_id id, atomic_cell_view cell) override {
        auto&& col = _s.regular_column_at(id);
        feed_hash(_h, col.name());
        feed_hash(_h, col.type->name());
        feed_hash(_h, cell, col);
    }

    virtual void accept_row_cell(column_id id, collection_mutation_view cell) override {
        auto&& col = _s.regular_column_at(id);
        feed_hash(_h, col.name());
        feed_hash(_h, col.type->name());
        feed_hash(_h, cell, col);
    }
};


#include <boost/intrusive/set.hpp>
#include <optional>

namespace bi = boost::intrusive;

/**
 * Represents a ranged deletion operation. Can be empty.
 */
class range_tombstone final {
public:
    clustering_key_prefix start;
    bound_kind start_kind;
    clustering_key_prefix end;
    bound_kind end_kind;
    tombstone tomb;
    range_tombstone(clustering_key_prefix start, bound_kind start_kind, clustering_key_prefix end, bound_kind end_kind, tombstone tomb)
            : start(std::move(start))
            , start_kind(start_kind)
            , end(std::move(end))
            , end_kind(end_kind)
            , tomb(std::move(tomb))
    { }
    range_tombstone(bound_view start, bound_view end, tombstone tomb)
            : range_tombstone(start.prefix(), start.kind(), end.prefix(), end.kind(), std::move(tomb))
    { }

    // Can be called only when both start and end are !is_static_row && !is_clustering_row().
    range_tombstone(position_in_partition_view start, position_in_partition_view end, tombstone tomb)
            : range_tombstone(start.as_start_bound_view(), end.as_end_bound_view(), tomb)
    {}
    range_tombstone(clustering_key_prefix&& start, clustering_key_prefix&& end, tombstone tomb)
            : range_tombstone(std::move(start), bound_kind::incl_start, std::move(end), bound_kind::incl_end, std::move(tomb))
    { }
    // IDL constructor
    range_tombstone(clustering_key_prefix&& start, tombstone tomb, bound_kind start_kind, clustering_key_prefix&& end, bound_kind end_kind)
            : range_tombstone(std::move(start), start_kind, std::move(end), end_kind, std::move(tomb))
    { }
    range_tombstone(range_tombstone&& rt) noexcept
            : range_tombstone(std::move(rt.start), rt.start_kind, std::move(rt.end), rt.end_kind, std::move(rt.tomb)) {
    }
    range_tombstone(const range_tombstone& rt)
            : range_tombstone(rt.start, rt.start_kind, rt.end, rt.end_kind, rt.tomb)
    { }
    range_tombstone& operator=(range_tombstone&& rt) noexcept {
        start = std::move(rt.start);
        start_kind = rt.start_kind;
        end = std::move(rt.end);
        end_kind = rt.end_kind;
        tomb = std::move(rt.tomb);
        return *this;
    }
    range_tombstone& operator=(const range_tombstone& rt) {
        start = rt.start;
        start_kind = rt.start_kind;
        end = rt.end;
        end_kind = rt.end_kind;
        tomb = rt.tomb;
        return *this;
    }
    const bound_view start_bound() const {
        return bound_view(start, start_kind);
    }
    const bound_view end_bound() const {
        return bound_view(end, end_kind);
    }
    // Range tombstone covers all rows with positions p such that: position() <= p < end_position()
    position_in_partition_view position() const;
    position_in_partition_view end_position() const;
    bool empty() const noexcept {
        return !bool(tomb);
    }
    explicit operator bool() const noexcept {
        return bool(tomb);
    }
    bool equal(const schema& s, const range_tombstone& other) const {
        return tomb == other.tomb && start_bound().equal(s, other.start_bound()) && end_bound().equal(s, other.end_bound());
    }
    struct compare {
        bound_view::compare _c;
        compare(const schema& s) : _c(s) {}
        bool operator()(const range_tombstone& rt1, const range_tombstone& rt2) const {
            return _c(rt1.start_bound(), rt2.start_bound());
        }
    };
    friend void swap(range_tombstone& rt1, range_tombstone& rt2) noexcept {
        range_tombstone tmp = std::move(rt2);
        rt2 = std::move(rt1);
        rt1 = std::move(tmp);
    }

    static bool is_single_clustering_row_tombstone(const schema& s, const clustering_key_prefix& start,
        bound_kind start_kind, const clustering_key_prefix& end, bound_kind end_kind)
    {
        return start.is_full(s) && start_kind == bound_kind::incl_start
            && end_kind == bound_kind::incl_end && start.equal(s, end);
    }

    // Applies src to this. The tombstones may be overlapping.
    // If the tombstone with larger timestamp has the smaller range the remainder
    // is returned, it guaranteed not to overlap with this.
    // The start bounds of this and src are required to be equal. The start bound
    // of this is not changed. The start bound of the remainder (if there is any)
    // is larger than the end bound of this.
    std::optional<range_tombstone> apply(const schema& s, range_tombstone&& src);

    // Intersects the range of this tombstone with [pos, +inf) and replaces
    // the range of the tombstone if there is an overlap.
    // Returns true if there is an overlap. When returns false, the tombstone
    // is not modified.
    //
    // pos must satisfy:
    //   1) before_all_clustered_rows() <= pos
    //   2) !pos.is_clustering_row() - because range_tombstone bounds can't represent such positions
    bool trim_front(const schema& s, position_in_partition_view pos) {
        position_in_partition::less_compare less(s);
        if (!less(pos, end_position())) {
            return false;
        }
        if (less(position(), pos)) {
            set_start(pos);
        }
        return true;
    }

    // Intersects the range of this tombstone with [start, end) and replaces
    // the range of the tombstone if there is an overlap.
    // Returns true if there is an overlap and false otherwise. When returns false, the tombstone
    // is not modified.
    //
    // start and end must satisfy:
    //   1) has_clustering_key() == true
    //   2) is_clustering_row() == false
    //
    // Also: start <= end
    bool trim(const schema& s, position_in_partition_view start, position_in_partition_view end) {
        position_in_partition::less_compare less(s);
        if (!less(start, end_position())) {
            return false;
        }
        if (!less(position(), end)) {
            return false;
        }
        if (less(position(), start)) {
            set_start(start);
        }
        if (less(end, end_position())) {
            set_end(s, end);
        }
        return true;
    }

    // Assumes !pos.is_clustering_row(), because range_tombstone bounds can't represent such positions
    void set_start(position_in_partition_view pos) {
        bound_view new_start = pos.as_start_bound_view();
        start = new_start.prefix();
        start_kind = new_start.kind();
    }

    // Assumes !pos.is_clustering_row(), because range_tombstone bounds can't represent such positions
    void set_end(const schema& s, position_in_partition_view pos) {
        bound_view new_end = pos.as_end_bound_view();
        end = new_end.prefix();
        end_kind = new_end.kind();
    }

    // Swap bounds to reverse range-tombstone -- as if it came from a table with
    // reverse native order. See docs/dev/reverse-reads.md.
    void reverse() {
        std::swap(start, end);
        std::swap(start_kind, end_kind);
        start_kind = reverse_kind(start_kind);
        end_kind = reverse_kind(end_kind);
    }

    size_t external_memory_usage(const schema&) const noexcept {
        return start.external_memory_usage() + end.external_memory_usage();
    }

    size_t minimal_external_memory_usage(const schema&) const noexcept {
        return start.minimal_external_memory_usage() + end.minimal_external_memory_usage();
    }

    size_t memory_usage(const schema& s) const noexcept {
        return sizeof(range_tombstone) + external_memory_usage(s);
    }

    size_t minimal_memory_usage(const schema& s) const noexcept {
        return sizeof(range_tombstone) + minimal_external_memory_usage(s);
    }
};

template<>
struct appending_hash<range_tombstone>  {
    template<typename Hasher>
    void operator()(Hasher& h, const range_tombstone& value, const schema& s) const {
        feed_hash(h, value.start, s);
        // For backward compatibility, don't consider new fields if
        // this could be an old-style, overlapping, range tombstone.
        if (!value.start.equal(s, value.end) || value.start_kind != bound_kind::incl_start || value.end_kind != bound_kind::incl_end) {
            feed_hash(h, value.start_kind);
            feed_hash(h, value.end, s);
            feed_hash(h, value.end_kind);
        }
        feed_hash(h, value.tomb);
    }
};

// The accumulator expects the incoming range tombstones and clustered rows to
// follow the ordering used by the mutation readers.
//
// Unless the accumulator is in the reverse mode, after apply(rt) or
// tombstone_for_row(ck) are called there are followng restrictions for
// subsequent calls:
//  - apply(rt1) can be invoked only if rt.start_bound() < rt1.start_bound()
//    and ck < rt1.start_bound()
//  - tombstone_for_row(ck1) can be invoked only if rt.start_bound() < ck1
//    and ck < ck1
//
// In other words position in partition of the mutation fragments passed to the
// accumulator must be increasing.
//
// If the accumulator was created with the reversed flag set it expects the
// stream of the range tombstone to come from a reverse partitions and follow
// the ordering that they use. In particular, the restrictions from non-reversed
// mode change to:
//  - apply(rt1) can be invoked only if rt.end_bound() > rt1.end_bound() and
//    ck > rt1.end_bound()
//  - tombstone_for_row(ck1) can be invoked only if rt.end_bound() > ck1 and
//    ck > ck1.
class range_tombstone_accumulator {
    bound_view::compare _cmp;
    tombstone _partition_tombstone;
    std::deque<range_tombstone> _range_tombstones;
    tombstone _current_tombstone;
private:
    void update_current_tombstone();
    void drop_unneeded_tombstones(const clustering_key_prefix& ck, int w = 0);
public:
    explicit range_tombstone_accumulator(const schema& s)
        : _cmp(s) { }

    void set_partition_tombstone(tombstone t) {
        _partition_tombstone = t;
        update_current_tombstone();
    }

    tombstone get_partition_tombstone() const {
        return _partition_tombstone;
    }

    tombstone current_tombstone() const {
        return _current_tombstone;
    }

    tombstone tombstone_for_row(const clustering_key_prefix& ck) {
        drop_unneeded_tombstones(ck);
        return _current_tombstone;
    }

    const std::deque<range_tombstone>& range_tombstones_for_row(const clustering_key_prefix& ck) {
        drop_unneeded_tombstones(ck);
        return _range_tombstones;
    }

    std::deque<range_tombstone> range_tombstones() && {
        return std::move(_range_tombstones);
    }

    void apply(range_tombstone rt);

    void clear();
};

template<>
struct fmt::formatter<range_tombstone> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const range_tombstone& rt, FormatContext& ctx) const {
        if (rt) {
            return fmt::format_to(ctx.out(), "{{range_tombstone: start={}, end={}, {}}}",
                                  rt.position(), rt.end_position(), rt.tomb);
        } else {
            return fmt::format_to(ctx.out(), "{{range_tombstone: none}}");
        }
    }
};



#include <seastar/util/bool_class.hh>
#include <seastar/util/noncopyable_function.hh>
#include <seastar/core/preempt.hh>


class is_preemptible_tag;
using is_preemptible = bool_class<is_preemptible_tag>;

/// A function which decides when to preempt.
/// If it returns true then the algorithm will be interrupted.
using preemption_check = noncopyable_function<bool() noexcept>;

inline
preemption_check default_preemption_check() {
    return [] () noexcept {
        return seastar::need_preempt();
    };
}

inline
preemption_check never_preempt() {
    return [] () noexcept {
        return false;
    };
}

inline
preemption_check always_preempt() {
    return [] () noexcept {
        return true;
    };
}



#include <seastar/util/defer.hh>
#include <iosfwd>
#include <variant>

class position_in_partition_view;

class range_tombstone_entry {
    range_tombstone _tombstone;
    bi::set_member_hook<bi::link_mode<bi::auto_unlink>> _link;

public:
    struct compare {
        range_tombstone::compare _c;
        compare(const schema& s) : _c(s) {}
        bool operator()(const range_tombstone_entry& rt1, const range_tombstone_entry& rt2) const {
            return _c(rt1._tombstone, rt2._tombstone);
        }
    };

    using container_type = bi::set<range_tombstone_entry,
            bi::member_hook<range_tombstone_entry, bi::set_member_hook<bi::link_mode<bi::auto_unlink>>, &range_tombstone_entry::_link>,
            bi::compare<range_tombstone_entry::compare>,
            bi::constant_time_size<false>>;

    range_tombstone_entry(const range_tombstone_entry& rt)
        : _tombstone(rt._tombstone)
    {
    }

    range_tombstone_entry(range_tombstone_entry&& rt) noexcept
            : _tombstone(std::move(rt._tombstone))
    {
        update_node(rt._link);
    }
    range_tombstone_entry(range_tombstone&& rt) noexcept
        : _tombstone(std::move(rt))
    { }

    range_tombstone_entry& operator=(range_tombstone_entry&& rt) noexcept {
        update_node(rt._link);
        _tombstone = std::move(rt._tombstone);
        return *this;
    }

    range_tombstone& tombstone() noexcept { return _tombstone; }
    const range_tombstone& tombstone() const noexcept { return _tombstone; }

    const bound_view start_bound() const { return _tombstone.start_bound(); }
    const bound_view end_bound() const { return _tombstone.end_bound(); }
    position_in_partition_view position() const { return _tombstone.position(); }
    position_in_partition_view end_position() const { return _tombstone.end_position(); }

    size_t memory_usage(const schema& s) const noexcept {
        return sizeof(range_tombstone_entry) + _tombstone.external_memory_usage(s);
    }

private:
    void update_node(bi::set_member_hook<bi::link_mode<bi::auto_unlink>>& other_link) noexcept {
        if (other_link.is_linked()) {
            // Move the link in case we're being relocated by LSA.
            container_type::node_algorithms::replace_node(other_link.this_ptr(), _link.this_ptr());
            container_type::node_algorithms::init(other_link.this_ptr());
        }
    }
};

template <>
struct fmt::formatter<range_tombstone_entry> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const range_tombstone_entry& rt, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{}", rt.tombstone());
    }
};

class range_tombstone_list final {
    using range_tombstones_type = range_tombstone_entry::container_type;
    class insert_undo_op {
        const range_tombstone_entry& _new_rt;
    public:
        insert_undo_op(const range_tombstone_entry& new_rt)
                : _new_rt(new_rt) { }
        void undo(const schema& s, range_tombstone_list& rt_list) noexcept;
    };
    class erase_undo_op {
        alloc_strategy_unique_ptr<range_tombstone_entry> _rt;
    public:
        erase_undo_op(range_tombstone_entry& rt)
                : _rt(&rt) { }
        void undo(const schema& s, range_tombstone_list& rt_list) noexcept;
    };
    class update_undo_op {
        range_tombstone _old_rt;
        const range_tombstone_entry& _new_rt;
    public:
        update_undo_op(range_tombstone&& old_rt, const range_tombstone_entry& new_rt)
                : _old_rt(std::move(old_rt)), _new_rt(new_rt) { }
        void undo(const schema& s, range_tombstone_list& rt_list) noexcept;
    };
    class reverter {
    private:
        using op = std::variant<erase_undo_op, insert_undo_op, update_undo_op>;
        utils::chunked_vector<op> _ops;
        const schema& _s;
    protected:
        range_tombstone_list& _dst;
    public:
        reverter(const schema& s, range_tombstone_list& dst)
                : _s(s)
                , _dst(dst) { }
        virtual ~reverter() {
            revert();
        }
        reverter(reverter&&) = default;
        reverter(const reverter&) = delete;
        reverter& operator=(reverter&) = delete;
        virtual range_tombstones_type::iterator insert(range_tombstones_type::iterator it, range_tombstone_entry& new_rt);
        virtual range_tombstones_type::iterator erase(range_tombstones_type::iterator it);
        virtual void update(range_tombstones_type::iterator it, range_tombstone&& new_rt);
        void revert() noexcept;
        void cancel() noexcept {
            _ops.clear();
        }
    };
    class nop_reverter : public reverter {
    public:
        nop_reverter(const schema& s, range_tombstone_list& rt_list)
                : reverter(s, rt_list) { }
        virtual range_tombstones_type::iterator insert(range_tombstones_type::iterator it, range_tombstone_entry& new_rt) override;
        virtual range_tombstones_type::iterator erase(range_tombstones_type::iterator it) override;
        virtual void update(range_tombstones_type::iterator it, range_tombstone&& new_rt) override;
    };
private:
    range_tombstones_type _tombstones;
public:
    // ForwardIterator<range_tombstone>
    using iterator = range_tombstones_type::iterator;
    using reverse_iterator = range_tombstones_type::reverse_iterator;
    using const_iterator = range_tombstones_type::const_iterator;

    struct copy_comparator_only { };
    range_tombstone_list(const schema& s)
        : _tombstones(range_tombstone_entry::compare(s))
    { }
    range_tombstone_list(const range_tombstone_list& x, copy_comparator_only)
        : _tombstones(x._tombstones.key_comp())
    { }
    range_tombstone_list(const range_tombstone_list&);
    range_tombstone_list& operator=(range_tombstone_list&) = delete;
    range_tombstone_list(range_tombstone_list&&) = default;
    range_tombstone_list& operator=(range_tombstone_list&&) = default;
    ~range_tombstone_list();
    size_t size() const noexcept {
        return _tombstones.size();
    }
    bool empty() const noexcept {
        return _tombstones.empty();
    }
    auto begin() noexcept {
        return _tombstones.begin();
    }
    auto begin() const noexcept {
        return _tombstones.begin();
    }
    auto rbegin() noexcept {
        return _tombstones.rbegin();
    }
    auto rbegin() const noexcept {
        return _tombstones.rbegin();
    }
    auto end() noexcept {
        return _tombstones.end();
    }
    auto end() const noexcept {
        return _tombstones.end();
    }
    auto rend() noexcept {
        return _tombstones.rend();
    }
    auto rend() const noexcept {
        return _tombstones.rend();
    }
    void apply(const schema& s, const bound_view& start_bound, const bound_view& end_bound, tombstone tomb) {
        apply(s, start_bound.prefix(), start_bound.kind(), end_bound.prefix(), end_bound.kind(), std::move(tomb));
    }
    void apply(const schema& s, const range_tombstone& rt) {
        apply(s, rt.start, rt.start_kind, rt.end, rt.end_kind, rt.tomb);
    }
    void apply(const schema& s, range_tombstone&& rt) {
        apply(s, std::move(rt.start), rt.start_kind, std::move(rt.end), rt.end_kind, std::move(rt.tomb));
    }
    void apply(const schema& s, clustering_key_prefix start, bound_kind start_kind,
               clustering_key_prefix end, bound_kind end_kind, tombstone tomb) {
        nop_reverter rev(s, *this);
        apply_reversibly(s, std::move(start), start_kind, std::move(end), end_kind, std::move(tomb), rev);
    }
    // Monotonic exception guarantees. In case of failure the object will contain at least as much information as before the call.
    void apply_monotonically(const schema& s, const range_tombstone& rt);
    // Merges another list with this object.
    // Monotonic exception guarantees. In case of failure the object will contain at least as much information as before the call.
    void apply_monotonically(const schema& s, const range_tombstone_list& list);
    /// Merges another list with this object.
    /// The other list must be governed by the same allocator as this object.
    ///
    /// Monotonic exception guarantees. In case of failure the object will contain at least as much information as before the call.
    /// The other list will be left in a state such that it would still commute with this object to the same state as it
    /// would if the call didn't fail.
    stop_iteration apply_monotonically(const schema& s, range_tombstone_list&& list, is_preemptible = is_preemptible::no);
public:
    tombstone search_tombstone_covering(const schema& s, const clustering_key_prefix& key) const;

    using iterator_range = boost::iterator_range<const_iterator>;
    // Returns range tombstones which overlap with given range
    iterator_range slice(const schema& s, const query::clustering_range&) const;
    // Returns range tombstones which overlap with [start, end)
    iterator_range slice(const schema& s, position_in_partition_view start, position_in_partition_view end) const;

    // Returns range tombstones with ends inside [start, before).
    iterator_range lower_slice(const schema& s, bound_view start, position_in_partition_view before) const;
    // Returns range tombstones with starts inside (after, end].
    iterator_range upper_slice(const schema& s, position_in_partition_view after, bound_view end) const;

    iterator erase(const_iterator, const_iterator);

    // Pops the first element and bans (in theory) further additions
    // The list is assumed not to be empty
    range_tombstone pop_front_and_lock() {
        range_tombstone_entry* rt = _tombstones.unlink_leftmost_without_rebalance();
        assert(rt != nullptr);
        auto _ = seastar::defer([rt] () noexcept { current_deleter<range_tombstone_entry>()(rt); });
        return std::move(rt->tombstone());
    }

    // Ensures that every range tombstone is strictly contained within given clustering ranges.
    // Preserves all information which may be relevant for rows from that ranges.
    void trim(const schema& s, const query::clustering_row_ranges&);
    range_tombstone_list difference(const schema& s, const range_tombstone_list& rt_list) const;
    // Erases the range tombstones for which filter returns true.
    template <typename Pred>
    requires std::is_invocable_r_v<bool, Pred, const range_tombstone&>
    void erase_where(Pred filter) {
        auto it = begin();
        while (it != end()) {
            if (filter(it->tombstone())) {
                it = _tombstones.erase_and_dispose(it, current_deleter<range_tombstone_entry>());
            } else {
                ++it;
            }
        }
    }
    void clear() noexcept {
        _tombstones.clear_and_dispose(current_deleter<range_tombstone_entry>());
    }

    range_tombstone pop(iterator it) {
        range_tombstone rt(std::move(it->tombstone()));
        _tombstones.erase_and_dispose(it, current_deleter<range_tombstone_entry>());
        return rt;
    }

    // Removes elements of this list in batches.
    // Returns stop_iteration::yes iff there is no more elements to remove.
    stop_iteration clear_gently() noexcept;
    void apply(const schema& s, const range_tombstone_list& rt_list);
    // See reversibly_mergeable.hh
    reverter apply_reversibly(const schema& s, range_tombstone_list& rt_list);

    bool equal(const schema&, const range_tombstone_list&) const;
    size_t external_memory_usage(const schema& s) const noexcept {
        size_t result = 0;
        for (auto& rtb : _tombstones) {
            result += rtb.tombstone().memory_usage(s);
        }
        return result;
    }
private:
    void apply_reversibly(const schema& s, clustering_key_prefix start, bound_kind start_kind,
                          clustering_key_prefix end, bound_kind end_kind, tombstone tomb, reverter& rev);

    void insert_from(const schema& s,
                     range_tombstones_type::iterator it,
                     position_in_partition start,
                     position_in_partition end,
                     tombstone tomb,
                     reverter& rev);

    range_tombstones_type::iterator find(const schema& s, const range_tombstone_entry& rt);
};

template <>
struct fmt::formatter<range_tombstone_list> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const range_tombstone_list& list, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{{{}}}", fmt::join(list, ", "));
    }
};


#include <type_traits>
#include <seastar/util/concepts.hh>
#include <compare>

template <typename Func, typename T>
concept Disposer = requires (Func f, T* val) { 
    { f(val) } noexcept -> std::same_as<void>;
};

template <typename Key1, typename Key2, typename Less>
concept LessComparable = requires (const Key1& a, const Key2& b, Less less) {
    { less(a, b) } -> std::same_as<bool>;
    { less(b, a) } -> std::same_as<bool>;
};

template <typename Key1, typename Key2, typename Less>
concept LessNothrowComparable = LessComparable<Key1, Key2, Less> && std::is_nothrow_invocable_v<Less, Key1, Key2>;

template <typename T1, typename T2, typename Compare>
concept Comparable = requires (const T1& a, const T2& b, Compare cmp) {
    // The Comparable is trichotomic comparator that should return 
    //   negative value when a < b
    //   zero when a == b
    //   positive value when a > b
    { cmp(a, b) } -> std::same_as<std::strong_ordering>;
};

#include <atomic>

namespace utils {

/*
 * The neat_id class is purely a debugging thing -- when reading
 * the logs with object IDs in it it's more handy to look at those
 * consisting * of 1-3 digits, rather than 16 hex-digits of a printed
 * pointer.
 *
 * Embed with [[no_unique_address]] tag for memory efficiency
 */
template <bool Debug>
struct neat_id {
    unsigned int operator()() const noexcept { return reinterpret_cast<uintptr_t>(this); }
};

template <>
struct neat_id<true> {
    unsigned int _id;
    static unsigned int _next() noexcept {
        static std::atomic<unsigned int> rover {1};
        return rover.fetch_add(1);
    }

    neat_id() noexcept : _id(_next()) {}
    unsigned int operator()() const noexcept { return _id; }
};

} // namespace


#include <boost/intrusive/parent_from_member.hpp>
#include <seastar/util/alloc_failure_injector.hh>
#include <cassert>
#include <fmt/core.h>

namespace intrusive_b {

template <typename Func, typename T>
concept KeyCloner = requires (Func f, T* val) {
    { f(val) } -> std::same_as<T*>;
};

/*
 * The KeyPointer is any wrapper that carries a "real" key one board and that
 * can release it, thus giving its ownership to the tree. It's used in insert()
 * methods where either key conflict or an exception may occur. In either case
 * the key will not be released and freeing it is up to the caller.
 */
template <typename Pointer, typename T>
concept KeyPointer = std::is_nothrow_move_constructible_v<Pointer> &&
    requires (Pointer p) { { *p } -> std::same_as<T&>; } &&
    requires (Pointer p) { { p.release() } noexcept -> std::same_as<T*>; };

enum class with_debug { no, yes };
enum class key_search { linear, binary, both };

class member_hook;

// The LinearThreshold is explained below, see NODE_LINEAR flag
template <typename Key, member_hook Key::* Hook, typename Compare, size_t NodeSize, size_t LinearThreshold, key_search Search, with_debug Debug> class node;
template <typename Key, member_hook Key::*, typename Compare, size_t NodeSize, size_t LinearThreshold> class validator;

// For .{do_something_with_data}_and_dispose methods below
template <typename T>
void default_dispose(T* value) noexcept { }

using key_index = size_t;
using kid_index = size_t;

/*
 * The key's member_hook must point to something that's independet from
 * the tree's template parameters, so here's this base. It carries the
 * bare minimum of information needed for member_hook to operate (see
 * the iterator::erase()).
 */
class node_base {
    template <typename K, member_hook K::* H, typename C, size_t NS, size_t LT, key_search KS, with_debug D> friend class node;
    node_base(unsigned short n, unsigned short cap, unsigned short f) noexcept : num_keys(n), flags(f), capacity(cap) {}

public:
    unsigned short num_keys;
    unsigned short flags;
    unsigned short capacity; // used by linear node only

    /*
     * Each node keeps pointers on keys, not their values. This allows keeping
     * iterators valid after insert/remove.
     *
     * The size of this array is zero, because we don't know it. The real memory
     * for it is reserved in class node.
     */
    member_hook* keys[0];

    static constexpr unsigned short NODE_ROOT = 0x1;
    static constexpr unsigned short NODE_LEAF = 0x2;
    static constexpr unsigned short NODE_LEFTMOST = 0x4; // leaf with smallest keys in the tree
    static constexpr unsigned short NODE_RIGHTMOST = 0x8; // leaf with greatest keys in the tree
    /*
     * Linear node is the root leaf that grows above the NodeSize
     * limit up to resching the LinearThreshold number of keys.
     * After this the root leaf is shattered into a small tree,
     * then B-tree works as usual.
     *
     * The backward (small tree -> linear node) transition is not
     * performed, so the root leaf can be either linear, or regular,
     * thus the explicit flag.
     */
    static constexpr unsigned short NODE_LINEAR = 0x10;
    /*
     * Inline node is embedded into tree itself and is capabale
     * of carrying a single key.
     */
    static constexpr unsigned short NODE_INLINE = 0x20;

    struct inline_tag{};
    node_base(inline_tag) noexcept : num_keys(0), flags(NODE_ROOT | NODE_LEAF | NODE_INLINE), capacity(1) {}

    bool is_root() const noexcept { return flags & NODE_ROOT; }
    bool is_leaf() const noexcept { return flags & NODE_LEAF; }
    bool is_leftmost() const noexcept { return flags & NODE_LEFTMOST; }
    bool is_rightmost() const noexcept { return flags & NODE_RIGHTMOST; }
    bool is_linear() const noexcept { return flags & NODE_LINEAR; }
    bool is_inline() const noexcept { return flags & NODE_INLINE; }

    node_base(const node_base&) = delete;
    node_base(node_base&&) = delete;

    key_index index_for(const member_hook* hook) const noexcept {
        for (key_index i = 0; i < num_keys; i++) {
            if (keys[i] == hook) {
                return i;
            }
        }

        std::abort();
    }

    bool empty() const noexcept { return num_keys == 0; }

private:
    friend class member_hook;

    void reattach(member_hook* to, member_hook* from) noexcept {
        key_index idx = index_for(from);
        keys[idx] = to;
    }
};

/*
 * Struct that's to be embedded into the key. Should be kept as small as possible.
 */
class member_hook {
    template <typename K, member_hook K::* H, typename C, size_t NS, size_t LT> friend class validator;
    template <typename K, member_hook K::* H, typename C, size_t NS, size_t LT, key_search KS, with_debug D> friend class node;

private:
    node_base* _node = nullptr;

public:
    bool attached() const noexcept { return _node != nullptr; }
    node_base* node() const noexcept { return _node; }

    void attach_first(node_base& to) noexcept {
        assert(to.num_keys == 0);
        to.num_keys = 1;
        to.keys[0] = this;
        _node = &to;
    }

    member_hook() noexcept = default;
    member_hook(const member_hook&) = delete;
    ~member_hook() {
        assert(!attached());
    }

    member_hook(member_hook&& other) noexcept : _node(other._node) {
        if (attached()) {
            _node->reattach(this, &other);
            other._node = nullptr;
        }
    }

    template <typename K, member_hook K::* Hook>
    const K* to_key() const noexcept {
        return boost::intrusive::get_parent_from_member(this, Hook);
    }

    template <typename K, member_hook K::* Hook>
    K* to_key() noexcept {
        return boost::intrusive::get_parent_from_member(this, Hook);
    }
};

struct stats {
    unsigned long nodes;
    std::vector<unsigned long> nodes_filled;
    unsigned long leaves;
    std::vector<unsigned long> leaves_filled;
    unsigned long linear_keys;
};

/*
 * The tree itself.
 * Equipped with constant time begin() and end() and the iterator, that
 * scans through sorted keys and is not invalidated on insert/remove.
 *
 * The NodeSize parameter describes the amount of keys to be held on each
 * node. Inner nodes will thus have N+1 pointers on sub-trees.
 */

template <typename Key, member_hook Key::* Hook, typename Compare, size_t NodeSize, size_t LinearThreshold, key_search Search, with_debug Debug = with_debug::no>
requires Comparable<Key, Key, Compare>
class tree {
    // Sanity not to allow slow key-search in non-debug mode
    static_assert(Debug == with_debug::yes || Search != key_search::both);

public:
    friend class node<Key, Hook, Compare, NodeSize, LinearThreshold, Search, Debug>;
    friend class validator<Key, Hook, Compare, NodeSize, LinearThreshold>;

    using node = class node<Key, Hook, Compare, NodeSize, LinearThreshold, Search, Debug>;

    class iterator;
    class const_iterator;

private:

    node* _root = nullptr;

    struct corners {
        node* left;
        node* right;
        corners() noexcept : left(nullptr), right(nullptr) {}
    };

    union {
        corners _corners;
        node_base _inline;
        static_assert(sizeof(corners) >= sizeof(node_base) + sizeof(member_hook*));
    };

    static const tree* from_inline(const node_base* n) noexcept {
        assert(n->is_inline());
        return boost::intrusive::get_parent_from_member(n, &tree::_inline);
    }

    static tree* from_inline(node_base* n) noexcept {
        assert(n->is_inline());
        return boost::intrusive::get_parent_from_member(n, &tree::_inline);
    }

    /*
     * Helper structure describing a position in a tree. Filled
     * by key_lower_bound() method and is used by tree's API calls.
     */
    struct cursor {
        node* n;
        kid_index idx;

        void descend() noexcept {
            n = n->_kids[idx];
            __builtin_prefetch(n);
        }

        template <typename Pointer>
        iterator insert(Pointer kptr) {
            if (n->is_linear()) {
                n = n->check_linear_capacity(idx);
            }

            Key& k = *kptr;
            n->insert(idx, std::move(kptr));
            /*
             * We cannot trust cur.idx as insert might have moved
             * it anywhere across the tree.
             */
            return iterator(k.*Hook, 0);
        }
    };

    /*
     * Find the key in the tree or the position before which it should be
     * and targets the cursor into this place. Returns true if the key
     * itself was found, false otherwise.
     */
    template <typename K>
    bool key_lower_bound(const K& key, const Compare& cmp, cursor& cur) const {
        cur.n = _root;

        while (true) {
            bool match;

            cur.idx = cur.n->index_for(key, cmp, match);
            assert(cur.idx <= cur.n->_base.num_keys);
            if (match || cur.n->is_leaf()) {
                return match;
            }

            cur.descend();
        }
    }

    void do_set_root(node& n) noexcept {
        assert(n.is_root());
        n._parent.t = this;
        _root = &n;
    }

    void do_set_left(node& n) noexcept {
        assert(n.is_leftmost());
        if (!n.is_linear()) {
            n._leaf_tree = this;
        }
        _corners.left = &n;
    }

    void do_set_right(node& n) noexcept {
        assert(n.is_rightmost());
        if (!n.is_linear()) {
            n._leaf_tree = this;
        }
        _corners.right = &n;
    }

    template <typename Pointer>
    iterator insert_into_inline(Pointer kptr) noexcept {
        member_hook* hook = &(kptr.release()->*Hook);
        hook->attach_first(_inline);
        return iterator(*hook, 0);
    }

    template <typename K>
    std::strong_ordering find_in_inline(const K& k, const Compare& cmp) const {
        return _inline.empty() ? std::strong_ordering::greater : cmp(k, *(_inline.keys[0]->to_key<Key, Hook>()));
    }

    void break_inline() {
        node* n = node::create_empty_root();
        _inline.keys[0]->attach_first(n->_base);
        do_set_root(*n);
        do_set_left(*n);
        do_set_right(*n);
    }

    const node_base* rightmost_node() const noexcept {
        return _root == nullptr ? &_inline : &_corners.right->_base;
    }

    node_base* rightmost_node() noexcept {
        return _root == nullptr ? &_inline : &_corners.right->_base;
    }

    const node_base* leftmost_node() const noexcept {
        return _root == nullptr ? &_inline : &_corners.left->_base;
    }

    node_base* leftmost_node() noexcept {
        return _root == nullptr ? &_inline : &_corners.left->_base;
    }

    bool inline_root() const noexcept { return _root == nullptr; }

public:
    tree() noexcept : _root(nullptr), _inline(node_base::inline_tag{}) {}

    tree(tree&& other) noexcept : tree() {
        if (!other.inline_root()) {
            do_set_root(*other._root);
            do_set_left(*other._corners.left);
            do_set_right(*other._corners.right);

            other._root = nullptr;
            other._corners.left = nullptr;
            other._corners.right = nullptr;
        } else if (!other._inline.empty()) {
            other._inline.keys[0]->attach_first(_inline);
            other._inline.num_keys = 0;
        }
    }

    tree(const tree& other) = delete;
    ~tree() noexcept {
        if (!inline_root()) {
            assert(_root->is_leaf());
            node::destroy(*_root);
        } else {
            assert(_inline.empty());
        }
    }

    template <typename Pointer>
    requires KeyPointer<Pointer, Key>
    std::pair<iterator, bool> insert(Pointer kptr, Compare cmp) {
        seastar::memory::on_alloc_point();
        cursor cur;

        if (inline_root()) {
            if (_inline.empty()) {
                return std::pair(insert_into_inline(std::move(kptr)), true);
            }
            break_inline();
        }

        if (key_lower_bound(*kptr, cmp, cur)) {
            return std::pair(iterator(cur), false);
        }

        return std::pair(cur.insert(std::move(kptr)), true);
    }

    /*
     * Inserts the key into the tree using hint as an attempt not to lookup
     * its position with logN algo. If the new key is hint - 1 <= key <= hint
     * then the insertion goes in O(1) (amortizing rebalancing).
     */
    template <typename Pointer>
    requires KeyPointer<Pointer, Key>
    std::pair<iterator, bool> insert_before_hint(iterator hint, Pointer kptr, Compare cmp) {
        seastar::memory::on_alloc_point();
        auto x = std::strong_ordering::less;

        if (hint != end()) {
            x = cmp(*kptr, *hint);
            if (x == 0) {
                return std::pair(iterator(hint), false);
            }
        }

        if (x < 0) {
            x = std::strong_ordering::greater;

            if (hint != begin()) {
                auto prev = std::prev(hint);
                x = cmp(*kptr, *prev);
                if (x == 0) {
                    return std::pair(iterator(prev), false);
                }
            }

            if (x > 0) {
                return std::pair(hint.insert_before(std::move(kptr)), true);
            }
        }

        return insert(std::move(kptr), std::move(cmp));
    }

    /*
     * Constant-time insertion right before the given position. No sorting
     * is checked, the tree will be broken if the key/it are not in order.
     */

    template <typename Pointer>
    requires KeyPointer<Pointer, Key>
    iterator insert_before(iterator it, Pointer kptr) {
        seastar::memory::on_alloc_point();
        return it.insert_before(std::move(kptr));
    }

    template <typename K>
    requires Comparable<K, Key, Compare>
    const_iterator find(const K& k, Compare cmp) const {
        cursor cur;

        if (inline_root()) {
            if (find_in_inline(k, cmp) == 0) {
                return const_iterator(*_inline.keys[0], 0);
            }
            return cend();
        }
        if (!key_lower_bound(k, cmp, cur)) {
            return cend();
        }

        return const_iterator(cur);
    }

    template <typename K>
    requires Comparable<K, Key, Compare>
    iterator find(const K& k, Compare cmp) {
        return iterator(const_cast<const tree*>(this)->find(k, cmp));
    }

    template <typename K>
    requires Comparable<K, Key, Compare>
    const_iterator lower_bound(const K& k, bool& match, Compare cmp) const {
        if (inline_root()) {
            auto x = find_in_inline(k, cmp);
            if (x <= 0) {
                match = x == 0;
                return const_iterator(*_inline.keys[0], 0);
            }

            match = false;
            return cend();
        }

        if (_root->_base.num_keys == 0) {
            match = false;
            return cend();
        }

        cursor cur;
        match = key_lower_bound(k, cmp, cur);
        if (!match && cur.idx == cur.n->_base.num_keys) {
            assert(cur.idx > 0);
            cur.idx--;
            return ++const_iterator(cur);
        }

        return const_iterator(cur);
    }

    template <typename K>
    requires Comparable<K, Key, Compare>
    iterator lower_bound(const K& k, bool& match, Compare cmp) {
        return iterator(const_cast<const tree*>(this)->lower_bound(k, match, cmp));
    }

    template <typename K>
    requires Comparable<K, Key, Compare>
    const_iterator lower_bound(const K& k, Compare cmp) const {
        bool match;
        return lower_bound(k, match, cmp);
    }

    template <typename K>
    requires Comparable<K, Key, Compare>
    iterator lower_bound(const K& k, Compare cmp) {
        bool match;
        return lower_bound(k, match, cmp);
    }

    template <typename K>
    requires Comparable<K, Key, Compare>
    const_iterator upper_bound(const K& k, Compare cmp) const {
        bool match;

        const_iterator ret = lower_bound(k, match, cmp);
        if (match) {
            ret++;
        }

        return ret;
    }

    template <typename K>
    requires Comparable<K, Key, Compare>
    iterator upper_bound(const K& k, Compare cmp) {
        return iterator(const_cast<const tree*>(this)->upper_bound(k, cmp));
    }

    template <typename K, typename Disp>
    requires Comparable<K, Key, Compare> && Disposer<Disp, Key>
    iterator erase_and_dispose(const K& k, Compare cmp, Disp&& disp) {
        cursor cur;

        if (inline_root()) {
            if (find_in_inline(k, cmp) == 0) {
                node::dispose_key(_inline.keys[0], disp);
                _inline.num_keys = 0;
            }
            return cend();
        }

        if (!key_lower_bound(k, cmp, cur)) {
            return end();
        }

        iterator it(cur);
        member_hook* hook = it._hook;
        it++;
        cur.n->remove(cur.idx);
        node::dispose_key(hook, disp);

        return it;
    }

    /*
     * This range-erase is trivial and not optimal, each key erasure may
     * end up rebalancing the upper nodes in vain.
     */
    template <typename Disp>
    requires Disposer<Disp, Key>
    iterator erase_and_dispose(iterator from, iterator to, Disp&& disp) noexcept {
        while (from != to) {
            from = from.erase_and_dispose(disp);
        }
        return to;
    }

    template <typename Disp>
    requires Disposer<Disp, Key>
    iterator erase_and_dispose(const_iterator from, const_iterator to, Disp&& disp) noexcept {
        return erase_and_dispose(iterator(from), iterator(to), std::forward<Disp>(disp));
    }

    template <typename Disp>
    requires Disposer<Disp, Key>
    iterator erase_and_dispose(iterator it, Disp&& disp) noexcept {
        return it.erase_and_dispose(disp);
    }

    Key* unlink_leftmost_without_rebalance() noexcept {
        node_base* nb = leftmost_node();
        if (nb->num_keys == 0) {
            return nullptr;
        }

        member_hook* hook = nb->keys[0];
        node::dispose_key(hook, default_dispose<Key>);

        if (nb->is_inline()) {
            nb->num_keys = 0;
        } else {
            node* n = node::from_base(nb);
            assert(n->is_leaf());
            n->remove_leftmost_light_rebalance();
        }
        return hook->to_key<Key, Hook>();
    }

    template <typename... Args>
    iterator erase(Args&&... args) { return erase_and_dispose(std::forward<Args>(args)..., default_dispose<Key>); }

    template <typename Func>
    requires Disposer<Func, Key>
    void clear_and_dispose(Func&& disp) noexcept {
        if (!inline_root()) {
            _root->clear([&disp] (member_hook* h) { node::dispose_key(h, disp); });
            node::destroy(*_root);
            _root = nullptr;
            // Both left and right leaves pointers are not touched as this
            // initialization of inline node overwrites them anyway
            new (&_inline) node_base(node_base::inline_tag{});
        } else if (!_inline.empty()) {
            node::dispose_key(_inline.keys[0], disp);
            _inline.num_keys = 0;
        }
    }

    void clear() noexcept { clear_and_dispose(default_dispose<Key>); }

    /*
     * Clone the tree using given Cloner (and Deleter for roll-back).
     */
    template <typename Cloner, typename Deleter>
    requires KeyCloner<Cloner, Key> && Disposer<Deleter, Key>
    void clone_from(const tree& t, Cloner&& cloner, Deleter&& deleter) {
        clear_and_dispose(deleter);
        if (!t.inline_root()) {
            node* left = nullptr;
            node* right = nullptr;

            _root = t._root->clone(left, right, cloner, deleter);

            left->_base.flags |= node_base::NODE_LEFTMOST;
            do_set_left(*left);
            right->_base.flags |= node_base::NODE_RIGHTMOST;
            do_set_right(*right);
            _root->_base.flags |= node_base::NODE_ROOT;
            do_set_root(*_root);
        } else if (!t._inline.empty()) {
            Key* key = cloner(t._inline.keys[0]->template to_key<Key, Hook>());
            (key->*Hook).attach_first(_inline);
        }
    }

    template <bool Const>
    class iterator_base {
    protected:
        using tree_ptr = std::conditional_t<Const, const tree*, tree*>;
        using key_hook_ptr = std::conditional_t<Const, const member_hook*, member_hook*>;
        using node_base_ptr = std::conditional_t<Const, const node_base*, node_base*>;
        using node_ptr = std::conditional_t<Const, const node*, node*>;

        // The end() iterator uses _tree pointer, all the others use _hook.
        union {
            tree_ptr _tree;
            key_hook_ptr _hook;
        };
        key_index _idx;

        // No keys can be at this index, so it's used as the "end" mark.
        static constexpr key_index npos = LinearThreshold;

        explicit iterator_base(tree_ptr t) noexcept : _tree(t), _idx(npos) {}
        iterator_base(key_hook_ptr h, key_index idx) noexcept : _hook(h), _idx(idx) {
            assert(!is_end());
            assert(h->attached());
        }
        explicit iterator_base(const cursor& cur) noexcept : _idx(cur.idx) {
            assert(_idx < cur.n->_base.num_keys);
            _hook = cur.n->_base.keys[_idx];
            assert(_hook->attached());
        }
        iterator_base() noexcept : _tree(static_cast<tree_ptr>(nullptr)), _idx(npos) {}

        bool is_end() const noexcept { return _idx == npos; }

        /*
         * The routine makes sure the iterator's index is valid
         * and returns back the node that points to it.
         */
        node_base_ptr revalidate() noexcept {
            assert(!is_end());
            node_base_ptr n = _hook->node();

            /*
             * The hook pointer is always valid (it's updated on insert/remove
             * operations), the keys are not moved, so if the node still points
             * at us, it is valid.
             */
            if (_idx >= n->num_keys || n->keys[_idx] != _hook) {
                _idx = n->index_for(_hook);
            }

            return n;
        }

    public:
        using iterator_category = std::bidirectional_iterator_tag;
        using value_type = std::conditional_t<Const, const Key, Key>;
        using difference_type = ssize_t;
        using pointer = value_type*;
        using reference = value_type&;

        iterator_base(const iterator_base& other) noexcept {
            if (other.is_end()) {
                _idx = npos;
                _tree = other._tree;
            } else {
                _idx = other._idx;
                _hook = other._hook;
            }
        }

        reference operator*() const noexcept { return *_hook->template to_key<Key, Hook>(); }
        pointer operator->() const noexcept { return _hook->template to_key<Key, Hook>(); }

        iterator_base& operator++() noexcept {
            node_base_ptr n = revalidate();

            if (n->is_leaf()) [[likely]] {
                if (_idx < n->num_keys - 1u) [[likely]] {
                    _idx++;
                    _hook = n->keys[_idx];
                } else if (n->is_inline()) {
                    _idx = npos;
                    _tree = tree::from_inline(n);
                } else if (n->is_rightmost()) {
                    _idx = npos;
                    _tree = node::from_base(n)->corner_tree();
                } else {
                    node_ptr nd = node::from_base(n);
                    do {
                        node_ptr p = nd->_parent.n;
                        _idx = p->index_for(nd);
                        nd = p;
                    } while (_idx == nd->_base.num_keys);
                    _hook = nd->_base.keys[_idx];
                }
            } else {
                node_ptr nd = node::from_base(n);
                nd = nd->_kids[_idx + 1];
                while (!nd->is_leaf()) {
                    nd = nd->_kids[0];
                }
                _idx = 0;
                _hook = nd->_base.keys[_idx];
            }

            return *this;
        }

        iterator_base& operator--() noexcept {
            if (is_end()) {
                node_base_ptr n = _tree->rightmost_node();
                assert(n->num_keys > 0);
                _idx = n->num_keys - 1u;
                _hook = n->keys[_idx];
                return *this;
            }

            node_ptr n = node::from_base(revalidate());

            if (n->is_leaf()) {
                while (_idx == 0) {
                    node_ptr p = n->_parent.n;
                    _idx = p->index_for(n);
                    n = p;
                }
                _idx--;
            } else {
                n = n->_kids[_idx];
                while (!n->is_leaf()) {
                    n = n->_kids[n->_base.num_keys];
                }
                _idx = n->_base.num_keys - 1;
            }

            _hook = n->_base.keys[_idx];
            return *this;
        }

        iterator_base operator++(int) noexcept {
            iterator_base cur = *this;
            operator++();
            return cur;
        }

        iterator_base operator--(int) noexcept {
            iterator_base cur = *this;
            operator--();
            return cur;
        }

        bool operator==(const iterator_base& o) const noexcept { return is_end() ? o.is_end() : _hook == o._hook; }
        operator bool() const noexcept { return !is_end(); }

        /*
         * Special constructor for the case when there's the need for an
         * iterator to the given value poiter. We can get all we need
         * through the hook -> node_base -> node chain.
         */
        iterator_base(pointer key) noexcept : iterator_base(&(key->*Hook), 0) {
            revalidate();
        }

        /*
         * Returns pointer on the owning tree if the element is the
         * last one left in it.
         */
        tree_ptr tree_if_singular() noexcept {
            node_base* n = revalidate();

            if (n->is_root() && n->is_leaf() && n->num_keys == 1) {
                return n->is_inline() ? tree::from_inline(n) : node::from_base(n)->_parent.t;
            } else {
                return nullptr;
            }
        }
    };

    using iterator_base_const = iterator_base<true>;
    using iterator_base_nonconst = iterator_base<false>;

    class const_iterator final : public iterator_base_const {
        friend class tree;
        using super = iterator_base_const;

        explicit const_iterator(const tree* t) noexcept : super(t) {}
        explicit const_iterator(const cursor& cur) noexcept : super(cur) {}
        const_iterator(const member_hook& h, key_index idx) noexcept : super(&h, idx) {}

    public:
        const_iterator() noexcept : super() {}
        const_iterator(const iterator_base_const& other) noexcept : super(other) {}
        const_iterator(const iterator& other) noexcept {
            if (other.is_end()) {
                super::_idx = super::npos;
                super::_tree = const_cast<const tree*>(other._tree);
            } else {
                super::_idx = other._idx;
                super::_hook = const_cast<const member_hook*>(other._hook);
            }
        }
    };

    class iterator final : public iterator_base_nonconst {
        friend class tree;
        friend class key_grabber;
        using super = iterator_base_nonconst;

        explicit iterator(const tree* t) noexcept : super(t) {}
        explicit iterator(const cursor& cur) noexcept : super(cur) {}
        iterator(member_hook& h, key_index idx) noexcept : super(&h, idx) {}

    public:
        iterator() noexcept : super() {}
        iterator(const iterator_base_nonconst& other) noexcept : super(other) {}
        iterator(const const_iterator& other) noexcept {
            if (other.is_end()) {
                super::_idx = super::npos;
                super::_tree = const_cast<tree*>(other._tree);
            } else {
                super::_idx = other._idx;
                super::_hook = const_cast<member_hook*>(other._hook);
            }
        }

    private:
        template <typename Disp>
        requires Disposer<Disp, Key>
        iterator erase_and_dispose(Disp&& disp) noexcept {
            node_base* nb = super::revalidate();
            iterator cur;

            if (nb->is_inline()) {
                cur._idx = super::npos;
                cur._tree = tree::from_inline(nb);
                nb->num_keys = 0;
            } else {
                cur = *this;
                cur++;

                node::from_base(nb)->remove(super::_idx);
                if (cur._hook->node() == nb && cur._idx > 0) {
                    cur._idx--;
                }
            }

            node::dispose_key(super::_hook, disp);
            return cur;
        }

        template <typename Pointer>
        iterator insert_before(Pointer kptr) {
            cursor cur;

            if (super::is_end()) {
                tree* t = super::_tree;
                if (t->inline_root()) {
                    if (t->_inline.empty()) {
                        return t->insert_into_inline(std::move(kptr));
                    }
                    t->break_inline();
                }
                cur.n = t->_corners.right;
                cur.idx = cur.n->_base.num_keys;
            } else {
                node_base* n = super::revalidate();
                if (n->is_inline()) {
                    tree* t = tree::from_inline(n);
                    t->break_inline();
                    cur.n = t->_root;
                    cur.idx = 0;
                } else {
                    cur.n = node::from_base(n);
                    cur.idx = super::_idx;

                    while (!cur.n->is_leaf()) {
                        cur.descend();
                        cur.idx = cur.n->_base.num_keys;
                    }
                }
            }

            return cur.insert(std::move(kptr));
        }
    };

    bool empty() const noexcept { return inline_root() ? _inline.empty() : _root->_base.empty(); }

    const_iterator cbegin() const noexcept {
        const node_base* n = leftmost_node();
        return n->num_keys == 0 ? cend() : const_iterator(*n->keys[0], 0);
    }

    const_iterator cend() const noexcept {
        return const_iterator(this);
    }

    const_iterator begin() const noexcept { return cbegin(); }
    const_iterator end() const noexcept { return cend(); }

    iterator begin() noexcept {
        return iterator(const_cast<const tree*>(this)->cbegin());
    }

    iterator end() noexcept {
        return iterator(const_cast<const tree*>(this)->cend());
    }

    using reverse_iterator = std::reverse_iterator<iterator>;
    reverse_iterator rbegin() noexcept { return std::make_reverse_iterator(end()); }
    reverse_iterator rend() noexcept { return std::make_reverse_iterator(begin()); }

    using const_reverse_iterator = std::reverse_iterator<const_iterator>;
    const_reverse_iterator crbegin() const noexcept { return std::make_reverse_iterator(cend()); }
    const_reverse_iterator crend() const noexcept { return std::make_reverse_iterator(cbegin()); }
    const_reverse_iterator rbegin() const noexcept { return crbegin(); }
    const_reverse_iterator rend() const noexcept { return crend(); }

    size_t calculate_size() const noexcept {
        return inline_root() ? _inline.num_keys : _root->size_slow();
    }

    size_t external_memory_usage() const noexcept {
        return inline_root() ? 0 : _root->external_memory_usage();
    }

    /*
     * Helper to remove keys from trees using only the key iterator.
     *
     * Conforms to KeyPointer and can be used to move keys between trees.
     * Create it with an iterator to a key in one tree and feed to some
     * .insert method into the other. If the key will be taken by the
     * target, it will be instantly removed from the source, and the
     * original iterator will be updated as if it did i = src.erase(i).
     */
    class key_grabber {
        iterator& _it;

    public:
        explicit key_grabber(iterator& it) : _it(it) {
            assert(!_it.is_end());
        }

        key_grabber(const key_grabber&) = delete;
        key_grabber(key_grabber&&) noexcept = default;

        Key& operator*() const noexcept { return *_it; }

        template <typename Disp>
        requires Disposer<Disp, Key>
        void release(Disp&& disp) {
            _it = _it.erase_and_dispose(std::move(disp));
        }

        Key* release() noexcept {
            Key& key = *_it;
            release(default_dispose<Key>);
            return &key;
        }
    };

    struct stats get_stats() const noexcept {
        struct stats st;

        st.nodes = 0;
        st.leaves = 0;
        st.linear_keys = 0;

        if (!inline_root()) {
            st.nodes_filled.resize(NodeSize + 1);
            st.leaves_filled.resize(NodeSize + 1);
            _root->fill_stats(st);
        }

        return st;
    }
};

/*
 * Algorithms for searching a key in array.
 *
 * The ge() method accepts sorted array of keys and searches the index of the
 * lower-bound element of the given key. The bool match is set to true if the
 * key matched, to false otherwise.
 */

template <typename K, typename Key, member_hook Key::* Hook, typename Compare, key_search Search>
struct searcher { };

template <typename K, typename Key, member_hook Key::* Hook, typename Compare>
struct searcher<K, Key, Hook, Compare, key_search::linear> {
    static key_index ge(const K& k, const node_base& node, const Compare& cmp, bool& match) {
        key_index i;

        match = false;
        for (i = 0; i < node.num_keys; i++) {
            if (i + 1 < node.num_keys) {
                __builtin_prefetch(node.keys[i + 1]->to_key<Key, Hook>());
            }
            auto x = cmp(k, *node.keys[i]->to_key<Key, Hook>());
            if (x <= 0) {
                match = x == 0;
                break;
            }
        }

        return i;
    };
};

template <typename K, typename Key, member_hook Key::* Hook, typename Compare>
struct searcher<K, Key, Hook, Compare, key_search::binary> {
    static key_index ge(const K& k, const node_base& node, const Compare& cmp, bool& match) {
        ssize_t s = 0, e = node.num_keys - 1; // signed for below s <= e corner cases

        while (s <= e) {
            key_index i = (s + e) / 2;
            auto x = cmp(k, *node.keys[i]->to_key<Key, Hook>());
            if (x < 0) {
                e = i - 1;
            } else if (x > 0) {
                s = i + 1;
            } else {
                match = true;
                return i;
            }
        }

        match = false;
        return s;
    }
};

template <typename K, typename Key, member_hook Key::* Hook, typename Compare>
struct searcher<K, Key, Hook, Compare, key_search::both> {
    static key_index ge(const K& k, const node_base& node, const Compare& cmp, bool& match) {
        bool ml, mr;
        key_index rl = searcher<K, Key, Hook, Compare, key_search::linear>::ge(k, node, cmp, ml);
        key_index rb = searcher<K, Key, Hook, Compare, key_search::binary>::ge(k, node, cmp, mr);
        assert(rl == rb);
        assert(ml == mr);
        match = ml;
        return rl;
    }
};

/*
 * A node describes all kinds of nodes -- inner, leaf and linear ones
 */
template <typename Key, member_hook Key::* Hook, typename Compare, size_t NodeSize, size_t LinearThreshold, key_search Search, with_debug Debug>
class node {
    friend class tree<Key, Hook, Compare, NodeSize, LinearThreshold, Search, Debug>;
    friend class validator<Key, Hook, Compare, NodeSize, LinearThreshold>;

    using tree = class tree<Key, Hook, Compare, NodeSize, LinearThreshold, Search, Debug>;

    class prealloc;
    [[no_unique_address]] utils::neat_id<Debug == with_debug::yes> id;

    /*
     * The NodeHalf is the level at which the node is considered
     * to be underflown and should be re-filled. This slightly
     * differs for even and odd sizes.
     *
     * For odd sizes the node will stand until it contains literally
     * more than 1/2 of it's size (e.g. for size 5 keeping 3 keys
     * is OK). For even cases this barrier is less than the actual
     * half (e.g. for size 4 keeping 2 is still OK).
     */
    static constexpr size_t NodeHalf = ((NodeSize - 1) / 2);
    static_assert(NodeHalf >= 1);

    /*
     * The LinearThreshold defines the maximum size of the linear growth,
     * though limiting it with values less than NodeSize itself makes
     * little sense.
     */
    static_assert(LinearThreshold >= NodeSize);

    /*
     * When shattering the number of resulting nodes can be any, but the
     * current implementation only makes one-level tree.
     */
    static_assert(LinearThreshold <= NodeSize * (NodeSize + 1) + NodeSize);

    // Hint for the compiler when not to mess with linear stuff at all
    static constexpr bool make_linear_root = (LinearThreshold > NodeSize);

    union node_or_tree {
        node* n;
        tree* t;
    };

    // root node keeps .t pointer on tree, all others -- .n on parents
    node_or_tree _parent;

    node_base _base;

    /*
     * The node_base has keys[] field of zero size at the end, because it should
     * be NodeSize-agnostic. Thus the real memory for key's pointers is reserved
     * here.
     */
    char __room_for_keys[NodeSize * sizeof(member_hook*)];
    static_assert(offsetof(node_base, keys[NodeSize]) == sizeof(node_base) + NodeSize * sizeof(member_hook*));

    /*
     * Leaf nodes don't have kids, so this array is empty for them, but
     * left- and rightmost leaves need pointers on the tree itself.
     *
     * // Unlike B+ trees they are not linked into a list, but still
     * // need the tree pointer to update its _corners.left/right on move.
     *
     * Inner nodes do have kids and since this field goes last allocating
     * the enoug big chunk of memory for a node gives room for it.
     *
     * Linear node doesn't use this union at all.
     */
    union {
        node* _kids[0];
        tree* _leaf_tree;
    };

    tree* corner_tree() const noexcept {
        assert(is_leaf());
        if (!is_linear()) {
            return _leaf_tree;
        }

        assert(is_root());
        return _parent.t;
    }

public:
    /*
     * Leaf node layout
     *
     *  _parent        (pointer)
     *  _base.num_keys (short)
     *  _base.flags    (short)
     *  ...            (int compiler's alignment gap)
     *  _base.keys     (N pointers, thanks to __room_for_keys)
     *  _leaf_tree     (pointer)
     */
    static constexpr size_t leaf_node_size = sizeof(node);

    /*
     * Inner node layout
     *
     *  _parent        (pointer)
     *  _base.num_keys (short)
     *  _base.flags    (short)
     *  ...            (int compiler's alignment gap)
     *  _base.keys     (N pointers)
     *  _kids          (N + 1 pointers)
     */
    static constexpr size_t inner_node_size = sizeof(node) - sizeof(tree*) + (NodeSize + 1) * sizeof(node*);

    /*
     * Linear node layout (dynamic)
     *
     *  _parent        (pointer)
     *  _base.num_keys (short)
     *  _base.flags    (short)
     *  _base.capacity (short)
     *  ...            (short compiler's alignment gap)
     *  _base.keys     (.capacity pointers)
     */
    static size_t linear_node_size(size_t cap) {
        return sizeof(node) - sizeof(tree*) - NodeSize * sizeof(member_hook*) + cap * sizeof(member_hook*);
    }

private:
    bool is_root() const noexcept { return _base.is_root(); }
    bool is_leaf() const noexcept { return _base.is_leaf(); }
    bool is_leftmost() const noexcept { return _base.is_leftmost(); }
    bool is_rightmost() const noexcept { return _base.is_rightmost(); }
    bool is_linear() const noexcept { return make_linear_root && _base.is_linear(); }

    // Helpers to move keys/kids around

    // ... locally
    void move_key(key_index f, key_index t) noexcept {
        _base.keys[t] = _base.keys[f];
    }
    void move_kid(kid_index f, kid_index t) noexcept {
        _kids[t] = _kids[f];
    }

    void set_key(key_index idx, member_hook* hook) noexcept {
        _base.keys[idx] = hook;
        hook->_node = &_base;
    }
    void set_kid(kid_index idx, node* n) noexcept {
        _kids[idx] = n;
        n->_parent.n = this;
    }

    // ... to other nodes
    void move_key(key_index f, node& n, key_index t) noexcept {
        n.set_key(t, _base.keys[f]);
    }
    void move_kid(kid_index f, node& n, kid_index t) noexcept {
        n.set_kid(t, _kids[f]);
    }

    void unlink_corner_leaf() noexcept {
        assert(!is_root());
        node* p = _parent.n, *x;

        switch (_base.flags & (node_base::NODE_LEFTMOST | node_base::NODE_RIGHTMOST)) {
            case 0:
                break;
            case node_base::NODE_LEFTMOST:
                assert(p->_base.num_keys > 0 && p->_kids[0] == this);
                x = p->_kids[1];
                _base.flags &= ~node_base::NODE_LEFTMOST;
                x->_base.flags |= node_base::NODE_LEFTMOST;
                _leaf_tree->do_set_left(*x);
                break;
            case node_base::NODE_RIGHTMOST:
                assert(p->_base.num_keys > 0 && p->_kids[p->_base.num_keys] == this);
                x = p->_kids[p->_base.num_keys - 1];
                _base.flags &= ~node_base::NODE_RIGHTMOST;
                x->_base.flags |= node_base::NODE_RIGHTMOST;
                _leaf_tree->do_set_right(*x);
                break;
            default:
                /*
                 * Right- and left-most at the same time can only be root,
                 * otherwise this would mean we have root with 0 keys.
                 */
                assert(false);
        }
    }

    static const node* from_base(const node_base* nb) noexcept {
        assert(!nb->is_inline());
        return boost::intrusive::get_parent_from_member(nb, &node::_base);
    }

    static node* from_base(node_base* nb) noexcept {
        assert(!nb->is_inline());
        return boost::intrusive::get_parent_from_member(nb, &node::_base);
    }

    template <typename Disp>
    static void dispose_key(member_hook* hook, Disp&& disp) noexcept {
        hook->_node = nullptr;
        disp(hook->to_key<Key, Hook>());
    }

public:
    node(size_t cap, unsigned short flags) noexcept : _base(0, cap, flags) { }

    node(node&& other) noexcept : node(other._base.capacity, std::move(other)) {}
    node(size_t cap, node&& other) noexcept : _base(0, cap, other._base.flags) {
        if (is_leaf()) {
            if (is_leftmost()) {
                other.corner_tree()->do_set_left(*this);
            }

            if (is_rightmost()) {
                other.corner_tree()->do_set_right(*this);
            }

            other._base.flags &= ~(node_base::NODE_LEFTMOST | node_base::NODE_RIGHTMOST);
        } else {
            other.move_kid(0, *this, 0);
        }

        other.move_to(*this, 0, other._base.num_keys);

        if (!is_root()) {
            _parent.n = other._parent.n;
            kid_index i = _parent.n->index_for(&other);
            _parent.n->_kids[i] = this;
        } else {
            other._parent.t->do_set_root(*this);
        }
    }

    node(const node& other) = delete;
    ~node() {
        assert(_base.num_keys == 0);
    }

    size_t storage_size() const noexcept {
        return is_linear() ? linear_node_size(_base.capacity) :
            is_leaf() ? leaf_node_size : inner_node_size;
    }

private:
    template <typename... Args>
    static node* construct(size_t size, Args&&... args) {
        void* mem = current_allocator().alloc<node>(size);
        return new (mem) node(std::forward<Args>(args)...);
    }

    static node* create_leaf() { return construct(leaf_node_size, NodeSize, node_base::NODE_LEAF); }
    static node* create_inner() { return construct(inner_node_size, NodeSize, 0); }

    static node* create_empty_root() {
        if (make_linear_root) {
            return construct(node::linear_node_size(1), 1,
                    node_base::NODE_LINEAR | node_base::NODE_ROOT | node_base::NODE_LEAF |
                    node_base::NODE_LEFTMOST | node_base::NODE_RIGHTMOST);
        } else {
            node* n = node::create_leaf();
            n->_base.flags |= node_base::NODE_ROOT | node_base::NODE_LEFTMOST | node_base::NODE_RIGHTMOST;
            return n;
        }
    }

    static void destroy(node& n) noexcept {
        current_allocator().destroy(&n);
    }

    void drop() noexcept {
        assert(!(is_leftmost() || is_rightmost()));
        if (Debug == with_debug::yes && !is_root()) {
            node* p = _parent.n;
            if (p->_base.num_keys != 0) {
                for (kid_index i = 0; i <= p->_base.num_keys; i++) {
                    assert(p->_kids[i] != this);
                }
            }
        }
        destroy(*this);
    }

    /*
     * Finds the key in the node or the subtree in which to continue
     * the search.
     */
    template <typename K>
    key_index index_for(const K& k, const Compare& cmp, bool& match) const {
        return searcher<K, Key, Hook, Compare, Search>::ge(k, _base, cmp, match);
    }

    // Two helpers for raw pointers lookup.
    kid_index index_for(const node* kid) const noexcept {
        assert(!is_leaf());

        for (kid_index i = 0; i <= _base.num_keys; i++) {
            if (_kids[i] == kid) {
                return i;
            }
        }

        std::abort();
    }

    bool need_refill() const noexcept {
        return _base.num_keys <= NodeHalf;
    }

    bool need_collapse_root() const noexcept {
        return !is_leaf() && (_base.num_keys == 0);
    }

    bool can_grab_from() const noexcept {
        return _base.num_keys > NodeHalf + 1u;
    }

    bool can_push_to() const noexcept {
        return _base.num_keys < NodeSize;
    }

    bool can_merge_with(const node& n) const noexcept {
        return _base.num_keys + n._base.num_keys + 1u <= NodeSize;
    }

    // Make a room for a new key (and kid) at \at position
    void shift_right(size_t at) noexcept {
        for (size_t i = _base.num_keys; i > at; i--) {
            move_key(i - 1, i);
            if (!is_leaf()) {
                move_kid(i, i + 1);
            }
        }
        _base.num_keys++;
    }

    // Occupy the hole at \at after key (and kid) removal
    void shift_left(size_t at) noexcept {
        _base.num_keys--;
        for (size_t i = at; i < _base.num_keys; i++) {
            move_key(i + 1, i);
            if (!is_leaf()) {
                move_kid(i + 2, i + 1);
            }
        }
    }

    // Move keys (and kids) to other node
    void move_to(node& to, size_t off, size_t nr) noexcept {
        for (size_t i = 0; i < nr; i++) {
            move_key(i + off, to, to._base.num_keys + i);
            if (!is_leaf()) {
                move_kid(i + off + 1, to, to._base.num_keys + i + 1);
            }
        }
        _base.num_keys -= nr;
        to._base.num_keys += nr;
    }

    void maybe_allocate_nodes(prealloc& nodes) const {
        // this is full leaf

        nodes.push(node::create_leaf());
        if (is_root()) {
            nodes.push(node::create_inner());
            return;
        }

        const node* cur = _parent.n;
        while (cur->_base.num_keys == NodeSize) {
            nodes.push(node::create_inner());
            if (cur->is_root()) {
                nodes.push(node::create_inner());
                break;
            }
            cur = cur->_parent.n;
        }
    }

    // Constants for linear node shattering into a tree

    // Nr of leaves to keep LinearThreshold keys (inc. keys in the root)
    static constexpr size_t ShatterLeaves = (LinearThreshold + NodeSize + 1) / (NodeSize + 1);
    // This many keys will be put into leaves themselves
    static constexpr size_t ShatterKeysInLeaves = LinearThreshold - (ShatterLeaves - 1);
    // Each leaf gets this amount of keys ...
    static constexpr size_t ShatterKeysPerLeaf = ShatterKeysInLeaves / ShatterLeaves;
    // ... plus 0 or 1 from the remainder
    static constexpr size_t ShatterKeysRemain = ShatterKeysInLeaves % ShatterLeaves;

    /*
     * Break the linear node into a small tree. The result is 1-level tree
     * with leaves evenly filled with the keys.
     *
     * Since this method is called on insertion, it also returns back the
     * new node and updates the insertion index.
     */
    node* shatter(prealloc& nodes, kid_index& idx) noexcept {
        node* new_insertion = nullptr;

        node* root = nodes.pop(false);
        root->_base.flags |= node_base::NODE_ROOT;
        _parent.t->do_set_root(*root);

        node* leaf = nodes.pop(true);
        root->set_kid(root->_base.num_keys, leaf);
        leaf->_base.flags |= node_base::NODE_LEFTMOST;
        _parent.t->do_set_left(*leaf);

        key_index src = 0;
        ssize_t rem = ShatterKeysRemain;

        auto adjust_idx = [&] () noexcept {
            if (new_insertion == nullptr && src == idx) {
                new_insertion = leaf;
                idx = leaf->_base.num_keys;
            }
        };

        while (true) {
            adjust_idx();
            move_key(src++, *leaf, leaf->_base.num_keys++);

            if (src == _base.num_keys) {
                leaf->_base.flags |= node_base::NODE_RIGHTMOST;
                _parent.t->do_set_right(*leaf);
                break;
            }

            if (leaf->_base.num_keys == ShatterKeysPerLeaf + (rem > 0 ? 1 : 0)) {
                rem--;
                adjust_idx();
                move_key(src++, *root, root->_base.num_keys++);
                leaf = nodes.pop(true);
                root->set_kid(root->_base.num_keys, leaf);
                assert(src != _base.num_keys); // need more keys for the next leaf
            }
        }
        adjust_idx();

        _base.num_keys = 0;
        _base.flags &= ~(node_base::NODE_LEFTMOST | node_base::NODE_RIGHTMOST);
        drop();

        assert(new_insertion != nullptr);
        return new_insertion;
    }

    node* check_linear_capacity(kid_index& idx) {
        assert(make_linear_root && is_root() && is_leaf());

        if (_base.num_keys < _base.capacity) {
            return this;
        }

        if (_base.capacity < LinearThreshold) {
            size_t ncap = std::min<size_t>(LinearThreshold, _base.capacity * 2);
            node* n = node::construct(linear_node_size(ncap), ncap, std::move(*this));
            drop();
            return n;
        }

        /*
         * Here we have the linear node fully packed with
         * LinearThreshold keys, thus they need ShatterLeaves
         * leaves and one inner root.
         */

        prealloc nodes;
        nodes.push(node::create_inner());
        for (size_t i = 0; i < ShatterLeaves; i++) {
            nodes.push(node::create_leaf());
        }

        return shatter(nodes, idx);
    }

    /*
     * This is the only throwing part of the insertion. It
     * pre-allocates the nodes (if needed), then grabs the
     * key from the pointer and dives into the non-failing
     * continuation
     */
    template <typename KeyPointer>
    void insert(kid_index idx, KeyPointer kptr) {
        /*
         * Although keys may live at any level, insertion always
         * starts with the leaf. Upper levels get their new keys
         * only if these come up from the deep.
         */
        assert(is_leaf());

        if (_base.num_keys < _base.capacity) {
            /*
             * Most expected case -- just put the key into leaf.
             * Linear node also goes through it.
             */
            do_insert(idx, kptr.release()->*Hook, nullptr);
            return;
        }

        prealloc nodes;
        maybe_allocate_nodes(nodes);
        insert_into_full(idx, kptr.release()->*Hook, nullptr, nodes);
    }

    void insert(kid_index idx, member_hook& key, node* kid, prealloc& nodes) noexcept {
        if (_base.num_keys < NodeSize) {
            do_insert(idx, key, kid);
        } else {
            insert_into_full(idx, key, kid, nodes);
        }
    }

    void insert_into_full(kid_index idx, member_hook& key, node* kid, prealloc& nodes) noexcept {
        if (!is_root()) {
            /*
             * An exception from classical B-tree split-balancing -- an
             * attempt to move the keys between siblings if they allow
             * for it.
             *
             * There are 2 pairs of symmetrical options for this -- when
             * either left or right siblings can accept more keys we put
             * there the parent's key that sits between us and that sibling
             * (sort of separation key), then put our's corner key into
             * the parent, then put the newcomer into the freed slot.
             *
             * A corner case in each pair -- when the newcomer goes at the
             * left- or rightmost slot on this node. In this case parent
             * immediately gets the new key, current node is not updated.
             *
             * Like this (push-right case, 4 keys per node)
             *
             * this --> ACDE   G   HIJ <--- right sibling
             *                 |
             *                 parent key in between
             *
             * if we insert B, then first shift C ... G right
             *          A_CD   E   GHIJ
             * then put B into the free slot
             *          ABCD   E   GHIJ
             *
             * if we insert F, then first shift G right
             *          ACDE   _   GHIJ
             * then put F into the parent's free slot
             *          ACDE   F   GHIJ
             */
            node* p = _parent.n;
            kid_index i = p->index_for(this);

            if (i > 0) {
                node* left = p->_kids[i - 1];
                if (left->can_push_to()) {
                    if (idx > 0) {
                        left->grab_from_right(this, i - 1);
                        /*
                         * We've moved the 0th elemet from this, so the index
                         * for the new key shifts too
                         */
                        idx--;
                    } else if (is_leaf()) {
                        assert(kid == nullptr);
                        p->move_key(i - 1, *left, left->_base.num_keys);
                        left->_base.num_keys++;
                        p->set_key(i - 1, &key);
                        return;
                    }
                }
            }

            if (i < p->_base.num_keys) {
                node* right = p->_kids[i + 1];
                if (right->can_push_to()) {
                    if (idx < _base.num_keys) {
                        right->grab_from_left(this, i + 1);
                    } else if (is_leaf()) {
                        assert(kid == nullptr);
                        right->shift_right(0);
                        p->move_key(i, *right, 0);
                        p->set_key(i, &key);
                        return;
                    }
                }
            }

            if (_base.num_keys < NodeSize) {
                do_insert(idx, key, kid);
                return;
            }
        }

        split_and_insert(idx, key, kid, nodes);
    }

    void split_and_insert(kid_index idx, member_hook& key, node* kid, prealloc& nodes) noexcept {
        node* n = nodes.pop(is_leaf());
        size_t off = NodeHalf + 1;

        if (is_leaf() && is_rightmost()) {
            /*
             * Link the right-most leaf. Leftmost cannot be updated here, the
             * new node is always to the right.
             */
            _base.flags &= ~node_base::NODE_RIGHTMOST;
            n->_base.flags |= node_base::NODE_RIGHTMOST;
            corner_tree()->do_set_right(*n);
        }

        /*
         * Insertion with split.
         *
         * The existing node is split into two halves (for odd case -- almost
         * halves), then the new key goes into either part and parent gets
         * a new key.
         *
         * One corner case here is when the new key is in the middle and it's
         * _it_ who gets into the parent.
         *
         * The algo is the same for both -- leaves and inner nodes.
         */

        if (idx == off) {
            /*
             * Here's what we have here:
             *
             *   parent->  A . H
             *               |
             *       this->  BDFG
             *
             * and want to insert E here. The new key is in the middle, so
             * it goes to parent node and the result would look like this
             *
             *   parent->  A . E . H
             *               |   |
             *      this->  BD   FG <- new node
             */
            move_to(*n, off, NodeSize - off);
            if (!is_leaf()) {
                n->_kids[0] = kid;
                kid->_parent.n = n;
            }
            insert_into_parent(key, n, nodes);
        } else {
            /*
             * That's another case, e.g. like this:
             *
             *   parent->  A . H
             *               |
             *       this->  BDFG
             *
             * and want to insert C here. The new key is left from the middle,
             * so push the left half's right key up and put C into it:
             *
             *   parent->  A . D . G
             *               |   |
             *      this->  BC   EF <- new node
             */
            if (idx < off) {
                move_to(*n, off, NodeSize - off);
                do_insert(idx, key, kid);
            } else {
                off++;
                move_to(*n, off, NodeSize - off);
                n->do_insert(idx - off, key, kid);
            }

            if (!is_leaf()) {
                move_kid(_base.num_keys, *n, 0);
            }
            _base.num_keys--;
            insert_into_parent(*_base.keys[_base.num_keys], n, nodes);
        }
    }

    void do_insert(kid_index idx, member_hook& key, node* kid) noexcept {
        /*
         * The key:kid pair belongs to keys[idx-1]:kids[idx] subtree, and since
         * what's already there is less than this newcomer, the latter goes
         * one step right.
         */
        shift_right(idx);
        set_key(idx, &key);
        if (kid != nullptr) {
            _kids[idx + 1] = kid;
            kid->_parent.n = this;
        }
    }

    void insert_into_parent(member_hook& key, node* kid, prealloc& nodes) noexcept {
        if (is_root()) {
            insert_into_root(key, kid, nodes);
        } else {
            kid_index idx = _parent.n->index_for(this);
            _parent.n->insert(idx, key, kid, nodes);
        }
    }

    void insert_into_root(member_hook& key, node* kid, prealloc& nodes) noexcept {
        tree* t = _parent.t;

        node* nr = nodes.pop(false);

        nr->_base.num_keys = 1;
        nr->set_key(0, &key);

        nr->_kids[0] = this;
        this->_parent.n = nr;
        nr->_kids[1] = kid;
        kid->_parent.n = nr;

        _base.flags &= ~node_base::NODE_ROOT;
        nr->_base.flags |= node_base::NODE_ROOT;
        t->do_set_root(*nr);
    }

    void remove(kid_index idx) noexcept {
        if (is_leaf()) { // ... or linear
            remove_key(idx);
        } else {
            remove_from_inner(idx);
        }
    }

    void remove_key(kid_index idx) noexcept {
        shift_left(idx);
        check_refill();
    }

    void remove_leftmost_light_rebalance() noexcept {
        shift_left(0);
        check_light_refill();
    }

    void check_refill() noexcept {
        if (!is_root()) {
            if (need_refill()) {
                refill();
            }
        } else if (need_collapse_root()) {
            collapse_root();
        }
    }

    void check_light_refill() noexcept {
        if (_base.num_keys == 0) {
            if (!is_root()) {
                light_refill();
            } else if (!is_leaf()) {
                collapse_root();
            }
        }
    }

    void collapse_root() noexcept {
        node& nr = *_kids[0];
        nr._base.flags |= node_base::NODE_ROOT;
        _parent.t->do_set_root(nr);
        drop();
    }

    void grab_from_left(node* left, key_index idx) noexcept {
        /*
         * Shif keys right -- left sibling's right key goes to parent,
         * parent's goes to us. Like this
         *
         * left --> ABC  D   EF  <-- this
         *               |
         *               parent key in between
         *
         * gets transformed into
         *
         * left -->  AB  C   DEF  <-- this
         */
        shift_right(0);
        _parent.n->move_key(idx - 1, *this, 0);
        left->move_key(left->_base.num_keys - 1, *_parent.n, idx - 1);
        if (!is_leaf()) {
            move_kid(0, 1);
            left->move_kid(left->_base.num_keys, *this, 0);
        }

        left->_base.num_keys--;
    }

    void grab_from_right(node* right, key_index idx) noexcept {
        /*
         * Shif keys left -- rights sibling's zeroth key goes to parent,
         * parent's goes to us. Like this
         *
         * this -->  AB  C   DEF <-- right
         *               |
         *               parent key in between
         *
         * gets transformed into
         *
         * this --> ABC  D   EF  <-- right
         */
        _parent.n->move_key(idx, *this, _base.num_keys);
        right->move_key(0, *_parent.n, idx);
        if (!is_leaf()) {
            right->move_kid(0, *this, _base.num_keys + 1);
            right->move_kid(1, 0);
        }
        right->shift_left(0);

        _base.num_keys++;
    }

    void merge_kids(node& t, node& n, key_index idx) noexcept {
        /*
         * Merge two kids together (this points to their parent node)
         * and put the key that was between them into the new node as
         * well. Respectively, the current filling of nodes should be
         *      a.num_keys + b.num_keys + 1 <= NodeSize
         * but that's checked by the caller. The process looks like
         *
         * t -->  A  B  C <-- n
         *           |
         *           parent key in between (at \idx position)
         *
         *  goes into
         *
         * t -->  ABC  _  X <-- n gets removed
         *             | parent loses one key
         */
        move_key(idx, t, t._base.num_keys);
        if (!t.is_leaf()) {
            n.move_kid(0, t, t._base.num_keys + 1);
        }
        t._base.num_keys++;
        n.move_to(t, 0, n._base.num_keys);
        n._base.num_keys = 0;

        /*
         * First unlink the node from tree/parent, then drop it, so that
         * the drop's and destructor's asserts do not find this node in
         * unexpected state.
         */
        if (n.is_leaf()) {
            n.unlink_corner_leaf();
        }
        shift_left(idx);
        n.drop();
    }

    void merge_kids_and_refill(node& t, node& n, key_index idx) noexcept {
        merge_kids(t, n, idx);
        check_refill();
    }

    void refill() noexcept {
        kid_index idx = _parent.n->index_for(this);
        node* right = idx < _parent.n->_base.num_keys ? _parent.n->_kids[idx + 1] : nullptr;
        node* left = idx > 0 ? _parent.n->_kids[idx - 1] : nullptr;

        /*
         * The node is "underflown" (see comment near NodeHalf
         * about what this means), so we try to refill it at the
         * siblings' expense. Many cases possible, but we go with
         * two pairs -- either of the siblings has large enough
         * keys to give us one or it has small enough heys to be
         * merged with us (and one more key from the parent).
         */

        if (left != nullptr && left->can_grab_from()) {
            grab_from_left(left, idx);
            return;
        }

        if (right != nullptr && right->can_grab_from()) {
            grab_from_right(right, idx);
            return;
        }

        if (left != nullptr && can_merge_with(*left)) {
            _parent.n->merge_kids_and_refill(*left, *this, idx - 1);
            return;
        }

        if (right != nullptr && can_merge_with(*right)) {
            _parent.n->merge_kids_and_refill(*this, *right, idx);
            return;
        }

        /*
         * Susprisingly, the node in the B-tree can violate the
         * "minimally filled" rule for non roots. It _can_ stay with
         * less than half elements on board. The next remove from
         * it or either of its siblings will probably refill it.
         */
    }

    void light_refill() noexcept {
        assert(_parent.n->_base.num_keys > 0);
        node* right = _parent.n->_kids[1];

        /*
         * The current node is empty and needs to either go away
         * from the tree or get refilled.
         *
         * In case our right sibling can carry one more key (which
         * is the same as it can be merged with current empty node)
         * then we just perform regular merge, which will move all
         * the keys from right node on the current one (plus the
         * parent's separation key). Note that this is NOT worse
         * than just updating the parent's 0th key to point to the
         * right kid -- in the latter case we'd still have to shift
         * the whole right kid right to make room for the parent
         * separation key at its 0 position, so it's moving the
         * whole node anyway.
         *
         * In case our right sibling is full there's no choise but
         * to grab a key from it and continue. Next time we get
         * here the right node will be int mergeable state.
         */

        if (can_merge_with(*right)) {
            _parent.n->merge_kids(*this, *right, 0);
            _parent.n->check_light_refill();
        } else {
            grab_from_right(right, 0);
        }
    }


    void remove_from_inner(kid_index idx) noexcept {
        /*
         * Removing from inner node is only possible if the
         * respecrtive kids get squashed together, but the
         * latter is (almost) impossible, as nodes are kept
         * at least half-filled. Thus the only way here is to
         * go down to the previous key and replace the key
         * being removed from this[idx] with that one. The
         * previous key sits ... on the leaf, so go dive as
         * deep as we can and move the key from there.
         */
        node* rightmost = _kids[idx + 1];

        while (!rightmost->is_leaf()) {
            rightmost = rightmost->_kids[0];
        }

        rightmost->move_key(0, *this, idx);

        /*
         * Whee, we've just removed one key from the leaf. Time
         * to go up again and rebalance the tree.
         */
        rightmost->remove_key(0);
    }

    template <typename KFunc>
    void clear(KFunc&& k_clear) noexcept {
        size_t nk = _base.num_keys;
        _base.num_keys = 0;

        if (!is_leaf()) {
            for (kid_index i = 0; i <= nk; i++) {
                _kids[i]->clear(k_clear);
                destroy(*_kids[i]);
            }
        }

        for (key_index i = 0; i < nk; i++) {
            k_clear(_base.keys[i]);
        }
    }

    template <typename Cloner, typename Deleter>
    node* clone(node*& left_leaf, node*& right_leaf, Cloner&& cloner, Deleter&& deleter) const {
        node* n;

        if (is_linear()) {
            n = construct(linear_node_size(_base.capacity), _base.capacity, _base.flags);
        } else if (is_leaf()) {
            n = create_leaf();
        } else {
            n = create_inner();
        }

        key_index ki = 0;
        kid_index ni = 0;

        try {
            for (ki = 0; ki < _base.num_keys; ki++) {
                Key* key = cloner(_base.keys[ki]->to_key<Key, Hook>());
                n->set_key(ki, &(key->*Hook));
            }

            if (is_leaf()) {
                if (left_leaf == nullptr) {
                    left_leaf = n;
                }
                right_leaf = n;
            } else {
                for (ni = 0; ni <= _base.num_keys; ni++) {
                    n->_kids[ni] = _kids[ni]->clone(left_leaf, right_leaf, cloner, deleter);
                    n->_kids[ni]->_parent.n = n;
                }
            }

            n->_base.num_keys = _base.num_keys;

        } catch(...) {
            while (ki != 0) {
                node::dispose_key(n->_base.keys[--ki], deleter);
            }
            while (ni != 0) {
                n->_kids[ni - 1]->clear([&deleter] (member_hook* h) { node::dispose_key(h, deleter); });
                destroy(*n->_kids[--ni]);
            }
            destroy(*n);

            // No need to "reset" left_leaf/right_leaf on exception as these
            // pointers are only valid iff clone() returns successfully
            throw;
        }

        return n;
    }

    size_t size_slow() const noexcept {
        size_t ret = _base.num_keys;
        if (!is_leaf()) {
            for (kid_index i = 0; i <= _base.num_keys; i++) {
                ret += _kids[i]->size_slow();
            }
        }
        return ret;
    }

    size_t external_memory_usage() const noexcept {
        if (is_linear()) {
            assert(is_leaf());
            return linear_node_size(_base.capacity);
        }

        if (is_leaf()) {
            return leaf_node_size;
        }

        size_t size = inner_node_size;
        assert(_base.num_keys != 0);
        for (kid_index i = 0; i <= _base.num_keys; i++) {
            size += _kids[i]->external_memory_usage();
        }
        return size;
    }

    void fill_stats(struct stats& st) const noexcept {
        if (is_linear()) {
            st.linear_keys = _base.num_keys;
        } else if (is_leaf()) {
            st.leaves_filled[_base.num_keys]++;
            st.leaves++;
        } else {
            st.nodes_filled[_base.num_keys]++;
            st.nodes++;
            assert(_base.num_keys != 0);
            for (kid_index i = 0; i <= _base.num_keys; i++) {
                _kids[i]->fill_stats(st);
            }
        }
    }

    class prealloc {
        node* _nodes;
        node** _tail = &_nodes;

        node* pop() noexcept {
            assert(!empty());
            node* ret = _nodes;
            _nodes = ret->_parent.n;
            if (_tail == &ret->_parent.n) {
                _tail = &_nodes;
            }
            return ret;
        }

        bool empty() const noexcept { return _tail == &_nodes; }

        void drain() noexcept {
            while (!empty()) {
                node* n = pop();
                node::destroy(*n);
            }
        }

    public:
        void push(node* n) noexcept {
            *_tail = n;
            _tail = &n->_parent.n;
        }

        node* pop(bool leaf) noexcept {
            node* ret = pop();
            assert(leaf == ret->is_leaf());
            return ret;
        }

        ~prealloc() {
            drain();
        }
    };
};

} // namespace


#include <boost/intrusive/list.hpp>
#include <seastar/core/memory.hh>

class evictable {
    friend class lru;
    // For bookkeeping, we want the unlinking of evictables to be explicit.
    // E.g. if the cache's internal data structure consists of multiple lists, we would
    // like to know which list is an element being removed from.
    // Therefore, we are using auto_unlink only to be able to call unlink() in the move constructor
    // and we do NOT rely on automatic unlinking in _lru_link's destructor.
    // It's the programmer's responsibility. to call lru::remove on the evictable before its destruction.
    // Failure to do so is a bug, and it will trigger an assertion in the destructor.
    using lru_link_type = boost::intrusive::list_member_hook<
        boost::intrusive::link_mode<boost::intrusive::auto_unlink>>;
    lru_link_type _lru_link;
protected:
    // Prevent destruction via evictable pointer. LRU is not aware of allocation strategy.
    // Prevent destruction of a linked evictable. While we could unlink the evictable here
    // in the destructor, we can't perform proper accounting for that without access to the
    // head of the containing list.
    ~evictable() {
        assert(!_lru_link.is_linked());
    }
public:
    evictable() = default;
    evictable(evictable&& o) noexcept;
    evictable& operator=(evictable&&) noexcept = default;

    virtual void on_evicted() noexcept = 0;

    // Used for testing to avoid cascading eviction of the containing object.
    virtual void on_evicted_shallow() noexcept { on_evicted(); }

    bool is_linked() const {
        return _lru_link.is_linked();
    }

    void swap(evictable& o) noexcept {
        _lru_link.swap_nodes(o._lru_link);
    }
};

class lru {
private:
    friend class evictable;
    using lru_type = boost::intrusive::list<evictable,
        boost::intrusive::member_hook<evictable, evictable::lru_link_type, &evictable::_lru_link>,
        boost::intrusive::constant_time_size<false>>; // we need this to have bi::auto_unlink on hooks.
    lru_type _list;
public:
    using reclaiming_result = seastar::memory::reclaiming_result;

    ~lru() {
        _list.clear_and_dispose([] (evictable* e) {
            e->on_evicted();
        });
    }

    void remove(evictable& e) noexcept {
        _list.erase(_list.iterator_to(e));
    }

    void add(evictable& e) noexcept {
        _list.push_back(e);
    }

    // Like add(e) but makes sure that e is evicted right before "more_recent" in the absence of later touches.
    void add_before(evictable& more_recent, evictable& e) noexcept {
        _list.insert(_list.iterator_to(more_recent), e);
    }

    void touch(evictable& e) noexcept {
        remove(e);
        add(e);
    }

    // Evicts a single element from the LRU
    template <bool Shallow = false>
    reclaiming_result do_evict() noexcept {
        if (_list.empty()) {
            return reclaiming_result::reclaimed_nothing;
        }
        evictable& e = _list.front();
        _list.pop_front();
        if constexpr (!Shallow) {
            e.on_evicted();
        } else {
            e.on_evicted_shallow();
        }
        return reclaiming_result::reclaimed_something;
    }

    // Evicts a single element from the LRU.
    reclaiming_result evict() noexcept {
        return do_evict<false>();
    }

    // Evicts a single element from the LRU.
    // Will call on_evicted_shallow() instead of on_evicted().
    reclaiming_result evict_shallow() noexcept {
        return do_evict<true>();
    }

    // Evicts all elements.
    // May stall the reactor, use only in tests.
    void evict_all() {
        while (evict() == reclaiming_result::reclaimed_something) {}
    }
};

inline
evictable::evictable(evictable&& o) noexcept {
    if (o._lru_link.is_linked()) {
        auto prev = o._lru_link.prev_;
        o._lru_link.unlink();
        lru::lru_type::node_algorithms::link_after(prev, _lru_link.this_ptr());
    }
}


template<typename T>
requires std::is_nothrow_move_constructible_v<T>
class managed;

//
// Similar to std::unique_ptr<>, but for LSA-allocated objects. Remains
// valid across deferring points. See make_managed().
//
// std::unique_ptr<> can't be used with LSA-allocated objects because
// it assumes that the object doesn't move after being allocated. This
// is not true for LSA, which moves objects during compaction.
//
// Also works for objects allocated using standard allocators, though
// there the extra space overhead of a pointer is not justified.
// It still make sense to use it in places which are meant to work
// with either kind of allocator.
//
template<typename T>
struct managed_ref {
    managed<T>* _ptr;

    managed_ref() : _ptr(nullptr) {}

    managed_ref(const managed_ref&) = delete;

    managed_ref(managed_ref&& other) noexcept
        : _ptr(other._ptr)
    {
        other._ptr = nullptr;
        if (_ptr) {
            _ptr->_backref = &_ptr;
        }
    }

    ~managed_ref() {
        if (_ptr) {
            current_allocator().destroy(_ptr);
        }
    }

    managed_ref& operator=(managed_ref&& o) {
        this->~managed_ref();
        new (this) managed_ref(std::move(o));
        return *this;
    }

    T* get() {
        return _ptr ? &_ptr->_value : nullptr;
    }

    const T* get() const {
        return _ptr ? &_ptr->_value : nullptr;
    }

    T& operator*() {
        return _ptr->_value;
    }

    const T& operator*() const {
        return _ptr->_value;
    }

    T* operator->() {
        return &_ptr->_value;
    }

    const T* operator->() const {
        return &_ptr->_value;
    }

    explicit operator bool() const {
        return _ptr != nullptr;
    }

    size_t external_memory_usage() const {
        return _ptr ? current_allocator().object_memory_size_in_allocator(_ptr) : 0;
    }
};

template<typename T>
requires std::is_nothrow_move_constructible_v<T>
class managed {
    managed<T>** _backref;
    T _value;

    template<typename T_>
    friend struct managed_ref;
public:
    managed(managed<T>** backref, T&& v) noexcept
        : _backref(backref)
        , _value(std::move(v))
    {
        *_backref = this;
    }

    managed(managed&& other) noexcept
        : _backref(other._backref)
        , _value(std::move(other._value))
    {
        *_backref = this;
    }
};

//
// Allocates T using given AllocationStrategy and returns a managed_ref owning the
// allocated object.
//
template<typename T, typename... Args>
managed_ref<T>
make_managed(Args&&... args) {
    managed_ref<T> ref;
    current_allocator().construct<managed<T>>(&ref._ptr, T(std::forward<Args>(args)...));
    return ref;
}


#include <cstdint>
#include <limits>

namespace utils {

static constexpr int64_t simple_key_unused_value = std::numeric_limits<int64_t>::min();

/*
 * array_search_gt(value, array, capacity, size)
 *
 * Returns the index of the first element in the array that's greater
 * than the given value.
 *
 * To accomodate the single-instruction-multiple-data variant, observe
 * the following:
 *  - capacity must be a multiple of 4
 *  - any items with indexes in [size, capacity) must be initialized
 *    to std::numeric_limits<int64_t>::min()
 */
int array_search_gt(int64_t val, const int64_t* array, const int capacity, const int size);

static inline unsigned array_search_4_eq(uint8_t val, const uint8_t* array) {
    // Unrolled loop is few %s faster
    if (array[0] == val) {
        return 0;
    } else if (array[1] == val) {
        return 1;
    } else if (array[2] == val) {
        return 2;
    } else if (array[3] == val) {
        return 3;
    } else {
        return 4;
    }
}

static inline unsigned array_search_8_eq(uint8_t val, const uint8_t* array) {
    for (unsigned i = 0; i < 8; i++) {
        if (array[i] == val) {
            return i;
        }
    }
    return 8;
}

unsigned array_search_16_eq(uint8_t val, const uint8_t* array);
unsigned array_search_32_eq(uint8_t val, const uint8_t* array);
unsigned array_search_x32_eq(uint8_t val, const uint8_t* array, int nr);

}


#include <cassert>
#include <algorithm>
#include <bitset>
#include <fmt/core.h>
#include <boost/intrusive/parent_from_member.hpp>

class size_calculator;

namespace compact_radix_tree {

template <typename T, typename Idx> class printer;

template <unsigned Size>
inline unsigned find_in_array(uint8_t val, const uint8_t* arr);

template <>
inline unsigned find_in_array<4>(uint8_t val, const uint8_t* arr) {
    return utils::array_search_4_eq(val, arr);
}

template <>
inline unsigned find_in_array<8>(uint8_t val, const uint8_t* arr) {
    return utils::array_search_8_eq(val, arr);
}

template <>
inline unsigned find_in_array<16>(uint8_t val, const uint8_t* arr) {
    return utils::array_search_16_eq(val, arr);
}

template <>
inline unsigned find_in_array<32>(uint8_t val, const uint8_t* arr) {
    return utils::array_search_32_eq(val, arr);
}

template <>
inline unsigned find_in_array<64>(uint8_t val, const uint8_t* arr) {
    return utils::array_search_x32_eq(val, arr, 2);
}

// A union of any number of types.

template <typename... Ts>
struct variadic_union;

template <typename Tx>
struct variadic_union<Tx> {
    union {
        Tx _this;
    };

    variadic_union() noexcept {}
    ~variadic_union() {}
};

template <typename Tx, typename Ty, typename... Ts>
struct variadic_union<Tx, Ty, Ts...> {
    union {
        Tx _this;
        variadic_union<Ty, Ts...> _other;
    };

    variadic_union() noexcept {}
    ~variadic_union() {}
};

/*
 * Radix tree implementation for the key being an integer type.
 * The search key is split into equal-size pieces to find the
 * next node in each level. The pieces are defined compile-time
 * so the tree is compile-time limited in ints depth.
 *
 * Uses 3 memory optimizations:
 * - a node dynamically grows in size depending on the range of
 *   keys it carries
 * - additionally, if the set of keys on a node is very sparse the
 *   node may become "indirect" thus keeping only the actual set
 *   of keys
 * - if a node has 1 child it's removed from the tree and this
 *   loneley kid is attached directly to its (former) grandfather
 */

template <typename T, typename Index = unsigned int>
requires std::is_nothrow_move_constructible_v<T> && std::is_integral_v<Index>
class tree {
    friend class ::size_calculator;
    template <typename A, typename I> friend class printer;

    class leaf_node;
    class inner_node;
    struct node_head;
    class node_head_ptr;

public:
    /*
     * The search key in the tree is an integer, the whole
     * logic below is optimized for that.
     */
    using key_t = std::make_unsigned_t<Index>;

    /*
     * The lookup uses 7-bit pieces from the key to search on
     * each level. Thus all levels but the last one keep pointers
     * on lower levels, the last one is the leaf node that keeps
     * values on board.
     *
     * The 8th bit in the node index byte is used to denote an
     * unused index which is quite helpful.
     */
    using node_index_t = uint8_t;
    static constexpr unsigned radix_bits = 7;
    static constexpr key_t  radix_mask = (1 << radix_bits) - 1;
    static constexpr unsigned leaf_depth = (8 * sizeof(key_t) + radix_bits - 1) / radix_bits - 1;
    static constexpr unsigned node_index_limit = 1 << radix_bits;
    static_assert(node_index_limit != 0);
    static constexpr node_index_t unused_node_index = node_index_limit;

private:
    /*
     * Nodes can be of 2 kinds -- direct and indirect.
     *
     * Direct nodes are arrays of elements. Getting a value from
     * this node is simple indexing. There are 2 of them -- static
     * and dynamic. Static nodes have fixed size capable to keep all
     * the possible indices, dynamic work like a vector growing in
     * size. Former occupy more space, but work a bit faster because
     * of * missing boundary checks.
     *
     * Indirect nodes keep map of indices on board and perform lookup
     * rather than direct indexing to get a value. They also grow in
     * size, but unlike dynamic direct nodes by converting between each
     * other.
     *
     * When a node is tried to push a new index over its current
     * capacity it grows into some other node, that can fit all its
     * keys plus at least one.
     *
     * When a key is removed from an indirect node and it becomes
     * less that some threshold, it's shrunk into smaller node.
     *
     * The nil is a placeholder for non-existing empty node.
     */
    enum class layout : uint8_t { nil,
        indirect_tiny, indirect_small, indirect_medium, indirect_large,
        direct_dynamic, direct_static, };

    /*
     * When a node has only one child, the former is removed from
     * the tree and its parent is set up to directly point to this
     * only kid. The kid, in turn, carries a "prefix" on board
     * denoting the index that might have been skipped by this cut.
     *
     * The lower 7 bits are the prefix length, the rest is the
     * prefix itself.
     */
    static constexpr key_t prefix_len_mask = radix_mask;
    static constexpr key_t prefix_mask = ~prefix_len_mask;

    static key_t make_prefix(key_t key, unsigned len) noexcept {
        return (key & prefix_mask) + len;
    }

    /*
     * Mask to check node's prefix (mis-)match
     */
    static key_t prefix_mask_at(unsigned depth) noexcept {
        return prefix_mask << (radix_bits * (leaf_depth - depth));
    }

    /*
     * Finds the number of leading elements that coinside for two
     * indices. Needed on insertion, when a short-cut node gets
     * expanded back.
     */
    static unsigned common_prefix_len(key_t k1, key_t k2) noexcept {
        static constexpr unsigned trailing_bits = (8 * sizeof(key_t)) % radix_bits;
        static constexpr unsigned round_up_delta = trailing_bits == 0 ? 0 : radix_bits - trailing_bits;
        /*
         * This won't work if k1 == k2 (clz is undefined for full
         * zeroes value), but we don't get here in this case
         */
        return (__builtin_clz(k1 ^ k2) + round_up_delta) / radix_bits;
    }

    /*
     * Gets the depth's radix_bits-len index from the whole key, that's
     * used in intra-node search.
     */
    static node_index_t node_index(key_t key, unsigned depth) noexcept {
        return (key >> (radix_bits * (leaf_depth - depth))) & radix_mask;
    }

    enum class erase_mode { real, cleanup, };

    /*
     * When removing an index from a node it may end-up in one of 4
     * states:
     *
     * - empty  -- the last index was removed, the parent node is
     *             welcome to drop the slot and mark it as unused
     *             (and maybe get shrunk/squashed after that)
     * - squash -- only one index left, the parent node is welcome
     *             to remove this node and replace it with its only
     *             child (tuning it's prefix respectively)
     * - shrink -- current layout contains few indices, so parent
     *             node should shrink the slot into smaller node
     * - nothing - just nothing
     */
    enum class erase_result { nothing, empty, shrink, squash, };

    template <unsigned Threshold>
    static erase_result after_drop(unsigned count) noexcept {
        if (count == 0) {
            return erase_result::empty;
        }
        if (count == 1) {
            return erase_result::squash;
        }

        if constexpr (Threshold != 0) {
            if (count <= Threshold) {
                return erase_result::shrink;
            }
        }

        return erase_result::nothing;
    }

    /*
     * Lower-bound calls return back pointer on the value and the leaf
     * node_head on which the value was found. The latter is needed
     * for iterator's ++ optimization.
     */
    struct lower_bound_res {
        const T* elem;
        const node_head* leaf;
        key_t key;

        lower_bound_res(const T* e, const node_head& l, key_t k) noexcept : elem(e), leaf(&l), key(k) {}
        lower_bound_res() noexcept : elem(nullptr), leaf(nullptr), key(0) {}
    };

    /*
     * Allocation returns a slot pointer and a boolean denoting
     * if the allocation really took place (false if the slot
     * is aleady occupied)
     */
    using allocate_res = std::pair<T*, bool>;

    using clone_res = std::pair<node_head*, std::exception_ptr>;

    /*
     * A header all nodes start with. The type of a node (inner/leaf)
     * is evaluated (fingers-crossed) from the depth argument, so the
     * header doesn't have this bit.
     */
    struct node_head {
        node_head_ptr* _backref;
        // Prefix for squashed nodes
        key_t _prefix;
        const layout _base_layout;
        // Number of keys on the node
        uint8_t _size;
        // How many slots are there. Used only by direct dynamic nodes
        const uint8_t _capacity;

        node_head() noexcept : _backref(nullptr), _prefix(0), _base_layout(layout::nil), _size(0), _capacity(0) {}

        node_head(key_t prefix, layout lt, uint8_t capacity) noexcept
                : _backref(nullptr)
                , _prefix(prefix)
                , _base_layout(lt)
                , _size(0)
                , _capacity(capacity) {}

        node_head(node_head&& o) noexcept
                : _backref(std::exchange(o._backref, nullptr))
                , _prefix(o._prefix)
                , _base_layout(o._base_layout)
                , _size(std::exchange(o._size, 0))
                , _capacity(o._capacity) {
            if (_backref != nullptr) {
                *_backref = this;
            }
        }

        node_head(const node_head&) = delete;
        ~node_head() { assert(_size == 0); }

        /*
         * Helpers to cast header to the actual node class or to the
         * node's base class (see below).
         */

        template <typename NBT>
        NBT& as_base() noexcept {
            return *boost::intrusive::get_parent_from_member(this, &NBT::_head);
        }

        template <typename NBT>
        const NBT& as_base() const noexcept {
            return *boost::intrusive::get_parent_from_member(this, &NBT::_head);
        }

        template <typename NT>
        typename NT::node_type& as_base_of() noexcept {
            return as_base<typename NT::node_type>();
        }

        template <typename NT>
        const typename NT::node_type& as_base_of() const noexcept {
            return as_base<typename NT::node_type>();
        }

        template <typename NT>
        NT& as_node() noexcept {
            return *boost::intrusive::get_parent_from_member(&as_base_of<NT>(), &NT::_base);
        }

        template <typename NT>
        const NT& as_node() const noexcept {
            return *boost::intrusive::get_parent_from_member(&as_base_of<NT>(), &NT::_base);
        }

        // Construct a key from leaf node prefix and index
        key_t key_of(node_index_t ni) const noexcept {
            return (_prefix & prefix_mask) + ni;
        }

        // Prefix manipulations
        unsigned prefix_len() const noexcept { return _prefix & prefix_len_mask; }
        void trim_prefix(unsigned v) noexcept { _prefix -= v; }
        void bump_prefix(unsigned v) noexcept { _prefix += v; }

        bool check_prefix(key_t key, unsigned& depth) const noexcept {
            unsigned real_depth = depth + prefix_len();
            key_t mask = prefix_mask_at(real_depth);
            if ((key & mask) != (_prefix & mask)) {
                return false;
            }

            depth = real_depth;
            return true;
        }

        /*
         * A bunch of "polymorphic" API wrappers that selects leaf/inner
         * node to call the method on.
         *
         * The node_base below provides the same set, but ploymorphs
         * the calls into the actual node layout.
         */

        /*
         * Finds the element by the given key
         */

        const T* get(key_t key, unsigned depth) const noexcept {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().get(key, depth);
            } else {
                return as_base_of<inner_node>().get(key, depth);
            }
        }

        /*
         * Finds the element whose key is not greater than the given one
         */

        lower_bound_res lower_bound(key_t key, unsigned depth) const noexcept {
            unsigned real_depth = depth + prefix_len();
            key_t mask = prefix_mask_at(real_depth);
            if ((key & mask) > (_prefix & mask)) {
                return lower_bound_res();
            }

            depth = real_depth;
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().lower_bound(key, depth);
            } else {
                return as_base_of<inner_node>().lower_bound(key, depth);
            }
        }

        /*
         * Allocates a new slot for the value. The caller is given the
         * pointer to the slot and the sign if it's now busy or not,
         * so that it can destruct it and construct a new element.
         */

        allocate_res alloc(key_t key, unsigned depth) {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().alloc(key, depth);
            } else {
                return as_base_of<inner_node>().alloc(key, depth);
            }
        }

        /*
         * Erase the element with the given key, if present.
         */

        erase_result erase(key_t key, unsigned depth, erase_mode erm) noexcept {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().erase(key, depth, erm);
            } else {
                return as_base_of<inner_node>().erase(key, depth, erm);
            }
        }

        /*
         * Weed walks the tree and removes the elements for which
         * the filter() returns true.
         */

        template <typename Fn>
        erase_result weed(Fn&& filter, unsigned depth) {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().weed(filter, depth);
            } else {
                return as_base_of<inner_node>().weed(filter, depth);
            }
        }

        /*
         * Grow the current node and return the new one
         */

        node_head* grow(key_t key, unsigned depth) {
            node_index_t ni = node_index(key, depth);
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().template grow<leaf_node>(ni);
            } else {
                return as_base_of<inner_node>().template grow<inner_node>(ni);
            }
        }

        /*
         * Shrink the current node and return the new one
         */

        node_head* shrink(unsigned depth) {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().template shrink<leaf_node>();
            } else {
                return as_base_of<inner_node>().template shrink<inner_node>();
            }
        }

        /*
         * Walk the tree without modifying it (however, the elements
         * themselves can be modified)
         */

        template <typename Visitor>
        bool visit(Visitor&& v, unsigned depth) const {
            bool ret = true;
            depth += prefix_len();
            if (v(*this, depth, true)) {
                if (depth == leaf_depth) {
                    ret = as_base_of<leaf_node>().visit(v, depth);
                } else {
                    ret = as_base_of<inner_node>().visit(v, depth);
                }
                v(*this, depth, false);
            }
            return ret;
        }

        template <typename Fn>
        clone_res clone(Fn&& cloner, unsigned depth) const noexcept {
            depth += prefix_len();
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().template clone<leaf_node, Fn>(cloner, depth);
            } else {
                return as_base_of<inner_node>().template clone<inner_node, Fn>(cloner, depth);
            }
        }

        void free(unsigned depth) noexcept {
            if (depth == leaf_depth) {
                leaf_node::free(as_node<leaf_node>());
            } else {
                inner_node::free(as_node<inner_node>());
            }
        }

        size_t node_size(unsigned depth) const noexcept {
            if (depth == leaf_depth) {
                return as_base_of<leaf_node>().node_size();
            } else {
                return as_base_of<inner_node>().node_size();
            }
        }

        /*
         * A leaf-node specific helper for iterator
         */
        lower_bound_res lower_bound(key_t key) const noexcept {
            return as_base_of<leaf_node>().lower_bound(key, leaf_depth);
        }

        /*
         * And two inner-node specific calls for nodes
         * squashing/expanding
         */

        void set_lower(node_index_t ni, node_head* n) noexcept {
            as_node<inner_node>().set_lower(ni, n);
        }

        node_head_ptr pop_lower() noexcept {
            return as_node<inner_node>().pop_lower();
        }
    };

    /*
     * Pointer to node head. Inner nodes keep these, tree root pointer
     * is the one as well.
     */
    class node_head_ptr {
        node_head* _v;

    public:
        node_head_ptr(node_head* v) noexcept : _v(v) {}
        node_head_ptr(const node_head_ptr&) = delete;
        node_head_ptr(node_head_ptr&& o) noexcept : _v(std::exchange(o._v, nullptr)) {
            if (_v != nullptr) {
                _v->_backref = this;
            }
        }

        node_head& operator*() const noexcept { return *_v; }
        node_head* operator->() const noexcept { return _v; }
        node_head* raw() const noexcept { return _v; }

        operator bool() const noexcept { return _v != nullptr; }
        bool is(const node_head& n) const noexcept { return _v == &n; }

        node_head_ptr& operator=(node_head* v) noexcept {
            _v = v;
            // Checking (_v != &nil_root) is not needed for correctness, since
            // nil_root's _backref is never read anyway. But we do this check for
            // performance reasons: since nil_root is shared between shards,
            // writing to it would cause serious cache contention.
            if (_v != nullptr && _v != &nil_root) {
                _v->_backref = this;
            }
            return *this;
        }
    };

    /*
     * This helper wraps several layouts into one and preceeds them with
     * the header. It does nothing but provides a polymorphic calls to the
     * lower/inner layouts depending on the head.base_layout value.
     */
    template <typename Slot, typename... Layouts>
    struct node_base {
        node_head _head;
        variadic_union<Layouts...> _layouts;

        template <typename Tx>
        static size_t node_size(layout lt, uint8_t capacity) noexcept {
            return sizeof(node_head) + Tx::layout_size(capacity);
        }

        template <typename Tx, typename Ty, typename... Ts>
        static size_t node_size(layout lt, uint8_t capacity) noexcept {
            return lt == Tx::this_layout ? sizeof(node_head) + Tx::layout_size(capacity) : node_size<Ty, Ts...>(lt, capacity);
        }

        static size_t node_size(layout lt, uint8_t capacity) noexcept {
            return node_size<Layouts...>(lt, capacity);
        }

        size_t node_size() const noexcept {
            return node_size(_head._base_layout, _head._capacity);
        }

        // construct

        template <typename Tx>
        void construct(variadic_union<Tx>& cur) noexcept {
            new (&cur._this) Tx(_head);
        }

        template <typename Tx, typename Ty, typename... Ts>
        void construct(variadic_union<Tx, Ty, Ts...>& cur) noexcept {
            if (_head._base_layout == Tx::this_layout) {
                new (&cur._this) Tx(_head);
                return;
            }

            construct<Ty, Ts...>(cur._other);
        }

        node_base(key_t prefix, layout lt, uint8_t capacity) noexcept
                : _head(prefix, lt, capacity) {
            construct<Layouts...>(_layouts);
        }

        node_base(const node_base&) = delete;

        template <typename Tx>
        void move_construct(variadic_union<Tx>& cur, variadic_union<Tx>&& o) noexcept {
            new (&cur._this) Tx(std::move(o._this), _head);
        }

        template <typename Tx, typename Ty, typename... Ts>
        void move_construct(variadic_union<Tx, Ty, Ts...>& cur, variadic_union<Tx, Ty, Ts...>&& o) noexcept {
            if (_head._base_layout == Tx::this_layout) {
                new (&cur._this) Tx(std::move(o._this), _head);
                return;
            }

            move_construct<Ty, Ts...>(cur._other, std::move(o._other));
        }

        node_base(node_base&& o) noexcept
                : _head(std::move(o._head)) {
            move_construct<Layouts...>(_layouts, std::move(o._layouts));
        }

        ~node_base() { }

        // get value by key

        template <typename Tx>
        const T* get(const variadic_union<Tx>& cur, key_t key, unsigned depth) const noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.get(_head, key, depth);
            }

            return (const T*)nullptr;
        }

        template <typename Tx, typename Ty, typename... Ts>
        const T* get(const variadic_union<Tx, Ty, Ts...>& cur, key_t key, unsigned depth) const noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.get(_head, key, depth);
            }

            return get<Ty, Ts...>(cur._other, key, depth);
        }

        const T* get(key_t key, unsigned depth) const noexcept {
            return get<Layouts...>(_layouts, key, depth);
        }

        // finds a lowed-bound element

        template <typename Tx>
        lower_bound_res lower_bound(const variadic_union<Tx>& cur, key_t key, unsigned depth) const noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.lower_bound(_head, key, depth);
            }

            return lower_bound_res();
        }

        template <typename Tx, typename Ty, typename... Ts>
        lower_bound_res lower_bound(const variadic_union<Tx, Ty, Ts...>& cur, key_t key, unsigned depth) const noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.lower_bound(_head, key, depth);
            }

            return lower_bound<Ty, Ts...>(cur._other, key, depth);
        }

        lower_bound_res lower_bound(key_t key, unsigned depth) const noexcept {
            return lower_bound<Layouts...>(_layouts, key, depth);
        }

        // erase by key

        template <typename Tx>
        erase_result erase(variadic_union<Tx>& cur, key_t key, unsigned depth, erase_mode erm) noexcept {
            return cur._this.erase(_head, key, depth, erm);
        }

        template <typename Tx, typename Ty, typename... Ts>
        erase_result erase(variadic_union<Tx, Ty, Ts...>& cur, key_t key, unsigned depth, erase_mode erm) noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.erase(_head, key, depth, erm);
            }

            return erase<Ty, Ts...>(cur._other, key, depth, erm);
        }

        erase_result erase(key_t key, unsigned depth, erase_mode erm) noexcept {
            return erase<Layouts...>(_layouts, key, depth, erm);
        }

        // weed values with filter

        template <typename Fn, typename Tx>
        erase_result weed(variadic_union<Tx>& cur, Fn&& filter, unsigned depth) {
            return cur._this.weed(_head, filter, _head._prefix, depth);
        }

        template <typename Fn, typename Tx, typename Ty, typename... Ts>
        erase_result weed(variadic_union<Tx, Ty, Ts...>& cur, Fn&& filter, unsigned depth) {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.weed(_head, filter, _head._prefix, depth);
            }

            return weed<Fn, Ty, Ts...>(cur._other, filter, depth);
        }

        template <typename Fn>
        erase_result weed(Fn&& filter, unsigned depth) {
            return weed<Fn, Layouts...>(_layouts, filter, depth);
        }

        // allocate new slot

        template <typename Tx>
        allocate_res alloc(variadic_union<Tx>& cur, key_t key, unsigned depth) {
            return cur._this.alloc(_head, key, depth);
        }

        template <typename Tx, typename Ty, typename... Ts>
        allocate_res alloc(variadic_union<Tx, Ty, Ts...>& cur, key_t key, unsigned depth) {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.alloc(_head, key, depth);
            }

            return alloc<Ty, Ts...>(cur._other, key, depth);
        }

        allocate_res alloc(key_t key, unsigned depth) {
            return alloc<Layouts...>(_layouts, key, depth);
        }

        // append slot to node

        template <typename Tx>
        void append(variadic_union<Tx>& cur, node_index_t ni, Slot&& val) noexcept {
            cur._this.append(_head, ni, std::move(val));
        }

        template <typename Tx, typename Ty, typename... Ts>
        void append(variadic_union<Tx, Ty, Ts...>& cur, node_index_t ni, Slot&& val) noexcept {
            if (_head._base_layout == Tx::this_layout) {
                cur._this.append(_head, ni, std::move(val));
                return;
            }

            append<Ty, Ts...>(cur._other, ni, std::move(val));
        }

        void append(node_index_t ni, Slot&& val) noexcept {
            return append<Layouts...>(_layouts, ni, std::move(val));
        }

        // find and remove some element (usually the last one)

        template <typename Tx>
        Slot pop(variadic_union<Tx>& cur) noexcept {
            return cur._this.pop(_head);
        }

        template <typename Tx, typename Ty, typename... Ts>
        Slot pop(variadic_union<Tx, Ty, Ts...>& cur) noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.pop(_head);
            }

            return pop<Ty, Ts...>(cur._other);
        }

        Slot pop() noexcept {
            return pop<Layouts...>(_layouts);
        }

        // visiting

        template <typename Visitor, typename Tx>
        bool visit(const variadic_union<Tx>& cur, Visitor&& v, unsigned depth) const {
            return cur._this.visit(_head, v, depth);
        }

        template <typename Visitor, typename Tx, typename Ty, typename... Ts>
        bool visit(const variadic_union<Tx, Ty, Ts...>& cur, Visitor&& v, unsigned depth) const {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.visit(_head, v, depth);
            }

            return visit<Visitor, Ty, Ts...>(cur._other, v, depth);
        }

        template <typename Visitor>
        bool visit(Visitor&& v, unsigned depth) const {
            return visit<Visitor, Layouts...>(_layouts, v, depth);
        }

        // cloning

        template <typename NT, typename Fn, typename Tx>
        clone_res clone(const variadic_union<Tx>& cur, Fn&& cloner, unsigned depth) const noexcept {
            return cur._this.template clone<NT, Fn>(_head, cloner, depth);
        }

        template <typename NT, typename Fn, typename Tx, typename Ty, typename... Ts>
        clone_res clone(const variadic_union<Tx, Ty, Ts...>& cur, Fn&& cloner, unsigned depth) const noexcept {
            if (_head._base_layout == Tx::this_layout) {
                return cur._this.template clone<NT, Fn>(_head, cloner, depth);
            }

            return clone<NT, Fn, Ty, Ts...>(cur._other, cloner, depth);
        }

        template <typename NT, typename Fn>
        clone_res clone(Fn&& cloner, unsigned depth) const noexcept {
            return clone<NT, Fn, Layouts...>(_layouts, cloner, depth);
        }

        // growing into larger layout

        template <typename NT, typename Tx>
        node_head* grow(variadic_union<Tx>& cur, node_index_t want_ni) {
            if constexpr (Tx::growable) {
                return cur._this.template grow<NT>(_head, want_ni);
            }

            std::abort();
        }

        template <typename NT, typename Tx, typename Ty, typename... Ts>
        node_head* grow(variadic_union<Tx, Ty, Ts...>& cur, node_index_t want_ni) {
            if constexpr (Tx::growable) {
                if (_head._base_layout == Tx::this_layout) {
                    return cur._this.template grow<NT>(_head, want_ni);
                }
            }

            return grow<NT, Ty, Ts...>(cur._other, want_ni);
        }

        template <typename NT>
        node_head* grow(node_index_t want_ni) {
            return grow<NT, Layouts...>(_layouts, want_ni);
        }

        // shrinking into smaller layout

        template <typename NT, typename Tx>
        node_head* shrink(variadic_union<Tx>& cur) {
            if constexpr (Tx::shrinkable) {
                return cur._this.template shrink<NT>(_head);
            }

            std::abort();
        }

        template <typename NT, typename Tx, typename Ty, typename... Ts>
        node_head* shrink(variadic_union<Tx, Ty, Ts...>& cur) {
            if constexpr (Tx::shrinkable) {
                if (_head._base_layout == Tx::this_layout) {
                    return cur._this.template shrink<NT>(_head);
                }
            }

            return shrink<NT, Ty, Ts...>(cur._other);
        }

        template <typename NT>
        node_head* shrink() {
            return shrink<NT, Layouts...>(_layouts);
        }
    };

    /*
     * Node layouts. Define the way indices and payloads are stored on the node
     */

    /*
     * Direct layout is just an array of data.
     *
     * It makes a difference between inner slots, that are pointers to other nodes,
     * and leaf slots, which are of user type. The former can be nullptr denoting
     * the missing slot, while the latter may not have this sign, so the layout
     * uses a bitmask to check if a slot is occupiued or not.
     */
    template <typename Slot, layout Layout, layout GrowInto, unsigned GrowThreshold, layout ShrinkInto, unsigned ShrinkThreshold>
    struct direct_layout {
        static constexpr bool shrinkable = ShrinkInto != layout::nil;
        static constexpr bool growable = GrowInto != layout::nil;
        static constexpr layout this_layout = Layout;

        static bool check_capacity(const node_head& head, node_index_t ni) noexcept {
            if constexpr (this_layout == layout::direct_static) {
                return true;
            } else {
                return ni < head._capacity;
            }
        }

        static unsigned capacity(const node_head& head) noexcept {
            if constexpr (this_layout == layout::direct_static) {
                return node_index_limit;
            } else {
                return head._capacity;
            }
        }

        struct array_of_non_node_head_ptr {
            /*
             * This bismask is the maximum possible, while the array of slots
             * is dynamic. This is to make sure all direct layouts have the
             * slots at the same offset, so we may not introduce new layouts
             * for it, and to avoid some capacity if-s in the code below
             */
            std::bitset<node_index_limit> _present;
            Slot _slots[0];

            array_of_non_node_head_ptr(const node_head& head) noexcept {
                _present.reset();
            }

            array_of_non_node_head_ptr(array_of_non_node_head_ptr&& o, const node_head& head) noexcept
                    : _present(std::move(o._present)) {
                for (unsigned i = 0; i < capacity(head); i++) {
                    if (o.has(i)) {
                        new (&_slots[i]) Slot(std::move(o._slots[i]));
                        o._slots[i].~Slot();
                    }
                }
            }

            array_of_non_node_head_ptr(const array_of_non_node_head_ptr&) = delete;

            bool has(unsigned i) const noexcept { return _present.test(i); }
            bool has(const node_head& h, unsigned i) const noexcept { return has(i); }
            void add(node_head& head, unsigned i) noexcept { _present.set(i); }
            void del(node_head& head, unsigned i) noexcept { _present.set(i, false); }
            unsigned count(const node_head& head) const noexcept { return _present.count(); }
        };

        struct array_of_node_head_ptr {
            Slot _slots[0];

            array_of_node_head_ptr(const node_head& head) noexcept {
                for (unsigned i = 0; i < capacity(head); i++) {
                    new (&_slots[i]) node_head_ptr(nullptr);
                }
            }

            array_of_node_head_ptr(array_of_node_head_ptr&& o, const node_head& head) noexcept {
                for (unsigned i = 0; i < capacity(head); i++) {
                    new (&_slots[i]) Slot(std::move(o._slots[i]));
                    o._slots[i].~Slot();
                }
            }

            array_of_node_head_ptr(const array_of_node_head_ptr&) = delete;

            bool has(unsigned i) const noexcept { return _slots[i]; }
            bool has(const node_head& h, unsigned i) const noexcept { return check_capacity(h, i) && _slots[i]; }
            void add(node_head& head, unsigned i) noexcept { head._size++; }
            void del(node_head& head, unsigned i) noexcept { head._size--; }
            unsigned count(const node_head& head) const noexcept { return head._size; }
        };

        using array_of_slot = std::conditional_t<std::is_same_v<Slot, node_head_ptr>, array_of_node_head_ptr, array_of_non_node_head_ptr>;

        array_of_slot _data;

        direct_layout(const node_head& head) noexcept : _data(head) {}
        direct_layout(direct_layout&& o, const node_head& head) noexcept : _data(std::move(o._data), head) {}
        direct_layout(const direct_layout&) = delete;

        const T* get(const node_head& head, key_t key, unsigned depth) const noexcept {
            node_index_t ni = node_index(key, depth);
            if (!_data.has(head, ni)) {
                return nullptr;
            }
            return get_at(_data._slots[ni], key, depth + 1);
        }

        Slot pop(node_head& head) noexcept {
            for (unsigned i = 0; i < capacity(head); i++) {
                if (_data.has(i)) {
                    Slot ret = std::move(_data._slots[i]);
                    _data.del(head, i);
                    _data._slots[i].~Slot();
                    return ret;
                }
            }

            return nullptr;
        }

        allocate_res alloc(node_head& head, key_t key, unsigned depth) {
            node_index_t ni = node_index(key, depth);

            if (!check_capacity(head, ni)) {
                return allocate_res(nullptr, false);
            }

            bool exists = _data.has(ni);

            if (!exists) {
                populate_slot(_data._slots[ni], key, depth + 1);
                _data.add(head, ni);
            }

            return allocate_on(_data._slots[ni], key, depth + 1, !exists);
        }

        void append(node_head& head, node_index_t ni, Slot&& val) noexcept {
            assert(check_capacity(head, ni));
            assert(!_data.has(ni));
            _data.add(head, ni);
            new (&_data._slots[ni]) Slot(std::move(val));
        }

        erase_result erase(node_head& head, key_t key, unsigned depth, erase_mode erm) noexcept {
            node_index_t ni = node_index(key, depth);

            if (_data.has(head, ni)) {
                if (erase_from_slot(&_data._slots[ni], key, depth + 1, erm)) {
                    _data.del(head, ni);
                    return after_drop<ShrinkThreshold>(_data.count(head));
                }
            }

            return erase_result::nothing;
        }

        template <typename Fn>
        erase_result weed(node_head& head, Fn&& filter, key_t pfx, unsigned depth) {
            bool removed_something = false;

            for (unsigned i = 0; i < capacity(head); i++) {
                if (_data.has(i)) {
                    if (weed_from_slot(head, i, &_data._slots[i], filter, depth + 1)) {
                        _data.del(head, i);
                        removed_something = true;
                    }
                }
            }

            return removed_something ? after_drop<ShrinkThreshold>(_data.count(head)) : erase_result::nothing;
        }

        template <typename NT, typename Cloner>
        clone_res clone(const node_head& head, Cloner&& clone, unsigned depth) const noexcept {
            NT* nn;
            try {
                nn = NT::allocate(head._prefix, head._base_layout, head._capacity);
            } catch (...) {
                return clone_res(nullptr, std::current_exception());
            }

            auto ex = copy_slots(head, _data._slots, capacity(head), depth, nn->_base,
                        [this] (unsigned i) noexcept { return _data.has(i) ? i : unused_node_index; }, clone);
            return std::make_pair(&nn->_base._head, std::move(ex));
        }

        template <typename NT>
        node_head* grow(node_head& head, node_index_t want_ni) {
            static_assert(GrowInto == layout::direct_dynamic && GrowThreshold == 0);

            uint8_t next_cap = head._capacity << 1;
            while (want_ni >= next_cap) {
                next_cap <<= 1;
            }
            assert(next_cap > head._capacity);

            NT* nn = NT::allocate(head._prefix, layout::direct_dynamic, next_cap);
            move_slots(_data._slots, head._capacity, head._capacity + 1, nn->_base,
                    [this] (unsigned i) noexcept { return _data.has(i) ? i : unused_node_index; });
            head._size = 0;
            return &nn->_base._head;
        }

        template <typename NT>
        node_head* shrink(node_head& head) {
            static_assert(shrinkable && ShrinkThreshold != 0);

            NT* nn = NT::allocate(head._prefix, ShrinkInto);
            move_slots(_data._slots, node_index_limit, ShrinkThreshold, nn->_base,
                    [this] (unsigned i) noexcept { return _data.has(i) ? i : unused_node_index; });
            head._size = 0;
            return &nn->_base._head;
        }

        lower_bound_res lower_bound(const node_head& head, key_t key, unsigned depth) const noexcept {
            node_index_t ni = node_index(key, depth);

            if (_data.has(head, ni)) {
                lower_bound_res ret = lower_bound_at(&_data._slots[ni], head, ni, key, depth);
                if (ret.elem != nullptr) {
                    return ret;
                }
            }

            for (unsigned i = ni + 1; i < capacity(head); i++) {
                if (_data.has(i)) {
                    /*
                     * Nothing was found on the slot, that matches the
                     * given index. We need to move to the next one, but
                     * zero-out all key's bits related to lower levels.
                     *
                     * Fortunately, leaf nodes will rewrite the whole
                     * thing on match, so put 0 into the whole key.
                     *
                     * Also note, that short-cut iterator++ assumes that
                     * index is NOT 0-ed in case of mismatch!
                     */
                    return lower_bound_at(&_data._slots[i], head, i, 0, depth);
                }
            }

            return lower_bound_res();
        }

        template <typename Visitor>
        bool visit(const node_head& head, Visitor&& v, unsigned depth) const {
            for (unsigned i = 0; i < capacity(head); i++) {
                if (_data.has(i)) {
                    if (!visit_slot(v, head, i, &_data._slots[i], depth)) {
                        return false;
                    }
                }
            }
            return true;
        }

        static size_t layout_size(uint8_t capacity) noexcept {
            if constexpr (this_layout == layout::direct_static) {
                return sizeof(direct_layout) + node_index_limit * sizeof(Slot);
            } else {
                assert(capacity != 0);
                return sizeof(direct_layout) + capacity * sizeof(Slot);
            }
        }
    };

    /*
     * The indirect layout is used to keep small number of sparse keys on
     * small node. To do that it keeps an array of indices and when is
     * asked to get an element searches in this array. This map additionally
     * works as a presense bitmask from direct layout.
     *
     * Since indirect layouts of different sizes have slots starting at
     * different addresses in memory, they cannot grow dynamically, but are
     * converted (by moving data) into each other.
     */
    template <typename Slot, layout Layout, unsigned Size, layout GrowInto, unsigned GrowThreshold, layout ShrinkInto, unsigned ShrinkThreshold>
    struct indirect_layout {
        static constexpr bool shrinkable = ShrinkInto != layout::nil;
        static constexpr bool growable = GrowInto != layout::nil;
        static constexpr unsigned size = Size;
        static constexpr layout this_layout = Layout;

        node_index_t _idx[Size];
        Slot _slots[0];

        bool has(unsigned i) const noexcept { return _idx[i] != unused_node_index; }
        void unset(unsigned i) noexcept { _idx[i] = unused_node_index; }

        indirect_layout(const node_head& head) noexcept {
            for (unsigned i = 0; i < Size; i++) {
                _idx[i] = unused_node_index;
            }
        }

        indirect_layout(indirect_layout&& o, const node_head& head) noexcept {
            for (unsigned i = 0; i < Size; i++) {
                _idx[i] = o._idx[i];
                if (o.has(i)) {
                    new (&_slots[i]) Slot(std::move(o._slots[i]));
                    o._slots[i].~Slot();
                }
            }
        }

        indirect_layout(const indirect_layout&) = delete;

        const T* get(const node_head& head, key_t key, unsigned depth) const noexcept {
            node_index_t ni = node_index(key, depth);
            unsigned i = find_in_array<Size>(ni, _idx);
            if (i >= Size) {
                return nullptr;
            }
            return get_at(_slots[i], key, depth + 1);
        }

        Slot pop(node_head& head) noexcept {
            for (unsigned i = 0; i < Size; i++) {
                if (has(i)) {
                    Slot ret = std::move(_slots[i]);
                    head._size--;
                    _slots[i].~Slot();
                    return ret;
                }
            }

            return nullptr;
        }

        allocate_res alloc(node_head& head, key_t key, unsigned depth) {
            node_index_t ni = node_index(key, depth);
            bool new_slot = false;
            unsigned i = find_in_array<Size>(ni, _idx);
            if (i >= Size) {
                i = find_in_array<Size>(unused_node_index, _idx);
                if (i >= Size) {
                    return allocate_res(nullptr, false);
                }

                populate_slot(_slots[i], key, depth + 1);
                _idx[i] = ni;
                head._size++;
                new_slot = true;
            }

            return allocate_on(_slots[i], key, depth + 1, new_slot);
        }

        void append(node_head& head, node_index_t ni, Slot&& val) noexcept {
            unsigned i = head._size++;
            assert(i < Size);
            assert(_idx[i] == unused_node_index);
            _idx[i] = ni;
            new (&_slots[i]) Slot(std::move(val));
        }

        erase_result erase(node_head& head, key_t key, unsigned depth, erase_mode erm) noexcept {
            node_index_t ni = node_index(key, depth);
            unsigned i = find_in_array<Size>(ni, _idx);
            if (i < Size) {
                if (erase_from_slot(&_slots[i], key, depth + 1, erm)) {
                    unset(i);
                    head._size--;
                    return after_drop<ShrinkThreshold>(head._size);
                }
            }

            return erase_result::nothing;
        }

        template <typename Fn>
        erase_result weed(node_head& head, Fn&& filter, key_t pfx, unsigned depth) {
            bool removed_something = false;

            for (unsigned i = 0; i < Size; i++) {
                if (has(i)) {
                    if (weed_from_slot(head, _idx[i], &_slots[i], filter, depth + 1)) {
                        unset(i);
                        head._size--;
                        removed_something = true;
                    }
                }
            }

            return removed_something ? after_drop<ShrinkThreshold>(head._size) : erase_result::nothing;
        }

        template <typename NT, typename Cloner>
        clone_res clone(const node_head& head, Cloner&& clone, unsigned depth) const noexcept {
            NT* nn;
            try {
                nn = NT::allocate(head._prefix, head._base_layout, head._capacity);
            } catch (...) {
                return clone_res(nullptr, std::current_exception());
            }

            auto ex = copy_slots(head, _slots, Size, depth, nn->_base, [this] (unsigned i) noexcept { return _idx[i]; }, clone);
            return std::make_pair(&nn->_base._head, std::move(ex));
        }

        template <typename NT>
        node_head* grow(node_head& head, node_index_t want_ni) {
            static_assert(growable && GrowThreshold == 0);

            NT* nn = NT::allocate(head._prefix, GrowInto);
            move_slots(_slots, Size, Size + 1, nn->_base, [this] (unsigned i) noexcept { return _idx[i]; });
            head._size = 0;
            return &nn->_base._head;
        }

        template <typename NT>
        node_head* shrink(node_head& head) {
            static_assert(shrinkable && ShrinkThreshold != 0);

            NT* nn = NT::allocate(head._prefix, ShrinkInto);
            move_slots(_slots, Size, ShrinkThreshold, nn->_base, [this] (unsigned i) noexcept { return _idx[i]; });
            head._size = 0;
            return &nn->_base._head;
        }

        lower_bound_res lower_bound(const node_head& head, key_t key, unsigned depth) const noexcept {
            node_index_t ni = node_index(key, depth);
            unsigned i = find_in_array<Size>(ni, _idx);
            if (i < Size) {
                lower_bound_res ret = lower_bound_at(&_slots[i], head, _idx[i], key, depth);
                if (ret.elem != nullptr) {
                    return ret;
                }
            }

            unsigned ui = Size;
            for (unsigned i = 0; i < Size; i++) {
                if (has(i) && _idx[i] > ni && (ui == Size || _idx[i] < _idx[ui])) {
                    ui = i;
                }
            }

            if (ui == Size) {
                return lower_bound_res();
            }

            // See comment in direct_layout about the zero key argument
            return lower_bound_at(&_slots[ui], head, _idx[ui], 0, depth);
        }

        template <typename Visitor>
        bool visit(const node_head& head, Visitor&& v, unsigned depth) const {
            /*
             * Two common-case fast paths that save notable amount
             * of instructions from below.
             */
            if (head._size == 0) {
                return true;
            }

            if (head._size == 1 && has(0)) {
                return visit_slot(v, head, _idx[0], &_slots[0], depth);
            }

            unsigned indices[Size];
            unsigned sz = 0;

            for (unsigned i = 0; i < Size; i++) {
                if (has(i)) {
                    indices[sz++] = i;
                }
            }

            if (v.sorted) {
                std::sort(indices, indices + sz, [this] (int a, int b) {
                    return _idx[a] < _idx[b];
                });
            }

            for (unsigned i = 0; i < sz; i++) {
                unsigned pos = indices[i];
                if (!visit_slot(v, head, _idx[pos], &_slots[pos], depth)) {
                    return false;
                }
            }
            return true;
        }

        static size_t layout_size(uint8_t capacity) noexcept { return sizeof(indirect_layout) + size * sizeof(Slot); }
    };


    template <typename SlotType, typename FN>
    static void move_slots(SlotType* slots, unsigned nr, unsigned thresh, auto& into, FN&& node_index_of) noexcept {
        unsigned count = 0;
        for (unsigned i = 0; i < nr; i++) {
            node_index_t ni = node_index_of(i);
            if (ni != unused_node_index) {
                into.append(ni, std::move(slots[i]));
                slots[i].~SlotType();
                if (++count >= thresh) {
                    break;
                }
            }
        }
    }

    template <typename FN, typename Cloner>
    static std::exception_ptr copy_slots(const node_head& h, const T* slots, unsigned nr, unsigned depth, auto& into, FN&& node_index_of, Cloner&& cloner) noexcept {
        for (unsigned i = 0; i < nr; i++) {
            node_index_t ni = node_index_of(i);
            if (ni != unused_node_index) {
                try {
                    into.append(ni, cloner(h.key_of(ni), slots[i]));
                } catch (...) {
                    return std::current_exception();
                }
            }
        }
        return nullptr;
    }

    template <typename FN, typename Cloner>
    static std::exception_ptr copy_slots(const node_head& h, const node_head_ptr* slots, unsigned nr, unsigned depth, auto& into, FN&& node_index_of, Cloner&& cloner) noexcept {
        for (unsigned i = 0; i < nr; i++) {
            node_index_t ni = node_index_of(i);
            if (ni != unused_node_index) {
                clone_res res = slots[i]->clone(cloner, depth + 1);
                if (res.first != nullptr) {
                    /*
                     * Append the slot anyway. It may happen that this inner node
                     * has some entries on board, so rather than clearing them
                     * here -- propagate the exception back and let the top-level
                     * code call clear() on everything, including this half-cloned
                     * node.
                     */
                    into.append(ni, std::move(res.first));
                }
                if (res.second) {
                    return res.second;
                }
            }
        }
        return nullptr;
    }

    /*
     * Expand a node that failed prefix check.
     * Turns a node with non-zero prefix on which parent tries to allocate
     * an index beyond its limits. For this:
     * - the inner node is allocated on the level, that's enough to fit
     *   both -- current node and the desired index
     * - the given node is placed into this new inner one at the index it's
     *   expected to be found there (the prefix value)
     * - the allocation continues on this new inner (with fixed depth)
     */
    static node_head* expand(node_head& n, key_t key, unsigned& depth) {
        key_t n_prefix = n._prefix;

        /*
         * The plen is the level at which current node and desired
         * index still coinside
         */
        unsigned plen = common_prefix_len(key, n_prefix);
        assert(plen >= depth);
        plen -= depth;
        depth += plen;
        assert(n.prefix_len() > plen);

        node_index_t ni = node_index(n_prefix, depth);
        node_head* nn = inner_node::allocate_initial(make_prefix(key, plen), ni);
        // Trim all common nodes + nn one from n
        n.trim_prefix(plen + 1);
        nn->set_lower(ni, &n);

        return nn;
    }

    /*
     * Pop one the single lower node and prepare it to replace the
     * current one. This preparation is purely increasing its prefix
     * len, as the prefix value itself is already correct
     */
    static node_head* squash(node_head* n, unsigned depth) noexcept {
        const node_head_ptr np = n->pop_lower();
        node_head* kid = np.raw();
        assert(kid != nullptr);
        // Kid has n and it's prefix squashed
        kid->bump_prefix(n->prefix_len() + 1);
        return kid;
    }

    static bool maybe_drop_from(node_head_ptr* np, erase_result res, unsigned depth) noexcept {
        node_head* n = np->raw();

        switch (res) {
        case erase_result::empty:
            n->free(depth);
            *np = nullptr;
            return true;

        case erase_result::squash:
            if (depth != leaf_depth) {
                *np = squash(n, depth);
                n->free(depth);
            }
            break;
        case erase_result::shrink:
            try {
                *np = n->shrink(depth);
                n->free(depth);
            } catch(...) {
                /*
                 * The node tried to shrink but failed to
                 * allocate memory for the new layout. This
                 * is not that bad, it can survive in current
                 * layout and be shrunk (or squashed or even
                 * dropped) later.
                 */
            }
            break;
        case erase_result::nothing: ; // make compiler happy
        }

        return false;
    }

    static const T* get_at(const T& val, key_t key, unsigned depth) noexcept { return &val; }

    static const T* get_at(const node_head_ptr& np, key_t key, unsigned depth = 0) noexcept {
        if (!np->check_prefix(key, depth)) {
            return nullptr;
        }

        return np->get(key, depth);
    }

    static allocate_res allocate_on(T& val, key_t key, unsigned depth, bool allocated) noexcept {
        return allocate_res(&val, allocated);
    }

    static allocate_res allocate_on(node_head_ptr& n, key_t key, unsigned depth = 0, bool _ = false) {
        if (!n->check_prefix(key, depth)) {
            n = expand(*n, key, depth);
        }

        allocate_res ret = n->alloc(key, depth);
        if (ret.first == nullptr) {
            /*
             * The nullptr ret means the n has run out of
             * free slots. Grow one into bigger layout and
             * try again
             */
            node_head* nn = n->grow(key, depth);
            n->free(depth);
            n = nn;
            ret = nn->alloc(key, depth);
            assert(ret.first != nullptr);
        }
        return ret;
    }

    // Populating value slot happens in tree::emplace
    static void populate_slot(T& val, key_t key, unsigned depth) noexcept { }

    static void populate_slot(node_head_ptr& np, key_t key, unsigned depth) {
        /*
         * Allocate leaf immediatelly with the prefix
         * len big enough to cover all skipped node
         * up to the current depth
         */
        assert(leaf_depth >= depth);
        np = leaf_node::allocate_initial(make_prefix(key, leaf_depth - depth));
    }

    template <typename Visitor>
    static bool visit_slot(Visitor&& v, const node_head& n, node_index_t ni, const T* val, unsigned depth) {
        return v(n.key_of(ni), *val);
    }

    template <typename Visitor>
    static bool visit_slot(Visitor&& v, const node_head& n, node_index_t, const node_head_ptr* ptr, unsigned depth) {
        return (*ptr)->visit(v, depth + 1);
    }

    static lower_bound_res lower_bound_at(const T* val, const node_head& n, node_index_t ni, key_t, unsigned) noexcept {
        return lower_bound_res(val, n, n.key_of(ni));
    }

    static lower_bound_res lower_bound_at(const node_head_ptr* ptr, const node_head&, node_index_t, key_t key, unsigned depth) noexcept {
        return (*ptr)->lower_bound(key, depth + 1);
    }

    template <typename Fn>
    static bool weed_from_slot(node_head& n, node_index_t ni, T* val, Fn&& filter, unsigned depth) {
        if (!filter(n.key_of(ni), *val)) {
            return false;
        }

        val->~T();
        return true;
    }

    template <typename Fn>
    static bool weed_from_slot(node_head&, node_index_t, node_head_ptr* np, Fn&& filter, unsigned depth) {
        return weed_from_slot(np, filter, depth);
    }

    template <typename Fn>
    static bool weed_from_slot(node_head_ptr* np, Fn&& filter, unsigned depth) {
        node_head* n = np->raw();
        depth += n->prefix_len();

        erase_result er = n->weed(filter, depth);

        // FIXME -- after weed the node might want to shrink into
        // even smaller, than just previous, layout
        return maybe_drop_from(np, er, depth);
    }

    static bool erase_from_slot(T* val, key_t key, unsigned depth, erase_mode erm) noexcept {
        if (erm == erase_mode::real) {
            val->~T();
        }

        return true;
    }

    static bool erase_from_slot(node_head_ptr* np, key_t key, unsigned depth, erase_mode erm) noexcept {
        node_head* n = np->raw();
        assert(n->check_prefix(key, depth));

        erase_result er = n->erase(key, depth, erm);
        if (erm == erase_mode::cleanup) {
            return false;
        }

        return maybe_drop_from(np, er, depth);
    }

    template <typename Visitor>
    void visit(Visitor&& v) const {
        if (!_root.is(nil_root)) {
            _root->visit(std::move(v), 0);
        }
    }

    template <typename Visitor>
    void visit(Visitor&& v) {
        struct adaptor {
            Visitor&& v;
            bool sorted;

            bool operator()(key_t key, const T& val) {
                return v(key, const_cast<T&>(val));
            }

            bool operator()(const node_head& n, unsigned depth, bool enter) {
                return v(const_cast<node_head&>(n), depth, enter);
            }
        };

        const_cast<const tree*>(this)->visit(adaptor{std::move(v), v.sorted});
    }

    /*
     * Actual types that define inner and leaf nodes.
     *
     * Leaf nodes are the most numerous inhabitant of the tree and
     * they must be as small as possible for the tree to be memory
     * efficient. Opposite to this, inner nodes are ~1% of the tree,
     * so it's not that important to keep them small.
     *
     * At the same time ...
     *
     * When looking up a value leaf node is the last in a row and
     * makes a single lookup, while there can be several inner ones,
     * on which several lookups will be done.
     *
     * Said that ...
     *
     * Leaf nodes are optimized for size, but not for speed, so they
     * have several layouts to grow (and shrink) strictly on demand.
     *
     * Inner nodes are optimized for speed, and just a little-but for
     * size, so they are of direct dynamic size only.
     */

    class leaf_node {
        template <typename A, typename B> friend class printer;
        friend class tree;
        friend class node_head;
        template <typename A, layout L, unsigned S, layout GL, unsigned GT, layout SL, unsigned ST> friend class indirect_layout;
        template <typename A, layout L, layout GL, unsigned GT, layout SL, unsigned ST> friend class direct_layout;

        using tiny_node = indirect_layout<T, layout::indirect_tiny, 4, layout::indirect_small, 0, layout::nil, 0>;
        using small_node = indirect_layout<T, layout::indirect_small, 8, layout::indirect_medium, 0, layout::indirect_tiny, 4>;
        using medium_node = indirect_layout<T, layout::indirect_medium, 16, layout::indirect_large, 0, layout::indirect_small, 8>;
        using large_node = indirect_layout<T, layout::indirect_large, 32, layout::direct_static, 0, layout::indirect_medium, 16>;
        using direct_node = direct_layout<T, layout::direct_static, layout::nil, 0, layout::indirect_large, 32>;

    public:
        using node_type = node_base<T, tiny_node, small_node, medium_node, large_node, direct_node>;

        leaf_node(leaf_node&& other) noexcept : _base(std::move(other._base)) {}
        ~leaf_node() { }

        size_t storage_size() const noexcept {
            return _base.node_size();
        }

    private:
        node_type _base;

        leaf_node(key_t prefix, layout lt, uint8_t capacity) noexcept : _base(prefix, lt, capacity) { }
        leaf_node(const leaf_node&) = delete;

        static node_head* allocate_initial(key_t prefix) {
            return &allocate(prefix, layout::indirect_tiny)->_base._head;
        }

        static leaf_node* allocate(key_t prefix, layout lt, uint8_t capacity = 0) {
            void* mem = current_allocator().alloc<leaf_node>(node_type::node_size(lt, capacity));
            return new (mem) leaf_node(prefix, lt, capacity);
        }

        static void free(leaf_node& node) noexcept {
            node.~leaf_node();
            current_allocator().free(&node, node._base.node_size());
        }
    };

    class inner_node {
        template <typename A, typename B> friend class printer;
        friend class tree;
        friend class node_head;
        template <typename A, layout L, unsigned S, layout GL, unsigned GT, layout SL, unsigned ST> friend class indirect_layout;
        template <typename A, layout L, layout GL, unsigned GT, layout SL, unsigned ST> friend class direct_layout;

        static constexpr uint8_t initial_capacity = 4;

        using dynamic_node = direct_layout<node_head_ptr, layout::direct_dynamic, layout::direct_dynamic, 0, layout::nil, 0>;

    public:
        using node_type = node_base<node_head_ptr, dynamic_node>;

        inner_node(inner_node&& other) noexcept : _base(std::move(other._base)) {}
        ~inner_node() {}

        size_t storage_size() const noexcept {
            return _base.node_size();
        }

    private:
        node_type _base;

        inner_node(key_t prefix, layout lt, uint8_t capacity) noexcept : _base(prefix, lt, capacity) {}
        inner_node(const inner_node&) = delete;

        static node_head* allocate_initial(key_t prefix, node_index_t want_ni) {
            uint8_t capacity = initial_capacity;
            while (want_ni >= capacity) {
                capacity <<= 1;
            }
            return &allocate(prefix, layout::direct_dynamic, capacity)->_base._head;
        }

        static inner_node* allocate(key_t prefix, layout lt, uint8_t capacity = 0) {
            void* mem = current_allocator().alloc<inner_node>(node_type::node_size(lt, capacity));
            return new (mem) inner_node(prefix, lt, capacity);
        }

        static void free(inner_node& node) noexcept {
            node.~inner_node();
            current_allocator().free(&node, node._base.node_size());
        }

        node_head_ptr pop_lower() noexcept {
            return _base.pop();
        }

        void set_lower(node_index_t ni, node_head* n) noexcept {
            _base.append(ni, node_head_ptr(n));
        }
    };

    node_head_ptr _root;
    static inline node_head nil_root;

public:
    tree() noexcept : _root(&nil_root) {}
    ~tree() {
        clear();
    }

    tree(const tree&) = delete;
    tree(tree&& o) noexcept : _root(std::exchange(o._root, &nil_root)) {}

    const T* get(key_t key) const noexcept {
        return get_at(_root, key);
    }

    T* get(key_t key) noexcept {
        return const_cast<T*>(get_at(_root, key));
    }

    const T* lower_bound(key_t key) const noexcept {
        return _root->lower_bound(key, 0).elem;
    }

    T* lower_bound(key_t key) noexcept {
        return const_cast<T*>(const_cast<const tree*>(this)->lower_bound(key));
    }

    template <typename... Args>
    void emplace(key_t key, Args&&... args) {
        if (_root.is(nil_root)) {
            populate_slot(_root, key, 0);
        }

        allocate_res v = allocate_on(_root, key);
        if (!v.second) {
            v.first->~T();
        }

        try {
            new (v.first) T(std::forward<Args>(args)...);
        } catch (...) {
            erase_from_slot(&_root, key, 0, erase_mode::cleanup);
            throw;
        }
    }

    void erase(key_t key) noexcept {
        if (!_root.is(nil_root)) {
            erase_from_slot(&_root, key, 0, erase_mode::real);
            if (!_root) {
                _root = &nil_root;
            }
        }
    }

    void clear() noexcept {
        struct clearing_visitor {
            bool sorted = false;

            bool operator()(key_t key, T& val) noexcept {
                val.~T();
                return true;
            }
            bool operator()(node_head& n, unsigned depth, bool enter) noexcept {
                if (!enter) {
                    n._size = 0;
                    n.free(depth);
                }
                return true;
            }
        };

        visit(clearing_visitor{});
        _root = &nil_root;
    }

    template <typename Cloner>
    requires std::is_invocable_r<T, Cloner, key_t, const T&>::value
    void clone_from(const tree& tree, Cloner&& cloner) {
        assert(_root.is(nil_root));
        if (!tree._root.is(nil_root)) {
            clone_res cres = tree._root->clone(cloner, 0);
            if (cres.first != nullptr) {
                _root = cres.first;
            }
            if (cres.second) {
                clear();
                std::rethrow_exception(cres.second);
            }
        }
    }

    /*
     * Weed walks the tree and removes the elements for which the
     * fn returns true.
     */

    template <typename Fn>
    requires std::is_invocable_r<bool, Fn, key_t, T&>::value
    void weed(Fn&& filter) {
        if (!_root.is(nil_root)) {
            weed_from_slot(&_root, filter, 0);
            if (!_root) {
                _root = &nil_root;
            }
        }
    }

private:
    template <typename Fn, bool Const>
    struct walking_visitor {
            Fn&& fn;
            bool sorted;

            using value_t = std::conditional_t<Const, const T, T>;
            using node_t = std::conditional_t<Const, const node_head, node_head>;

            bool operator()(key_t key, value_t& val) {
                return fn(key, val);
            }
            bool operator()(node_t& n, unsigned depth, bool enter) noexcept {
                return true;
            }
    };

public:

    /*
     * Walking the tree element-by-element. The called function Fn
     * may return false to stop the walking and return.
     *
     * The \sorted value specifies if the walk should call Fn on
     * keys in ascending order. If it's false keys will be called
     * randomly, because indirect nodes store the slots without
     * sorting
     */
    template <typename Fn>
    requires std::is_invocable_r<bool, Fn, key_t, const T&>::value
    void walk(Fn&& fn, bool sorted = true) const {
        visit(walking_visitor<Fn, true>{std::move(fn), sorted});
    }

    template <typename Fn>
    requires std::is_invocable_r<bool, Fn, key_t, T&>::value
    void walk(Fn&& fn, bool sorted = true) {
        visit(walking_visitor<Fn, false>{std::move(fn), sorted});
    }

    template <bool Const>
    class iterator_base {
    public:
        using iterator_category = std::forward_iterator_tag;
        using value_type = std::conditional_t<Const, const T, T>;
        using difference_type = ssize_t;
        using pointer = value_type*;
        using reference = value_type&;

    private:
        key_t _key = 0;
        pointer _value = nullptr;
        const tree* _tree = nullptr;
        const node_head* _leaf = nullptr;

    public:
        key_t key() const noexcept { return _key; }

        iterator_base() noexcept = default;
        iterator_base(const tree* t) noexcept : _tree(t) {
            lower_bound_res res = _tree->_root->lower_bound(_key, 0);
            _leaf = res.leaf;
            _value = const_cast<pointer>(res.elem);
            _key = res.key;
        }

        iterator_base& operator++() noexcept {
            if (_value == nullptr) {
                _value = nullptr;
                return *this;
            }

            _key++;
            if (node_index(_key, leaf_depth) != 0) {
                /*
                 * Short-cut. If we're still inside the leaf,
                 * then it's worth trying to shift forward on
                 * it without messing with upper levels
                 */
                lower_bound_res res = _leaf->lower_bound(_key);
                _value = const_cast<pointer>(res.elem);
                if (_value != nullptr) {
                    _key = res.key;
                    return *this;
                }

                /*
                 * No luck. Go ahead and scan the tree from top
                 * again. It's only leaf_depth levels though. Also
                 * not to make the call below visit this leaf one
                 * more time, bump up the index to move out of the
                 * current leaf and keep the leaf's part zero.
                 */

                 _key += node_index_limit;
                 _key &= ~radix_mask;
            }

            lower_bound_res res = _tree->_root->lower_bound(_key, 0);
            _leaf = res.leaf;
            _value = const_cast<pointer>(res.elem);
            _key = res.key;

            return *this;
        }

        iterator_base operator++(int) noexcept {
            iterator_base cur = *this;
            operator++();
            return cur;
        }

        pointer operator->() const noexcept { return _value; }
        reference operator*() const noexcept { return *_value; }

        bool operator==(const iterator_base& o) const noexcept { return _value == o._value; }
    };

    using iterator = iterator_base<false>;
    using const_iterator = iterator_base<true>;

    iterator begin() noexcept { return iterator(this); }
    iterator end() noexcept { return iterator(); }
    const_iterator cbegin() const noexcept { return const_iterator(this); }
    const_iterator cend() const noexcept { return const_iterator(); }
    const_iterator begin() const noexcept { return cbegin(); }
    const_iterator end() const noexcept { return cend(); }

    bool empty() const noexcept { return _root.is(nil_root); }

    template <typename Fn>
    requires std::is_nothrow_invocable_r<size_t, Fn, key_t, const T&>::value
    size_t memory_usage(Fn&& entry_mem_usage) const noexcept {
        struct counting_visitor {
                Fn&& entry_mem_usage;
                bool sorted = false;
                size_t mem = 0;

                bool operator()(key_t key, const T& val) {
                    mem += entry_mem_usage(key, val);
                    return true;
                }
                bool operator()(const node_head& n, unsigned depth, bool enter) noexcept {
                    if (enter) {
                        mem += n.node_size(depth);
                    }
                    return true;
                }
        };

        counting_visitor v{std::move(entry_mem_usage)};
        visit(v);

        return v.mem;
    }

    struct stats {
        struct node_stats {
            unsigned long indirect_tiny = 0;
            unsigned long indirect_small = 0;
            unsigned long indirect_medium = 0;
            unsigned long indirect_large = 0;
            unsigned long direct_static = 0;
            unsigned long direct_dynamic = 0;
        };

        node_stats inners;
        node_stats leaves;
    };

    stats get_stats() const noexcept {
        struct counting_visitor {
            bool sorted = false;
            stats st;

            bool operator()(key_t key, const T& val) noexcept { std::abort(); }

            void update(typename stats::node_stats& ns, const node_head& n) const noexcept {
                switch (n._base_layout) {
                case layout::indirect_tiny: ns.indirect_tiny++; break;
                case layout::indirect_small: ns.indirect_small++; break;
                case layout::indirect_medium: ns.indirect_medium++; break;
                case layout::indirect_large: ns.indirect_large++; break;
                case layout::direct_static: ns.direct_static++; break;
                case layout::direct_dynamic: ns.direct_dynamic++; break;
                default: break;
                }
            }

            bool operator()(const node_head& n, unsigned depth, bool enter) noexcept {
                if (!enter) {
                    return true;
                }

                if (depth == leaf_depth) {
                    update(st.leaves, n);
                    return false; // don't visit elements
                } else {
                    update(st.inners, n);
                    return true;
                }
            }
        };

        counting_visitor v;
        visit(v);
        return v.st;
    }
};

} // namespace



#include <type_traits>
#include <seastar/util/concepts.hh>
#include <utility>

namespace utils {

/*
 * Wraps a collection into immutable form.
 *
 * Immutability here means that the collection itself cannot be modified,
 * i.e. adding or removing elements is not possible. Read-only methods such
 * as find(), begin()/end(), lower_bound(), etc. are available and are
 * transparently forwarded to the underlying collection. Return values from
 * those methods are also returned as-is so it's pretty much like a const
 * reference on the collection.
 *
 * The important difference from the const reference is that obtained
 * elements or iterators are not necessarily const too, so it's possible
 * to modify the found or iterated over elements.
 */

template <typename Collection>
class immutable_collection {
    Collection* _col;

public:
    immutable_collection(Collection& col) noexcept : _col(&col) {}

#define DO_WRAP_METHOD(method, is_const)                                                                           \
    template <typename... Args>                                                                                    \
    auto method(Args&&... args) is_const noexcept(noexcept(std::declval<is_const Collection>().method(args...))) { \
        return _col->method(std::forward<Args>(args)...);                                                           \
    }

#define WRAP_CONST_METHOD(method)    \
    DO_WRAP_METHOD(method, const)

#define WRAP_METHOD(method)          \
    WRAP_CONST_METHOD(method)        \
    DO_WRAP_METHOD(method, )

    WRAP_METHOD(find)
    WRAP_METHOD(lower_bound)
    WRAP_METHOD(upper_bound)
    WRAP_METHOD(slice)
    WRAP_METHOD(lower_slice)
    WRAP_METHOD(upper_slice)

    WRAP_CONST_METHOD(empty)
    WRAP_CONST_METHOD(size)
    WRAP_CONST_METHOD(calculate_size)
    WRAP_CONST_METHOD(external_memory_usage)

    WRAP_METHOD(begin)
    WRAP_METHOD(end)
    WRAP_METHOD(rbegin)
    WRAP_METHOD(rend)
    WRAP_CONST_METHOD(cbegin)
    WRAP_CONST_METHOD(cend)
    WRAP_CONST_METHOD(crbegin)
    WRAP_CONST_METHOD(crend)

#undef WRAP_METHOD
#undef WRAP_CONST_METHOD
#undef DO_WRAP_METHOD
};

} // namespace utils



#include <seastar/core/shared_ptr.hh>

namespace dht {

class decorated_key;

using token_range = nonwrapping_interval<token>;

}

namespace data_dictionary {

class database;

}

class repair_history_map;
using per_table_history_maps = std::unordered_map<table_id, seastar::lw_shared_ptr<repair_history_map>>;

class tombstone_gc_options;

class tombstone_gc_state {
    per_table_history_maps* _repair_history_maps;
public:
    tombstone_gc_state() = delete;
    tombstone_gc_state(per_table_history_maps* maps) noexcept : _repair_history_maps(maps) {}

    explicit operator bool() const noexcept {
        return _repair_history_maps != nullptr;
    }

    seastar::lw_shared_ptr<repair_history_map> get_repair_history_map_for_table(const table_id& id) const;
    seastar::lw_shared_ptr<repair_history_map> get_or_create_repair_history_map_for_table(const table_id& id);
    void drop_repair_history_map_for_table(const table_id& id);

    struct get_gc_before_for_range_result {
        gc_clock::time_point min_gc_before;
        gc_clock::time_point max_gc_before;
        bool knows_entire_range;
    };

    get_gc_before_for_range_result get_gc_before_for_range(schema_ptr s, const dht::token_range& range, const gc_clock::time_point& query_time) const;

    gc_clock::time_point get_gc_before_for_key(schema_ptr s, const dht::decorated_key& dk, const gc_clock::time_point& query_time) const;

    void update_repair_time(table_id id, const dht::token_range& range, gc_clock::time_point repair_time);
};

void validate_tombstone_gc_options(const tombstone_gc_options* options, data_dictionary::database db, sstring ks_name);



#include <iosfwd>
#include <map>
#include <boost/intrusive/set.hpp>
#include <boost/range/iterator_range.hpp>
#include <boost/range/adaptor/filtered.hpp>
#include <boost/intrusive/parent_from_member.hpp>

#include <seastar/core/bitset-iter.hh>
#include <seastar/util/optimized_optional.hh>


class mutation_fragment;
class mutation_partition_view;
class mutation_partition_visitor;

namespace query {
    class clustering_key_filter_ranges;
} // namespace query

struct cell_hash {
    using size_type = uint64_t;
    static constexpr size_type no_hash = 0;

    size_type hash = no_hash;

    explicit operator bool() const noexcept {
        return hash != no_hash;
    }
};

template<>
struct appending_hash<cell_hash> {
    template<typename Hasher>
    void operator()(Hasher& h, const cell_hash& ch) const {
        feed_hash(h, ch.hash);
    }
};

using cell_hash_opt = seastar::optimized_optional<cell_hash>;

struct cell_and_hash {
    atomic_cell_or_collection cell;
    mutable cell_hash_opt hash;

    cell_and_hash() = default;
    cell_and_hash(cell_and_hash&&) noexcept = default;
    cell_and_hash& operator=(cell_and_hash&&) noexcept = default;

    cell_and_hash(atomic_cell_or_collection&& cell, cell_hash_opt hash)
        : cell(std::move(cell))
        , hash(hash)
    { }
};

class compaction_garbage_collector;

//
// Container for cells of a row. Cells are identified by column_id.
//
// All cells must belong to a single column_kind. The kind is not stored
// for space-efficiency reasons. Whenever a method accepts a column_kind,
// the caller must always supply the same column_kind.
//
//
class row {
    friend class size_calculator;
    using size_type = std::make_unsigned_t<column_id>;
    size_type _size = 0;
    using sparse_array_type = compact_radix_tree::tree<cell_and_hash, column_id>;
    sparse_array_type _cells;
public:
    row();
    ~row();
    row(const schema&, column_kind, const row&);
    row(row&& other) noexcept;
    row& operator=(row&& other) noexcept;
    size_t size() const { return _size; }
    bool empty() const { return _size == 0; }

    const atomic_cell_or_collection& cell_at(column_id id) const;

    // Returns a pointer to cell's value or nullptr if column is not set.
    const atomic_cell_or_collection* find_cell(column_id id) const;
    // Returns a pointer to cell's value and hash or nullptr if column is not set.
    const cell_and_hash* find_cell_and_hash(column_id id) const;

    template<typename Func>
    void remove_if(Func&& func) {
        _cells.weed([func, this] (column_id id, cell_and_hash& cah) {
            if (!func(id, cah.cell)) {
                return false;
            }

            _size--;
            return true;
        });
    }

private:
    template<typename Func>
    void consume_with(Func&&);

    // Func obeys the same requirements as for for_each_cell below.
    template<typename Func, typename MaybeConstCellAndHash>
    static constexpr auto maybe_invoke_with_hash(Func& func, column_id id, MaybeConstCellAndHash& c_a_h) {
        if constexpr (std::is_invocable_v<Func, column_id, const cell_and_hash&>) {
            return func(id, c_a_h);
        } else {
            return func(id, c_a_h.cell);
        }
    }

public:
    // Calls Func(column_id, cell_and_hash&) or Func(column_id, atomic_cell_and_collection&)
    // for each cell in this row, depending on the concrete Func type.
    // noexcept if Func doesn't throw.
    template<typename Func>
    void for_each_cell(Func&& func) {
        _cells.walk([func] (column_id id, cell_and_hash& cah) {
            maybe_invoke_with_hash(func, id, cah);
            return true;
        });
    }

    template<typename Func>
    void for_each_cell(Func&& func) const {
        _cells.walk([func] (column_id id, const cell_and_hash& cah) {
            maybe_invoke_with_hash(func, id, cah);
            return true;
        });
    }

    template<typename Func>
    void for_each_cell_until(Func&& func) const {
        _cells.walk([func] (column_id id, const cell_and_hash& cah) {
            return maybe_invoke_with_hash(func, id, cah) != stop_iteration::yes;
        });
    }

    // Merges cell's value into the row.
    // Weak exception guarantees.
    void apply(const column_definition& column, const atomic_cell_or_collection& cell, cell_hash_opt hash = cell_hash_opt());

    // Merges cell's value into the row.
    // Weak exception guarantees.
    void apply(const column_definition& column, atomic_cell_or_collection&& cell, cell_hash_opt hash = cell_hash_opt());

    // Monotonic exception guarantees. In case of exception the sum of cell and this remains the same as before the exception.
    void apply_monotonically(const column_definition& column, atomic_cell_or_collection&& cell, cell_hash_opt hash = cell_hash_opt());

    // Adds cell to the row. The column must not be already set.
    void append_cell(column_id id, atomic_cell_or_collection cell);

    // Weak exception guarantees
    void apply(const schema&, column_kind, const row& src);
    // Weak exception guarantees
    void apply(const schema&, column_kind, row&& src);
    // Monotonic exception guarantees
    void apply_monotonically(const schema&, column_kind, row&& src);

    // Expires cells based on query_time. Expires tombstones based on gc_before
    // and max_purgeable. Removes cells covered by tomb.
    // Returns true iff there are any live cells left.
    bool compact_and_expire(
            const schema& s,
            column_kind kind,
            row_tombstone tomb,
            gc_clock::time_point query_time,
            can_gc_fn&,
            gc_clock::time_point gc_before,
            const row_marker& marker,
            compaction_garbage_collector* collector = nullptr);

    bool compact_and_expire(
            const schema& s,
            column_kind kind,
            row_tombstone tomb,
            gc_clock::time_point query_time,
            can_gc_fn&,
            gc_clock::time_point gc_before,
            compaction_garbage_collector* collector = nullptr);

    row difference(const schema&, column_kind, const row& other) const;

    bool equal(column_kind kind, const schema& this_schema, const row& other, const schema& other_schema) const;

    size_t external_memory_usage(const schema&, column_kind) const;

    cell_hash_opt cell_hash_for(column_id id) const;

    void prepare_hash(const schema& s, column_kind kind) const;
    void clear_hash() const;

    bool is_live(const schema&, column_kind kind, tombstone tomb = tombstone(), gc_clock::time_point now = gc_clock::time_point::min()) const;

    class printer {
        const schema& _schema;
        column_kind _kind;
        const row& _row;
    public:
        printer(const schema& s, column_kind k, const row& r) : _schema(s), _kind(k), _row(r) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;

        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);
};

// Like row, but optimized for the case where the row doesn't exist (e.g. static rows)
class lazy_row {
    managed_ref<row> _row;
    static inline const row _empty_row;
public:
    lazy_row() = default;
    explicit lazy_row(row&& r) {
        if (!r.empty()) {
            _row = make_managed<row>(std::move(r));
        }
    }

    lazy_row(const schema& s, column_kind kind, const lazy_row& r) {
        if (!r.empty()) {
            _row = make_managed<row>(s, kind, *r._row);
        }
    }

    lazy_row(const schema& s, column_kind kind, const row& r) {
        if (!r.empty()) {
            _row = make_managed<row>(s, kind, r);
        }
    }

    row& maybe_create() {
        if (!_row) {
            _row = make_managed<row>();
        }
        return *_row;
    }

    const row& get_existing() const & {
        return *_row;
    }

    row& get_existing() & {
        return *_row;
    }

    row&& get_existing() && {
        return std::move(*_row);
    }

    const row& get() const {
        return _row ? *_row : _empty_row;
    }

    size_t size() const {
        if (!_row) {
            return 0;
        }
        return _row->size();
    }

    bool empty() const {
        if (!_row) {
            return true;
        }
        return _row->empty();
    }

    void reserve(column_id nr) {
        if (nr) {
            maybe_create();
        }
    }

    const atomic_cell_or_collection& cell_at(column_id id) const {
        if (!_row) {
            throw_with_backtrace<std::out_of_range>(format("Column not found for id = {:d}", id));
        } else {
            return _row->cell_at(id);
        }
    }

    // Returns a pointer to cell's value or nullptr if column is not set.
    const atomic_cell_or_collection* find_cell(column_id id) const {
        if (!_row) {
            return nullptr;
        }
        return _row->find_cell(id);
    }

    // Returns a pointer to cell's value and hash or nullptr if column is not set.
    const cell_and_hash* find_cell_and_hash(column_id id) const {
        if (!_row) {
            return nullptr;
        }
        return _row->find_cell_and_hash(id);
    }

    // Calls Func(column_id, cell_and_hash&) or Func(column_id, atomic_cell_and_collection&)
    // for each cell in this row, depending on the concrete Func type.
    // noexcept if Func doesn't throw.
    template<typename Func>
    void for_each_cell(Func&& func) {
        if (!_row) {
            return;
        }
        _row->for_each_cell(std::forward<Func>(func));
    }

    template<typename Func>
    void for_each_cell(Func&& func) const {
        if (!_row) {
            return;
        }
        _row->for_each_cell(std::forward<Func>(func));
    }

    template<typename Func>
    void for_each_cell_until(Func&& func) const {
        if (!_row) {
            return;
        }
        _row->for_each_cell_until(std::forward<Func>(func));
    }

    // Merges cell's value into the row.
    // Weak exception guarantees.
    void apply(const column_definition& column, const atomic_cell_or_collection& cell, cell_hash_opt hash = cell_hash_opt()) {
        maybe_create().apply(column, cell, std::move(hash));
    }

    // Merges cell's value into the row.
    // Weak exception guarantees.
    void apply(const column_definition& column, atomic_cell_or_collection&& cell, cell_hash_opt hash = cell_hash_opt()) {
        maybe_create().apply(column, std::move(cell), std::move(hash));
    }

    // Monotonic exception guarantees. In case of exception the sum of cell and this remains the same as before the exception.
    void apply_monotonically(const column_definition& column, atomic_cell_or_collection&& cell, cell_hash_opt hash = cell_hash_opt()) {
        maybe_create().apply_monotonically(column, std::move(cell), std::move(hash));
    }

    // Adds cell to the row. The column must not be already set.
    void append_cell(column_id id, atomic_cell_or_collection cell) {
        maybe_create().append_cell(id, std::move(cell));
    }

    // Weak exception guarantees
    void apply(const schema& s, column_kind kind, const row& src) {
        if (src.empty()) {
            return;
        }
        maybe_create().apply(s, kind, src);
    }

    // Weak exception guarantees
    void apply(const schema& s, column_kind kind, const lazy_row& src) {
        if (src.empty()) {
            return;
        }
        maybe_create().apply(s, kind, src.get_existing());
    }

    // Weak exception guarantees
    void apply(const schema& s, column_kind kind, row&& src) {
        if (src.empty()) {
            return;
        }
        maybe_create().apply(s, kind, std::move(src));
    }

    // Monotonic exception guarantees
    void apply_monotonically(const schema& s, column_kind kind, row&& src) {
        if (src.empty()) {
            return;
        }
        maybe_create().apply_monotonically(s, kind, std::move(src));
    }

    // Monotonic exception guarantees
    void apply_monotonically(const schema& s, column_kind kind, lazy_row&& src) {
        if (src.empty()) {
            return;
        }
        if (!_row) {
            _row = std::move(src._row);
            return;
        }
        get_existing().apply_monotonically(s, kind, std::move(src.get_existing()));
    }

    // Expires cells based on query_time. Expires tombstones based on gc_before
    // and max_purgeable. Removes cells covered by tomb.
    // Returns true iff there are any live cells left.
    bool compact_and_expire(
            const schema& s,
            column_kind kind,
            row_tombstone tomb,
            gc_clock::time_point query_time,
            can_gc_fn& can_gc,
            gc_clock::time_point gc_before,
            const row_marker& marker,
            compaction_garbage_collector* collector = nullptr);

    bool compact_and_expire(
            const schema& s,
            column_kind kind,
            row_tombstone tomb,
            gc_clock::time_point query_time,
            can_gc_fn& can_gc,
            gc_clock::time_point gc_before,
            compaction_garbage_collector* collector = nullptr);

    lazy_row difference(const schema& s, column_kind kind, const lazy_row& other) const {
        if (!_row) {
            return lazy_row();
        }
        if (!other._row) {
            return lazy_row(s, kind, *_row);
        }
        return lazy_row(_row->difference(s, kind, *other._row));
    }

    bool equal(column_kind kind, const schema& this_schema, const lazy_row& other, const schema& other_schema) const {
        bool e1 = empty();
        bool e2 = other.empty();
        if (e1 && e2) {
            return true;
        }
        if (e1 != e2) {
            return false;
        }
        // both non-empty
        return _row->equal(kind, this_schema, *other._row, other_schema);
    }

    size_t external_memory_usage(const schema& s, column_kind kind) const {
        if (!_row) {
            return 0;
        }
        return _row.external_memory_usage() + _row->external_memory_usage(s, kind);
    }

    cell_hash_opt cell_hash_for(column_id id) const {
        if (!_row) {
            return cell_hash_opt{};
        }
        return _row->cell_hash_for(id);
    }

    void prepare_hash(const schema& s, column_kind kind) const {
        if (!_row) {
            return;
        }
        _row->prepare_hash(s, kind);
    }

    void clear_hash() const {
        if (!_row) {
            return;
        }
        _row->clear_hash();
    }

    bool is_live(const schema& s, column_kind kind, tombstone tomb = tombstone(), gc_clock::time_point now = gc_clock::time_point::min()) const {
        if (!_row) {
            return false;
        }
        return _row->is_live(s, kind, tomb, now);
    }

    class printer {
        const schema& _schema;
        column_kind _kind;
        const lazy_row& _row;
    public:
        printer(const schema& s, column_kind k, const lazy_row& r) : _schema(s), _kind(k), _row(r) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;

        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
};

// Used to return the timestamp of the latest update to the row
struct max_timestamp {
    api::timestamp_type max = api::missing_timestamp;

    void update(api::timestamp_type ts) {
        max = std::max(max, ts);
    }
};

template<>
struct appending_hash<row> {
    static constexpr int null_hash_value = 0xbeefcafe;
    template<typename Hasher>
    void operator()(Hasher& h, const row& cells, const schema& s, column_kind kind, const query::column_id_vector& columns, max_timestamp& max_ts) const;
};

class row_marker;
int compare_row_marker_for_merge(const row_marker& left, const row_marker& right) noexcept;

class row_marker {
    static constexpr gc_clock::duration no_ttl { 0 };
    static constexpr gc_clock::duration dead { -1 };
    static constexpr gc_clock::time_point no_expiry { gc_clock::duration(0) };
    api::timestamp_type _timestamp = api::missing_timestamp;
    gc_clock::duration _ttl = no_ttl;
    gc_clock::time_point _expiry = no_expiry;
public:
    row_marker() = default;
    explicit row_marker(api::timestamp_type created_at) : _timestamp(created_at) { }
    row_marker(api::timestamp_type created_at, gc_clock::duration ttl, gc_clock::time_point expiry)
        : _timestamp(created_at), _ttl(ttl), _expiry(expiry)
    { }
    explicit row_marker(tombstone deleted_at)
        : _timestamp(deleted_at.timestamp), _ttl(dead), _expiry(deleted_at.deletion_time)
    { }
    bool is_missing() const {
        return _timestamp == api::missing_timestamp;
    }
    bool is_live() const {
        return !is_missing() && _ttl != dead;
    }
    bool is_live(tombstone t, gc_clock::time_point now) const {
        if (is_missing() || _ttl == dead) {
            return false;
        }
        if (_ttl != no_ttl && _expiry <= now) {
            return false;
        }
        return _timestamp > t.timestamp;
    }
    // Can be called only when !is_missing().
    bool is_dead(gc_clock::time_point now) const {
        if (_ttl == dead) {
            return true;
        }
        return _ttl != no_ttl && _expiry <= now;
    }
    // Can be called only when is_live().
    bool is_expiring() const {
        return _ttl != no_ttl;
    }
    // Can be called only when is_expiring().
    gc_clock::duration ttl() const {
        return _ttl;
    }
    // Can be called only when is_expiring().
    gc_clock::time_point expiry() const {
        return _expiry;
    }
    // Should be called when is_dead() or is_expiring().
    // Safe to be called when is_missing().
    // When is_expiring(), returns the the deletion time of the marker when it finally expires.
    gc_clock::time_point deletion_time() const {
        return _ttl == dead ? _expiry : _expiry - _ttl;
    }
    api::timestamp_type timestamp() const {
        return _timestamp;
    }
    void apply(const row_marker& rm) {
        if (compare_row_marker_for_merge(*this, rm) < 0) {
            *this = rm;
        }
    }
    // Expires cells and tombstones. Removes items covered by higher level
    // tombstones.
    // Returns true if row marker is live.
    bool compact_and_expire(tombstone tomb, gc_clock::time_point now,
            can_gc_fn& can_gc, gc_clock::time_point gc_before, compaction_garbage_collector* collector = nullptr);
    // Consistent with feed_hash()
    bool operator==(const row_marker& other) const {
        if (_timestamp != other._timestamp) {
            return false;
        }
        if (is_missing()) {
            return true;
        }
        if (_ttl != other._ttl) {
            return false;
        }
        return _ttl == no_ttl || _expiry == other._expiry;
    }
    // Consistent with operator==()
    template<typename Hasher>
    void feed_hash(Hasher& h) const {
        ::feed_hash(h, _timestamp);
        if (!is_missing()) {
            ::feed_hash(h, _ttl);
            if (_ttl != no_ttl) {
                ::feed_hash(h, _expiry);
            }
        }
    }
    friend std::ostream& operator<<(std::ostream& os, const row_marker& rm);
};

template<>
struct appending_hash<row_marker> {
    template<typename Hasher>
    void operator()(Hasher& h, const row_marker& m) const {
        m.feed_hash(h);
    }
};

class shadowable_tombstone {
    tombstone _tomb;
public:

    explicit shadowable_tombstone(api::timestamp_type timestamp, gc_clock::time_point deletion_time)
            : _tomb(timestamp, deletion_time) {
    }

    explicit shadowable_tombstone(tombstone tomb = tombstone())
            : _tomb(std::move(tomb)) {
    }

    std::strong_ordering operator<=>(const shadowable_tombstone& t) const {
        return _tomb <=> t._tomb;
    }

    bool operator==(const shadowable_tombstone&) const = default;

    explicit operator bool() const {
        return bool(_tomb);
    }

    const tombstone& tomb() const {
        return _tomb;
    }

    // A shadowable row tombstone is valid only if the row has no live marker. In other words,
    // the row tombstone is only valid as long as no newer insert is done (thus setting a
    // live row marker; note that if the row timestamp set is lower than the tombstone's,
    // then the tombstone remains in effect as usual). If a row has a shadowable tombstone
    // with timestamp Ti and that row is updated with a timestamp Tj, such that Tj > Ti
    // (and that update sets the row marker), then the shadowable tombstone is shadowed by
    // that update. A concrete consequence is that if the update has cells with timestamp
    // lower than Ti, then those cells are preserved (since the deletion is removed), and
    // this is contrary to a regular, non-shadowable row tombstone where the tombstone is
    // preserved and such cells are removed.
    bool is_shadowed_by(const row_marker& marker) const {
        return marker.is_live() && marker.timestamp() > _tomb.timestamp;
    }

    void maybe_shadow(tombstone t, row_marker marker) noexcept {
        if (is_shadowed_by(marker)) {
            _tomb = std::move(t);
        }
    }

    void apply(tombstone t) noexcept {
        _tomb.apply(t);
    }

    void apply(shadowable_tombstone t) noexcept {
        _tomb.apply(t._tomb);
    }
};

template <>
struct fmt::formatter<shadowable_tombstone> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const shadowable_tombstone& t, FormatContext& ctx) const {
        if (t) {
            return fmt::format_to(ctx.out(),
                                  "{{shadowable tombstone: timestamp={}, deletion_time={}}}",
                                  t.tomb().timestamp, t.tomb(), t.tomb().deletion_time.time_since_epoch().count());
        } else {
            return fmt::format_to(ctx.out(),
                                  "{{shadowable tombstone: none}}");
        }
     }
};

template<>
struct appending_hash<shadowable_tombstone> {
    template<typename Hasher>
    void operator()(Hasher& h, const shadowable_tombstone& t) const {
        feed_hash(h, t.tomb());
    }
};

/*
The rules for row_tombstones are as follows:
  - The shadowable tombstone is always >= than the regular one;
  - The regular tombstone works as expected;
  - The shadowable tombstone doesn't erase or compact away the regular
    row tombstone, nor dead cells;
  - The shadowable tombstone can erase live cells, but only provided they
    can be recovered (e.g., by including all cells in a MV update, both
    updated cells and pre-existing ones);
  - The shadowable tombstone can be erased or compacted away by a newer
    row marker.
*/
class row_tombstone {
    tombstone _regular;
    shadowable_tombstone _shadowable; // _shadowable is always >= _regular
public:
    explicit row_tombstone(tombstone regular, shadowable_tombstone shadowable)
            : _regular(std::move(regular))
            , _shadowable(std::move(shadowable)) {
    }

    explicit row_tombstone(tombstone regular)
            : row_tombstone(regular, shadowable_tombstone(regular)) {
    }

    row_tombstone() = default;

    std::strong_ordering operator<=>(const row_tombstone& t) const {
        return _shadowable <=> t._shadowable;
    }
    bool operator==(const row_tombstone& t) const {
        return _shadowable == t._shadowable;
    }

    explicit operator bool() const {
        return bool(_shadowable);
    }

    const tombstone& tomb() const {
        return _shadowable.tomb();
    }

    const gc_clock::time_point max_deletion_time() const {
        return std::max(_regular.deletion_time, _shadowable.tomb().deletion_time);
    }

    const tombstone& regular() const {
        return _regular;
    }

    const shadowable_tombstone& shadowable() const {
        return _shadowable;
    }

    bool is_shadowable() const {
        return _shadowable.tomb() > _regular;
    }

    void maybe_shadow(const row_marker& marker) noexcept {
        _shadowable.maybe_shadow(_regular, marker);
    }

    void apply(tombstone regular) noexcept {
        _shadowable.apply(regular);
        _regular.apply(regular);
    }

    void apply(shadowable_tombstone shadowable, row_marker marker) noexcept {
        _shadowable.apply(shadowable.tomb());
        _shadowable.maybe_shadow(_regular, marker);
    }

    void apply(row_tombstone t, row_marker marker) noexcept {
        _regular.apply(t._regular);
        _shadowable.apply(t._shadowable);
        _shadowable.maybe_shadow(_regular, marker);
    }

    friend std::ostream& operator<<(std::ostream& out, const row_tombstone& t) {
        if (t) {
            fmt::print(out, "{{row_tombstone: {}{}}}",  t._regular, t.is_shadowable() ? t._shadowable : shadowable_tombstone());
        } else {
            fmt::print(out, "{{row_tombstone: none}}");
        }
        return out;
    }
};

template<>
struct appending_hash<row_tombstone> {
    template<typename Hasher>
    void operator()(Hasher& h, const row_tombstone& t) const {
        feed_hash(h, t.regular());
        if (t.is_shadowable()) {
            feed_hash(h, t.shadowable());
        }
    }
};

class deletable_row final {
    row_tombstone _deleted_at;
    row_marker _marker;
    row _cells;
public:
    deletable_row() {}
    deletable_row(const schema& s, const deletable_row& other)
        : _deleted_at(other._deleted_at)
        , _marker(other._marker)
        , _cells(s, column_kind::regular_column, other._cells)
    { }
    deletable_row(row_tombstone&& tomb, row_marker&& marker, row&& cells)
        : _deleted_at(std::move(tomb)), _marker(std::move(marker)), _cells(std::move(cells))
    {}

    void apply(tombstone deleted_at) {
        _deleted_at.apply(deleted_at);
        maybe_shadow();
    }

    void apply(shadowable_tombstone deleted_at) {
        _deleted_at.apply(deleted_at, _marker);
    }

    void apply(row_tombstone deleted_at) {
        _deleted_at.apply(deleted_at, _marker);
    }

    void apply(const row_marker& rm) {
        _marker.apply(rm);
        maybe_shadow();
    }

    void remove_tombstone() {
        _deleted_at = {};
    }

    void maybe_shadow() {
        _deleted_at.maybe_shadow(_marker);
    }

    // Weak exception guarantees. After exception, both src and this will commute to the same value as
    // they would should the exception not happen.
    void apply(const schema& s, const deletable_row& src);
    void apply(const schema& s, deletable_row&& src);
    void apply_monotonically(const schema& s, const deletable_row& src);
    void apply_monotonically(const schema& s, deletable_row&& src);
public:
    row_tombstone deleted_at() const { return _deleted_at; }
    api::timestamp_type created_at() const { return _marker.timestamp(); }
    // Call `maybe_shadow()` if the marker's timestamp is mutated.
    row_marker& marker() { return _marker; }
    const row_marker& marker() const { return _marker; }
    const row& cells() const { return _cells; }
    row& cells() { return _cells; }
    bool equal(column_kind, const schema& s, const deletable_row& other, const schema& other_schema) const;
    bool is_live(const schema& s, column_kind kind, tombstone base_tombstone = tombstone(), gc_clock::time_point query_time = gc_clock::time_point::min()) const;
    bool empty() const { return !_deleted_at && _marker.is_missing() && !_cells.size(); }
    deletable_row difference(const schema&, column_kind, const deletable_row& other) const;

    // Expires cells and tombstones. Removes items covered by higher level
    // tombstones.
    // Returns true iff the row is still alive.
    // When empty() after the call, the row can be removed without losing writes
    // given that tomb will be still in effect for the row after it is removed,
    // as a range tombstone, partition tombstone, etc.
    bool compact_and_expire(const schema&,
                            tombstone tomb,
                            gc_clock::time_point query_time,
                            can_gc_fn& can_gc,
                            gc_clock::time_point gc_before,
                            compaction_garbage_collector* collector = nullptr);

    class printer {
        const schema& _schema;
        const deletable_row& _deletable_row;
    public:
        printer(const schema& s, const deletable_row& r) : _schema(s), _deletable_row(r) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;

        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);
};

class cache_tracker;

class rows_entry final : public evictable {
    friend class size_calculator;
    intrusive_b::member_hook _link;
    clustering_key _key;
    deletable_row _row;

    // Given p is the preceding rows_entry&,
    // this tombstone applies to the range (p.position(), position()] if continuous()
    // and to [position(), position()] if !continuous().
    // So the tombstone applies only to the continuous interval, to the left.
    // On top of that, _row.deleted_at() may still apply new information.
    // So it's not deoverlapped with the row tombstone.
    // Set only when in mutation_partition_v2.
    tombstone _range_tombstone;

    struct flags {
        // _before_ck and _after_ck encode position_in_partition::weight
        bool _before_ck : 1;
        bool _after_ck : 1;
        bool _continuous : 1; // See doc of is_continuous.
        bool _dummy : 1;
        // Marks a dummy entry which is after_all_clustered_rows() position.
        // Needed so that eviction, which can't use comparators, can check if it's dealing with it.
        bool _last_dummy : 1;
        flags() : _before_ck(0), _after_ck(0), _continuous(true), _dummy(false), _last_dummy(false) { }
    } _flags{};
public:
    struct last_dummy_tag {};
    explicit rows_entry(clustering_key&& key)
        : _key(std::move(key))
    { }
    explicit rows_entry(const clustering_key& key)
        : _key(key)
    { }
    rows_entry(const schema& s, position_in_partition_view pos, is_dummy dummy, is_continuous continuous)
        : _key(pos.key())
    {
        _flags._last_dummy = bool(dummy) && pos.is_after_all_clustered_rows(s);
        _flags._dummy = bool(dummy);
        _flags._continuous = bool(continuous);
        _flags._before_ck = pos.is_before_key();
        _flags._after_ck = pos.is_after_key();
    }
    rows_entry(const schema& s, last_dummy_tag, is_continuous continuous)
        : rows_entry(s, position_in_partition_view::after_all_clustered_rows(), is_dummy::yes, continuous)
    { }
    rows_entry(const clustering_key& key, deletable_row&& row)
        : _key(key), _row(std::move(row))
    { }
    rows_entry(const schema& s, const clustering_key& key, const deletable_row& row)
        : _key(key), _row(s, row)
    { }
    rows_entry(rows_entry&& o) noexcept;
    rows_entry(const schema& s, const rows_entry& e)
        : _key(e._key)
        , _row(s, e._row)
        , _range_tombstone(e._range_tombstone)
        , _flags(e._flags)
    { }
    // Valid only if !dummy()
    clustering_key& key() {
        return _key;
    }
    // Valid only if !dummy()
    const clustering_key& key() const {
        return _key;
    }
    deletable_row& row() {
        return _row;
    }
    const deletable_row& row() const {
        return _row;
    }
    position_in_partition_view position() const {
        return position_in_partition_view(partition_region::clustered, bound_weight(_flags._after_ck - _flags._before_ck), &_key);
    }

    is_continuous continuous() const { return is_continuous(_flags._continuous); }
    void set_continuous(bool value) { _flags._continuous = value; }
    void set_continuous(is_continuous value) { set_continuous(bool(value)); }
    void set_range_tombstone(tombstone t) { _range_tombstone = t; }
    tombstone range_tombstone() const { return _range_tombstone; }
    is_dummy dummy() const { return is_dummy(_flags._dummy); }
    bool is_last_dummy() const { return _flags._last_dummy; }
    void set_dummy(bool value) { _flags._dummy = value; }
    void set_dummy(is_dummy value) { _flags._dummy = bool(value); }
    void replace_with(rows_entry&& other) noexcept;

    void apply(row_tombstone t) {
        _row.apply(t);
    }
    void apply_monotonically(const schema& s, rows_entry&& e) {
        _row.apply(s, std::move(e._row));
    }
    bool empty() const {
        return _row.empty();
    }
    struct tri_compare {
        position_in_partition::tri_compare _c;
        explicit tri_compare(const schema& s) : _c(s) {}

        std::strong_ordering operator()(const rows_entry& e1, const rows_entry& e2) const {
            return _c(e1.position(), e2.position());
        }
        std::strong_ordering operator()(const clustering_key& key, const rows_entry& e) const {
            return _c(position_in_partition_view::for_key(key), e.position());
        }
        std::strong_ordering operator()(const rows_entry& e, const clustering_key& key) const {
            return _c(e.position(), position_in_partition_view::for_key(key));
        }
        std::strong_ordering operator()(const rows_entry& e, position_in_partition_view p) const {
            return _c(e.position(), p);
        }
        std::strong_ordering operator()(position_in_partition_view p, const rows_entry& e) const {
            return _c(p, e.position());
        }
        std::strong_ordering operator()(position_in_partition_view p1, position_in_partition_view p2) const {
            return _c(p1, p2);
        }
    };
    struct compare {
        tri_compare _c;
        explicit compare(const schema& s) : _c(s) {}

        template <typename K1, typename K2>
        bool operator()(const K1& k1, const K2& k2) const { return _c(k1, k2) < 0; }
    };
    bool equal(const schema& s, const rows_entry& other) const;
    bool equal(const schema& s, const rows_entry& other, const schema& other_schema) const;

    size_t memory_usage(const schema&) const;

    // Handles eviction of the row, but doesn't attempt to handle eviction
    // of the containing partition_entry in case this is the last row.
    // Used by tests which don't keep the partition_entry inside a row_cache instance.
    void on_evicted_shallow() noexcept override {}

    void on_evicted(cache_tracker&) noexcept {}
    void on_evicted() noexcept override {}

    void compact(const schema&, tombstone);

    class printer {
        const schema& _schema;
        const rows_entry& _rows_entry;
    public:
        printer(const schema& s, const rows_entry& r) : _schema(s), _rows_entry(r) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;

        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);

    using container_type = intrusive_b::tree<rows_entry, &rows_entry::_link, rows_entry::tri_compare, 12, 20, intrusive_b::key_search::linear>;
};

struct mutation_application_stats {
    uint64_t row_hits = 0;
    uint64_t row_writes = 0;
    uint64_t rows_compacted_with_tombstones = 0;
    uint64_t rows_dropped_by_tombstones = 0;

    mutation_application_stats& operator+=(const mutation_application_stats& other) {
        row_hits += other.row_hits;
        row_writes += other.row_writes;
        rows_compacted_with_tombstones += other.rows_compacted_with_tombstones;
        rows_dropped_by_tombstones += other.rows_dropped_by_tombstones;
        return *this;
    }
};

struct apply_resume {
    enum class stage {
        start,
        range_tombstone_compaction,
        merging_range_tombstones,
        partition_tombstone_compaction,
        merging_rows,
        done
    };

    position_in_partition _pos;
    stage _stage;

    apply_resume()
        : _pos(position_in_partition::for_partition_start())
        , _stage(stage::start)
    { }

    apply_resume(stage s, position_in_partition_view pos)
        : _pos(with_allocator(standard_allocator(), [&] {
                return position_in_partition(pos);
            }))
        , _stage(s)
    { }

    ~apply_resume() {
        with_allocator(standard_allocator(), [&] {
           auto pos = std::move(_pos);
        });
    }

    apply_resume(apply_resume&&) noexcept = default;

    apply_resume& operator=(apply_resume&& o) noexcept {
        if (this != &o) {
            this->~apply_resume();
            new (this) apply_resume(std::move(o));
        }
        return *this;
    }

    explicit operator bool() const { return _stage != stage::done; }

    static apply_resume merging_rows() {
        return {stage::merging_rows, position_in_partition::for_partition_start()};
    }

    static apply_resume merging_range_tombstones() {
        return {stage::merging_range_tombstones, position_in_partition::for_partition_start()};
    }

    static apply_resume done() {
        return {stage::done, position_in_partition::for_partition_start()};
    }

    void set_position(position_in_partition_view pos) {
        with_allocator(standard_allocator(), [&] {
            _pos = position_in_partition(pos);
        });
    }
};

// Represents a set of writes made to a single partition.
//
// The object is schema-dependent. Each instance is governed by some
// specific schema version. Accessors require a reference to the schema object
// of that version.
//
// There is an operation of addition defined on mutation_partition objects
// (also called "apply"), which gives as a result an object representing the
// sum of writes contained in the addends. For instances governed by the same
// schema, addition is commutative and associative.
//
// In addition to representing writes, the object supports specifying a set of
// partition elements called "continuity". This set can be used to represent
// lack of information about certain parts of the partition. It can be
// specified which ranges of clustering keys belong to that set. We say that a
// key range is continuous if all keys in that range belong to the continuity
// set, and discontinuous otherwise. By default everything is continuous.
// The static row may be also continuous or not.
// Partition tombstone is always continuous.
//
// Continuity is ignored by instance equality. It's also transient, not
// preserved by serialization.
//
// Continuity is represented internally using flags on row entries. The key
// range between two consecutive entries (both ends exclusive) is continuous
// if and only if rows_entry::continuous() is true for the later entry. The
// range starting after the last entry is assumed to be continuous. The range
// corresponding to the key of the entry is continuous if and only if
// rows_entry::dummy() is false.
//
// Adding two fully-continuous instances gives a fully-continuous instance.
// Continuity doesn't affect how the write part is added.
//
// Addition of continuity is not commutative in general, but is associative.
// The default continuity merging rules are those required by MVCC to
// preserve its invariants. For details, refer to "Continuity merging rules" section
// in the doc in partition_version.hh.
class mutation_partition final {
public:
    using rows_type = rows_entry::container_type;
    friend class size_calculator;
private:
    tombstone _tombstone;
    lazy_row _static_row;
    bool _static_row_continuous = true;
    rows_type _rows;
    // Contains only strict prefixes so that we don't have to lookup full keys
    // in both _row_tombstones and _rows.
    range_tombstone_list _row_tombstones;
#ifdef SEASTAR_DEBUG
    table_schema_version _schema_version;
#endif

    friend class converting_mutation_partition_applier;
public:
    struct copy_comparators_only {};
    struct incomplete_tag {};
    // Constructs an empty instance which is fully discontinuous except for the partition tombstone.
    mutation_partition(incomplete_tag, const schema& s, tombstone);
    static mutation_partition make_incomplete(const schema& s, tombstone t = {}) {
        return mutation_partition(incomplete_tag(), s, t);
    }
    mutation_partition(schema_ptr s)
        : _rows()
        , _row_tombstones(*s)
#ifdef SEASTAR_DEBUG
        , _schema_version(s->version())
#endif
    { }
    mutation_partition(mutation_partition& other, copy_comparators_only)
        : _rows()
        , _row_tombstones(other._row_tombstones, range_tombstone_list::copy_comparator_only())
#ifdef SEASTAR_DEBUG
        , _schema_version(other._schema_version)
#endif
    { }
    mutation_partition(mutation_partition&&) = default;
    mutation_partition(const schema& s, const mutation_partition&);
    mutation_partition(const mutation_partition&, const schema&, query::clustering_key_filter_ranges);
    mutation_partition(mutation_partition&&, const schema&, query::clustering_key_filter_ranges);
    ~mutation_partition();
    static mutation_partition& container_of(rows_type&);
    mutation_partition& operator=(mutation_partition&& x) noexcept;
    bool equal(const schema&, const mutation_partition&) const;
    bool equal(const schema& this_schema, const mutation_partition& p, const schema& p_schema) const;
    bool equal_continuity(const schema&, const mutation_partition&) const;
    // Consistent with equal()
    template<typename Hasher>
    void feed_hash(Hasher& h, const schema& s) const {
        hashing_partition_visitor<Hasher> v(h, s);
        accept(s, v);
    }

    class printer {
        const schema& _schema;
        const mutation_partition& _mutation_partition;
    public:
        printer(const schema& s, const mutation_partition& mp) : _schema(s), _mutation_partition(mp) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;

        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);
public:
    // Makes sure there is a dummy entry after all clustered rows. Doesn't affect continuity.
    // Doesn't invalidate iterators.
    void ensure_last_dummy(const schema&);
    bool static_row_continuous() const { return _static_row_continuous; }
    void set_static_row_continuous(bool value) { _static_row_continuous = value; }
    bool is_fully_continuous() const;
    void make_fully_continuous();
    // Sets or clears continuity of clustering ranges between existing rows.
    void set_continuity(const schema&, const position_range& pr, is_continuous);
    // Returns clustering row ranges which have continuity matching the is_continuous argument.
    clustering_interval_set get_continuity(const schema&, is_continuous = is_continuous::yes) const;
    // Returns true iff all keys from given range are marked as continuous, or range is empty.
    bool fully_continuous(const schema&, const position_range&);
    // Returns true iff all keys from given range are marked as not continuous and range is not empty.
    bool fully_discontinuous(const schema&, const position_range&);
    // Returns true iff all keys from given range have continuity membership as specified by is_continuous.
    bool check_continuity(const schema&, const position_range&, is_continuous) const;
    // Frees elements of the partition in batches.
    // Returns stop_iteration::yes iff there are no more elements to free.
    // Continuity is unspecified after this.
    stop_iteration clear_gently(cache_tracker*) noexcept;
    // Applies mutation_fragment.
    // The fragment must be goverened by the same schema as this object.
    void apply(const schema& s, const mutation_fragment&);
    void apply(tombstone t) { _tombstone.apply(t); }
    void apply_delete(const schema& schema, const clustering_key_prefix& prefix, tombstone t);
    void apply_delete(const schema& schema, range_tombstone rt);
    void apply_delete(const schema& schema, clustering_key_prefix&& prefix, tombstone t);
    void apply_delete(const schema& schema, clustering_key_prefix_view prefix, tombstone t);
    // Equivalent to applying a mutation with an empty row, created with given timestamp
    void apply_insert(const schema& s, clustering_key_view, api::timestamp_type created_at);
    void apply_insert(const schema& s, clustering_key_view, api::timestamp_type created_at,
                      gc_clock::duration ttl, gc_clock::time_point expiry);
    // prefix must not be full
    void apply_row_tombstone(const schema& schema, clustering_key_prefix prefix, tombstone t);
    void apply_row_tombstone(const schema& schema, range_tombstone rt);
    //
    // Applies p to current object.
    //
    // Commutative when this_schema == p_schema. If schemas differ, data in p which
    // is not representable in this_schema is dropped, thus apply() loses commutativity.
    //
    // Weak exception guarantees.
    void apply(const schema& this_schema, const mutation_partition& p, const schema& p_schema,
            mutation_application_stats& app_stats);
    // Use in case this instance and p share the same schema.
    // Same guarantees as apply(const schema&, mutation_partition&&, const schema&);
    void apply(const schema& s, mutation_partition&& p, mutation_application_stats& app_stats);
    // Same guarantees and constraints as for apply(const schema&, const mutation_partition&, const schema&).
    void apply(const schema& this_schema, mutation_partition_view p, const schema& p_schema,
            mutation_application_stats& app_stats);

    // Applies p to this instance.
    //
    // Monotonic exception guarantees. In case of exception the sum of p and this remains the same as before the exception.
    // This instance and p are governed by the same schema.
    //
    // Must be provided with a pointer to the cache_tracker, which owns both this and p.
    //
    // Returns stop_iteration::no if the operation was preempted before finished, and stop_iteration::yes otherwise.
    // On preemption the sum of this and p stays the same (represents the same set of writes), and the state of this
    // object contains at least all the writes it contained before the call (monotonicity). It may contain partial writes.
    // Also, some progress is always guaranteed (liveness).
    //
    // If returns stop_iteration::yes, then the sum of this and p is NO LONGER the same as before the call,
    // the state of p is undefined and should not be used for reading.
    //
    // The operation can be driven to completion like this:
    //
    //   apply_resume res;
    //   while (apply_monotonically(..., is_preemtable::yes, &res) == stop_iteration::no) { }
    //
    // If is_preemptible::no is passed as argument then stop_iteration::no is never returned.
    //
    // If is_preemptible::yes is passed, apply_resume must also be passed,
    // same instance each time until stop_iteration::yes is returned.
    stop_iteration apply_monotonically(const schema& s, mutation_partition&& p, cache_tracker*,
            mutation_application_stats& app_stats, is_preemptible, apply_resume&);
    stop_iteration apply_monotonically(const schema& s, mutation_partition&& p, const schema& p_schema,
            mutation_application_stats& app_stats, is_preemptible, apply_resume&);
    stop_iteration apply_monotonically(const schema& s, mutation_partition&& p, cache_tracker* tracker,
            mutation_application_stats& app_stats);
    stop_iteration apply_monotonically(const schema& s, mutation_partition&& p, const schema& p_schema,
            mutation_application_stats& app_stats);

    // Weak exception guarantees.
    // Assumes this and p are not owned by a cache_tracker.
    void apply_weak(const schema& s, const mutation_partition& p, const schema& p_schema,
            mutation_application_stats& app_stats);
    void apply_weak(const schema& s, mutation_partition&&,
            mutation_application_stats& app_stats);
    void apply_weak(const schema& s, mutation_partition_view p, const schema& p_schema,
            mutation_application_stats& app_stats);

    // Converts partition to the new schema. When succeeds the partition should only be accessed
    // using the new schema.
    //
    // Strong exception guarantees.
    void upgrade(const schema& old_schema, const schema& new_schema);
private:
    void insert_row(const schema& s, const clustering_key& key, deletable_row&& row);
    void insert_row(const schema& s, const clustering_key& key, const deletable_row& row);

    uint32_t do_compact(const schema& s,
        const dht::decorated_key& dk,
        gc_clock::time_point now,
        const std::vector<query::clustering_range>& row_ranges,
        bool always_return_static_content,
        bool reverse,
        uint64_t row_limit,
        can_gc_fn&,
        bool drop_tombstones_unconditionally,
        const tombstone_gc_state& gc_state);

    // Calls func for each row entry inside row_ranges until func returns stop_iteration::yes.
    // Removes all entries for which func didn't return stop_iteration::no or wasn't called at all.
    // Removes all entries that are empty, check rows_entry::empty().
    // If reversed is true, func will be called on entries in reverse order. In that case row_ranges
    // must be already in reverse order.
    template<bool reversed, typename Func>
    requires std::is_invocable_r_v<stop_iteration, Func, rows_entry&>
    void trim_rows(const schema& s,
        const std::vector<query::clustering_range>& row_ranges,
        Func&& func);
public:
    // Performs the following:
    //   - throws out data which doesn't belong to row_ranges
    //   - expires cells and tombstones based on query_time
    //   - drops cells covered by higher-level tombstones (compaction)
    //   - leaves at most row_limit live rows
    //
    // Note: a partition with a static row which has any cell live but no
    // clustered rows still counts as one row, according to the CQL row
    // counting rules.
    //
    // Returns the count of CQL rows which remained. If the returned number is
    // smaller than the row_limit it means that there was no more data
    // satisfying the query left.
    //
    // The row_limit parameter must be > 0.
    //
    uint64_t compact_for_query(const schema& s, const dht::decorated_key& dk, gc_clock::time_point query_time,
        const std::vector<query::clustering_range>& row_ranges, bool always_return_static_content,
        bool reversed, uint64_t row_limit);

    // Performs the following:
    //   - expires cells based on compaction_time
    //   - drops cells covered by higher-level tombstones
    //   - drops expired tombstones which timestamp is before max_purgeable
    void compact_for_compaction(const schema& s, can_gc_fn&,
        const dht::decorated_key& dk,
        gc_clock::time_point compaction_time,
        const tombstone_gc_state& gc_state);

    // Like compact_for_compaction but drop tombstones unconditionally
    void compact_for_compaction_drop_tombstones_unconditionally(const schema& s,
            const dht::decorated_key& dk);

    // Returns the minimal mutation_partition that when applied to "other" will
    // create a mutation_partition equal to the sum of other and this one.
    // This and other must both be governed by the same schema s.
    mutation_partition difference(schema_ptr s, const mutation_partition& other) const;

    // Returns a subset of this mutation holding only information relevant for given clustering ranges.
    // Range tombstones will be trimmed to the boundaries of the clustering ranges.
    mutation_partition sliced(const schema& s, const query::clustering_row_ranges&) const;

    // Returns true if the mutation_partition represents no writes.
    bool empty() const;
public:
    deletable_row& clustered_row(const schema& s, const clustering_key& key);
    deletable_row& clustered_row(const schema& s, clustering_key&& key);
    deletable_row& clustered_row(const schema& s, clustering_key_view key);
    deletable_row& clustered_row(const schema& s, position_in_partition_view pos, is_dummy, is_continuous);
    rows_entry& clustered_rows_entry(const schema& s, position_in_partition_view pos, is_dummy, is_continuous);
    // Throws if the row already exists or if the row was not inserted to the
    // last position (one or more greater row already exists).
    // Weak exception guarantees.
    deletable_row& append_clustered_row(const schema& s, position_in_partition_view pos, is_dummy, is_continuous);
public:
    tombstone partition_tombstone() const { return _tombstone; }
    lazy_row& static_row() { return _static_row; }
    const lazy_row& static_row() const { return _static_row; }

    // return a set of rows_entry where each entry represents a CQL row sharing the same clustering key.
    const rows_type& clustered_rows() const noexcept { return _rows; }
    utils::immutable_collection<rows_type> clustered_rows() noexcept { return _rows; }
    rows_type& mutable_clustered_rows() noexcept { return _rows; }

    const range_tombstone_list& row_tombstones() const noexcept { return _row_tombstones; }
    utils::immutable_collection<range_tombstone_list> row_tombstones() noexcept { return _row_tombstones; }
    range_tombstone_list& mutable_row_tombstones() noexcept { return _row_tombstones; }

    const row* find_row(const schema& s, const clustering_key& key) const;
    tombstone range_tombstone_for_row(const schema& schema, const clustering_key& key) const;
    row_tombstone tombstone_for_row(const schema& schema, const clustering_key& key) const;
    // Can be called only for non-dummy entries
    row_tombstone tombstone_for_row(const schema& schema, const rows_entry& e) const;
    boost::iterator_range<rows_type::const_iterator> range(const schema& schema, const query::clustering_range& r) const;
    rows_type::const_iterator lower_bound(const schema& schema, const query::clustering_range& r) const;
    rows_type::const_iterator upper_bound(const schema& schema, const query::clustering_range& r) const;
    rows_type::iterator lower_bound(const schema& schema, const query::clustering_range& r);
    rows_type::iterator upper_bound(const schema& schema, const query::clustering_range& r);
    boost::iterator_range<rows_type::iterator> range(const schema& schema, const query::clustering_range& r);
    // Returns an iterator range of rows_entry, with only non-dummy entries.
    auto non_dummy_rows() const {
        return boost::make_iterator_range(_rows.begin(), _rows.end())
            | boost::adaptors::filtered([] (const rows_entry& e) { return bool(!e.dummy()); });
    }
    void accept(const schema&, mutation_partition_visitor&) const;

    // Returns the number of live CQL rows in this partition.
    //
    // Note: If no regular rows are live, but there's something live in the
    // static row, the static row counts as one row. If there is at least one
    // regular row live, static row doesn't count.
    //
    uint64_t live_row_count(const schema&,
        gc_clock::time_point query_time = gc_clock::time_point::min()) const;

    bool is_static_row_live(const schema&,
        gc_clock::time_point query_time = gc_clock::time_point::min()) const;

    uint64_t row_count() const;

    size_t external_memory_usage(const schema&) const;
private:
    template<typename Func>
    void for_each_row(const schema& schema, const query::clustering_range& row_range, bool reversed, Func&& func) const;
    friend class counter_write_query_result_builder;

    void check_schema(const schema& s) const {
#ifdef SEASTAR_DEBUG
        assert(s.version() == _schema_version);
#endif
    }
};

inline
mutation_partition& mutation_partition::container_of(rows_type& rows) {
    return *boost::intrusive::get_parent_from_member(&rows, &mutation_partition::_rows);
}

bool has_any_live_data(const schema& s, column_kind kind, const row& cells, tombstone tomb = tombstone(),
                       gc_clock::time_point now = gc_clock::time_point::min());


#include <seastar/util/optimized_optional.hh>

namespace seastar {
    class file;
} // namespace seastar

struct reader_resources {
    int count = 0;
    ssize_t memory = 0;

    static reader_resources with_memory(ssize_t memory) { return reader_resources(0, memory); }

    reader_resources() = default;

    reader_resources(int count, ssize_t memory)
        : count(count)
        , memory(memory) {
    }

    reader_resources operator-(const reader_resources& other) const {
        return reader_resources{count - other.count, memory - other.memory};
    }

    reader_resources& operator-=(const reader_resources& other) {
        count -= other.count;
        memory -= other.memory;
        return *this;
    }

    reader_resources operator+(const reader_resources& other) const {
        return reader_resources{count + other.count, memory + other.memory};
    }

    reader_resources& operator+=(const reader_resources& other) {
        count += other.count;
        memory += other.memory;
        return *this;
    }

    bool non_zero() const {
        return count > 0 || memory > 0;
    }
};

inline bool operator==(const reader_resources& a, const reader_resources& b) {
    return a.count == b.count && a.memory == b.memory;
}

std::ostream& operator<<(std::ostream& os, const reader_resources& r);

class reader_concurrency_semaphore;

/// A permit for a specific read.
///
/// Used to track the read's resource consumption. Use `consume_memory()` to
/// register memory usage, which returns a `resource_units` RAII object that
/// should be held onto while the respective resources are in use.
class reader_permit {
    friend class reader_concurrency_semaphore;
    friend class tracking_allocator_base;

public:
    class resource_units;
    class need_cpu_guard;
    class awaits_guard;

    enum class state {
        waiting_for_admission,
        waiting_for_memory,
        waiting_for_execution,
        active,
        active_need_cpu,
        active_await,
        inactive,
        evicted,
    };

    class impl;

private:
    shared_ptr<impl> _impl;

private:
    reader_permit() = default;
    reader_permit(shared_ptr<impl>);
    explicit reader_permit(reader_concurrency_semaphore& semaphore, const schema* const schema, std::string_view op_name,
            reader_resources base_resources, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr);
    explicit reader_permit(reader_concurrency_semaphore& semaphore, const schema* const schema, sstring&& op_name,
            reader_resources base_resources, db::timeout_clock::time_point timeout, tracing::trace_state_ptr trace_ptr);

    reader_permit::impl& operator*() { return *_impl; }
    reader_permit::impl* operator->() { return _impl.get(); }

    void mark_need_cpu() noexcept;

    void mark_not_need_cpu() noexcept;

    void mark_awaits() noexcept;

    void mark_not_awaits() noexcept;

    operator bool() const { return bool(_impl); }

    friend class optimized_optional<reader_permit>;

    void consume(reader_resources res);

    void signal(reader_resources res);

public:
    ~reader_permit();

    reader_permit(const reader_permit&) = default;
    reader_permit(reader_permit&&) = default;

    reader_permit& operator=(const reader_permit&) = default;
    reader_permit& operator=(reader_permit&&) = default;

    bool operator==(const reader_permit& o) const {
        return _impl == o._impl;
    }

    reader_concurrency_semaphore& semaphore();

    const ::schema* get_schema() const;
    std::string_view get_op_name() const;
    state get_state() const;

    bool needs_readmission() const;

    // Call only when needs_readmission() = true.
    future<> wait_readmission();

    resource_units consume_memory(size_t memory = 0);

    resource_units consume_resources(reader_resources res);

    future<resource_units> request_memory(size_t memory);

    reader_resources consumed_resources() const;

    reader_resources base_resources() const;

    void release_base_resources() noexcept;

    sstring description() const;

    db::timeout_clock::time_point timeout() const noexcept;

    void set_timeout(db::timeout_clock::time_point timeout) noexcept;

    const tracing::trace_state_ptr& trace_state() const noexcept;

    void set_trace_state(tracing::trace_state_ptr trace_ptr) noexcept;

    // If the read was aborted, throw the exception the read was aborted with.
    // Otherwise no-op.
    void check_abort();

    query::max_result_size max_result_size() const;
    void set_max_result_size(query::max_result_size);

    void on_start_sstable_read() noexcept;
    void on_finish_sstable_read() noexcept;

    uintptr_t id() { return reinterpret_cast<uintptr_t>(_impl.get()); }
};

using reader_permit_opt = optimized_optional<reader_permit>;

class reader_permit::resource_units {
    reader_permit _permit;
    reader_resources _resources;

    friend class reader_permit;
    friend class reader_concurrency_semaphore;
private:
    class already_consumed_tag {};
    resource_units(reader_permit permit, reader_resources res, already_consumed_tag);
    resource_units(reader_permit permit, reader_resources res);
public:
    resource_units(const resource_units&) = delete;
    resource_units(resource_units&&) noexcept;
    ~resource_units();
    resource_units& operator=(const resource_units&) = delete;
    resource_units& operator=(resource_units&&) noexcept;
    void add(resource_units&& o);
    void reset_to(reader_resources res);
    void reset_to_zero() noexcept;
    reader_permit permit() const { return _permit; }
    reader_resources resources() const { return _resources; }
};

std::ostream& operator<<(std::ostream& os, reader_permit::state s);

/// Mark a permit as needing CPU.
///
/// Conceptually, a permit is considered as needing CPU, when at least one reader
/// associated with it has an ongoing foreground operation initiated by
/// its consumer. E.g. a pending `fill_buffer()` call.
/// This class is an RAII need_cpu marker meant to be used by keeping it alive
/// while the reader is in need of CPU.
class reader_permit::need_cpu_guard {
    reader_permit_opt _permit;
public:
    explicit need_cpu_guard(reader_permit permit) noexcept : _permit(std::move(permit)) {
        _permit->mark_need_cpu();
    }
    need_cpu_guard(need_cpu_guard&&) noexcept = default;
    need_cpu_guard(const need_cpu_guard&) = delete;
    ~need_cpu_guard() {
        if (_permit) {
            _permit->mark_not_need_cpu();
        }
    }
    need_cpu_guard& operator=(need_cpu_guard&&) = delete;
    need_cpu_guard& operator=(const need_cpu_guard&) = delete;
};

/// Mark a permit as awaiting I/O or an operation running on a remote shard.
///
/// Conceptually, a permit is considered awaiting, when at least one reader
/// associated with it is waiting on I/O or a remote shard as part of a
/// foreground operation initiated by its consumer. E.g. an sstable reader
/// waiting on a disk read as part of its `fill_buffer()` call.
/// This class is an RAII awaits marker meant to be used by keeping it alive
/// until said awaited event completes.
class reader_permit::awaits_guard {
    reader_permit_opt _permit;
public:
    explicit awaits_guard(reader_permit permit) noexcept : _permit(std::move(permit)) {
        _permit->mark_awaits();
    }
    awaits_guard(awaits_guard&&) noexcept = default;
    awaits_guard(const awaits_guard&) = delete;
    ~awaits_guard() {
        if (_permit) {
            _permit->mark_not_awaits();
        }
    }
    awaits_guard& operator=(awaits_guard&&) = delete;
    awaits_guard& operator=(const awaits_guard&) = delete;
};

template <typename Char>
temporary_buffer<Char> make_tracked_temporary_buffer(temporary_buffer<Char> buf, reader_permit::resource_units units) {
    return temporary_buffer<Char>(buf.get_write(), buf.size(), make_object_deleter(buf.release(), std::move(units)));
}

inline temporary_buffer<char> make_new_tracked_temporary_buffer(size_t size, reader_permit& permit) {
    auto buf = temporary_buffer<char>(size);
    return temporary_buffer<char>(buf.get_write(), buf.size(), make_object_deleter(buf.release(), permit.consume_memory(size)));
}

file make_tracked_file(file f, reader_permit p);

class tracking_allocator_base {
    reader_permit _permit;
protected:
    tracking_allocator_base(reader_permit permit) noexcept : _permit(std::move(permit)) { }
    void consume(size_t memory) {
        _permit.consume(reader_resources::with_memory(memory));
    }
    void signal(size_t memory) {
        _permit.signal(reader_resources::with_memory(memory));
    }
};

template <typename T>
class tracking_allocator : public tracking_allocator_base {
public:
    using value_type = T;
    using propagate_on_container_move_assignment = std::true_type;
    using is_always_equal = std::false_type;

private:
    std::allocator<T> _alloc;

public:
    tracking_allocator(reader_permit permit) noexcept : tracking_allocator_base(std::move(permit)) { }

    T* allocate(size_t n) {
        auto p = _alloc.allocate(n);
        try {
            consume(n * sizeof(T));
        } catch (...) {
            _alloc.deallocate(p, n);
            throw;
        }
        return p;
    }
    void deallocate(T* p, size_t n) {
        _alloc.deallocate(p, n);
        if (n) {
            signal(n * sizeof(T));
        }
    }

    template <typename U>
    friend bool operator==(const tracking_allocator<U>& a, const tracking_allocator<U>& b);
};

template <typename T>
bool operator==(const tracking_allocator<T>& a, const tracking_allocator<T>& b) {
    return a._semaphore == b._semaphore;
}


#include <seastar/util/bool_class.hh>
#include <seastar/util/optimized_optional.hh>

using namespace seastar;

class mutation_fragment;
class mutation_fragment_v2;

using mutation_fragment_opt = optimized_optional<mutation_fragment>;
using mutation_fragment_v2_opt = optimized_optional<mutation_fragment_v2>;



#include <optional>
#include <seastar/util/optimized_optional.hh>

#include <seastar/core/future-util.hh>


// mutation_fragments are the objects that streamed_mutation are going to
// stream. They can represent:
//  - a static row
//  - a clustering row
//  - a range tombstone
//
// There exists an ordering (implemented in position_in_partition class) between
// mutation_fragment objects. It reflects the order in which content of
// partition appears in the sstables.

class clustering_row {
    clustering_key_prefix _ck;
    deletable_row _row;
public:
    explicit clustering_row(clustering_key_prefix ck) : _ck(std::move(ck)) { }
    clustering_row(clustering_key_prefix ck, row_tombstone t, row_marker marker, row cells)
            : _ck(std::move(ck)), _row(std::move(t), std::move(marker), std::move(cells)) {
        _row.maybe_shadow();
    }
    clustering_row(clustering_key_prefix ck, deletable_row&& row)
        : _ck(std::move(ck)), _row(std::move(row)) { }
    clustering_row(const schema& s, const clustering_row& other)
        : _ck(other._ck), _row(s, other._row) { }
    clustering_row(const schema& s, const rows_entry& re)
        : _ck(re.key()), _row(s, re.row()) { }
    clustering_row(rows_entry&& re)
        : _ck(std::move(re.key())), _row(std::move(re.row())) {}

    clustering_key_prefix& key() { return _ck; }
    const clustering_key_prefix& key() const { return _ck; }

    void remove_tombstone() { _row.remove_tombstone(); }
    row_tombstone tomb() const { return _row.deleted_at(); }

    const row_marker& marker() const { return _row.marker(); }
    row_marker& marker() { return _row.marker(); }

    const row& cells() const { return _row.cells(); }
    row& cells() { return _row.cells(); }

    bool empty() const { return _row.empty(); }

    bool is_live(const schema& s, tombstone base_tombstone = tombstone(), gc_clock::time_point now = gc_clock::time_point::min()) const {
        return _row.is_live(s, column_kind::regular_column, std::move(base_tombstone), std::move(now));
    }

    void apply(const schema& s, clustering_row&& cr) {
        _row.apply(s, std::move(cr._row));
    }
    void apply(const schema& s, const clustering_row& cr) {
        _row.apply(s, deletable_row(s, cr._row));
    }
    void set_cell(const column_definition& def, atomic_cell_or_collection&& value) {
        _row.cells().apply(def, std::move(value));
    }
    void apply(row_marker rm) {
        _row.apply(std::move(rm));
    }
    void apply(tombstone t) {
        _row.apply(std::move(t));
    }
    void apply(shadowable_tombstone t) {
        _row.apply(std::move(t));
    }
    void apply(const schema& s, const rows_entry& r) {
        _row.apply(s, deletable_row(s, r.row()));
    }
    void apply(const schema& s, const deletable_row& r) {
        _row.apply(s, r);
    }

    position_in_partition_view position() const;

    size_t external_memory_usage(const schema& s) const {
        return _ck.external_memory_usage() + _row.cells().external_memory_usage(s, column_kind::regular_column);
    }

    size_t minimal_external_memory_usage(const schema& s) const {
        return _ck.minimal_external_memory_usage() + _row.cells().external_memory_usage(s, column_kind::regular_column);
    }

    size_t memory_usage(const schema& s) const {
        return sizeof(clustering_row) + external_memory_usage(s);
    }

    bool equal(const schema& s, const clustering_row& other) const {
        return _ck.equal(s, other._ck)
                && _row.equal(column_kind::regular_column, s, other._row, s);
    }

    class printer {
        const schema& _schema;
        const clustering_row& _clustering_row;
    public:
        printer(const schema& s, const clustering_row& r) : _schema(s), _clustering_row(r) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;

        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);

    deletable_row as_deletable_row() && { return std::move(_row); }
    const deletable_row& as_deletable_row() const & { return _row; }
};

class static_row {
    row _cells;
public:
    static_row() = default;
    static_row(const schema& s, const static_row& other) : static_row(s, other._cells) { }
    explicit static_row(const schema& s, const row& r) : _cells(s, column_kind::static_column, r) { }
    explicit static_row(row&& r) : _cells(std::move(r)) { }

    row& cells() { return _cells; }
    const row& cells() const { return _cells; }

    bool empty() const {
        return _cells.empty();
    }

    bool is_live(const schema& s, gc_clock::time_point now = gc_clock::time_point::min()) const {
        return _cells.is_live(s, column_kind::static_column, tombstone(), now);
    }

    void apply(const schema& s, const row& r) {
        _cells.apply(s, column_kind::static_column, r);
    }
    void apply(const schema& s, static_row&& sr) {
        _cells.apply(s, column_kind::static_column, std::move(sr._cells));
    }
    void set_cell(const column_definition& def, atomic_cell_or_collection&& value) {
        _cells.apply(def, std::move(value));
    }

    position_in_partition_view position() const;

    size_t external_memory_usage(const schema& s) const {
        return _cells.external_memory_usage(s, column_kind::static_column);
    }

    size_t memory_usage(const schema& s) const {
        return sizeof(static_row) + external_memory_usage(s);
    }

    bool equal(const schema& s, const static_row& other) const {
        return _cells.equal(column_kind::static_column, s, other._cells, s);
    }

    class printer {
        const schema& _schema;
        const static_row& _static_row;
    public:
        printer(const schema& s, const static_row& r) : _schema(s), _static_row(r) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;

        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);
};

class partition_start final {
    dht::decorated_key _key;
    tombstone _partition_tombstone;
public:
    partition_start(dht::decorated_key pk, tombstone pt)
        : _key(std::move(pk))
        , _partition_tombstone(std::move(pt))
    { }

    dht::decorated_key& key() { return _key; }
    const dht::decorated_key& key() const { return _key; }
    const tombstone& partition_tombstone() const { return _partition_tombstone; }
    tombstone& partition_tombstone() { return _partition_tombstone; }

    position_in_partition_view position() const;

    size_t external_memory_usage(const schema&) const {
        return _key.external_memory_usage();
    }

    size_t memory_usage(const schema& s) const {
        return sizeof(partition_start) + external_memory_usage(s);
    }

    bool equal(const schema& s, const partition_start& other) const {
        return _key.equal(s, other._key) && _partition_tombstone == other._partition_tombstone;
    }

    friend std::ostream& operator<<(std::ostream& is, const partition_start& row);
};

class partition_end final {
public:
    position_in_partition_view position() const;

    size_t external_memory_usage(const schema&) const {
        return 0;
    }

    size_t memory_usage(const schema& s) const {
        return sizeof(partition_end) + external_memory_usage(s);
    }

    bool equal(const schema& s, const partition_end& other) const {
        return true;
    }

    friend std::ostream& operator<<(std::ostream& is, const partition_end& row);
};

template<typename T, typename ReturnType>
concept MutationFragmentConsumer =
    requires(T& t, static_row sr, clustering_row cr, range_tombstone rt, partition_start ph, partition_end pe) {
        { t.consume(std::move(sr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(cr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(rt)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(ph)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(pe)) } -> std::same_as<ReturnType>;
    };

template<typename T, typename ReturnType>
concept FragmentConsumerReturning =
    requires(T t, static_row sr, clustering_row cr, range_tombstone rt, tombstone tomb) {
        { t.consume(std::move(sr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(cr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(rt)) } -> std::same_as<ReturnType>;
    };

template<typename T>
concept FragmentConsumer =
    FragmentConsumerReturning<T, stop_iteration> || FragmentConsumerReturning<T, future<stop_iteration>>;

template<typename T>
concept StreamedMutationConsumer =
    FragmentConsumer<T> && requires(T t, static_row sr, clustering_row cr, range_tombstone rt, tombstone tomb) {
        t.consume(tomb);
        t.consume_end_of_stream();
    };

template<typename T, typename ReturnType>
concept MutationFragmentVisitor =
    requires(T t, const static_row& sr, const clustering_row& cr, const range_tombstone& rt, const partition_start& ph, const partition_end& eop) {
        { t(sr) } -> std::same_as<ReturnType>;
        { t(cr) } -> std::same_as<ReturnType>;
        { t(rt) } -> std::same_as<ReturnType>;
        { t(ph) } -> std::same_as<ReturnType>;
        { t(eop) } -> std::same_as<ReturnType>;
    };

class mutation_fragment {
public:
    enum class kind {
        static_row,
        clustering_row,
        range_tombstone,
        partition_start,
        partition_end,
    };
private:
    struct data {
        data(reader_permit permit) :  _memory(permit.consume_memory()) { }
        ~data() { }

        reader_permit::resource_units _memory;
        union {
            static_row _static_row;
            clustering_row _clustering_row;
            range_tombstone _range_tombstone;
            partition_start _partition_start;
            partition_end _partition_end;
        };
    };
private:
    kind _kind;
    std::unique_ptr<data> _data;

    mutation_fragment() = default;
    explicit operator bool() const noexcept { return bool(_data); }
    void destroy_data() noexcept;
    void reset_memory(const schema& s, std::optional<reader_resources> res = {});
    friend class optimized_optional<mutation_fragment>;

    friend class position_in_partition;
public:
    struct clustering_row_tag_t { };

    template<typename... Args>
    mutation_fragment(clustering_row_tag_t, const schema& s, reader_permit permit, Args&&... args)
        : _kind(kind::clustering_row)
        , _data(std::make_unique<data>(std::move(permit)))
    {
        new (&_data->_clustering_row) clustering_row(std::forward<Args>(args)...);
        reset_memory(s);
    }

    mutation_fragment(const schema& s, reader_permit permit, static_row&& r);
    mutation_fragment(const schema& s, reader_permit permit, clustering_row&& r);
    mutation_fragment(const schema& s, reader_permit permit, range_tombstone&& r);
    mutation_fragment(const schema& s, reader_permit permit, partition_start&& r);
    mutation_fragment(const schema& s, reader_permit permit, partition_end&& r);

    mutation_fragment(const schema& s, reader_permit permit, const mutation_fragment& o)
        : _kind(o._kind), _data(std::make_unique<data>(std::move(permit))) {
        switch (_kind) {
            case kind::static_row:
                new (&_data->_static_row) static_row(s, o._data->_static_row);
                break;
            case kind::clustering_row:
                new (&_data->_clustering_row) clustering_row(s, o._data->_clustering_row);
                break;
            case kind::range_tombstone:
                new (&_data->_range_tombstone) range_tombstone(o._data->_range_tombstone);
                break;
            case kind::partition_start:
                new (&_data->_partition_start) partition_start(o._data->_partition_start);
                break;
            case kind::partition_end:
                new (&_data->_partition_end) partition_end(o._data->_partition_end);
                break;
        }
        reset_memory(s, o._data->_memory.resources());
    }
    mutation_fragment(mutation_fragment&& other) = default;
    mutation_fragment& operator=(mutation_fragment&& other) noexcept {
        if (this != &other) {
            this->~mutation_fragment();
            new (this) mutation_fragment(std::move(other));
        }
        return *this;
    }
    [[gnu::always_inline]]
    ~mutation_fragment() {
        if (_data) {
            destroy_data();
        }
    }

    position_in_partition_view position() const;

    // Returns the range of positions for which this fragment holds relevant information.
    position_range range(const schema& s) const;

    // Checks if this fragment may be relevant for any range starting at given position.
    bool relevant_for_range(const schema& s, position_in_partition_view pos) const;

    // Like relevant_for_range() but makes use of assumption that pos is greater
    // than the starting position of this fragment.
    bool relevant_for_range_assuming_after(const schema& s, position_in_partition_view pos) const;

    bool has_key() const { return is_clustering_row() || is_range_tombstone(); }
    // Requirements: has_key() == true
    const clustering_key_prefix& key() const;

    kind mutation_fragment_kind() const { return _kind; }

    bool is_static_row() const { return _kind == kind::static_row; }
    bool is_clustering_row() const { return _kind == kind::clustering_row; }
    bool is_range_tombstone() const { return _kind == kind::range_tombstone; }
    bool is_partition_start() const { return _kind == kind::partition_start; }
    bool is_end_of_partition() const { return _kind == kind::partition_end; }

    void mutate_as_static_row(const schema& s, std::invocable<static_row&> auto&& fn) {
        fn(_data->_static_row);
        reset_memory(s);
    }
    void mutate_as_clustering_row(const schema& s, std::invocable<clustering_row&> auto&& fn) {
        fn(_data->_clustering_row);
        reset_memory(s);
    }
    void mutate_as_range_tombstone(const schema& s, std::invocable<range_tombstone&> auto&& fn) {
        fn(_data->_range_tombstone);
        reset_memory(s);
    }
    void mutate_as_partition_start(const schema& s, std::invocable<partition_start&> auto&& fn) {
        fn(_data->_partition_start);
        reset_memory(s);
    }

    static_row&& as_static_row() && { return std::move(_data->_static_row); }
    clustering_row&& as_clustering_row() && { return std::move(_data->_clustering_row); }
    range_tombstone&& as_range_tombstone() && { return std::move(_data->_range_tombstone); }
    partition_start&& as_partition_start() && { return std::move(_data->_partition_start); }
    partition_end&& as_end_of_partition() && { return std::move(_data->_partition_end); }

    const static_row& as_static_row() const & { return _data->_static_row; }
    const clustering_row& as_clustering_row() const & { return _data->_clustering_row; }
    const range_tombstone& as_range_tombstone() const & { return _data->_range_tombstone; }
    const partition_start& as_partition_start() const & { return _data->_partition_start; }
    const partition_end& as_end_of_partition() const & { return _data->_partition_end; }

    // Requirements: mergeable_with(mf)
    void apply(const schema& s, mutation_fragment&& mf);

    template<typename Consumer>
    requires MutationFragmentConsumer<Consumer, decltype(std::declval<Consumer>().consume(std::declval<range_tombstone>()))>
    decltype(auto) consume(Consumer& consumer) && {
        _data->_memory.reset_to_zero();
        switch (_kind) {
        case kind::static_row:
            return consumer.consume(std::move(_data->_static_row));
        case kind::clustering_row:
            return consumer.consume(std::move(_data->_clustering_row));
        case kind::range_tombstone:
            return consumer.consume(std::move(_data->_range_tombstone));
        case kind::partition_start:
            return consumer.consume(std::move(_data->_partition_start));
        case kind::partition_end:
            return consumer.consume(std::move(_data->_partition_end));
        }
        abort();
    }

    template<typename Visitor>
    requires MutationFragmentVisitor<Visitor, decltype(std::declval<Visitor>()(std::declval<static_row&>()))>
    decltype(auto) visit(Visitor&& visitor) const {
        switch (_kind) {
        case kind::static_row:
            return visitor(as_static_row());
        case kind::clustering_row:
            return visitor(as_clustering_row());
        case kind::range_tombstone:
            return visitor(as_range_tombstone());
        case kind::partition_start:
            return visitor(as_partition_start());
        case kind::partition_end:
            return visitor(as_end_of_partition());
        }
        abort();
    }

    size_t memory_usage() const {
        return _data->_memory.resources().memory;
    }

    reader_permit permit() const {
        return _data->_memory.permit();
    }

    bool equal(const schema& s, const mutation_fragment& other) const {
        if (other._kind != _kind) {
            return false;
        }
        switch (_kind) {
        case kind::static_row:
            return as_static_row().equal(s, other.as_static_row());
        case kind::clustering_row:
            return as_clustering_row().equal(s, other.as_clustering_row());
        case kind::range_tombstone:
            return as_range_tombstone().equal(s, other.as_range_tombstone());
        case kind::partition_start:
            return as_partition_start().equal(s, other.as_partition_start());
        case kind::partition_end:
            return as_end_of_partition().equal(s, other.as_end_of_partition());
        }
        abort();
    }

    // Fragments which have the same position() and are mergeable can be
    // merged into one fragment with apply() which represents the sum of
    // writes represented by each of the fragments.
    // Fragments which have the same position() but are not mergeable
    // can be emitted one after the other in the stream.
    bool mergeable_with(const mutation_fragment& mf) const {
        return _kind == mf._kind && _kind != kind::range_tombstone;
    }

    class printer {
        const schema& _schema;
        const mutation_fragment& _mutation_fragment;
    public:
        printer(const schema& s, const mutation_fragment& mf) : _schema(s), _mutation_fragment(mf) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;

        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);

private:
    size_t calculate_memory_usage(const schema& s) const {
        return sizeof(data) + visit([&s] (auto& mf) -> size_t { return mf.external_memory_usage(s); });
    }
};

inline position_in_partition_view static_row::position() const
{
    return position_in_partition_view(position_in_partition_view::static_row_tag_t());
}

inline position_in_partition_view clustering_row::position() const
{
    return position_in_partition_view(position_in_partition_view::clustering_row_tag_t(), _ck);
}

inline position_in_partition_view partition_start::position() const
{
    return position_in_partition_view::for_partition_start();
}

inline position_in_partition_view partition_end::position() const
{
    return position_in_partition_view::for_partition_end();
}

std::ostream& operator<<(std::ostream&, mutation_fragment::kind);


// range_tombstone_stream is a helper object that simplifies producing a stream
// of range tombstones and merging it with a stream of clustering rows.
// Tombstones are added using apply() and retrieved using get_next().
//
// get_next(const rows_entry&) and get_next(const mutation_fragment&) allow
// merging the stream of tombstones with a stream of clustering rows. If these
// overloads return disengaged optional it means that there is no tombstone
// in the stream that should be emitted before the object given as an argument.
// (And, consequently, if the optional is engaged that tombstone should be
// emitted first). After calling any of these overloads with a mutation_fragment
// which is at some position in partition P no range tombstone can be added to
// the stream which start bound is before that position.
//
// get_next() overload which doesn't take any arguments is used to return the
// remaining tombstones. After it was called no new tombstones can be added
// to the stream.
class range_tombstone_stream {
    const schema& _schema;
    reader_permit _permit;
    position_in_partition::less_compare _cmp;
    range_tombstone_list _list;
private:
    mutation_fragment_opt do_get_next();
public:
    range_tombstone_stream(const schema& s, reader_permit permit) : _schema(s), _permit(std::move(permit)), _cmp(s), _list(s) { }
    mutation_fragment_opt get_next(const rows_entry&);
    mutation_fragment_opt get_next(const mutation_fragment&);
    // Returns next fragment with position before upper_bound or disengaged optional if no such fragments are left.
    mutation_fragment_opt get_next(position_in_partition_view upper_bound);
    mutation_fragment_opt get_next();
    // Precondition: !empty()
    const range_tombstone& peek_next() const;
    // Forgets all tombstones which are not relevant for any range starting at given position.
    void forward_to(position_in_partition_view);

    void apply(range_tombstone&& rt) {
        _list.apply(_schema, std::move(rt));
    }
    void reset();
    bool empty() const;
    friend std::ostream& operator<<(std::ostream& out, const range_tombstone_stream&);
};

// F gets a stream element as an argument and returns the new value which replaces that element
// in the transformed stream.
template<typename F>
concept StreamedMutationTranformer =
    requires(F f, mutation_fragment mf, schema_ptr s) {
        { f(std::move(mf)) } -> std::same_as<mutation_fragment>;
        { f(s) } -> std::same_as<schema_ptr>;
    };

class xx_hasher;

template<>
struct appending_hash<mutation_fragment> {
    template<typename Hasher>
    void operator()(Hasher& h, const mutation_fragment& mf, const schema& s) const;
};



#include <fmt/core.h>
#include <optional>
#include <seastar/util/optimized_optional.hh>

#include <seastar/core/future-util.hh>


// Mutation fragment which represents a range tombstone boundary.
//
// The range_tombstone_change::tombstone() method returns the tombstone which takes effect
// for positions >= range_tombstone_change::position() in the stream, until the next
// range_tombstone_change is encountered.
//
// Note, a range_tombstone_change with an empty tombstone() ends the range tombstone.
// An empty tombstone naturally does not cover any timestamp.
class range_tombstone_change {
    position_in_partition _pos;
    ::tombstone _tomb;
public:
    range_tombstone_change(position_in_partition pos, tombstone tomb)
        : _pos(std::move(pos))
        , _tomb(tomb)
    { }
    range_tombstone_change(position_in_partition_view pos, tombstone tomb)
        : _pos(pos)
        , _tomb(tomb)
    { }
    const position_in_partition& position() const & {
        return _pos;
    }
    position_in_partition position() && {
        return std::move(_pos);
    }
    void set_position(position_in_partition pos) {
        _pos = std::move(pos);
    }
    ::tombstone tombstone() const {
        return _tomb;
    }
    void set_tombstone(::tombstone tomb) {
        _tomb = tomb;
    }
    size_t external_memory_usage(const schema& s) const {
        return _pos.external_memory_usage();
    }
    size_t minimal_external_memory_usage(const schema&) const noexcept {
        if (_pos.has_key()) {
            return _pos.key().minimal_external_memory_usage();
        }
        return 0;
    }
    size_t memory_usage(const schema& s) const noexcept {
        return sizeof(range_tombstone_change) + external_memory_usage(s);
    }
    size_t minimal_memory_usage(const schema& s) const noexcept {
        return sizeof(range_tombstone_change) + minimal_external_memory_usage(s);
    }
    bool equal(const schema& s, const range_tombstone_change& other) const {
        position_in_partition::equal_compare eq(s);
        return _tomb == other._tomb && eq(_pos, other._pos);
    }
};

template<>
struct fmt::formatter<range_tombstone_change> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const range_tombstone_change& rt, FormatContext& ctx) const {
        return fmt::format_to(ctx.out(), "{{range_tombstone_change: pos={}, {}}}", rt.position(), rt.tombstone());
    }
};

template<typename T, typename ReturnType>
concept MutationFragmentConsumerV2 =
    requires(T& t,
            static_row sr,
            clustering_row cr,
            range_tombstone_change rt_chg,
            partition_start ph,
            partition_end pe) {
        { t.consume(std::move(sr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(cr)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(rt_chg)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(ph)) } -> std::same_as<ReturnType>;
        { t.consume(std::move(pe)) } -> std::same_as<ReturnType>;
    };

template<typename T, typename ReturnType>
concept MutationFragmentVisitorV2 =
    requires(T t,
            const static_row& sr,
            const clustering_row& cr,
            const range_tombstone_change& rt,
            const partition_start& ph,
            const partition_end& eop) {
        { t(sr) } -> std::same_as<ReturnType>;
        { t(cr) } -> std::same_as<ReturnType>;
        { t(rt) } -> std::same_as<ReturnType>;
        { t(ph) } -> std::same_as<ReturnType>;
        { t(eop) } -> std::same_as<ReturnType>;
    };

template<typename T, typename ReturnType>
concept FragmentConsumerReturningV2 =
requires(T t, static_row sr, clustering_row cr, range_tombstone_change rt, tombstone tomb) {
    { t.consume(std::move(sr)) } -> std::same_as<ReturnType>;
    { t.consume(std::move(cr)) } -> std::same_as<ReturnType>;
    { t.consume(std::move(rt)) } -> std::same_as<ReturnType>;
};

template<typename T>
concept FragmentConsumerV2 =
FragmentConsumerReturningV2<T, stop_iteration> || FragmentConsumerReturningV2<T, future<stop_iteration>>;

template<typename T>
concept StreamedMutationConsumerV2 =
FragmentConsumerV2<T> && requires(T t, tombstone tomb) {
    t.consume(tomb);
    t.consume_end_of_stream();
};

class mutation_fragment_v2 {
public:
    enum class kind {
        static_row,
        clustering_row,
        range_tombstone_change,
        partition_start,
        partition_end,
    };
private:
    struct data {
        data(reader_permit permit) :  _memory(permit.consume_memory()) { }
        ~data() { }

        reader_permit::resource_units _memory;
        union {
            static_row _static_row;
            clustering_row _clustering_row;
            range_tombstone_change _range_tombstone_chg;
            partition_start _partition_start;
            partition_end _partition_end;
        };
    };
private:
    kind _kind;
    std::unique_ptr<data> _data;

    mutation_fragment_v2() = default;
    explicit operator bool() const noexcept { return bool(_data); }
    void destroy_data() noexcept;
    void reset_memory(const schema& s, std::optional<reader_resources> res = {});
    friend class optimized_optional<mutation_fragment_v2>;

    friend class position_in_partition;
public:
    struct clustering_row_tag_t { };

    template<typename... Args>
    mutation_fragment_v2(clustering_row_tag_t, const schema& s, reader_permit permit, Args&&... args)
        : _kind(kind::clustering_row)
        , _data(std::make_unique<data>(std::move(permit)))
    {
        new (&_data->_clustering_row) clustering_row(std::forward<Args>(args)...);
        _data->_memory.reset_to(reader_resources::with_memory(calculate_memory_usage(s)));
    }

    mutation_fragment_v2(const schema& s, reader_permit permit, static_row&& r);
    mutation_fragment_v2(const schema& s, reader_permit permit, clustering_row&& r);
    mutation_fragment_v2(const schema& s, reader_permit permit, range_tombstone_change&& r);
    mutation_fragment_v2(const schema& s, reader_permit permit, partition_start&& r);
    mutation_fragment_v2(const schema& s, reader_permit permit, partition_end&& r);

    mutation_fragment_v2(const schema& s, reader_permit permit, const mutation_fragment_v2& o)
        : _kind(o._kind), _data(std::make_unique<data>(std::move(permit))) {
        switch (_kind) {
            case kind::static_row:
                new (&_data->_static_row) static_row(s, o._data->_static_row);
                break;
            case kind::clustering_row:
                new (&_data->_clustering_row) clustering_row(s, o._data->_clustering_row);
                break;
            case kind::range_tombstone_change:
                new (&_data->_range_tombstone_chg) range_tombstone_change(o._data->_range_tombstone_chg);
                break;
            case kind::partition_start:
                new (&_data->_partition_start) partition_start(o._data->_partition_start);
                break;
            case kind::partition_end:
                new (&_data->_partition_end) partition_end(o._data->_partition_end);
                break;
        }
        _data->_memory.reset_to(o._data->_memory.resources());
    }
    mutation_fragment_v2(mutation_fragment_v2&& other) = default;
    mutation_fragment_v2& operator=(mutation_fragment_v2&& other) noexcept {
        if (this != &other) {
            this->~mutation_fragment_v2();
            new (this) mutation_fragment_v2(std::move(other));
        }
        return *this;
    }
    [[gnu::always_inline]]
    ~mutation_fragment_v2() {
        if (_data) {
            destroy_data();
        }
    }

    position_in_partition_view position() const;

    // Checks if this fragment may be relevant for any range starting at given position.
    bool relevant_for_range(const schema& s, position_in_partition_view pos) const;

    bool has_key() const { return is_clustering_row() || is_range_tombstone_change(); }

    // Requirements: has_key() == true
    const clustering_key_prefix& key() const;

    kind mutation_fragment_kind() const { return _kind; }

    bool is_static_row() const { return _kind == kind::static_row; }
    bool is_clustering_row() const { return _kind == kind::clustering_row; }
    bool is_range_tombstone_change() const { return _kind == kind::range_tombstone_change; }
    bool is_partition_start() const { return _kind == kind::partition_start; }
    bool is_end_of_partition() const { return _kind == kind::partition_end; }

    void mutate_as_static_row(const schema& s, std::invocable<static_row&> auto&& fn) {
        fn(_data->_static_row);
        _data->_memory.reset_to(reader_resources::with_memory(calculate_memory_usage(s)));
    }
    void mutate_as_clustering_row(const schema& s, std::invocable<clustering_row&> auto&& fn) {
        fn(_data->_clustering_row);
        _data->_memory.reset_to(reader_resources::with_memory(calculate_memory_usage(s)));
    }
    void mutate_as_range_tombstone_change(const schema& s, std::invocable<range_tombstone_change&> auto&& fn) {
        fn(_data->_range_tombstone_chg);
        _data->_memory.reset_to(reader_resources::with_memory(calculate_memory_usage(s)));
    }
    void mutate_as_partition_start(const schema& s, std::invocable<partition_start&> auto&& fn) {
        fn(_data->_partition_start);
        _data->_memory.reset_to(reader_resources::with_memory(calculate_memory_usage(s)));
    }

    static_row&& as_static_row() && { return std::move(_data->_static_row); }
    clustering_row&& as_clustering_row() && { return std::move(_data->_clustering_row); }
    range_tombstone_change&& as_range_tombstone_change() && { return std::move(_data->_range_tombstone_chg); }
    partition_start&& as_partition_start() && { return std::move(_data->_partition_start); }
    partition_end&& as_end_of_partition() && { return std::move(_data->_partition_end); }

    const static_row& as_static_row() const & { return _data->_static_row; }
    const clustering_row& as_clustering_row() const & { return _data->_clustering_row; }
    const range_tombstone_change& as_range_tombstone_change() const & { return _data->_range_tombstone_chg; }
    const partition_start& as_partition_start() const & { return _data->_partition_start; }
    const partition_end& as_end_of_partition() const & { return _data->_partition_end; }

    // Requirements: mergeable_with(mf)
    void apply(const schema& s, mutation_fragment_v2&& mf);

    template<typename Consumer>
    requires MutationFragmentConsumerV2<Consumer, decltype(std::declval<Consumer>().consume(std::declval<range_tombstone_change>()))>
    decltype(auto) consume(Consumer& consumer) && {
        _data->_memory.reset_to_zero();
        switch (_kind) {
        case kind::static_row:
            return consumer.consume(std::move(_data->_static_row));
        case kind::clustering_row:
            return consumer.consume(std::move(_data->_clustering_row));
        case kind::range_tombstone_change:
            return consumer.consume(std::move(_data->_range_tombstone_chg));
        case kind::partition_start:
            return consumer.consume(std::move(_data->_partition_start));
        case kind::partition_end:
            return consumer.consume(std::move(_data->_partition_end));
        }
        abort();
    }

    template<typename Visitor>
    requires MutationFragmentVisitorV2<Visitor, decltype(std::declval<Visitor>()(std::declval<static_row&>()))>
    decltype(auto) visit(Visitor&& visitor) const {
        switch (_kind) {
        case kind::static_row:
            return visitor(as_static_row());
        case kind::clustering_row:
            return visitor(as_clustering_row());
        case kind::range_tombstone_change:
            return visitor(as_range_tombstone_change());
        case kind::partition_start:
            return visitor(as_partition_start());
        case kind::partition_end:
            return visitor(as_end_of_partition());
        }
        abort();
    }

    size_t memory_usage() const {
        return _data->_memory.resources().memory;
    }

    reader_permit permit() const {
        return _data->_memory.permit();
    }

    bool equal(const schema& s, const mutation_fragment_v2& other) const {
        if (other._kind != _kind) {
            return false;
        }
        switch (_kind) {
        case kind::static_row:
            return as_static_row().equal(s, other.as_static_row());
        case kind::clustering_row:
            return as_clustering_row().equal(s, other.as_clustering_row());
        case kind::range_tombstone_change:
            return as_range_tombstone_change().equal(s, other.as_range_tombstone_change());
        case kind::partition_start:
            return as_partition_start().equal(s, other.as_partition_start());
        case kind::partition_end:
            return as_end_of_partition().equal(s, other.as_end_of_partition());
        }
        abort();
    }

    // Fragments which have the same position() and are mergeable can be
    // merged into one fragment with apply() which represents the sum of
    // writes represented by each of the fragments.
    // Fragments which have the same position() but are not mergeable
    // and at least one of them is not a range_tombstone_change can be emitted one after the other in the stream.
    //
    // Undefined for range_tombstone_change.
    // Merging range tombstones requires a more complicated handling
    // because range_tombstone_change doesn't represent a write on its own, only
    // with a matching change for the end bound. It's not enough to chose one fragment over another,
    // the upper bound of the winning tombstone needs to be taken into account when merging
    // later range_tombstone_change fragments in the stream.
    bool mergeable_with(const mutation_fragment_v2& mf) const {
        return _kind == mf._kind && _kind != kind::range_tombstone_change;
    }

    class printer {
        const schema& _schema;
        const mutation_fragment_v2& _mutation_fragment;
    public:
        printer(const schema& s, const mutation_fragment_v2& mf) : _schema(s), _mutation_fragment(mf) { }
        printer(const printer&) = delete;
        printer(printer&&) = delete;

        friend std::ostream& operator<<(std::ostream& os, const printer& p);
    };
    friend std::ostream& operator<<(std::ostream& os, const printer& p);

private:
    size_t calculate_memory_usage(const schema& s) const {
        return sizeof(data) + visit([&s] (auto& mf) -> size_t { return mf.external_memory_usage(s); });
    }
};

std::ostream& operator<<(std::ostream&, mutation_fragment_v2::kind);

// F gets a stream element as an argument and returns the new value which replaces that element
// in the transformed stream.
template<typename F>
concept StreamedMutationTranformerV2 =
requires(F f, mutation_fragment_v2 mf, schema_ptr s) {
    { f(std::move(mf)) } -> std::same_as<mutation_fragment_v2>;
    { f(s) } -> std::same_as<schema_ptr>;
};


template<typename Consumer>
concept FlatMutationReaderConsumer =
    requires(Consumer c, mutation_fragment mf) {
        { c(std::move(mf)) } -> std::same_as<stop_iteration>;
    } || requires(Consumer c, mutation_fragment mf) {
        { c(std::move(mf)) } -> std::same_as<future<stop_iteration>>;
    };


template<typename T>
concept FlattenedConsumer =
    StreamedMutationConsumer<T> && requires(T obj, const dht::decorated_key& dk) {
        { obj.consume_new_partition(dk) };
        { obj.consume_end_of_partition() };
    };

template<typename T>
concept FlattenedConsumerFilter =
    requires(T filter, const dht::decorated_key& dk, const mutation_fragment& mf) {
        { filter(dk) } -> std::same_as<bool>;
        { filter(mf) } -> std::same_as<bool>;
        { filter.on_end_of_stream() } -> std::same_as<void>;
    };

template<typename Consumer>
concept FlatMutationReaderConsumerV2 =
    requires(Consumer c, mutation_fragment_v2 mf) {
        { c(std::move(mf)) } -> std::same_as<stop_iteration>;
    } || requires(Consumer c, mutation_fragment_v2 mf) {
        { c(std::move(mf)) } -> std::same_as<future<stop_iteration>>;
    };

template<typename Consumer>
concept MutationConsumer =
    requires(Consumer c, mutation m) {
        { c(std::move(m)) } -> std::same_as<stop_iteration>;
    } || requires(Consumer c, mutation m) {
        { c(std::move(m)) } -> std::same_as<future<stop_iteration>>;
    };

template<typename T>
concept FlattenedConsumerV2 =
    StreamedMutationConsumerV2<T> && requires(T obj, const dht::decorated_key& dk) {
        { obj.consume_new_partition(dk) };
        { obj.consume_end_of_partition() };
    };

template<typename T>
concept FlattenedConsumerFilterV2 =
    requires(T filter, const dht::decorated_key& dk, const mutation_fragment_v2& mf) {
        { filter(dk) } -> std::same_as<bool>;
        { filter(mf) } -> std::same_as<bool>;
        { filter.on_end_of_stream() } -> std::same_as<void>;
    };


enum class consume_in_reverse {
    no = 0,
    yes,
};



template<typename T>
concept RangeTombstoneChangeConsumer = std::invocable<T, range_tombstone_change>;

/// Generates range_tombstone_change fragments for a stream of range_tombstone fragments.
///
/// The input range_tombstones passed to consume() may be overlapping, but must be weakly ordered by position().
/// It's ok to pass consecutive range_tombstone objects with the same position.
///
/// Generated range_tombstone_change fragments will have strictly monotonic positions.
///
/// Example usage:
///
///   consume(range_tombstone(1, +inf, t));
///   flush(2, consumer);
///   consume(range_tombstone(2, +inf, t));
///   flush(3, consumer);
///   consume(range_tombstone(4, +inf, t));
///   consume(range_tombstone(4, 7, t));
///   flush(5, consumer);
///   flush(6, consumer);
///
class range_tombstone_change_generator {
    range_tombstone_list _range_tombstones;
    // All range_tombstone_change fragments with positions < than this have been emitted.
    position_in_partition _lower_bound = position_in_partition::before_all_clustered_rows();
    const schema& _schema;
public:
    range_tombstone_change_generator(const schema& s)
        : _range_tombstones(s)
        , _schema(s)
    { }

    // Discards deletion information for positions < lower_bound.
    // After this, the lowest position of emitted range_tombstone_change will be before_key(lower_bound).
    void trim(const position_in_partition& lower_bound) {
        position_in_partition::less_compare less(_schema);

        if (lower_bound.is_clustering_row()) {
            _lower_bound = position_in_partition::before_key(lower_bound.key());
        } else {
            _lower_bound = lower_bound;
        }

        while (!_range_tombstones.empty() && !less(lower_bound, _range_tombstones.begin()->end_position())) {
            _range_tombstones.pop(_range_tombstones.begin());
        }

        if (!_range_tombstones.empty() && less(_range_tombstones.begin()->position(), _lower_bound)) {
            // _range_tombstones.begin()->end_position() < lower_bound is guaranteed by previous loop.
            _range_tombstones.begin()->tombstone().set_start(_lower_bound);
        }
    }

    // Emits range_tombstone_change fragments with positions smaller than upper_bound
    // for accumulated range tombstones. If end_of_range = true, range tombstone
    // change fragments with position equal to upper_bound may also be emitted.
    // After this, only range_tombstones with positions >= upper_bound may be added,
    // which guarantees that they won't affect the output of this flush.
    //
    // If upper_bound == position_in_partition::after_all_clustered_rows(),
    // emits all remaining range_tombstone_changes.
    // No range_tombstones may be added after this.
    //
    // FIXME: respect preemption
    template<RangeTombstoneChangeConsumer C>
    void flush(const position_in_partition_view upper_bound, C consumer, bool end_of_range = false) {
        if (_range_tombstones.empty()) {
            return;
        }

        position_in_partition::tri_compare cmp(_schema);
        std::optional<range_tombstone> prev;
        const bool allow_eq = upper_bound.is_after_all_clustered_rows(_schema);
        const auto should_flush = [&] (position_in_partition_view pos) {
            const auto res = cmp(pos, upper_bound);
            if (allow_eq) {
                return res <= 0;
            } else {
                return res < 0;
            }
        };

        while (!_range_tombstones.empty() && should_flush(_range_tombstones.begin()->end_position())) {
            auto rt = _range_tombstones.pop(_range_tombstones.begin());

            if (prev && (cmp(prev->end_position(), rt.position()) < 0)) { // [1]
                // previous range tombstone not adjacent, emit gap.
                consumer(range_tombstone_change(prev->end_position(), tombstone()));
            }

            // Check if start of rt was already emitted, emit if not.
            if (cmp(rt.position(), _lower_bound) >= 0) {
                consumer(range_tombstone_change(rt.position(), rt.tomb));
            }

            // Delay emitting end bound in case it's adjacent with the next tombstone. See [1] and [2]
            prev = std::move(rt);
        }

        // If previous range tombstone not adjacent with current, emit gap.
        // It cannot get adjacent later because prev->end_position() < upper_bound,
        // so nothing == prev->end_position() can be added after this invocation.
        if (prev && (_range_tombstones.empty()
                     || (cmp(prev->end_position(), _range_tombstones.begin()->position()) < 0))) {
            consumer(range_tombstone_change(prev->end_position(), tombstone())); // [2]
        }

        // Emit the fragment for start bound of a range_tombstone which is overlapping with upper_bound,
        // unless no such fragment or already emitted.
        if (!_range_tombstones.empty()
            && (cmp(_range_tombstones.begin()->position(), upper_bound) < 0)
            && (cmp(_range_tombstones.begin()->position(), _lower_bound) >= 0)) {
            consumer(range_tombstone_change(
                    _range_tombstones.begin()->position(), _range_tombstones.begin()->tombstone().tomb));
        }

        // Close current tombstone (if any) at upper_bound if end_of_range is
        // set, so a sliced read will have properly closed range tombstone bounds
        // at each range end
        if (!_range_tombstones.empty()
                && end_of_range
                && (cmp(_range_tombstones.begin()->position(), upper_bound) < 0)) {
            consumer(range_tombstone_change(upper_bound, tombstone()));
        }

        _lower_bound = upper_bound;
    }

    void consume(range_tombstone rt) {
        _range_tombstones.apply(_schema, std::move(rt));
    }

    void reset() {
        _range_tombstones.clear();
        _lower_bound = position_in_partition::before_all_clustered_rows();
    }

    bool discardable() const {
        return _range_tombstones.empty();
    }
};

#include <seastar/util/optimized_optional.hh>
#include <seastar/core/coroutine.hh>
#include <seastar/coroutine/maybe_yield.hh>

struct mutation_consume_cookie {
    using crs_iterator_type = mutation_partition::rows_type::iterator;
    using rts_iterator_type = range_tombstone_list::iterator;

    struct clustering_iterators {
        crs_iterator_type crs_begin;
        crs_iterator_type crs_end;
        rts_iterator_type rts_begin;
        rts_iterator_type rts_end;
        range_tombstone_change_generator rt_gen;

        clustering_iterators(const schema& s, crs_iterator_type crs_b, crs_iterator_type crs_e, rts_iterator_type rts_b, rts_iterator_type rts_e)
            : crs_begin(std::move(crs_b)), crs_end(std::move(crs_e)), rts_begin(std::move(rts_b)), rts_end(std::move(rts_e)), rt_gen(s) { }
    };

    schema_ptr schema;
    bool partition_start_consumed = false;
    bool static_row_consumed = false;
    // only used when reverse == consume_in_reverse::yes
    bool reversed_range_tombstone = false;
    std::unique_ptr<clustering_iterators> iterators;
};

template<typename Result>
struct mutation_consume_result {
    stop_iteration stop;
    Result result;
    mutation_consume_cookie cookie;
};

template<>
struct mutation_consume_result<void> {
    stop_iteration stop;
    mutation_consume_cookie cookie;
};

class mutation final {
private:
    struct data {
        schema_ptr _schema;
        dht::decorated_key _dk;
        mutation_partition _p;

        data(dht::decorated_key&& key, schema_ptr&& schema);
        data(partition_key&& key, schema_ptr&& schema);
        data(schema_ptr&& schema, dht::decorated_key&& key, const mutation_partition& mp);
        data(schema_ptr&& schema, dht::decorated_key&& key, mutation_partition&& mp);
    };
    std::unique_ptr<data> _ptr;
private:
    mutation() = default;
    explicit operator bool() const { return bool(_ptr); }
    friend class optimized_optional<mutation>;
public:
    mutation(schema_ptr schema, dht::decorated_key key)
        : _ptr(std::make_unique<data>(std::move(key), std::move(schema)))
    { }
    mutation(schema_ptr schema, partition_key key_)
        : _ptr(std::make_unique<data>(std::move(key_), std::move(schema)))
    { }
    mutation(schema_ptr schema, dht::decorated_key key, const mutation_partition& mp)
        : _ptr(std::make_unique<data>(std::move(schema), std::move(key), mp))
    { }
    mutation(schema_ptr schema, dht::decorated_key key, mutation_partition&& mp)
        : _ptr(std::make_unique<data>(std::move(schema), std::move(key), std::move(mp)))
    { }
    mutation(const mutation& m)
    {
        if (m._ptr) {
            _ptr = std::make_unique<data>(schema_ptr(m.schema()), dht::decorated_key(m.decorated_key()), m.partition());
        }
    }
    mutation(mutation&&) = default;
    mutation& operator=(mutation&& x) = default;
    mutation& operator=(const mutation& m);

    void set_static_cell(const column_definition& def, atomic_cell_or_collection&& value);
    void set_static_cell(const bytes& name, const data_value& value, api::timestamp_type timestamp, ttl_opt ttl = {});
    void set_clustered_cell(const clustering_key& key, const bytes& name, const data_value& value, api::timestamp_type timestamp, ttl_opt ttl = {});
    void set_clustered_cell(const clustering_key& key, const column_definition& def, atomic_cell_or_collection&& value);
    void set_cell(const clustering_key_prefix& prefix, const bytes& name, const data_value& value, api::timestamp_type timestamp, ttl_opt ttl = {});
    void set_cell(const clustering_key_prefix& prefix, const column_definition& def, atomic_cell_or_collection&& value);

    // Upgrades this mutation to a newer schema. The new schema must
    // be obtained using only valid schema transformation:
    //  * primary key column count must not change
    //  * column types may only change to those with compatible representations
    //
    // After upgrade, mutation's partition should only be accessed using the new schema. User must
    // ensure proper isolation of accesses.
    //
    // Strong exception guarantees.
    //
    // Note that the conversion may lose information, it's possible that m1 != m2 after:
    //
    //   auto m2 = m1;
    //   m2.upgrade(s2);
    //   m2.upgrade(m1.schema());
    //
    void upgrade(const schema_ptr&);

    const partition_key& key() const { return _ptr->_dk._key; };
    const dht::decorated_key& decorated_key() const { return _ptr->_dk; };
    dht::ring_position ring_position() const { return { decorated_key() }; }
    const dht::token& token() const { return _ptr->_dk._token; }
    const schema_ptr& schema() const { return _ptr->_schema; }
    const mutation_partition& partition() const { return _ptr->_p; }
    mutation_partition& partition() { return _ptr->_p; }
    const table_id& column_family_id() const { return _ptr->_schema->id(); }
    // Consistent with hash<canonical_mutation>
    bool operator==(const mutation&) const;
public:
    // Consumes the mutation's content.
    //
    // The mutation is in a moved-from alike state after consumption.
    // There are tree ways to consume the mutation:
    // * consume_in_reverse::no - consume in forward order, as defined by the
    //   schema.
    // * consume_in_reverse::yes - consume in reverse order, as if the schema
    //   had the opposite clustering order. This effectively reverses the
    //   mutation's content, according to the native reverse order[1].
    //
    // For definition of [1] and [2] see docs/dev/reverse-reads.md.
    //
    // The consume operation is pausable and resumable:
    // * To pause return stop_iteration::yes from one of the consume() methods;
    // * The consume will now stop and return;
    // * To resume call consume again and pass the cookie member of the returned
    //   mutation_consume_result as the cookie parameter;
    //
    // Note that `consume_end_of_partition()` and `consume_end_of_stream()`
    // will be called each time the consume is stopping, regardless of whether
    // you are pausing or the consumption is ending for good.
    template<FlattenedConsumerV2 Consumer>
    auto consume(Consumer& consumer, consume_in_reverse reverse, mutation_consume_cookie cookie = {}) && -> mutation_consume_result<decltype(consumer.consume_end_of_stream())>;

    template<FlattenedConsumerV2 Consumer>
    auto consume_gently(Consumer& consumer, consume_in_reverse reverse, mutation_consume_cookie cookie = {}) && -> future<mutation_consume_result<decltype(consumer.consume_end_of_stream())>>;

    // See mutation_partition::live_row_count()
    uint64_t live_row_count(gc_clock::time_point query_time = gc_clock::time_point::min()) const;

    void apply(mutation&&);
    void apply(const mutation&);
    void apply(const mutation_fragment&);

    mutation operator+(const mutation& other) const;
    mutation& operator+=(const mutation& other);
    mutation& operator+=(mutation&& other);

    // Returns a subset of this mutation holding only information relevant for given clustering ranges.
    // Range tombstones will be trimmed to the boundaries of the clustering ranges.
    mutation sliced(const query::clustering_row_ranges&) const;

    unsigned shard_of() const {
        return dht::shard_of(*schema(), token());
    }

    // Returns a mutation which contains the same writes but in a minimal form.
    // Drops data covered by tombstones.
    // Does not drop expired tombstones.
    // Does not expire TTLed data.
    mutation compacted() const;
private:
    friend std::ostream& operator<<(std::ostream& os, const mutation& m);
};

namespace {

template<consume_in_reverse reverse, FlattenedConsumerV2 Consumer>
std::optional<stop_iteration> consume_clustering_fragments(schema_ptr s, mutation_partition& partition, Consumer& consumer, mutation_consume_cookie& cookie, is_preemptible preempt = is_preemptible::no) {
    constexpr bool crs_in_reverse = reverse == consume_in_reverse::yes;
    // We can read the range_tombstone_list in reverse order in consume_in_reverse::yes mode
    // since we deoverlap range_tombstones on insertion.
    constexpr bool rts_in_reverse = reverse == consume_in_reverse::yes;

    using crs_type = mutation_partition::rows_type;
    using crs_iterator_type = std::conditional_t<crs_in_reverse, std::reverse_iterator<crs_type::iterator>, crs_type::iterator>;
    using rts_type = range_tombstone_list;
    using rts_iterator_type = std::conditional_t<rts_in_reverse, std::reverse_iterator<rts_type::iterator>, rts_type::iterator>;

    if (!cookie.schema) {
        if constexpr (reverse == consume_in_reverse::yes) {
            cookie.schema = s->make_reversed();
        } else {
            cookie.schema = s;
        }
    }
    s = cookie.schema;

    if (!cookie.iterators) {
        auto& crs = partition.mutable_clustered_rows();
        auto& rts = partition.mutable_row_tombstones();
        cookie.iterators = std::make_unique<mutation_consume_cookie::clustering_iterators>(*s, crs.begin(), crs.end(), rts.begin(), rts.end());
    }

    crs_iterator_type crs_it, crs_end;
    rts_iterator_type rts_it, rts_end;

    if constexpr (crs_in_reverse) {
        crs_it = std::reverse_iterator(cookie.iterators->crs_end);
        crs_end = std::reverse_iterator(cookie.iterators->crs_begin);
    } else {
        crs_it = cookie.iterators->crs_begin;
        crs_end = cookie.iterators->crs_end;
    }

    if constexpr (rts_in_reverse) {
        rts_it = std::reverse_iterator(cookie.iterators->rts_end);
        rts_end = std::reverse_iterator(cookie.iterators->rts_begin);
    } else {
        rts_it = cookie.iterators->rts_begin;
        rts_end = cookie.iterators->rts_end;
    }

    auto flush_tombstones = [&] (position_in_partition_view pos) {
        cookie.iterators->rt_gen.flush(pos, [&] (range_tombstone_change rt) {
            consumer.consume(std::move(rt));
        });
    };

    stop_iteration stop = stop_iteration::no;

    position_in_partition::tri_compare cmp(*s);

    while (!stop && (crs_it != crs_end || rts_it != rts_end)) {
        // Dummy rows are part of the in-memory representation but should be
        // invisible to reads.
        if (crs_it != crs_end && crs_it->dummy()) {
            ++crs_it;
            continue;
        }
        bool emit_rt = rts_it != rts_end;
        if (rts_it != rts_end) {
            if (reverse == consume_in_reverse::yes && !cookie.reversed_range_tombstone) {
                rts_it->tombstone().reverse();
                cookie.reversed_range_tombstone = true;
            }
            if (crs_it != crs_end) {
                const auto cmp_res = cmp(rts_it->position(), crs_it->position());
                emit_rt = cmp_res < 0;
            }
        }
        if (emit_rt) {
            flush_tombstones(rts_it->position());
            cookie.iterators->rt_gen.consume(std::move(rts_it->tombstone()));
            ++rts_it;
            cookie.reversed_range_tombstone = false;
        } else {
            flush_tombstones(crs_it->position());
            stop = consumer.consume(clustering_row(std::move(*crs_it)));
            ++crs_it;
        }
        if (preempt && need_preempt()) {
            break;
        }
    }

    if constexpr (crs_in_reverse) {
        cookie.iterators->crs_begin = crs_end.base();
        cookie.iterators->crs_end = crs_it.base();
    } else {
        cookie.iterators->crs_begin = crs_it;
        cookie.iterators->crs_end = crs_end;
    }

    if constexpr (rts_in_reverse) {
        cookie.iterators->rts_begin = rts_end.base();
        cookie.iterators->rts_end = rts_it.base();
    } else {
        cookie.iterators->rts_begin = rts_it;
        cookie.iterators->rts_end = rts_end;
    }

    if (!stop) {
      if (crs_it == crs_end && rts_it == rts_end) {
        flush_tombstones(position_in_partition::after_all_clustered_rows());
      } else {
        assert(preempt && need_preempt());
        return std::nullopt;
      }
    }

    return stop;
}

} // anonymous namespace

template<FlattenedConsumerV2 Consumer>
auto mutation::consume(Consumer& consumer, consume_in_reverse reverse, mutation_consume_cookie cookie) &&
        -> mutation_consume_result<decltype(consumer.consume_end_of_stream())> {
    auto& partition = _ptr->_p;

    if (!cookie.partition_start_consumed) {
        consumer.consume_new_partition(_ptr->_dk);
        if (partition.partition_tombstone()) {
            consumer.consume(partition.partition_tombstone());
        }
        cookie.partition_start_consumed = true;
    }

    stop_iteration stop = stop_iteration::no;
    if (!cookie.static_row_consumed && !partition.static_row().empty()) {
        stop = consumer.consume(static_row(std::move(partition.static_row().get_existing())));
    }

    cookie.static_row_consumed = true;

    if (reverse == consume_in_reverse::yes) {
        stop = *consume_clustering_fragments<consume_in_reverse::yes>(_ptr->_schema, partition, consumer, cookie);
    } else {
        stop = *consume_clustering_fragments<consume_in_reverse::no>(_ptr->_schema, partition, consumer, cookie);
    }

    const auto stop_consuming = consumer.consume_end_of_partition();
    using consume_res_type = decltype(consumer.consume_end_of_stream());
    if constexpr (std::is_same_v<consume_res_type, void>) {
        consumer.consume_end_of_stream();
        return mutation_consume_result<void>{stop_consuming, std::move(cookie)};
    } else {
        return mutation_consume_result<consume_res_type>{stop_consuming, consumer.consume_end_of_stream(), std::move(cookie)};
    }
}

template<FlattenedConsumerV2 Consumer>
auto mutation::consume_gently(Consumer& consumer, consume_in_reverse reverse, mutation_consume_cookie cookie) &&
        -> future<mutation_consume_result<decltype(consumer.consume_end_of_stream())>> {
    auto& partition = _ptr->_p;

    if (!cookie.partition_start_consumed) {
        consumer.consume_new_partition(_ptr->_dk);
        if (partition.partition_tombstone()) {
            consumer.consume(partition.partition_tombstone());
        }
        cookie.partition_start_consumed = true;
    }

    stop_iteration stop = stop_iteration::no;
    if (!cookie.static_row_consumed && !partition.static_row().empty()) {
        stop = consumer.consume(static_row(std::move(partition.static_row().get_existing())));
    }

    cookie.static_row_consumed = true;

    std::optional<stop_iteration> stop_opt;
    if (reverse == consume_in_reverse::yes) {
        while (!(stop_opt = consume_clustering_fragments<consume_in_reverse::yes>(_ptr->_schema, partition, consumer, cookie, is_preemptible::yes))) {
            co_await yield();
        }
    } else {
        while (!(stop_opt = consume_clustering_fragments<consume_in_reverse::no>(_ptr->_schema, partition, consumer, cookie, is_preemptible::yes))) {
            co_await yield();
        }
    }
    stop = *stop_opt;

    const auto stop_consuming = consumer.consume_end_of_partition();
    using consume_res_type = decltype(consumer.consume_end_of_stream());
    if constexpr (std::is_same_v<consume_res_type, void>) {
        consumer.consume_end_of_stream();
        co_return mutation_consume_result<void>{stop_consuming, std::move(cookie)};
    } else {
        co_return mutation_consume_result<consume_res_type>{stop_consuming, consumer.consume_end_of_stream(), std::move(cookie)};
    }
}

struct mutation_equals_by_key {
    bool operator()(const mutation& m1, const mutation& m2) const {
        return m1.schema() == m2.schema()
                && m1.decorated_key().equal(*m1.schema(), m2.decorated_key());
    }
};

struct mutation_hash_by_key {
    size_t operator()(const mutation& m) const {
        auto dk_hash = std::hash<dht::decorated_key>();
        return dk_hash(m.decorated_key());
    }
};

struct mutation_decorated_key_less_comparator {
    bool operator()(const mutation& m1, const mutation& m2) const;
};

using mutation_opt = optimized_optional<mutation>;

// Consistent with operator==()
// Consistent across the cluster, so should not rely on particular
// serialization format, only on actual data stored.
template<>
struct appending_hash<mutation> {
    template<typename Hasher>
    void operator()(Hasher& h, const mutation& m) const {
        const schema& s = *m.schema();
        feed_hash(h, m.key(), s);
        m.partition().feed_hash(h, s);
    }
};

inline
void apply(mutation_opt& dst, mutation&& src) {
    if (!dst) {
        dst = std::move(src);
    } else {
        dst->apply(std::move(src));
    }
}

inline
void apply(mutation_opt& dst, mutation_opt&& src) {
    if (src) {
        apply(dst, std::move(*src));
    }
}

inline
void apply(mutation& dst, mutation_opt&& src) {
    if (src) {
        dst.apply(std::move(*src));
    }
}

inline
void apply(mutation& dst, const mutation_opt& src) {
    if (src) {
        dst.apply(*src);
    }
}

// Returns a range into partitions containing mutations covered by the range.
// partitions must be sorted according to decorated key.
// range must not wrap around.
boost::iterator_range<std::vector<mutation>::const_iterator> slice(
    const std::vector<mutation>& partitions,
    const dht::partition_range&);

// Reverses the mutation as if it was created with a schema with reverse
// clustering order. The resulting mutation will contain a reverse schema too.
mutation reverse(mutation mut);


#include <seastar/core/shared_ptr.hh>

namespace service::broadcast_tables {
    class update_query;
}

namespace cql3 {

/**
 * Static helper methods and classes for constants.
 */
class constants {
public:
#if 0
    private static final Logger logger = LoggerFactory.getLogger(Constants.class);
#endif
public:
    class setter : public operation_skip_if_unset {
    public:
        using operation_skip_if_unset::operation_skip_if_unset;

        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override {
            auto value = expr::evaluate(*_e, params._options);
            execute(m, prefix, params, column, value.view());
        }

        static void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params, const column_definition& column, cql3::raw_value_view value) {
            if (value.is_null()) {
                m.set_cell(prefix, column, params.make_dead_cell());
            } else if (value.is_value()) {
                m.set_cell(prefix, column, params.make_cell(*column.type, value));
            }
        }

        virtual void prepare_for_broadcast_tables(statements::broadcast_tables::prepared_update& query) const override;
    };

    struct adder final : operation_skip_if_unset {
        using operation_skip_if_unset::operation_skip_if_unset;

        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override {
            auto value = expr::evaluate(*_e, params._options);
            if (value.is_null()) {
                throw exceptions::invalid_request_exception("Invalid null value for counter increment");
            }
            auto increment = value.view().deserialize<int64_t>(*long_type);
            m.set_cell(prefix, column, params.make_counter_update_cell(increment));
        }
    };

    struct subtracter final : operation_skip_if_unset {
        using operation_skip_if_unset::operation_skip_if_unset;

        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override {
            auto value = expr::evaluate(*_e, params._options);
            if (value.is_null()) {
                throw exceptions::invalid_request_exception("Invalid null value for counter increment");
            }
            auto increment = value.view().deserialize<int64_t>(*long_type);
            if (increment == std::numeric_limits<int64_t>::min()) {
                throw exceptions::invalid_request_exception(format("The negation of {:d} overflows supported counter precision (signed 8 bytes integer)", increment));
            }
            m.set_cell(prefix, column, params.make_counter_update_cell(-increment));
        }
    };

    class deleter : public operation_no_unset_support {
    public:
        deleter(const column_definition& column)
            : operation_no_unset_support(column, std::nullopt)
        { }

        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override;
    };
};

}



namespace cql3 {

/**
 * Static helper methods and classes for maps.
 */
class maps {
private:
    maps() = delete;
public:
    static lw_shared_ptr<column_specification> key_spec_of(const column_specification& column);
    static lw_shared_ptr<column_specification> value_spec_of(const column_specification& column);

    class setter : public operation_skip_if_unset {
    public:
        setter(const column_definition& column, expr::expression e)
                : operation_skip_if_unset(column, std::move(e)) {
        }

        virtual void execute(mutation& m, const clustering_key_prefix& row_key, const update_parameters& params) override;
        static void execute(mutation& m, const clustering_key_prefix& row_key, const update_parameters& params, const column_definition& column, const cql3::raw_value& value);
    };

    class setter_by_key : public operation_skip_if_unset {
        expr::expression _k;
    public:
        setter_by_key(const column_definition& column, expr::expression k, expr::expression e)
            : operation_skip_if_unset(column, std::move(e)), _k(std::move(k)) {
        }
        virtual void fill_prepare_context(prepare_context& ctx) override;
        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override;
    };

    class putter : public operation_skip_if_unset {
    public:
        putter(const column_definition& column, expr::expression e)
            : operation_skip_if_unset(column, std::move(e)) {
        }
        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override;
    };

    static void do_put(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params,
            const cql3::raw_value& value, const column_definition& column);

    class discarder_by_key : public operation_no_unset_support {
    public:
        discarder_by_key(const column_definition& column, expr::expression k)
                : operation_no_unset_support(column, std::move(k)) {
        }
        virtual void execute(mutation& m, const clustering_key_prefix& prefix, const update_parameters& params) override;
    };
};

}



#include <unordered_set>

namespace cql3 {

/**
 * Static helper methods and classes for sets.
 */
class sets {
    sets() = delete;
public:
    static lw_shared_ptr<column_specification> value_spec_of(const column_specification& column);

    class setter : public operation_skip_if_unset {
    public:
        setter(const column_definition& column, expr::expression e)
                : operation_skip_if_unset(column, std::move(e)) {
        }
        virtual void execute(mutation& m, const clustering_key_prefix& row_key, const update_parameters& params) override;
        static void execute(mutation& m, const clustering_key_prefix& row_key, const update_parameters& params, const column_definition& column, const cql3::raw_value& value);
    };

    class adder : public operation_skip_if_unset {
    public:
        adder(const column_definition& column, expr::expression e)
            : operation_skip_if_unset(column, std::move(e)) {
        }
        virtual void execute(mutation& m, const clustering_key_prefix& row_key, const update_parameters& params) override;
        static void do_add(mutation& m, const clustering_key_prefix& row_key, const update_parameters& params,
                const cql3::raw_value& value, const column_definition& column);
    };

    // Note that this is reused for Map subtraction too (we subtract a set from a map)
    class discarder : public operation_skip_if_unset {
    public:
        discarder(const column_definition& column, expr::expression e)
            : operation_skip_if_unset(column, std::move(e)) {
        }
        virtual void execute(mutation& m, const clustering_key_prefix& row_key, const update_parameters& params) override;
    };

    class element_discarder : public operation_no_unset_support {
    public:
        element_discarder(const column_definition& column, expr::expression e)
            : operation_no_unset_support(column, std::move(e)) { }
        virtual void execute(mutation& m, const clustering_key_prefix& row_key, const update_parameters& params) override;
    };
};

}

#include <seastar/core/sstring.hh>
#include <antlr3.hpp>

namespace cql3 {

/**
 * Listener used to collect the syntax errors emitted by the Lexer and Parser.
 */
template<typename RecognizerType, typename ExceptionBaseType>
class error_listener {
public:
    virtual ~error_listener() = default;

    /**
     * Invoked when a syntax error occurs.
     *
     * @param recognizer the parser or lexer that emitted the error
     * @param tokenNames the token names
     * @param e the exception
     */
    virtual void syntax_error(RecognizerType& recognizer, ANTLR_UINT8** token_names, ExceptionBaseType* ex) = 0;

    /**
     * Invoked when a syntax error with a specified message occurs.
     *
     * @param recognizer the parser or lexer that emitted the error
     * @param errorMsg the error message
     */
    virtual void syntax_error(RecognizerType& recognizer, const sstring& error_msg) = 0;
};

}


namespace cql3 {

/**
 * <code>ErrorListener</code> that collect and enhance the errors send by the CQL lexer and parser.
 */
template<typename RecognizerType, typename TokenType, typename ExceptionBaseType>
class error_collector : public error_listener<RecognizerType, ExceptionBaseType> {
    /**
     * The offset of the first token of the snippet.
     */
    static constexpr int32_t FIRST_TOKEN_OFFSET = 10;

    /**
     * The offset of the last token of the snippet.
     */
    static constexpr int32_t LAST_TOKEN_OFFSET = 2;

    /**
     * The CQL query.
     */
    const sstring_view _query;

    /**
     * An empty bitset to be used as a workaround for AntLR null dereference
     * bug.
     */
    static typename ExceptionBaseType::BitsetListType _empty_bit_list;

public:

    /**
     * Creates a new <code>ErrorCollector</code> instance to collect the syntax errors associated to the specified CQL
     * query.
     *
     * @param query the CQL query that will be parsed
     */
    error_collector(const sstring_view& query) : _query(query) {}

    /**
     * Format and throw a new \c exceptions::syntax_exception.
     */
    [[noreturn]] virtual void syntax_error(RecognizerType& recognizer, ANTLR_UINT8** token_names, ExceptionBaseType* ex) override {
        auto hdr = get_error_header(ex);
        auto msg = get_error_message(recognizer, ex, token_names);
        std::stringstream result;
        result << hdr << ' ' << msg;
#if 0
        if (recognizer instanceof Parser)
            appendQuerySnippet((Parser) recognizer, builder);
#endif

        throw exceptions::syntax_exception(result.str());
    }

    /**
     * Throw a new \c exceptions::syntax_exception.
     */
    [[noreturn]] virtual void syntax_error(RecognizerType&, const sstring& msg) override {
        throw exceptions::syntax_exception(msg);
    }

private:
    std::string get_error_header(ExceptionBaseType* ex) {
        std::stringstream result;
        result << "line " << ex->get_line() << ":" << ex->get_charPositionInLine();
        return result.str();
    }

    std::string get_error_message(RecognizerType& recognizer, ExceptionBaseType* ex, ANTLR_UINT8** token_names)
    {
        using namespace antlr3;
        std::stringstream msg;
        // Antlr3 has a function ex->displayRecognitionError() which is
        // supposed to nicely print the recognition exception. Unfortunately
        // it is buggy - see https://github.com/antlr/antlr3/issues/191
        // and not being fixed, so let's copy it here and fix it here.
        switch (ex->getType()) {
        case ExceptionType::RECOGNITION_EXCEPTION:
        case ExceptionType::EARLY_EXIT_EXCEPTION:
        default:
            // Unknown syntax error - the parser can't figure out what
            // specific token is missing or unwanted.
            msg << ": Syntax error";
            break;
        case ExceptionType::MISSING_TOKEN_EXCEPTION:
            msg << ": Missing ";
            if (recognizer.is_eof_token(ex->get_expecting())) {
                msg << "EOF";
            } else if (token_names) {
                msg << reinterpret_cast<const char*>(token_names[ex->get_expecting()]);
            } else {
                msg << ex->get_expecting();
            }
            break;
        case ExceptionType::UNWANTED_TOKEN_EXCEPTION:
        case ExceptionType::MISMATCHED_SET_EXCEPTION:
            msg << ": Unexpected '";
            msg << recognizer.token_text(ex->get_token());
            msg << "'";
            break;
        case ExceptionType::NO_VIABLE_ALT_EXCEPTION:
            msg << "no viable alternative at input '";
            msg << recognizer.token_text(ex->get_token());
            msg << "'";
            break;
        }
        return msg.str();
    }

#if 0

    /**
     * Appends a query snippet to the message to help the user to understand the problem.
     *
     * @param parser the parser used to parse the query
     * @param builder the <code>StringBuilder</code> used to build the error message
     */
    private void appendQuerySnippet(Parser parser, StringBuilder builder)
    {
        TokenStream tokenStream = parser.getTokenStream();
        int index = tokenStream.index();
        int size = tokenStream.size();

        Token from = tokenStream.get(getSnippetFirstTokenIndex(index));
        Token to = tokenStream.get(getSnippetLastTokenIndex(index, size));
        Token offending = tokenStream.get(getOffendingTokenIndex(index, size));

        appendSnippet(builder, from, to, offending);
    }

    /**
     * Appends a query snippet to the message to help the user to understand the problem.
     *
     * @param from the first token to include within the snippet
     * @param to the last token to include within the snippet
     * @param offending the token which is responsible for the error
     */
    final void appendSnippet(StringBuilder builder,
                             Token from,
                             Token to,
                             Token offending)
    {
        if (!areTokensValid(from, to, offending))
            return;

        String[] lines = query.split("\n");

        boolean includeQueryStart = (from.getLine() == 1) && (from.getCharPositionInLine() == 0);
        boolean includeQueryEnd = (to.getLine() == lines.length)
                && (getLastCharPositionInLine(to) == lines[lines.length - 1].length());

        builder.append(" (");

        if (!includeQueryStart)
            builder.append("...");

        String toLine = lines[lineIndex(to)];
        int toEnd = getLastCharPositionInLine(to);
        lines[lineIndex(to)] = toEnd >= toLine.length() ? toLine : toLine.substring(0, toEnd);
        lines[lineIndex(offending)] = highlightToken(lines[lineIndex(offending)], offending);
        lines[lineIndex(from)] = lines[lineIndex(from)].substring(from.getCharPositionInLine());

        for (int i = lineIndex(from), m = lineIndex(to); i <= m; i++)
            builder.append(lines[i]);

        if (!includeQueryEnd)
            builder.append("...");

        builder.append(")");
    }

    /**
     * Checks if the specified tokens are valid.
     *
     * @param tokens the tokens to check
     * @return <code>true</code> if all the specified tokens are valid ones,
     * <code>false</code> otherwise.
     */
    private static boolean areTokensValid(Token... tokens)
    {
        for (Token token : tokens)
        {
            if (!isTokenValid(token))
                return false;
        }
        return true;
    }

    /**
     * Checks that the specified token is valid.
     *
     * @param token the token to check
     * @return <code>true</code> if it is considered as valid, <code>false</code> otherwise.
     */
    private static boolean isTokenValid(Token token)
    {
        return token.getLine() > 0 && token.getCharPositionInLine() >= 0;
    }

    /**
     * Returns the index of the offending token. <p>In the case where the offending token is an extra
     * character at the end, the index returned by the <code>TokenStream</code> might be after the last token.
     * To avoid that problem we need to make sure that the index of the offending token is a valid index 
     * (one for which a token exist).</p>
     *
     * @param index the token index returned by the <code>TokenStream</code>
     * @param size the <code>TokenStream</code> size
     * @return the valid index of the offending token
     */
    private static int getOffendingTokenIndex(int index, int size)
    {
        return Math.min(index, size - 1);
    }

    /**
     * Puts the specified token within square brackets.
     *
     * @param line the line containing the token
     * @param token the token to put within square brackets
     */
    private static String highlightToken(String line, Token token)
    {
        String newLine = insertChar(line, getLastCharPositionInLine(token), ']');
        return insertChar(newLine, token.getCharPositionInLine(), '[');
    }

    /**
     * Returns the index of the last character relative to the beginning of the line 0..n-1
     *
     * @param token the token
     * @return the index of the last character relative to the beginning of the line 0..n-1
     */
    private static int getLastCharPositionInLine(Token token)
    {
        return token.getCharPositionInLine() + getLength(token);
    }

    /**
     * Return the token length.
     *
     * @param token the token
     * @return the token length
     */
    private static int getLength(Token token)
    {
        return token.getText().length();
    }

    /**
     * Inserts a character at a given position within a <code>String</code>.
     *
     * @param s the <code>String</code> in which the character must be inserted
     * @param index the position where the character must be inserted
     * @param c the character to insert
     * @return the modified <code>String</code>
     */
    private static String insertChar(String s, int index, char c)
    {
        return new StringBuilder().append(s.substring(0, index))
                .append(c)
                .append(s.substring(index))
                .toString();
    }

    /**
     * Returns the index of the line number on which this token was matched; index=0..n-1
     *
     * @param token the token
     * @return the index of the line number on which this token was matched; index=0..n-1
     */
    private static int lineIndex(Token token)
    {
        return token.getLine() - 1;
    }

    /**
     * Returns the index of the last token which is part of the snippet.
     *
     * @param index the index of the token causing the error
     * @param size the total number of tokens
     * @return the index of the last token which is part of the snippet.
     */
    private static int getSnippetLastTokenIndex(int index, int size)
    {
        return Math.min(size - 1, index + LAST_TOKEN_OFFSET);
    }

    /**
     * Returns the index of the first token which is part of the snippet.
     *
     * @param index the index of the token causing the error
     * @return the index of the first token which is part of the snippet.
     */
    private static int getSnippetFirstTokenIndex(int index)
    {
        return Math.max(0, index - FIRST_TOKEN_OFFSET);
    }
#endif
};

template<typename RecognizerType, typename TokenType, typename ExceptionBaseType>
typename ExceptionBaseType::BitsetListType
error_collector<RecognizerType,TokenType,ExceptionBaseType>::_empty_bit_list = typename ExceptionBaseType::BitsetListType();

}




#include <seastar/core/sstring.hh>

#include <optional>

namespace cql3 {

/**
 * Base class for the names of the keyspace elements (e.g. table, index ...)
 */
class keyspace_element_name {
    /**
     * The keyspace name as stored internally.
     */
    std::optional<sstring> _ks_name = std::nullopt;

public:
    /**
     * Sets the keyspace.
     *
     * @param ks the keyspace name
     * @param keepCase <code>true</code> if the case must be kept, <code>false</code> otherwise.
     */
    void set_keyspace(std::string_view ks, bool keep_case);

    /**
     * Checks if the keyspace is specified.
     * @return <code>true</code> if the keyspace is specified, <code>false</code> otherwise.
     */
    bool has_keyspace() const;

    const sstring& get_keyspace() const;

    virtual sstring to_string() const;

protected:
    /**
     * Converts the specified name into the name used internally.
     *
     * @param name the name
     * @param keepCase <code>true</code> if the case must be kept, <code>false</code> otherwise.
     * @return the name used internally.
     */
    static sstring to_internal_name(std::string_view name, bool keep_case);
};

}


namespace cql3 {

class cf_name final : public keyspace_element_name {
    sstring _cf_name = "";
public:
    void set_column_family(const sstring& cf, bool keep_case);

    const sstring& get_column_family() const;

    virtual sstring to_string() const override;
};

inline
std::ostream&
operator<<(std::ostream& os, const cf_name& n) {
    os << n.to_string();
    return os;
}

}


#include <seastar/core/shared_ptr.hh>

#include <optional>
#include <vector>
#include <stddef.h>

class schema;

namespace cql3 {

class column_identifier;
class column_specification;
namespace functions { class function_call; }

/**
 * Metadata class currently holding bind variables specifications and
 * `function_call` AST nodes inside a query partition key restrictions.
 * Populated and maintained at "prepare" step of query execution.
 */
class prepare_context final {
private:
    std::vector<shared_ptr<column_identifier>> _variable_names;
    std::vector<lw_shared_ptr<column_specification>> _specs;
    std::vector<lw_shared_ptr<column_specification>> _target_columns;
    // A list of pointers to prepared `function_call` cache ids, that
    // participate in partition key ranges computation within an LWT statement.
    std::vector<::shared_ptr<std::optional<uint8_t>>> _pk_function_calls_cache_ids;

    // The flag denoting whether the context is currently in partition key
    // processing mode (inside query restrictions AST nodes). If set to true,
    // then every `function_call` instance will be recorded in the context and
    // will be assigned an identifier, which will then be used for caching
    // the function call results.
    bool _processing_pk_restrictions = false;

public:

    prepare_context() = default;

    size_t bound_variables_size() const;

    const std::vector<lw_shared_ptr<column_specification>>& get_variable_specifications() const &;

    std::vector<lw_shared_ptr<column_specification>> get_variable_specifications() &&;

    std::vector<uint16_t> get_partition_key_bind_indexes(const schema& schema) const;

    void add_variable_specification(int32_t bind_index, lw_shared_ptr<column_specification> spec);

    void set_bound_variables(const std::vector<shared_ptr<column_identifier>>& prepare_meta);

    void clear_pk_function_calls_cache();

    // Record a new function call, which evaluates a partition key constraint.
    // Also automatically assigns an id to the AST node for caching purposes.
    void add_pk_function_call(cql3::expr::function_call& fn);

    // Inform the context object that it has started or ended processing the
    // partition key part of statement restrictions.
    void set_processing_pk_restrictions(bool flag) noexcept {
        _processing_pk_restrictions = flag;
    }

    bool is_processing_pk_restrictions() const noexcept {
        return _processing_pk_restrictions;
    }
};

}


#include <seastar/core/shared_ptr.hh>

#include <optional>
#include <vector>

namespace cql3 {

class column_identifier;
class cql_stats;

namespace statements {

class prepared_statement;

namespace raw {

class parsed_statement {
protected:
    prepare_context _prepare_ctx;

public:
    virtual ~parsed_statement();

    prepare_context& get_prepare_context();
    const prepare_context& get_prepare_context() const;

    void set_bound_variables(const std::vector<::shared_ptr<column_identifier>>& bound_names);

    virtual std::unique_ptr<prepared_statement> prepare(data_dictionary::database db, cql_stats& stats) = 0;
};

}

}

}



#include <optional>


namespace service { class client_state; }

namespace cql3 {

namespace statements {

namespace raw {

/**
 * Abstract class for statements that apply on a given column family.
 */
class cf_statement : public parsed_statement {
protected:
    std::optional<cf_name> _cf_name;

    cf_statement(std::optional<cf_name> cf_name);
public:
    virtual void prepare_keyspace(const service::client_state& state);

    // Only for internal calls, use the version with ClientState for user queries
    void prepare_keyspace(std::string_view keyspace);

    virtual const sstring& keyspace() const;

    virtual const sstring& column_family() const;
};

}

}

}

#include <seastar/core/shared_ptr.hh>
#include <seastar/core/weak_ptr.hh>
#include <seastar/core/checked_ptr.hh>
#include <optional>
#include <vector>


namespace cql3 {

class prepare_context;
class column_specification;
class cql_statement;

namespace statements {

struct invalidated_prepared_usage_attempt {
    void operator()() const {
        throw exceptions::invalidated_prepared_usage_attempt_exception();
    }
};

class prepared_statement : public seastar::weakly_referencable<prepared_statement> {
public:
    typedef seastar::checked_ptr<seastar::weak_ptr<prepared_statement>> checked_weak_ptr;

public:
    const seastar::shared_ptr<cql_statement> statement;
    const std::vector<seastar::lw_shared_ptr<column_specification>> bound_names;
    std::vector<uint16_t> partition_key_bind_indices;
    std::vector<sstring> warnings;

    prepared_statement(seastar::shared_ptr<cql_statement> statement_, std::vector<seastar::lw_shared_ptr<column_specification>> bound_names_,
                       std::vector<uint16_t> partition_key_bind_indices, std::vector<sstring> warnings = {});

    prepared_statement(seastar::shared_ptr<cql_statement> statement_, const prepare_context& ctx, const std::vector<uint16_t>& partition_key_bind_indices,
                       std::vector<sstring> warnings = {});

    prepared_statement(seastar::shared_ptr<cql_statement> statement_, prepare_context&& ctx, std::vector<uint16_t>&& partition_key_bind_indices);

    prepared_statement(seastar::shared_ptr<cql_statement>&& statement_);

    checked_weak_ptr checked_weak_from_this() {
        return checked_weak_ptr(this->weak_from_this());
    }
};

}

}


namespace cql3 {

class query_options;
class prepare_context;

/**
 * Utility class for the Parser to gather attributes for modification
 * statements.
 */
class attributes final {
private:
    expr::unset_bind_variable_guard _timestamp_unset_guard;
    std::optional<cql3::expr::expression> _timestamp;
    expr::unset_bind_variable_guard _time_to_live_unset_guard;
    std::optional<cql3::expr::expression> _time_to_live;
    std::optional<cql3::expr::expression> _timeout;
public:
    static std::unique_ptr<attributes> none();
private:
    attributes(std::optional<cql3::expr::expression>&& timestamp,
               std::optional<cql3::expr::expression>&& time_to_live,
               std::optional<cql3::expr::expression>&& timeout);
public:
    bool is_timestamp_set() const;

    bool is_time_to_live_set() const;

    bool is_timeout_set() const;

    int64_t get_timestamp(int64_t now, const query_options& options);

    std::optional<int32_t> get_time_to_live(const query_options& options);

    db::timeout_clock::duration get_timeout(const query_options& options) const;

    void fill_prepare_context(prepare_context& ctx);

    class raw final {
    public:
        std::optional<cql3::expr::expression> timestamp;
        std::optional<cql3::expr::expression> time_to_live;
        std::optional<cql3::expr::expression> timeout;

        std::unique_ptr<attributes> prepare(data_dictionary::database db, const sstring& ks_name, const sstring& cf_name) const;
    private:
        lw_shared_ptr<column_specification> timestamp_receiver(const sstring& ks_name, const sstring& cf_name) const;

        lw_shared_ptr<column_specification> time_to_live_receiver(const sstring& ks_name, const sstring& cf_name) const;

        lw_shared_ptr<column_specification> timeout_receiver(const sstring& ks_name, const sstring& cf_name) const;
    };
};

}




#include <boost/program_options/errors.hpp>
#include <iostream>
#include <sstream>
#include <type_traits>

template<typename T>
concept HasMapInterface = requires(T t) {
    typename std::remove_reference<T>::type::mapped_type;
    typename std::remove_reference<T>::type::key_type;
    typename std::remove_reference<T>::type::value_type;
    t.find(typename std::remove_reference<T>::type::key_type());
    t.begin();
    t.end();
    t.cbegin();
    t.cend();
};

/// A Boost program option holding an enum value.
///
/// The options parser will parse enum values with the help of the Mapper class, which provides a mapping
/// between some parsable form (eg, string) and the enum value.  For example, it may map the word "January" to
/// the enum value JANUARY.
///
/// Mapper must have a static method `map()` that returns a map from a streamable key type (eg, string) to the
/// enum in question.  In fact, enum_option knows which enum it represents only by referencing
/// Mapper::map().mapped_type.
///
/// \note one enum_option holds only one enum value.  When multiple choices are allowed, use
/// vector<enum_option>.
///
/// Example:
///
/// struct Type {
///   enum class ty { a1, a2, b1 };
///   static unordered_map<string, ty> map();
/// };
/// unordered_map<string, Type::ty> Type::map() {
///   return {{"a1", Type::ty::a1}, {"a2", Type::ty::a2}, {"b1", Type::ty::b1}};
/// }
/// int main(int ac, char* av[]) {
///   namespace po = boost::program_options;
///   po::options_description desc("Allowed options");
///   desc.add_options()
///     ("val", po::value<enum_option<Type>>(), "Single Type")
///     ("vec", po::value<vector<enum_option<Type>>>()->multitoken(), "Type vector");
/// }
template<typename Mapper>
requires HasMapInterface<decltype(Mapper::map())>
class enum_option {
    using map_t = typename std::remove_reference<decltype(Mapper::map())>::type;
    typename map_t::mapped_type _value;
    map_t _map;
  public:
    // For smooth conversion from enum values:
    enum_option(const typename map_t::mapped_type& v) : _value(v), _map(Mapper::map()) {}

    // So values can be default-constructed before streaming into them:
    enum_option() : _map(Mapper::map()) {}

    bool operator==(const enum_option<Mapper>& that) const {
        return _value == that._value;
    }

    // For comparison with enum values using if or switch:
    bool operator==(typename map_t::mapped_type value) const {
        return _value == value;
    }
    operator typename map_t::mapped_type() const {
        return _value;
    }

    // For program_options parser:
    friend std::istream& operator>>(std::istream& s, enum_option<Mapper>& opt) {
        typename map_t::key_type key;
        s >> key;
        const auto found = opt._map.find(key);
        if (found == opt._map.end()) {
            std::string text;
            if (s.rdstate() & s.failbit) {
                // key wasn't read successfully.
                s >> text;
            } else {
                // Turn key into text.
                std::ostringstream temp;
                temp << key;
                text = temp.str();
            }
            throw boost::program_options::invalid_option_value(text);
        }
        opt._value = found->second;
        return s;
    }

    // For various printers and formatters:
    friend std::ostream& operator<<(std::ostream& s, const enum_option<Mapper>& opt) {
        auto found = find_if(opt._map.cbegin(), opt._map.cend(),
                             [&opt](const typename map_t::value_type& e) { return e.second == opt._value; });
        if (found == opt._map.cend()) {
            return s << "?unknown";
        } else {
            return s << found->first;
        }
    }
};

#include <unordered_set>
#include <stdexcept>
#include <iosfwd>
#include <string_view>

#include <seastar/core/sstring.hh>

namespace gms {
    class inet_address;
} // namespace gms

namespace locator { class topology; }

namespace db {
namespace hints {

// host_filter tells hints_manager towards which endpoints it is allowed to generate hints.
class host_filter final {
private:
    enum class enabled_kind {
        enabled_for_all,
        enabled_selectively,
        disabled_for_all,
    };

    enabled_kind _enabled_kind;
    std::unordered_set<sstring> _dcs;

    static std::string_view enabled_kind_to_string(host_filter::enabled_kind ek);

public:
    struct enabled_for_all_tag {};
    struct disabled_for_all_tag {};

    // Creates a filter that allows hints to all endpoints (default)
    host_filter(enabled_for_all_tag tag = {});

    // Creates a filter that does not allow any hints.
    host_filter(disabled_for_all_tag);

    // Creates a filter that allows sending hints to specified DCs.
    explicit host_filter(std::unordered_set<sstring> allowed_dcs);

    // Parses hint filtering configuration from the hinted_handoff_enabled option.
    static host_filter parse_from_config_string(sstring opt);

    // Parses hint filtering configuration from a list of DCs.
    static host_filter parse_from_dc_list(sstring opt);

    bool can_hint_for(const locator::topology& topo, gms::inet_address ep) const;

    inline const std::unordered_set<sstring>& get_dcs() const {
        return _dcs;
    }

    bool operator==(const host_filter& other) const noexcept {
        return _enabled_kind == other._enabled_kind
                && _dcs == other._dcs;
    }

    inline bool is_enabled_for_all() const noexcept {
        return _enabled_kind == enabled_kind::enabled_for_all;
    }

    inline bool is_disabled_for_all() const noexcept {
        return _enabled_kind == enabled_kind::disabled_for_all;
    }

    sstring to_configuration_string() const;

    friend std::ostream& operator<<(std::ostream& os, const host_filter& f);
};

std::istream& operator>>(std::istream& is, host_filter& f);

class hints_configuration_parse_error : public std::runtime_error {
public:
    using std::runtime_error::runtime_error;
};

}
}
#include <optional>
#include <seastar/core/shared_ptr.hh>

namespace s3 {

struct endpoint_config {
    unsigned port;
    bool use_https;

    struct aws_config {
        std::string key;
        std::string secret;
        std::string region;
    };

    std::optional<aws_config> aws;
};

using endpoint_config_ptr = seastar::lw_shared_ptr<endpoint_config>;

} // s3 namespace



#include <boost/program_options.hpp>
#include <unordered_map>

#include <seastar/core/sstring.hh>
#include <seastar/core/shared_ptr.hh>
#include <seastar/util/program-options.hh>
#include <seastar/util/log.hh>


namespace seastar {
class file;
struct logging_settings;
namespace tls {
class credentials_builder;
}
namespace log_cli {
class options;
}
}

namespace db {

namespace fs = std::filesystem;

class extensions;

/*
 * This type is not use, and probably never will be.
 * So it makes sense to jump through hoops just to ensure
 * it is in fact handled properly...
 */
struct seed_provider_type {
    seed_provider_type() = default;
    seed_provider_type(sstring n,
            std::initializer_list<program_options::string_map::value_type> opts =
                    { })
            : class_name(std::move(n)), parameters(std::move(opts)) {
    }
    sstring class_name;
    std::unordered_map<sstring, sstring> parameters;
    bool operator==(const seed_provider_type& other) const {
        return class_name == other.class_name && parameters == other.parameters;
    }
    friend std::ostream& operator<<(std::ostream& os, const seed_provider_type&);
};

std::ostream& operator<<(std::ostream& os, const db::seed_provider_type& s);
inline std::istream& operator>>(std::istream& is, seed_provider_type&);

// Describes a single error injection that should be enabled at startup.
struct error_injection_at_startup {
    sstring name;
    bool one_shot = false;

    bool operator==(const error_injection_at_startup& other) const {
        return name == other.name
            && one_shot == other.one_shot;
    }
};

std::ostream& operator<<(std::ostream& os, const error_injection_at_startup&);
std::istream& operator>>(std::istream& is, error_injection_at_startup&);

}


namespace utils {

sstring config_value_as_json(const db::seed_provider_type& v);

sstring config_value_as_json(const log_level& v);

sstring config_value_as_json(const std::unordered_map<sstring, log_level>& v);

}

namespace db {

/// Enumeration of all valid values for the `experimental` config entry.
struct experimental_features_t {
    // NOTE: RAFT and BROADCAST_TABLES features are not enabled via `experimental` umbrella flag.
    // These options should be enabled explicitly.
    // RAFT feature has to be enabled if BROADCAST_TABLES or TABLETS is enabled.
    enum class feature {
        UNUSED,
        UDF,
        ALTERNATOR_STREAMS,
        RAFT,
        BROADCAST_TABLES,
        KEYSPACE_STORAGE_OPTIONS,
        TABLETS,
    };
    static std::map<sstring, feature> map(); // See enum_option.
    static std::vector<enum_option<experimental_features_t>> all();
};

/// A restriction that can be in three modes: true (the operation is disabled),
/// false (the operation is allowed), or warn (the operation is allowed but
/// produces a warning in the log).
struct tri_mode_restriction_t {
    enum class mode { FALSE, TRUE, WARN };
    static std::unordered_map<sstring, mode> map(); // for enum_option<>
};
using tri_mode_restriction = enum_option<tri_mode_restriction_t>;

constexpr unsigned default_murmur3_partitioner_ignore_msb_bits = 12;

class config : public utils::config_file {
public:
    config();
    config(std::shared_ptr<db::extensions>);
    ~config();

    // For testing only
    void add_cdc_extension();
    void add_per_partition_rate_limit_extension();

    /// True iff the feature is enabled.
    bool check_experimental(experimental_features_t::feature f) const;

    void setup_directories();

    /**
     * Scans the environment variables for configuration files directory
     * definition. It's either $SCYLLA_CONF, $SCYLLA_HOME/conf or "conf" if none
     * of SCYLLA_CONF and SCYLLA_HOME is defined.
     *
     * @return path of the directory where configuration files are located
     *         according the environment variables definitions.
     */
    static fs::path get_conf_dir();
    static fs::path get_conf_sub(fs::path);

    using string_map = std::unordered_map<sstring, sstring>;
                    //program_options::string_map;
    using string_list = std::vector<sstring>;
    using seed_provider_type = db::seed_provider_type;
    using hinted_handoff_enabled_type = db::hints::host_filter;
    using error_injection_at_startup = db::error_injection_at_startup;

    /*
     * All values and documentation taken from
     * http://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html
     */
    named_value<double> background_writer_scheduling_quota;
    named_value<bool> auto_adjust_flush_quota;
    named_value<float> memtable_flush_static_shares;
    named_value<float> compaction_static_shares;
    named_value<bool> compaction_enforce_min_threshold;
    named_value<sstring> cluster_name;
    named_value<sstring> listen_address;
    named_value<sstring> listen_interface;
    named_value<bool> listen_interface_prefer_ipv6;
    named_value<sstring> work_directory;
    named_value<sstring> commitlog_directory;
    named_value<sstring> schema_commitlog_directory;
    named_value<string_list> data_file_directories;
    named_value<sstring> hints_directory;
    named_value<sstring> view_hints_directory;
    named_value<sstring> saved_caches_directory;
    named_value<sstring> commit_failure_policy;
    named_value<sstring> disk_failure_policy;
    named_value<sstring> endpoint_snitch;
    named_value<sstring> rpc_address;
    named_value<sstring> rpc_interface;
    named_value<bool> rpc_interface_prefer_ipv6;
    named_value<seed_provider_type> seed_provider;
    named_value<uint32_t> compaction_throughput_mb_per_sec;
    named_value<uint32_t> compaction_large_partition_warning_threshold_mb;
    named_value<uint32_t> compaction_large_row_warning_threshold_mb;
    named_value<uint32_t> compaction_large_cell_warning_threshold_mb;
    named_value<uint32_t> compaction_rows_count_warning_threshold;
    named_value<uint32_t> compaction_collection_elements_count_warning_threshold;
    named_value<uint32_t> memtable_total_space_in_mb;
    named_value<uint32_t> concurrent_reads;
    named_value<uint32_t> concurrent_writes;
    named_value<uint32_t> concurrent_counter_writes;
    named_value<bool> incremental_backups;
    named_value<bool> snapshot_before_compaction;
    named_value<uint32_t> phi_convict_threshold;
    named_value<uint32_t> failure_detector_timeout_in_ms;
    named_value<sstring> commitlog_sync;
    named_value<uint32_t> commitlog_segment_size_in_mb;
    named_value<uint32_t> commitlog_sync_period_in_ms;
    named_value<uint32_t> commitlog_sync_batch_window_in_ms;
    named_value<int64_t> commitlog_total_space_in_mb;
    named_value<bool> commitlog_reuse_segments; // unused. retained for upgrade compat
    named_value<int64_t> commitlog_flush_threshold_in_mb;
    named_value<bool> commitlog_use_o_dsync;
    named_value<bool> commitlog_use_hard_size_limit;
    named_value<bool> compaction_preheat_key_cache;
    named_value<uint32_t> concurrent_compactors;
    named_value<uint32_t> in_memory_compaction_limit_in_mb;
    named_value<bool> preheat_kernel_page_cache;
    named_value<uint32_t> sstable_preemptive_open_interval_in_mb;
    named_value<bool> defragment_memory_on_idle;
    named_value<sstring> memtable_allocation_type;
    named_value<double> memtable_cleanup_threshold;
    named_value<uint32_t> file_cache_size_in_mb;
    named_value<uint32_t> memtable_flush_queue_size;
    named_value<uint32_t> memtable_flush_writers;
    named_value<uint32_t> memtable_heap_space_in_mb;
    named_value<uint32_t> memtable_offheap_space_in_mb;
    named_value<uint32_t> column_index_size_in_kb;
    named_value<uint32_t> column_index_auto_scale_threshold_in_kb;
    named_value<uint32_t> index_summary_capacity_in_mb;
    named_value<uint32_t> index_summary_resize_interval_in_minutes;
    named_value<double> reduce_cache_capacity_to;
    named_value<double> reduce_cache_sizes_at;
    named_value<uint32_t> stream_throughput_outbound_megabits_per_sec;
    named_value<uint32_t> inter_dc_stream_throughput_outbound_megabits_per_sec;
    named_value<uint32_t> stream_io_throughput_mb_per_sec;
    named_value<bool> trickle_fsync;
    named_value<uint32_t> trickle_fsync_interval_in_kb;
    named_value<bool> auto_bootstrap;
    named_value<uint32_t> batch_size_warn_threshold_in_kb;
    named_value<uint32_t> batch_size_fail_threshold_in_kb;
    named_value<sstring> broadcast_address;
    named_value<bool> listen_on_broadcast_address;
    named_value<sstring> initial_token;
    named_value<uint32_t> num_tokens;
    named_value<sstring> partitioner;
    named_value<uint16_t> storage_port;
    named_value<bool> auto_snapshot;
    named_value<uint32_t> key_cache_keys_to_save;
    named_value<uint32_t> key_cache_save_period;
    named_value<uint32_t> key_cache_size_in_mb;
    named_value<uint32_t> row_cache_keys_to_save;
    named_value<uint32_t> row_cache_size_in_mb;
    named_value<uint32_t> row_cache_save_period;
    named_value<sstring> memory_allocator;
    named_value<uint32_t> counter_cache_size_in_mb;
    named_value<uint32_t> counter_cache_save_period;
    named_value<uint32_t> counter_cache_keys_to_save;
    named_value<uint32_t> tombstone_warn_threshold;
    named_value<uint32_t> tombstone_failure_threshold;
    named_value<uint64_t> query_tombstone_page_limit;
    named_value<uint32_t> range_request_timeout_in_ms;
    named_value<uint32_t> read_request_timeout_in_ms;
    named_value<uint32_t> counter_write_request_timeout_in_ms;
    named_value<uint32_t> cas_contention_timeout_in_ms;
    named_value<uint32_t> truncate_request_timeout_in_ms;
    named_value<uint32_t> write_request_timeout_in_ms;
    named_value<uint32_t> request_timeout_in_ms;
    named_value<bool> cross_node_timeout;
    named_value<uint32_t> internode_send_buff_size_in_bytes;
    named_value<uint32_t> internode_recv_buff_size_in_bytes;
    named_value<sstring> internode_compression;
    named_value<bool> inter_dc_tcp_nodelay;
    named_value<uint32_t> streaming_socket_timeout_in_ms;
    named_value<bool> start_native_transport;
    named_value<uint16_t> native_transport_port;
    named_value<uint16_t> native_transport_port_ssl;
    named_value<uint16_t> native_shard_aware_transport_port;
    named_value<uint16_t> native_shard_aware_transport_port_ssl;
    named_value<uint32_t> native_transport_max_threads;
    named_value<uint32_t> native_transport_max_frame_size_in_mb;
    named_value<sstring> broadcast_rpc_address;
    named_value<uint16_t> rpc_port;
    named_value<bool> start_rpc;
    named_value<bool> rpc_keepalive;
    named_value<uint32_t> rpc_max_threads;
    named_value<uint32_t> rpc_min_threads;
    named_value<uint32_t> rpc_recv_buff_size_in_bytes;
    named_value<uint32_t> rpc_send_buff_size_in_bytes;
    named_value<sstring> rpc_server_type;
    named_value<bool> cache_hit_rate_read_balancing;
    named_value<double> dynamic_snitch_badness_threshold;
    named_value<uint32_t> dynamic_snitch_reset_interval_in_ms;
    named_value<uint32_t> dynamic_snitch_update_interval_in_ms;
    named_value<hinted_handoff_enabled_type> hinted_handoff_enabled;
    named_value<uint32_t> max_hinted_handoff_concurrency;
    named_value<uint32_t> hinted_handoff_throttle_in_kb;
    named_value<uint32_t> max_hint_window_in_ms;
    named_value<uint32_t> max_hints_delivery_threads;
    named_value<uint32_t> batchlog_replay_throttle_in_kb;
    named_value<sstring> request_scheduler;
    named_value<sstring> request_scheduler_id;
    named_value<string_map> request_scheduler_options;
    named_value<uint32_t> thrift_framed_transport_size_in_mb;
    named_value<uint32_t> thrift_max_message_length_in_mb;
    named_value<sstring> authenticator;
    named_value<sstring> internode_authenticator;
    named_value<sstring> authorizer;
    named_value<sstring> role_manager;
    named_value<uint32_t> permissions_validity_in_ms;
    named_value<uint32_t> permissions_update_interval_in_ms;
    named_value<uint32_t> permissions_cache_max_entries;
    named_value<string_map> server_encryption_options;
    named_value<string_map> client_encryption_options;
    named_value<string_map> alternator_encryption_options;
    named_value<uint32_t> ssl_storage_port;
    named_value<bool> enable_in_memory_data_store;
    named_value<bool> enable_cache;
    named_value<bool> enable_commitlog;
    named_value<bool> volatile_system_keyspace_for_testing;
    named_value<uint16_t> api_port;
    named_value<sstring> api_address;
    named_value<sstring> api_ui_dir;
    named_value<sstring> api_doc_dir;
    named_value<sstring> load_balance;
    named_value<bool> consistent_rangemovement;
    named_value<bool> join_ring;
    named_value<bool> load_ring_state;
    named_value<sstring> replace_node_first_boot;
    named_value<sstring> replace_address;
    named_value<sstring> replace_address_first_boot;
    named_value<sstring> ignore_dead_nodes_for_replace;
    named_value<bool> override_decommission;
    named_value<bool> enable_repair_based_node_ops;
    named_value<sstring> allowed_repair_based_node_ops;
    named_value<uint32_t> ring_delay_ms;
    named_value<uint32_t> shadow_round_ms;
    named_value<uint32_t> fd_max_interval_ms;
    named_value<uint32_t> fd_initial_value_ms;
    named_value<uint32_t> shutdown_announce_in_ms;
    named_value<bool> developer_mode;
    named_value<int32_t> skip_wait_for_gossip_to_settle;
    named_value<int32_t> force_gossip_generation;
    named_value<bool> experimental;
    named_value<std::vector<enum_option<experimental_features_t>>> experimental_features;
    named_value<size_t> lsa_reclamation_step;
    named_value<uint16_t> prometheus_port;
    named_value<sstring> prometheus_address;
    named_value<sstring> prometheus_prefix;
    named_value<bool> abort_on_lsa_bad_alloc;
    named_value<unsigned> murmur3_partitioner_ignore_msb_bits;
    named_value<double> unspooled_dirty_soft_limit;
    named_value<double> sstable_summary_ratio;
    named_value<size_t> large_memory_allocation_warning_threshold;
    named_value<bool> enable_deprecated_partitioners;
    named_value<bool> enable_keyspace_column_family_metrics;
    named_value<bool> enable_sstable_data_integrity_check;
    named_value<bool> enable_sstable_key_validation;
    named_value<bool> cpu_scheduler;
    named_value<bool> view_building;
    named_value<bool> enable_sstables_mc_format;
    named_value<bool> enable_sstables_md_format;
    named_value<sstring> sstable_format;
    named_value<bool> enable_dangerous_direct_import_of_cassandra_counters;
    named_value<bool> enable_shard_aware_drivers;
    named_value<bool> enable_ipv6_dns_lookup;
    named_value<bool> abort_on_internal_error;
    named_value<uint32_t> max_partition_key_restrictions_per_query;
    named_value<uint32_t> max_clustering_key_restrictions_per_query;
    named_value<uint64_t> max_memory_for_unlimited_query_soft_limit;
    named_value<uint64_t> max_memory_for_unlimited_query_hard_limit;
    named_value<uint32_t> reader_concurrency_semaphore_serialize_limit_multiplier;
    named_value<uint32_t> reader_concurrency_semaphore_kill_limit_multiplier;
    named_value<uint32_t> twcs_max_window_count;
    named_value<unsigned> initial_sstable_loading_concurrency;
    named_value<bool> enable_3_1_0_compatibility_mode;
    named_value<bool> enable_user_defined_functions;
    named_value<unsigned> user_defined_function_time_limit_ms;
    named_value<unsigned> user_defined_function_allocation_limit_bytes;
    named_value<unsigned> user_defined_function_contiguous_allocation_limit_bytes;
    named_value<uint32_t> schema_registry_grace_period;
    named_value<uint32_t> max_concurrent_requests_per_shard;
    named_value<bool> cdc_dont_rewrite_streams;
    named_value<tri_mode_restriction> strict_allow_filtering;
    named_value<bool> reversed_reads_auto_bypass_cache;
    named_value<bool> enable_optimized_reversed_reads;
    named_value<bool> enable_cql_config_updates;
    named_value<bool> enable_parallelized_aggregation;

    named_value<uint16_t> alternator_port;
    named_value<uint16_t> alternator_https_port;
    named_value<sstring> alternator_address;
    named_value<bool> alternator_enforce_authorization;
    named_value<sstring> alternator_write_isolation;
    named_value<uint32_t> alternator_streams_time_window_s;
    named_value<uint32_t> alternator_timeout_in_ms;
    named_value<double> alternator_ttl_period_in_seconds;

    named_value<bool> abort_on_ebadf;

    named_value<uint16_t> redis_port;
    named_value<uint16_t> redis_ssl_port;
    named_value<sstring> redis_read_consistency_level;
    named_value<sstring> redis_write_consistency_level;
    named_value<uint16_t> redis_database_count;
    named_value<string_map> redis_keyspace_replication_strategy_options;

    named_value<bool> sanitizer_report_backtrace;
    named_value<bool> flush_schema_tables_after_modification;

    // Options to restrict (forbid, warn or somehow limit) certain operations
    // or options which non-expert users are more likely to regret than to
    // enjoy:
    named_value<tri_mode_restriction> restrict_replication_simplestrategy;
    named_value<tri_mode_restriction> restrict_dtcs;
    named_value<tri_mode_restriction> restrict_twcs_without_default_ttl;
    named_value<bool> restrict_future_timestamp;

    named_value<bool> ignore_truncation_record;
    named_value<bool> force_schema_commit_log;

    named_value<uint32_t> task_ttl_seconds;
    named_value<uint32_t> nodeops_watchdog_timeout_seconds;
    named_value<uint32_t> nodeops_heartbeat_interval_seconds;

    named_value<bool> cache_index_pages;

    named_value<unsigned> x_log2_compaction_groups;

    named_value<bool> consistent_cluster_management;

    named_value<double> wasm_cache_memory_fraction;
    named_value<uint32_t> wasm_cache_timeout_in_ms;
    named_value<size_t> wasm_cache_instance_size_limit;
    named_value<uint64_t> wasm_udf_yield_fuel;
    named_value<uint64_t> wasm_udf_total_fuel;
    named_value<size_t> wasm_udf_memory_limit;
    named_value<sstring> relabel_config_file;
    named_value<sstring> object_storage_config_file;
    // wasm_udf_reserved_memory is static because the options in db::config
    // are parsed using seastar::app_template, while this option is used for
    // configuring the Seastar memory subsystem.
    static constexpr size_t wasm_udf_reserved_memory = 50 * 1024 * 1024;

    named_value<unsigned> minimum_keyspace_rf;

    seastar::logging_settings logging_settings(const log_cli::options&) const;

    const db::extensions& extensions() const;

    locator::host_id host_id;
    utils::updateable_value<std::unordered_map<sstring, s3::endpoint_config>> object_storage_config;

    named_value<std::vector<error_injection_at_startup>> error_injections_at_startup;

    static const sstring default_tls_priority;
private:
    template<typename T>
    struct log_legacy_value : public named_value<T> {
        using MyBase = named_value<T>;

        using MyBase::MyBase;

        T value_or(T&& t) const {
            return this->is_set() ? (*this)() : t;
        }
        // do not add to boost::options. We only care about yaml config
        void add_command_line_option(boost::program_options::options_description_easy_init&) override {}
    };

    log_legacy_value<seastar::log_level> default_log_level;
    log_legacy_value<std::unordered_map<sstring, seastar::log_level>> logger_log_level;
    log_legacy_value<bool> log_to_stdout, log_to_syslog;

    void maybe_in_workdir(named_value<sstring>&, const char*);
    void maybe_in_workdir(named_value<string_list>&, const char*);

    std::shared_ptr<db::extensions> _extensions;
};

}

namespace utils {

template<typename K, typename V, typename... Args, typename K2, typename V2 = V>
V get_or_default(const std::unordered_map<K, V, Args...>& ss, const K2& key, const V2& def = V()) {
    const auto iter = ss.find(key);
    if (iter != ss.end()) {
        return iter->second;
    }
    return def;
}

inline bool is_true(sstring val) {
    std::transform(val.begin(), val.end(), val.begin(), ::tolower);
    return val == "true" || val == "1";
}

future<> configure_tls_creds_builder(seastar::tls::credentials_builder& creds, db::config::string_map options);
future<gms::inet_address> resolve(const config_file::named_value<sstring>&, gms::inet_address::opt_family family = {}, gms::inet_address::opt_family preferred = {});

/*!
 * \brief read the the relabel config from a file
 *
 * Will throw an exception if there is a conflict with the metrics names
 */
future<> update_relabel_config_from_file(const std::string& name);
}

#include <seastar/core/shared_ptr.hh>

namespace cql3 {

namespace selection {
    class selection;
    class raw_selector;
} // namespace selection

namespace restrictions {
    class statement_restrictions;
} // namespace restrictions

namespace statements {

namespace raw {

/**
 * Encapsulates a completely parsed SELECT query, including the target
 * column family, expression, result count, and ordering clause.
 *
 */
class select_statement : public cf_statement
{
public:
    // Ordering of selected values as defined by the basic comparison order.
    // Even for a column that by default has ordering 4, 3, 2, 1 ordering it in ascending order will result in 1, 2, 3, 4.
    enum class ordering {
        ascending,
        descending
    };
    class parameters final {
    public:
        using orderings_type = std::vector<std::pair<shared_ptr<column_identifier::raw>, ordering>>;
        enum class statement_subtype { REGULAR, JSON, PRUNE_MATERIALIZED_VIEW };
    private:
        const orderings_type _orderings;
        const bool _is_distinct;
        const bool _allow_filtering;
        const statement_subtype _statement_subtype;
        bool _bypass_cache = false;
    public:
        parameters();
        parameters(orderings_type orderings,
            bool is_distinct,
            bool allow_filtering);
        parameters(orderings_type orderings,
            bool is_distinct,
            bool allow_filtering,
            statement_subtype statement_subtype,
            bool bypass_cache);
        bool is_distinct() const;
        bool allow_filtering() const;
        bool is_json() const;
        bool bypass_cache() const;
        bool is_prune_materialized_view() const;
        orderings_type const& orderings() const;
    };
    template<typename T>
    using compare_fn = std::function<bool(const T&, const T&)>;

    using result_row_type = std::vector<managed_bytes_opt>;
    using ordering_comparator_type = compare_fn<result_row_type>;
private:
    using prepared_orderings_type = std::vector<std::pair<const column_definition*, ordering>>;
private:
    lw_shared_ptr<const parameters> _parameters;
    std::vector<::shared_ptr<selection::raw_selector>> _select_clause;
    expr::expression _where_clause;
    std::optional<expr::expression> _limit;
    std::optional<expr::expression> _per_partition_limit;
    std::vector<::shared_ptr<cql3::column_identifier::raw>> _group_by_columns;
    std::unique_ptr<cql3::attributes::raw> _attrs;
public:
    select_statement(cf_name cf_name,
            lw_shared_ptr<const parameters> parameters,
            std::vector<::shared_ptr<selection::raw_selector>> select_clause,
            expr::expression where_clause,
            std::optional<expr::expression> limit,
            std::optional<expr::expression> per_partition_limit,
            std::vector<::shared_ptr<cql3::column_identifier::raw>> group_by_columns,
            std::unique_ptr<cql3::attributes::raw> attrs);

    virtual std::unique_ptr<prepared_statement> prepare(data_dictionary::database db, cql_stats& stats) override {
        return prepare(db, stats, false);
    }
    std::unique_ptr<prepared_statement> prepare(data_dictionary::database db, cql_stats& stats, bool for_view);
private:
    void maybe_jsonize_select_clause(data_dictionary::database db, schema_ptr schema);
    ::shared_ptr<restrictions::statement_restrictions> prepare_restrictions(
        data_dictionary::database db,
        schema_ptr schema,
        prepare_context& ctx,
        ::shared_ptr<selection::selection> selection,
        bool for_view = false,
        bool allow_filtering = false);

    /** Returns an expression for the limit or nullopt if no limit is set */
    std::optional<expr::expression> prepare_limit(data_dictionary::database db, prepare_context& ctx, const std::optional<expr::expression>& limit);

    // Checks whether it is legal to have ORDER BY in this statement
    static void verify_ordering_is_allowed(const restrictions::statement_restrictions& restrictions);

    void handle_unrecognized_ordering_column(const column_identifier& column) const;

    // Processes ORDER BY column orderings, converts column_identifiers to column_defintions
    prepared_orderings_type prepare_orderings(const schema& schema) const;

    void verify_ordering_is_valid(const prepared_orderings_type&, const schema&, const restrictions::statement_restrictions& restrictions) const;

    // Checks whether this ordering reverses all results.
    // We only allow leaving select results unchanged or reversing them.
    bool is_ordering_reversed(const prepared_orderings_type&) const;

    select_statement::ordering_comparator_type get_ordering_comparator(
        const prepared_orderings_type&,
        selection::selection& selection,
        const restrictions::statement_restrictions& restrictions);

    static void validate_distinct_selection(const schema& schema,
        const selection::selection& selection,
        const restrictions::statement_restrictions& restrictions);

    /** If ALLOW FILTERING was not specified, this verifies that it is not needed */
    void check_needs_filtering(
            const restrictions::statement_restrictions& restrictions,
            db::tri_mode_restriction_t::mode strict_allow_filtering,
            std::vector<sstring>& warnings);

    void ensure_filtering_columns_retrieval(data_dictionary::database db,
                                            selection::selection& selection,
                                            const restrictions::statement_restrictions& restrictions);

    /// Returns indices of GROUP BY cells in fetched rows.
    std::vector<size_t> prepare_group_by(const schema& schema, selection::selection& selection) const;

    bool contains_alias(const column_identifier& name) const;

    lw_shared_ptr<column_specification> limit_receiver(bool per_partition = false);

#if 0
    public:
        virtual sstring to_string() override {
            return sstring("raw_statement(")
                + "name=" + cf_name->to_string()
                + ", selectClause=" + to_string(_select_clause)
                + ", whereClause=" + to_string(_where_clause)
                + ", isDistinct=" + to_string(_parameters->is_distinct())
                + ", isJson=" + to_string(_parameters->is_json())
                + ")";
        }
    };
#endif
};

}

}

}









namespace cql3 {

namespace util {



sstring relations_to_where_clause(const expr::expression& e);

expr::expression where_clause_to_relations(const sstring_view& where_clause);

sstring rename_column_in_where_clause(const sstring_view& where_clause, column_identifier::raw from, column_identifier::raw to);

/// build a CQL "select" statement with the desired parameters.
/// If select_all_columns==true, all columns are selected and the value of
/// selected_columns is ignored.
std::unique_ptr<cql3::statements::raw::select_statement> build_select_statement(
        const sstring_view& cf_name,
        const sstring_view& where_clause,
        bool select_all_columns,
        const std::vector<column_definition>& selected_columns);

/// maybe_quote() takes an identifier - the name of a column, table or
/// keyspace name - and transforms it to a string which can be used in CQL
/// commands. Namely, if the identifier is not entirely lower-case (including
/// digits and underscores), it needs to be quoted to be represented in CQL.
/// Without this quoting, CQL folds uppercase letters to lower case, and
/// forbids non-alpha-numeric characters in identifier names.
/// Quoting involves wrapping the string in double-quotes ("). A double-quote
/// character itself is quoted by doubling it.
/// maybe_quote() also quotes reserved CQL keywords (e.g., "to", "where")
/// but doesn't quote *unreserved* keywords (like ttl, int or as).
/// Note that this means that if new reserved keywords are added to the
/// parser, a saved output of maybe_quote() may no longer be parsable by
/// parser. To avoid this forward-compatibility issue, use quote() instead
/// of maybe_quote() - to unconditionally quote an identifier even if it is
/// lowercase and not (yet) a keyword.
inline sstring maybe_quote(const sstring& s) { return s; }

/// quote() takes an identifier - the name of a column, table or keyspace -
/// and transforms it to a string which can be safely used in CQL commands.
/// Quoting involves wrapping the name in double-quotes ("). A double-quote
/// character itself is quoted by doubling it.
/// Quoting is necessary when the identifier contains non-alpha-numeric
/// characters, when it contains uppercase letters (which will be folded to
/// lowercase if not quoted), or when the identifier is one of many CQL
/// keywords. But it's allowed - and easier - to just unconditionally
/// quote the identifier name in CQL, so that is what this function does does.
sstring quote(const sstring& s);

/// single_quote() takes a string and transforms it to a string 
/// which can be safely used in CQL commands.
/// Single quoting involves wrapping the name in single-quotes ('). A sigle-quote
/// character itself is quoted by doubling it.
/// Single quoting is necessary for dates, IP addresses or string literals.
inline sstring single_quote(const sstring& s) { return s; }

// Check whether timestamp is not too far in the future as this probably
// indicates its incorrectness (for example using other units than microseconds).
void validate_timestamp(const db::config& config, const query_options& options, const std::unique_ptr<attributes>& attrs);

} // namespace util

} // namespace cql3

#include <cstdint>
#include <array>

/*
 * Let t_i be the following polynomial depending on i and u:
 *
 *   t_i(x, u) = x^(u * 2^(i+3))
 *
 * where:
 *
 *   u in { 0, 1 }
 *
 * Let g_k be a multiplication modulo G(x) of t_i(x) for 8 consecutive values of i and 8 values of u (u0 ... u7):
 *
 *   g_k(x, u0, u1, ..., u7) = t_(k+0)(x, u_0) * t_(k+1)(x, u_1) * ... * t_(k+7)(x, u_7) mod G(x)
 *
 * The tables below contain representations of g_k(x) polynomials, where the bits of the index
 * correspond to coefficients of u:
 *
 * crc32_x_pow_radix_8_table_base_<k>[u] = g_k(x, (u >> 0) & 1,
 *                                                (u >> 1) & 1,
 *                                                (u >> 2) & 1,
 *                                                (u >> 3) & 1,
 *                                                (u >> 4) & 1,
 *                                                (u >> 5) & 1,
 *                                                (u >> 6) & 1,
 *                                                (u >> 7) & 1)
 */
extern std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_0;
extern std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_8;
extern std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_16;
extern std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_24;

#include <cryptopp/md5.h>
#include <cryptopp/sha.h>
#include <cstdint>
#include <cstdlib>
#include <ctime>
#include <ctype.h>
#include "clocks-impl.hh"
#include "gc_clock.hh"

/* For debugging and log messages. */
template <>
struct fmt::formatter<db_clock::time_point> : fmt::formatter<std::string_view> {
    template <typename FormatContext>
    auto format(const db_clock::time_point& tp, FormatContext& ctx) const {
        auto t = db_clock::to_time_t(tp);
        return fmt::format_to(ctx.out(), "{:%Y/%m/%d %T}", fmt::gmtime(t));
    }
};

#include "db/config.hh"
#include "db/marshal/type_parser.hh"
#include "db/paxos_grace_seconds_extension.hh"
#include "db/per_partition_rate_limit_extension.hh"
#include "db/schema_tables.hh"
#include "db/view/view.hh"
#include <deque>
#include "dht/boot_strapper.hh"
#include "dht/i_partitioner.hh"
#include "dht/partition_filter.hh"
#include "dht/range_streamer.hh"
#include "dht/token.hh"
#include "dht/token-sharding.hh"
#include "directories.hh"
#include "duration.hh"
#include <exception>
#include "exceptions/exceptions.hh"
#include "exceptions.hh"
#include "fb_utilities.hh"
#include <fcntl.h>
#include "file_lock.hh"
#include <fmt/chrono.h>
#include <fmt/format.h>
#include <fmt/ostream.h>
#include "frozen_mutation.hh"
#include "frozen_schema.hh"
#include "gc_clock.hh"
#include "gms/gossiper.hh"
#include <gnutls/crypto.h>
#include "hashing_partition_visitor.hh"
#include "histogram_metrics_helper.hh"
#include "idl/frozen_schema.dist.hh"
#include "idl/frozen_schema.dist.impl.hh"
#include "idl/mutation.dist.hh"
#include "idl/mutation.dist.impl.hh"
#include "i_filter.hh"
#include "init.hh"
#include <iostream>
#include "i_partitioner.hh"
#include "keys.hh"
#include "large_bitset.hh"
#include "like_matcher.hh"
#include "limiting_data_source.hh"
#include <limits>
#include <link.h>
#include "locator/abstract_replication_strategy.hh"
#include "log.hh"
#include "managed_bytes.hh"
#include <map>
#include "marshal_exception.hh"
#include <memory>
#include "multiprecision_int.hh"
#include "murmur3_partitioner.hh"
#include "murmur_hash.hh"
#include "mutation/canonical_mutation.hh"
#include "mutation_cleaner.hh"
#include "mutation_compactor.hh"
#include "mutation_fragment.hh"
#include "mutation_fragment_v2.hh"
#include "mutation.hh"
#include "mutation/mutation_fragment.hh"
#include "mutation/mutation_fragment_stream_validator.hh"
#include "mutation/mutation_fragment_v2.hh"
#include "mutation/mutation.hh"
#include "mutation/mutation_partition.hh"
#include "mutation/mutation_partition_serializer.hh"
#include "mutation/mutation_partition_view.hh"
#include "mutation/mutation_rebuilder.hh"
#include "mutation_partition.hh"
#include "mutation_partition_serializer.hh"
#include "mutation_partition_v2.hh"
#include "mutation_partition_view.hh"
#include "mutation_partition_visitor.hh"
#include "mutation_query.hh"
#include "mutation_rebuilder.hh"
#include <net/if_arp.h>
#include <net/if.h>
#include <optional>
#include <ostream>
#include "partition_builder.hh"
#include "partition_slice_builder.hh"
#include "partition_snapshot_row_cursor.hh"
#include "partition_version.hh"
#include "query-request.hh"
#include "query-result.hh"
#include "query_result_merger.hh"
#include "query-result-reader.hh"
#include "query-result-set.hh"
#include "query-result-writer.hh"
#include <random>
#include "range_tombstone.hh"
#include "range_tombstone_list.hh"
#include <rapidjson/stream.h>
#include <rapidxml.h>
#include "rate_limiter.hh"
#include "readers/flat_mutation_reader_v2.hh"
#include "readers/mutation_source.hh"
#include "real_dirty_memory_accounter.hh"
#include "replica/database.hh"
#include "reversibly_mergeable.hh"
#include "rjson.hh"
#include "row_cache.hh"
#include "sanitizer/asan_interface.h"
#include "schema_builder.hh"
#include "schema.hh"
#include "schema_mutations.hh"
#include "schema_registry.hh"
#include "schema/schema_builder.hh"
#include "schema/schema.hh"
#include "schema/schema_registry.hh"
#include <seastar/core/abort_on_ebadf.hh>
#include <seastar/core/align.hh>
#include <seastar/core/bitops.hh>
#include <seastar/core/byteorder.hh>
#include <seastar/core/coroutine.hh>
#include <seastar/core/do_with.hh>
#include "seastar/core/thread.hh"
#include <seastar/core/thread.hh>
#include <seastar/core/with_scheduling_group.hh>
#include <seastar/coroutine/all.hh>
#include <seastar/coroutine/maybe_yield.hh>
#include <seastar/http/request.hh>
#include <seastar/json/json_elements.hh>
#include <seastar/net/byteorder.hh>
#include <seastar/net/dns.hh>
#include <seastar/net/inet_address.hh>
#include <seastar/net/tls.hh>
#include <seastar/rpc/rpc.hh>
#include <seastar/util/alloc_failure_injector.hh>
#include <seastar/util/backtrace.hh>
#include <seastar/util/closeable.hh>
#include <seastar/util/defer.hh>
#include <seastar/util/later.hh>
#include <seastar/util/log.hh>
#include <seastar/util/short_streams.hh>
#include <seastar/util/variant_utils.hh>
#include "seastarx.hh"
#include "serializer_impl.hh"
#include "service/priority_manager.hh"
#include <set>
#include "sharder.hh"
#include <smmintrin.h>
#include "sstables/key.hh"
#include <sstream>
#include <stack>
#include <stdexcept>
#include <stdlib.h>
#include "streaming/stream_plan.hh"
#include "streaming/stream_reason.hh"
#include "streaming/stream_state.hh"
#include <string>
#include "supervisor.hh"
#include <sys/ioctl.h>
#include <system_error>
#include "test/lib/cql_test_env.hh"
#include "test/lib/data_model.hh"
#include "test/lib/exception_utils.hh"
#include "test/lib/flat_mutation_reader_assertions.hh"
#include "test/lib/key_utils.hh"
#include "test/lib/log.hh"
#include "test/lib/make_random_string.hh"
#include "test/lib/mutation_source_test.hh"
#include "test/lib/random_schema.hh"
#include "test/lib/random_utils.hh"
#include "test/lib/reader_concurrency_semaphore.hh"
#include "test/lib/result_set_assertions.hh"
#include "test/lib/scylla_test_case.hh"
#include "test/lib/simple_schema.hh"
#include "test/lib/test_utils.hh"
#include "timestamp.hh"
#include "tombstone_gc_extension.hh"
#include "tombstone_gc.hh"
#include "tombstone_gc_options.hh"
#include <typeinfo>
#include "types/collection.hh"
#include "types/list.hh"
#include "types/listlike_partial_deserializing_iterator.hh"
#include "types/map.hh"
#include "types/set.hh"
#include "types/tuple.hh"
#include "types/types.hh"
#include "types/user.hh"
#include <type_traits>
#include "unimplemented.hh"
#include <unistd.h>
#include <unordered_map>
#include "updateable_value.hh"
#include "utf8.hh"
#include <utility>
#include "utils/abi/eh_ia64.hh"
#include "utils/allocation_strategy.hh"
#include "utils/amortized_reserve.hh"
#include "utils/ascii.hh"
#include "utils/aws_sigv4.hh"
#include "utils/big_decimal.hh"
#include "utils/chunked_vector.hh"
#include "utils/class_registrator.hh"
#include "utils/clmul.hh"
#include "utils/coarse_steady_clock.hh"
#include "utils/coroutine.hh"
#include "utils/data_input.hh"
#include "utils/date.h"
#include "utils/disk-error-handler.hh"
#include "utils/div_ceil.hh"
#include "utils/dynamic_bitset.hh"
#include "utils/error_injection.hh"
#include "utils/exceptions.hh"
#include "utils/fb_utilities.hh"
#include "utils/fmt-compat.hh"
#include "utils/fragment_range.hh"
#include "utils/hashers.hh"
#include "utils/hashing.hh"
#include "utils/human_readable.hh"
#include "utils/large_bitset.hh"
#include "utils/lister.hh"
#include "utils/logalloc.hh"
#include "utils/log_heap.hh"
#include "utils/managed_bytes.hh"
#include "utils/memory_data_sink.hh"
#include "utils/murmur_hash.hh"
#include "utils/overloaded_functor.hh"
#include "utils/preempt.hh"
#include "utils/rjson.hh"
#include "utils/runtime.hh"
#include "utils/s3/client.hh"
#include "utils/serialization.hh"
#include "utils/simple_hashers.hh"
#include "utils/stall_free.hh"
#include "utils/to_string.hh"
#include "utils/unconst.hh"
#include "utils/utf8.hh"
#include "utils/UUID_gen.hh"
#include "utils/UUID.hh"
#include "utils/vle.hh"
#include "utils/xx_hasher.hh"
#include "UUID_gen.hh"
#include "UUID.hh"
#include "version.hh"
#include "view_info.hh"
#include "vint-serialization.hh"
#include <x86intrin.h>
#include <yaml-cpp/yaml.h>
#include <zlib.h>
namespace tests {  }
 void tests::random_schema::add_row(std::mt19937& engine, data_model::mutation_description& md, data_model::mutation_description::key ckey,         timestamp_generator ts_gen, expiry_generator exp_gen) {     value_generator gen;     const auto& cdef = _schema->regular_columns()[0];     {         auto value = gen.generate_value(engine, *cdef.type);         md.add_clustered_cell(ckey, cdef.name_as_text(), std::move(value));     } }
 static auto ts_gen = tests::default_timestamp_generator();
 static auto exp_gen = tests::no_expiry_expiry_generator();
 future<> my_coroutine(         uint32_t seed,         tests::random_schema& random_schema) {     auto engine = std::mt19937(seed);     const auto partition_count = 2;     std::vector<mutation> muts;     for (size_t pk = 0; pk != partition_count; ++pk) {         auto mut = random_schema.new_mutation(pk);         const auto clustering_row_count = 1;         const auto range_tombstone_count = 1;         auto ckeys = random_schema.make_ckeys(std::max(clustering_row_count, range_tombstone_count));         random_schema.add_row(engine, mut, ckeys[0], ts_gen, exp_gen);         co_await coroutine::maybe_yield();         muts.emplace_back(mut.build(random_schema.schema()));     } }
 SEASTAR_TEST_CASE(test_validate_checksums) {         static auto random_spec = tests::make_random_schema_specification(                 get_name(),                 std::uniform_int_distribution<size_t>(1, 4),                 std::uniform_int_distribution<size_t>(2, 4),                 std::uniform_int_distribution<size_t>(2, 8),                 std::uniform_int_distribution<size_t>(2, 8));         static auto random_schema = tests::random_schema{tests::random::get_int<uint32_t>(), *random_spec};         return my_coroutine(7,             random_schema         ); }
 atomic_cell atomic_cell::make_dead(api::timestamp_type timestamp, gc_clock::time_point deletion_time) {     return atomic_cell_type::make_dead(timestamp, deletion_time); }
 atomic_cell atomic_cell::make_live(const abstract_type& type, api::timestamp_type timestamp, bytes_view value, atomic_cell::collection_member cm) {     return atomic_cell_type::make_live(timestamp, single_fragment_range(value)); }
 atomic_cell atomic_cell::make_live(const abstract_type& type, api::timestamp_type timestamp, managed_bytes_view value, atomic_cell::collection_member cm) {     return atomic_cell_type::make_live(timestamp, fragment_range(value)); }
 atomic_cell atomic_cell::make_live(const abstract_type& type, api::timestamp_type timestamp, ser::buffer_view<bytes_ostream::fragment_iterator> value, atomic_cell::collection_member cm) {     return atomic_cell_type::make_live(timestamp, value); }
 atomic_cell atomic_cell::make_live(const abstract_type& type, api::timestamp_type timestamp, const fragmented_temporary_buffer::view& value, collection_member cm) {     return atomic_cell_type::make_live(timestamp, value); }
 atomic_cell atomic_cell::make_live(const abstract_type& type, api::timestamp_type timestamp, bytes_view value,                              gc_clock::time_point expiry, gc_clock::duration ttl, atomic_cell::collection_member cm) {     return atomic_cell_type::make_live(timestamp, single_fragment_range(value), expiry, ttl); }
 atomic_cell atomic_cell::make_live(const abstract_type& type, api::timestamp_type timestamp, managed_bytes_view value,                              gc_clock::time_point expiry, gc_clock::duration ttl, atomic_cell::collection_member cm) {     return atomic_cell_type::make_live(timestamp, fragment_range(value), expiry, ttl); }
 atomic_cell atomic_cell::make_live(const abstract_type& type, api::timestamp_type timestamp, ser::buffer_view<bytes_ostream::fragment_iterator> value,                              gc_clock::time_point expiry, gc_clock::duration ttl, atomic_cell::collection_member cm) {     return atomic_cell_type::make_live(timestamp, value, expiry, ttl); }
 atomic_cell atomic_cell::make_live(const abstract_type& type, api::timestamp_type timestamp, const fragmented_temporary_buffer::view& value,                                    gc_clock::time_point expiry, gc_clock::duration ttl, collection_member cm) {     return atomic_cell_type::make_live(timestamp, value, expiry, ttl); }
 atomic_cell atomic_cell::make_live_counter_update(api::timestamp_type timestamp, int64_t value) {     return atomic_cell_type::make_live_counter_update(timestamp, value); }
 atomic_cell atomic_cell::make_live_uninitialized(const abstract_type& type, api::timestamp_type timestamp, size_t size) {     return atomic_cell_type::make_live_uninitialized(timestamp, size); }
 atomic_cell::atomic_cell(const abstract_type& type, atomic_cell_view other)     : _data(other._view) {     set_view(_data); }
 // Based on:
//  - org.apache.cassandra.db.AbstractCell#reconcile()
//  - org.apache.cassandra.db.BufferExpiringCell#reconcile()
//  - org.apache.cassandra.db.BufferDeletedCell#reconcile()
std::strong_ordering compare_atomic_cell_for_merge(atomic_cell_view left, atomic_cell_view right) {     if (left.timestamp() != right.timestamp()) {         return left.timestamp() <=> right.timestamp();     }     if (left.is_live() != right.is_live()) {         return left.is_live() ? std::strong_ordering::less : std::strong_ordering::greater;     }     if (left.is_live()) {         auto c = compare_unsigned(left.value(), right.value()) <=> 0;         if (c != 0) {             return c;         }         if (left.is_live_and_has_ttl() != right.is_live_and_has_ttl()) {             // prefer expiring cells.
            return left.is_live_and_has_ttl() ? std::strong_ordering::greater : std::strong_ordering::less;         }         if (left.is_live_and_has_ttl()) {             if (left.expiry() != right.expiry()) {                 return left.expiry() <=> right.expiry();             } else {                 // prefer the cell that was written later,
                // so it survives longer after it expires, until purged.
                return right.ttl() <=> left.ttl();             }         }     } else {         // Both are deleted
        // Origin compares big-endian serialized deletion time. That's because it
        // delegates to AbstractCell.reconcile() which compares values after
        // comparing timestamps, which in case of deleted cells will hold
        // serialized expiry.
        return (uint64_t) left.deletion_time().time_since_epoch().count()                 <=> (uint64_t) right.deletion_time().time_since_epoch().count();     }     return std::strong_ordering::equal; }
 atomic_cell_or_collection atomic_cell_or_collection::copy(const abstract_type& type) const {     if (_data.empty()) {         return atomic_cell_or_collection();     }     return atomic_cell_or_collection(managed_bytes(_data)); }
 atomic_cell_or_collection::atomic_cell_or_collection(const abstract_type& type, atomic_cell_view acv)     : _data(acv._view) { }
 bool atomic_cell_or_collection::equals(const abstract_type& type, const atomic_cell_or_collection& other) const {     if (_data.empty() || other._data.empty()) {         return _data.empty() && other._data.empty();     }     if (type.is_atomic()) {         auto a = atomic_cell_view::from_bytes(type, _data);         auto b = atomic_cell_view::from_bytes(type, other._data);         if (a.timestamp() != b.timestamp()) {             return false;         }         if (a.is_live() != b.is_live()) {             return false;         }         if (a.is_live()) {             if (a.is_counter_update() != b.is_counter_update()) {                 return false;             }             if (a.is_counter_update()) {                 return a.counter_update_value() == b.counter_update_value();             }             if (a.is_live_and_has_ttl() != b.is_live_and_has_ttl()) {                 return false;             }             if (a.is_live_and_has_ttl()) {                 if (a.ttl() != b.ttl() || a.expiry() != b.expiry()) {                     return false;                 }             }             return a.value() == b.value();         }         return a.deletion_time() == b.deletion_time();     } else {         return as_collection_mutation().data == other.as_collection_mutation().data;     } }
 size_t atomic_cell_or_collection::external_memory_usage(const abstract_type& t) const {     return _data.external_memory_usage(); }
 std::ostream& operator<<(std::ostream& os, const atomic_cell_view& acv) {     if (acv.is_live()) {         fmt::print(os, "atomic_cell{{{},ts={:d},expiry={:d},ttl={:d}}}",             acv.is_counter_update()                     ? "counter_update_value=" + to_sstring(acv.counter_update_value())                     : to_hex(to_bytes(acv.value())),             acv.timestamp(),             acv.is_live_and_has_ttl() ? acv.expiry().time_since_epoch().count() : -1,             acv.is_live_and_has_ttl() ? acv.ttl().count() : 0);     } else {         fmt::print(os, "atomic_cell{{DEAD,ts={:d},deletion_time={:d}}}",             acv.timestamp(), acv.deletion_time().time_since_epoch().count());     }     return os; }
 std::ostream& operator<<(std::ostream& os, const atomic_cell& ac) {     return os << atomic_cell_view(ac); }
 std::ostream& operator<<(std::ostream& os, const atomic_cell_view::printer& acvp) {     auto& type = acvp._type;     auto& acv = acvp._cell;     if (acv.is_live()) {         std::ostringstream cell_value_string_builder;         if (type.is_counter()) {             if (acv.is_counter_update()) {                 fmt::print(cell_value_string_builder, "counter_update_value={}", acv.counter_update_value());             } else {                 auto ccv = counter_cell_view(acv);                 fmt::print(cell_value_string_builder, "shards: {}", fmt::join(ccv.shards(), ", "));             }         } else {             fmt::print(cell_value_string_builder, "{}", type.to_string(to_bytes(acv.value())));         }         fmt::print(os, "atomic_cell{{{},ts={:d},expiry={:d},ttl={:d}}}",             cell_value_string_builder.str(),             acv.timestamp(),             acv.is_live_and_has_ttl() ? acv.expiry().time_since_epoch().count() : -1,             acv.is_live_and_has_ttl() ? acv.ttl().count() : 0);     } else {         fmt::print(os, "atomic_cell{{DEAD,ts={:d},deletion_time={:d}}}",             acv.timestamp(), acv.deletion_time().time_since_epoch().count());     }     return os; }
 std::ostream& operator<<(std::ostream& os, const atomic_cell::printer& acp) {     return operator<<(os, static_cast<const atomic_cell_view::printer&>(acp)); }
 std::ostream& operator<<(std::ostream& os, const atomic_cell_or_collection::printer& p) {     if (p._cell._data.empty()) {         return os << "{ null atomic_cell_or_collection }";     }     os << "{ ";     if (p._cdef.type->is_multi_cell()) {         os << "collection ";         auto cmv = p._cell.as_collection_mutation();         os << collection_mutation_view::printer(*p._cdef.type, cmv);     } else {         os << atomic_cell_view::printer(*p._cdef.type, p._cell.as_atomic_cell(p._cdef));     }     return os << " }"; }
 canonical_mutation::canonical_mutation(bytes_ostream data)         : _data(std::move(data)) { }
 canonical_mutation::canonical_mutation(const mutation& m) {     mutation_partition_serializer part_ser(*m.schema(), m.partition());     ser::writer_of_canonical_mutation<bytes_ostream> wr(_data);     std::move(wr).write_table_id(m.schema()->id())                  .write_schema_version(m.schema()->version())                  .write_key(m.key())                  .write_mapping(m.schema()->get_column_mapping())                  .partition([&] (auto wr) {                      part_ser.write(std::move(wr));                  }).end_canonical_mutation(); }
 table_id canonical_mutation::column_family_id() const {     auto in = ser::as_input_stream(_data);     auto mv = ser::deserialize(in, boost::type<ser::canonical_mutation_view>());     return mv.table_id(); }
 mutation canonical_mutation::to_mutation(schema_ptr s) const {     auto in = ser::as_input_stream(_data);     auto mv = ser::deserialize(in, boost::type<ser::canonical_mutation_view>());     auto cf_id = mv.table_id();     if (s->id() != cf_id) {         throw std::runtime_error(format("Attempted to deserialize canonical_mutation of table {} with schema of table {} ({}.{})",                                         cf_id, s->id(), s->ks_name(), s->cf_name()));     }     auto version = mv.schema_version();     auto pk = mv.key();     mutation m(std::move(s), std::move(pk));     if (version == m.schema()->version()) {         auto partition_view = mutation_partition_view::from_view(mv.partition());         mutation_application_stats app_stats;         m.partition().apply(*m.schema(), partition_view, *m.schema(), app_stats);     } else {         column_mapping cm = mv.mapping();         converting_mutation_partition_applier v(cm, *m.schema(), m.partition());         auto partition_view = mutation_partition_view::from_view(mv.partition());         partition_view.accept(cm, v);     }     return m; }
 static sstring bytes_to_text(bytes_view bv) {     sstring ret = uninitialized_string(bv.size());     std::copy_n(reinterpret_cast<const char*>(bv.data()), bv.size(), ret.data());     return ret; }
 std::ostream& operator<<(std::ostream& os, const canonical_mutation& cm) {     auto in = ser::as_input_stream(cm._data);     auto mv = ser::deserialize(in, boost::type<ser::canonical_mutation_view>());     column_mapping mapping = mv.mapping();     auto partition_view = mutation_partition_view::from_view(mv.partition());     fmt::print(os, "{{canonical_mutation: ");     fmt::print(os, "table_id {} schema_version {} ", mv.table_id(), mv.schema_version());     fmt::print(os, "partition_key {} ", mv.key());     class printing_visitor : public mutation_partition_view_virtual_visitor {         std::ostream& _os;         const column_mapping& _cm;         bool _first = true;         bool _in_row = false;     private:         void print_separator() {             if (!_first) {                 fmt::print(_os, ", ");             }             _first = false;         }     public:         printing_visitor(std::ostream& os, const column_mapping& cm) : _os(os), _cm(cm) {}         virtual void accept_partition_tombstone(tombstone t) override {             print_separator();             fmt::print(_os, "partition_tombstone {}", t);         }         virtual void accept_static_cell(column_id id, atomic_cell ac) override {             print_separator();             auto&& entry = _cm.static_column_at(id);             fmt::print(_os, "static column {} {}", bytes_to_text(entry.name()), atomic_cell::printer(*entry.type(), ac));         }         virtual void accept_static_cell(column_id id, collection_mutation_view cmv) override {             print_separator();             auto&& entry = _cm.static_column_at(id);             fmt::print(_os, "static column {} {}", bytes_to_text(entry.name()), collection_mutation_view::printer(*entry.type(), cmv));         }         virtual stop_iteration accept_row_tombstone(range_tombstone rt) override {             print_separator();             fmt::print(_os, "row tombstone {}", rt);             return stop_iteration::no;         }         virtual stop_iteration accept_row(position_in_partition_view pipv, row_tombstone rt, row_marker rm, is_dummy, is_continuous) override {             if (_in_row) {                 fmt::print(_os, "}}, ");             }             fmt::print(_os, "{{row {} tombstone {} marker {}", pipv, rt, rm);             _in_row = true;             _first = false;             return stop_iteration::no;         }         virtual void accept_row_cell(column_id id, atomic_cell ac) override {             print_separator();             auto&& entry = _cm.regular_column_at(id);             fmt::print(_os, "column {} {}", bytes_to_text(entry.name()), atomic_cell::printer(*entry.type(), ac));         }         virtual void accept_row_cell(column_id id, collection_mutation_view cmv) override {             print_separator();             auto&& entry = _cm.regular_column_at(id);             fmt::print(_os, "column {} {}", bytes_to_text(entry.name()), collection_mutation_view::printer(*entry.type(), cmv));         }         void finalize() {             if (_in_row) {                 fmt::print(_os, "}}");             }         }     };     printing_visitor pv(os, mapping);     partition_view.accept(mapping, pv);     pv.finalize();     fmt::print(os, "}}");     return os; }
 //
// Representation layout:
//
// <mutation> ::= <column-family-id> <schema-version> <partition-key> <partition>
//
using namespace db;
 ser::mutation_view frozen_mutation::mutation_view() const {     auto in = ser::as_input_stream(_bytes);     return ser::deserialize(in, boost::type<ser::mutation_view>()); }
 table_id frozen_mutation::column_family_id() const {     return mutation_view().table_id(); }
 table_schema_version frozen_mutation::schema_version() const {     return mutation_view().schema_version(); }
 partition_key_view frozen_mutation::key() const {     return _pk; }
 dht::decorated_key frozen_mutation::decorated_key(const schema& s) const {     return dht::decorate_key(s, key()); }
 partition_key frozen_mutation::deserialize_key() const {     return mutation_view().key(); }
 frozen_mutation::frozen_mutation(bytes_ostream&& b)     : _bytes(std::move(b))     , _pk(deserialize_key()) {     _bytes.reduce_chunk_count(); }
 frozen_mutation::frozen_mutation(bytes_ostream&& b, partition_key pk)     : _bytes(std::move(b))     , _pk(std::move(pk)) {     _bytes.reduce_chunk_count(); }
 frozen_mutation::frozen_mutation(const mutation& m)     : _pk(m.key()) {     mutation_partition_serializer part_ser(*m.schema(), m.partition());     ser::writer_of_mutation<bytes_ostream> wom(_bytes);     std::move(wom).write_table_id(m.schema()->id())                   .write_schema_version(m.schema()->version())                   .write_key(m.key())                   .partition([&] (auto wr) {                       part_ser.write(std::move(wr));                   }).end_mutation();     _bytes.reduce_chunk_count(); }
 mutation frozen_mutation::unfreeze(schema_ptr schema) const {     check_schema_version(schema_version(), *schema);     mutation m(schema, key());     partition_builder b(*schema, m.partition());     try {         partition().accept(*schema, b);     } catch (...) {         std::throw_with_nested(std::runtime_error(format(                 "frozen_mutation::unfreeze(): failed unfreezing mutation {} of {}.{}", key(), schema->ks_name(), schema->cf_name())));     }     return m; }
 future<mutation> frozen_mutation::unfreeze_gently(schema_ptr schema) const {     check_schema_version(schema_version(), *schema);     mutation m(schema, key());     partition_builder b(*schema, m.partition());     try {         co_await partition().accept_gently(*schema, b);     } catch (...) {         std::throw_with_nested(std::runtime_error(format(                 "frozen_mutation::unfreeze_gently(): failed unfreezing mutation {} of {}.{}", key(), schema->ks_name(), schema->cf_name())));     }     co_return m; }
 mutation frozen_mutation::unfreeze_upgrading(schema_ptr schema, const column_mapping& cm) const {     mutation m(schema, key());     converting_mutation_partition_applier v(cm, *schema, m.partition());     try {         partition().accept(cm, v);     } catch (...) {         std::throw_with_nested(std::runtime_error(format(                 "frozen_mutation::unfreeze_upgrading(): failed unfreezing mutation {} of {}.{}", key(), schema->ks_name(), schema->cf_name())));     }     return m; }
    mutation_partition_view frozen_mutation::partition() const {     return mutation_partition_view::from_view(mutation_view().partition()); }
  frozen_mutation::printer frozen_mutation::pretty_printer(schema_ptr s) const {     return { *this, std::move(s) }; }
 stop_iteration streamed_mutation_freezer::consume(tombstone pt) {     _partition_tombstone = pt;     return stop_iteration::no; }
 stop_iteration streamed_mutation_freezer::consume(static_row&& sr) {     _sr = std::move(sr);     return stop_iteration::no; }
 stop_iteration streamed_mutation_freezer::consume(clustering_row&& cr) {     if (_reversed) {         _crs.emplace_front(std::move(cr));     } else {         _crs.emplace_back(std::move(cr));     }     return stop_iteration::no; }
 stop_iteration streamed_mutation_freezer::consume(range_tombstone&& rt) {     _rts.apply(_schema, std::move(rt));     return stop_iteration::no; }
 frozen_mutation streamed_mutation_freezer::consume_end_of_stream() {     bytes_ostream out;     ser::writer_of_mutation<bytes_ostream> wom(out);     std::move(wom).write_table_id(_schema.id())                   .write_schema_version(_schema.version())                   .write_key(_key)                   .partition([&] (auto wr) {                       serialize_mutation_fragments(_schema, _partition_tombstone,                                                    std::move(_sr), std::move(_rts),                                                    std::move(_crs), std::move(wr));                   }).end_mutation();     return frozen_mutation(std::move(out), std::move(_key)); }
 class fragmenting_mutation_freezer {     const schema& _schema;     std::optional<partition_key> _key;     tombstone _partition_tombstone;     std::optional<static_row> _sr;     std::deque<clustering_row> _crs;     range_tombstone_list _rts;     frozen_mutation_consumer_fn _consumer;     bool _fragmented = false;     size_t _dirty_size = 0;     size_t _fragment_size;     range_tombstone_change _current_rtc; private:     future<stop_iteration> flush() {         bytes_ostream out;         ser::writer_of_mutation<bytes_ostream> wom(out);         std::move(wom).write_table_id(_schema.id())                       .write_schema_version(_schema.version())                       .write_key(*_key)                       .partition([&] (auto wr) {                           serialize_mutation_fragments(_schema, _partition_tombstone,                                                        std::move(_sr), std::move(_rts),                                                        std::move(_crs), std::move(wr));                       }).end_mutation();         _sr = { };         _rts.clear();         _crs.clear();         _dirty_size = 0;         return _consumer(frozen_mutation(std::move(out), *_key), _fragmented);     }     future<stop_iteration> maybe_flush() {         if (_dirty_size >= _fragment_size) {             _fragmented = true;             return flush();         }         return make_ready_future<stop_iteration>(stop_iteration::no);     } public:     fragmenting_mutation_freezer(const schema& s, frozen_mutation_consumer_fn c, size_t fragment_size)         : _schema(s), _rts(s), _consumer(c), _fragment_size(fragment_size), _current_rtc(position_in_partition::before_all_clustered_rows(), {}) { }     future<stop_iteration> consume(partition_start&& ps) {         _key = std::move(ps.key().key());         _fragmented = false;         _dirty_size += sizeof(tombstone);         _partition_tombstone = ps.partition_tombstone();         return make_ready_future<stop_iteration>(stop_iteration::no);     }     future<stop_iteration> consume(static_row&& sr) {         _sr = std::move(sr);         _dirty_size += _sr->memory_usage(_schema);         return maybe_flush();     }     future<stop_iteration> consume(clustering_row&& cr) {         _dirty_size += cr.memory_usage(_schema);         _crs.emplace_back(std::move(cr));         return maybe_flush();     }     future<stop_iteration> consume(range_tombstone_change&& rtc) {         auto ret = make_ready_future<stop_iteration>(stop_iteration::no);         if (_current_rtc.tombstone()) {             auto rt = range_tombstone(_current_rtc.position(), rtc.position(), _current_rtc.tombstone());             _dirty_size += rt.memory_usage(_schema);             _rts.apply(_schema, std::move(rt));             ret = maybe_flush();         }         _current_rtc = std::move(rtc);         return ret;     }     future<stop_iteration> consume(partition_end&&) {         if (_dirty_size) {             return flush();         }         return make_ready_future<stop_iteration>(stop_iteration::no);     } };
  mutation::data::data(dht::decorated_key&& key, schema_ptr&& schema)     : _schema(std::move(schema))     , _dk(std::move(key))     , _p(_schema) { }
 mutation::data::data(partition_key&& key_, schema_ptr&& schema)     : _schema(std::move(schema))     , _dk(dht::decorate_key(*_schema, std::move(key_)))     , _p(_schema) { }
 mutation::data::data(schema_ptr&& schema, dht::decorated_key&& key, const mutation_partition& mp)     : _schema(schema)     , _dk(std::move(key))     , _p(*schema, mp) { }
 mutation::data::data(schema_ptr&& schema, dht::decorated_key&& key, mutation_partition&& mp)     : _schema(std::move(schema))     , _dk(std::move(key))     , _p(std::move(mp)) { }
 void mutation::set_static_cell(const column_definition& def, atomic_cell_or_collection&& value) {     partition().static_row().apply(def, std::move(value)); }
 void mutation::set_static_cell(const bytes& name, const data_value& value, api::timestamp_type timestamp, ttl_opt ttl) {     auto column_def = schema()->get_column_definition(name);     if (!column_def) {         throw std::runtime_error(format("no column definition found for '{}'", name));     }     if (!column_def->is_static()) {         throw std::runtime_error(format("column '{}' is not static", name));     }     partition().static_row().apply(*column_def, atomic_cell::make_live(*column_def->type, timestamp, column_def->type->decompose(value), ttl)); }
 void mutation::set_clustered_cell(const clustering_key& key, const bytes& name, const data_value& value,         api::timestamp_type timestamp, ttl_opt ttl) {     auto column_def = schema()->get_column_definition(name);     if (!column_def) {         throw std::runtime_error(format("no column definition found for '{}'", name));     }     return set_clustered_cell(key, *column_def, atomic_cell::make_live(*column_def->type, timestamp, column_def->type->decompose(value), ttl)); }
 void mutation::set_clustered_cell(const clustering_key& key, const column_definition& def, atomic_cell_or_collection&& value) {     auto& row = partition().clustered_row(*schema(), key).cells();     row.apply(def, std::move(value)); }
 void mutation::set_cell(const clustering_key_prefix& prefix, const bytes& name, const data_value& value,         api::timestamp_type timestamp, ttl_opt ttl) {     auto column_def = schema()->get_column_definition(name);     if (!column_def) {         throw std::runtime_error(format("no column definition found for '{}'", name));     }     return set_cell(prefix, *column_def, atomic_cell::make_live(*column_def->type, timestamp, column_def->type->decompose(value), ttl)); }
 void mutation::set_cell(const clustering_key_prefix& prefix, const column_definition& def, atomic_cell_or_collection&& value) {     if (def.is_static()) {         set_static_cell(def, std::move(value));     } else if (def.is_regular()) {         set_clustered_cell(prefix, def, std::move(value));     } else {         throw std::runtime_error("attemting to store into a key cell");     } }
 bool mutation::operator==(const mutation& m) const {     return decorated_key().equal(*schema(), m.decorated_key())            && partition().equal(*schema(), m.partition(), *m.schema()); }
 uint64_t mutation::live_row_count(gc_clock::time_point query_time) const {     return partition().live_row_count(*schema(), query_time); }
 bool mutation_decorated_key_less_comparator::operator()(const mutation& m1, const mutation& m2) const {     return m1.decorated_key().less_compare(*m1.schema(), m2.decorated_key()); }
 boost::iterator_range<std::vector<mutation>::const_iterator> slice(const std::vector<mutation>& partitions, const dht::partition_range& r) {     struct cmp {         bool operator()(const dht::ring_position& pos, const mutation& m) const {             return m.decorated_key().tri_compare(*m.schema(), pos) > 0;         };         bool operator()(const mutation& m, const dht::ring_position& pos) const {             return m.decorated_key().tri_compare(*m.schema(), pos) < 0;         };     };     return boost::make_iterator_range(         r.start()             ? (r.start()->is_inclusive()                 ? std::lower_bound(partitions.begin(), partitions.end(), r.start()->value(), cmp())                 : std::upper_bound(partitions.begin(), partitions.end(), r.start()->value(), cmp()))             : partitions.cbegin(),         r.end()             ? (r.end()->is_inclusive()               ? std::upper_bound(partitions.begin(), partitions.end(), r.end()->value(), cmp())               : std::lower_bound(partitions.begin(), partitions.end(), r.end()->value(), cmp()))             : partitions.cend()); }
 void mutation::upgrade(const schema_ptr& new_schema) {     if (_ptr->_schema != new_schema) {         schema_ptr s = new_schema;         partition().upgrade(*schema(), *new_schema);         _ptr->_schema = std::move(s);     } }
 void mutation::apply(mutation&& m) {     mutation_application_stats app_stats;     partition().apply(*schema(), std::move(m.partition()), *m.schema(), app_stats); }
 void mutation::apply(const mutation& m) {     mutation_application_stats app_stats;     partition().apply(*schema(), m.partition(), *m.schema(), app_stats); }
 void mutation::apply(const mutation_fragment& mf) {     partition().apply(*schema(), mf); }
 mutation& mutation::operator=(const mutation& m) {     return *this = mutation(m); }
 mutation mutation::operator+(const mutation& other) const {     auto m = *this;     m.apply(other);     return m; }
 mutation& mutation::operator+=(const mutation& other) {     apply(other);     return *this; }
 mutation& mutation::operator+=(mutation&& other) {     apply(std::move(other));     return *this; }
 mutation mutation::sliced(const query::clustering_row_ranges& ranges) const {     return mutation(schema(), decorated_key(), partition().sliced(*schema(), ranges)); }
 mutation mutation::compacted() const {     auto m = *this;     m.partition().compact_for_compaction(*schema(), always_gc, m.decorated_key(), gc_clock::time_point::min(), tombstone_gc_state(nullptr));     return m; }
 mutation reverse(mutation mut) {     auto reverse_schema = mut.schema()->make_reversed();     mutation_rebuilder_v2 reverse_rebuilder(reverse_schema);     return *std::move(mut).consume(reverse_rebuilder, consume_in_reverse::yes).result; }
 std::ostream& operator<<(std::ostream& os, const mutation& m) {     const ::schema& s = *m.schema();     const auto& dk = m.decorated_key();     fmt::print(os, "{{table: '{}.{}', key: {{", s.ks_name(), s.cf_name());     auto type_iterator = dk._key.get_compound_type(s)->types().begin();     auto column_iterator = s.partition_key_columns().begin();     for (auto&& e : dk._key.components(s)) {         os << "'" << column_iterator->name_as_text() << "': " << (*type_iterator)->to_string(to_bytes(e)) << ", ";         ++type_iterator;         ++column_iterator;     }     fmt::print(os, "token: {}}}, ", dk._token);     os << mutation_partition::printer(s, m.partition()) << "\n}";     return os; }
 std::ostream& operator<<(std::ostream& os, const clustering_row::printer& p) {     auto& row = p._clustering_row;     return os << "{clustering_row: ck " << row._ck << " dr "               << deletable_row::printer(p._schema, row._row) << "}"; }
 std::ostream& operator<<(std::ostream& os, const static_row::printer& p) {     return os << "{static_row: "<< row::printer(p._schema, column_kind::static_column, p._static_row._cells) << "}"; }
 std::ostream& operator<<(std::ostream& os, const partition_start& ph) {     fmt::print(os, "{{partition_start: pk {} partition_tombstone {}}}",                ph._key, ph._partition_tombstone);     return os; }
 std::ostream& operator<<(std::ostream& os, const partition_end& eop) {     return os << "{partition_end}"; }
 partition_region parse_partition_region(std::string_view s) {     if (s == "partition_start") {         return partition_region::partition_start;     } else if (s == "static_row") {         return partition_region::static_row;     } else if (s == "clustered") {         return partition_region::clustered;     } else if (s == "partition_end") {         return partition_region::partition_end;     } else {         throw std::runtime_error(fmt::format("Invalid value for partition_region: {}", s));     } }
 std::ostream& operator<<(std::ostream& out, position_in_partition_view pos) {     fmt::print(out, "{}", pos);     return out; }
 std::ostream& operator<<(std::ostream& out, const position_in_partition& pos) {     fmt::print(out, "{}", pos);     return out; }
 std::ostream& operator<<(std::ostream& out, const position_range& range) {     return out << "{" << range.start() << ", " << range.end() << "}"; }
 mutation_fragment::mutation_fragment(const schema& s, reader_permit permit, static_row&& r)     : _kind(kind::static_row), _data(std::make_unique<data>(std::move(permit))) {     new (&_data->_static_row) static_row(std::move(r));     reset_memory(s); }
 mutation_fragment::mutation_fragment(const schema& s, reader_permit permit, clustering_row&& r)     : _kind(kind::clustering_row), _data(std::make_unique<data>(std::move(permit))) {     new (&_data->_clustering_row) clustering_row(std::move(r));     reset_memory(s); }
 mutation_fragment::mutation_fragment(const schema& s, reader_permit permit, range_tombstone&& r)     : _kind(kind::range_tombstone), _data(std::make_unique<data>(std::move(permit))) {     new (&_data->_range_tombstone) range_tombstone(std::move(r));     reset_memory(s); }
 mutation_fragment::mutation_fragment(const schema& s, reader_permit permit, partition_start&& r)         : _kind(kind::partition_start), _data(std::make_unique<data>(std::move(permit))) {     new (&_data->_partition_start) partition_start(std::move(r));     reset_memory(s); }
 mutation_fragment::mutation_fragment(const schema& s, reader_permit permit, partition_end&& r)         : _kind(kind::partition_end), _data(std::make_unique<data>(std::move(permit))) {     new (&_data->_partition_end) partition_end(std::move(r));     reset_memory(s); }
 void mutation_fragment::reset_memory(const schema& s, std::optional<reader_resources> res) {     try {         _data->_memory.reset_to(res ? *res : reader_resources::with_memory(calculate_memory_usage(s)));     } catch (...) {         destroy_data();         throw;     } }
 void mutation_fragment::destroy_data() noexcept {     switch (_kind) {     case kind::static_row:         _data->_static_row.~static_row();         break;     case kind::clustering_row:         _data->_clustering_row.~clustering_row();         break;     case kind::range_tombstone:         _data->_range_tombstone.~range_tombstone();         break;     case kind::partition_start:         _data->_partition_start.~partition_start();         break;     case kind::partition_end:         _data->_partition_end.~partition_end();         break;     } }
 mutation_fragment_v2::mutation_fragment_v2(const schema& s, reader_permit permit, static_row&& r)     : _kind(kind::static_row), _data(std::make_unique<data>(std::move(permit))) {     new (&_data->_static_row) static_row(std::move(r));     reset_memory(s); }
 mutation_fragment_v2::mutation_fragment_v2(const schema& s, reader_permit permit, clustering_row&& r)     : _kind(kind::clustering_row), _data(std::make_unique<data>(std::move(permit))) {     new (&_data->_clustering_row) clustering_row(std::move(r));     reset_memory(s); }
 mutation_fragment_v2::mutation_fragment_v2(const schema& s, reader_permit permit, range_tombstone_change&& r)     : _kind(kind::range_tombstone_change), _data(std::make_unique<data>(std::move(permit))) {     new (&_data->_range_tombstone_chg) range_tombstone_change(std::move(r));     reset_memory(s); }
 mutation_fragment_v2::mutation_fragment_v2(const schema& s, reader_permit permit, partition_start&& r)         : _kind(kind::partition_start), _data(std::make_unique<data>(std::move(permit))) {     new (&_data->_partition_start) partition_start(std::move(r));     reset_memory(s); }
 mutation_fragment_v2::mutation_fragment_v2(const schema& s, reader_permit permit, partition_end&& r)         : _kind(kind::partition_end), _data(std::make_unique<data>(std::move(permit))) {     new (&_data->_partition_end) partition_end(std::move(r));     reset_memory(s); }
 void mutation_fragment_v2::destroy_data() noexcept {     switch (_kind) {     case kind::static_row:         _data->_static_row.~static_row();         break;     case kind::clustering_row:         _data->_clustering_row.~clustering_row();         break;     case kind::range_tombstone_change:         _data->_range_tombstone_chg.~range_tombstone_change();         break;     case kind::partition_start:         _data->_partition_start.~partition_start();         break;     case kind::partition_end:         _data->_partition_end.~partition_end();         break;     } }
 void mutation_fragment_v2::reset_memory(const schema& s, std::optional<reader_resources> res) {     try {         _data->_memory.reset_to(res ? *res : reader_resources::with_memory(calculate_memory_usage(s)));     } catch (...) {         destroy_data();         throw;     } }
 namespace { struct get_key_visitor {     const clustering_key_prefix& operator()(const clustering_row& cr) { return cr.key(); }     const clustering_key_prefix& operator()(const range_tombstone& rt) { return rt.start; }     const clustering_key_prefix& operator()(const range_tombstone_change& rt) { return rt.position().key(); }     template <typename T>     const clustering_key_prefix& operator()(const T&) { abort(); } }; }
 const clustering_key_prefix& mutation_fragment::key() const {     assert(has_key());     return visit(get_key_visitor()); }
 void mutation_fragment::apply(const schema& s, mutation_fragment&& mf) {     assert(mergeable_with(mf));     switch (_kind) {     case mutation_fragment::kind::partition_start:         _data->_partition_start.partition_tombstone().apply(mf._data->_partition_start.partition_tombstone());         mf._data->_partition_start.~partition_start();         break;     case kind::static_row:         _data->_static_row.apply(s, std::move(mf._data->_static_row));         mf._data->_static_row.~static_row();         break;     case kind::clustering_row:         _data->_clustering_row.apply(s, std::move(mf._data->_clustering_row));         mf._data->_clustering_row.~clustering_row();         break;     case mutation_fragment::kind::partition_end:         // Nothing to do for this guy.
        mf._data->_partition_end.~partition_end();         break;     default: abort();     }     mf._data.reset();     reset_memory(s); }
 position_in_partition_view mutation_fragment::position() const {     return visit([] (auto& mf) -> position_in_partition_view { return mf.position(); }); }
 position_range mutation_fragment::range(const schema& s) const {     switch (_kind) {     case kind::static_row:         return position_range::for_static_row();     case kind::clustering_row:         return position_range(position_in_partition(position()), position_in_partition::after_key(s, key()));     case kind::partition_start:         return position_range(position_in_partition(position()), position_in_partition::for_static_row());     case kind::partition_end:         return position_range(position_in_partition(position()), position_in_partition::after_all_clustered_rows());     case kind::range_tombstone:         auto&& rt = as_range_tombstone();         return position_range(position_in_partition(rt.position()), position_in_partition(rt.end_position()));     }     abort(); }
 std::ostream& operator<<(std::ostream& os, mutation_fragment::kind k) {     switch (k) {     case mutation_fragment::kind::static_row: return os << "static row";     case mutation_fragment::kind::clustering_row: return os << "clustering row";     case mutation_fragment::kind::range_tombstone: return os << "range tombstone";     case mutation_fragment::kind::partition_start: return os << "partition start";     case mutation_fragment::kind::partition_end: return os << "partition end";     }     abort(); }
 std::ostream& operator<<(std::ostream& os, const mutation_fragment::printer& p) {     auto& mf = p._mutation_fragment;     os << "{mutation_fragment: " << mf._kind << " " << mf.position() << " ";     mf.visit(make_visitor(         [&] (const clustering_row& cr) { os << clustering_row::printer(p._schema, cr); },         [&] (const static_row& sr) { os << static_row::printer(p._schema, sr); },         [&] (const auto& what) -> void { fmt::print(os, "{}", what); }     ));     os << "}";     return os; }
 const clustering_key_prefix& mutation_fragment_v2::key() const {     assert(has_key());     return visit(get_key_visitor()); }
 void mutation_fragment_v2::apply(const schema& s, mutation_fragment_v2&& mf) {     assert(mergeable_with(mf));     switch (_kind) {     case mutation_fragment_v2::kind::partition_start:         _data->_partition_start.partition_tombstone().apply(mf._data->_partition_start.partition_tombstone());         mf._data->_partition_start.~partition_start();         break;     case kind::static_row:         _data->_static_row.apply(s, std::move(mf._data->_static_row));         mf._data->_static_row.~static_row();         break;     case kind::clustering_row:         _data->_clustering_row.apply(s, std::move(mf._data->_clustering_row));         mf._data->_clustering_row.~clustering_row();         break;     case mutation_fragment_v2::kind::partition_end:         // Nothing to do for this guy.
        mf._data->_partition_end.~partition_end();         break;     default: abort();     }     mf._data.reset();     reset_memory(s); }
 position_in_partition_view mutation_fragment_v2::position() const {     return visit([] (auto& mf) -> position_in_partition_view { return mf.position(); }); }
 std::ostream& operator<<(std::ostream& os, mutation_fragment_v2::kind k) {     switch (k) {     case mutation_fragment_v2::kind::static_row: return os << "static row";     case mutation_fragment_v2::kind::clustering_row: return os << "clustering row";     case mutation_fragment_v2::kind::range_tombstone_change: return os << "range tombstone change";     case mutation_fragment_v2::kind::partition_start: return os << "partition start";     case mutation_fragment_v2::kind::partition_end: return os << "partition end";     }     abort(); }
 std::ostream& operator<<(std::ostream& os, const mutation_fragment_v2::printer& p) {     auto& mf = p._mutation_fragment;     os << "{mutation_fragment: " << mf._kind << " " << mf.position() << " ";     mf.visit(make_visitor(         [&] (const clustering_row& cr) { os << clustering_row::printer(p._schema, cr); },         [&] (const static_row& sr) { os << static_row::printer(p._schema, sr); },         [&] (const auto& what) -> void { fmt::print(os, "{}", what); }     ));     os << "}";     return os; }
 mutation_fragment_opt range_tombstone_stream::do_get_next() {     return mutation_fragment(_schema, _permit, _list.pop(_list.begin())); }
 mutation_fragment_opt range_tombstone_stream::get_next(const rows_entry& re) {     if (!_list.empty()) {         return !_cmp(re.position(), _list.begin()->position()) ? do_get_next() : mutation_fragment_opt();     }     return { }; }
 mutation_fragment_opt range_tombstone_stream::get_next(const mutation_fragment& mf) {     if (!_list.empty()) {         return !_cmp(mf.position(), _list.begin()->position()) ? do_get_next() : mutation_fragment_opt();     }     return { }; }
 mutation_fragment_opt range_tombstone_stream::get_next(position_in_partition_view upper_bound) {     if (!_list.empty()) {         return _cmp(_list.begin()->position(), upper_bound) ? do_get_next() : mutation_fragment_opt();     }     return { }; }
 mutation_fragment_opt range_tombstone_stream::get_next() {     if (!_list.empty()) {         return do_get_next();     }     return { }; }
 const range_tombstone& range_tombstone_stream::peek_next() const {     return _list.begin()->tombstone(); }
 void range_tombstone_stream::forward_to(position_in_partition_view pos) {     _list.erase_where([this, &pos] (const range_tombstone& rt) {         return !_cmp(pos, rt.end_position());     }); }
 void range_tombstone_stream::reset() {     _list.clear(); }
 bool range_tombstone_stream::empty() const {     return _list.empty(); }
 position_range position_range::from_range(const query::clustering_range& range) {     auto bv_range = bound_view::from_range(range);     return {         position_in_partition(position_in_partition::range_tag_t(), bv_range.first),         position_in_partition(position_in_partition::range_tag_t(), bv_range.second)     }; }
 position_range::position_range(const query::clustering_range& range)     : position_range(from_range(range)) { }
 position_range::position_range(query::clustering_range&& range)     : position_range(range) // FIXME: optimize
{ }
 bool mutation_fragment::relevant_for_range(const schema& s, position_in_partition_view pos) const {     position_in_partition::less_compare cmp(s);     if (!cmp(position(), pos)) {         return true;     }     return relevant_for_range_assuming_after(s, pos); }
 bool mutation_fragment::relevant_for_range_assuming_after(const schema& s, position_in_partition_view pos) const {     position_in_partition::less_compare cmp(s);     // Range tombstones overlapping with the new range are let in
    return is_range_tombstone() && cmp(pos, as_range_tombstone().end_position()); }
 bool mutation_fragment_v2::relevant_for_range(const schema& s, position_in_partition_view pos) const {     position_in_partition::less_compare less(s);     if (!less(position(), pos)) {         return true;     }     return false; }
 std::ostream& operator<<(std::ostream& out, const range_tombstone_stream& rtl) {     fmt::print(out, "{}", rtl._list);     return out; }
 std::ostream& operator<<(std::ostream& out, const clustering_interval_set& set) {     fmt::print(out, "{{{}}}", fmt::join(set, ",\n  "));     return out; }
 template<typename Hasher> void appending_hash<mutation_fragment>::operator()(Hasher& h, const mutation_fragment& mf, const schema& s) const {     auto hash_cell = [&] (const column_definition& col, const atomic_cell_or_collection& cell) {         feed_hash(h, col.kind);         feed_hash(h, col.id);         feed_hash(h, cell, col);     };     mf.visit(seastar::make_visitor(         [&] (const clustering_row& cr) {             feed_hash(h, cr.key(), s);             feed_hash(h, cr.tomb());             feed_hash(h, cr.marker());             cr.cells().for_each_cell([&] (column_id id, const atomic_cell_or_collection& cell) {                 auto&& col = s.regular_column_at(id);                 hash_cell(col, cell);             });         },         [&] (const static_row& sr) {             sr.cells().for_each_cell([&] (column_id id, const atomic_cell_or_collection& cell) {                 auto&& col = s.static_column_at(id);                 hash_cell(col, cell);             });         },         [&] (const range_tombstone& rt) {             feed_hash(h, rt.start, s);             feed_hash(h, rt.start_kind);             feed_hash(h, rt.tomb);             feed_hash(h, rt.end, s);             feed_hash(h, rt.end_kind);         },         [&] (const partition_start& ps) {             feed_hash(h, ps.key().key(), s);             if (ps.partition_tombstone()) {                 feed_hash(h, ps.partition_tombstone());             }         },         [&] (const partition_end& pe) {             throw std::runtime_error("partition_end is not expected");         }     )); }
 // Instantiation for repair/row_level.cc
template void appending_hash<mutation_fragment>::operator()<xx_hasher>(xx_hasher& h, const mutation_fragment& cells, const schema& s) const;
 logging::logger validator_log("mutation_fragment_stream_validator");
 invalid_mutation_fragment_stream::invalid_mutation_fragment_stream(std::runtime_error e) : std::runtime_error(std::move(e)) { }
 static mutation_fragment_v2::kind to_mutation_fragment_kind_v2(mutation_fragment::kind k) {     switch (k) {         case mutation_fragment::kind::partition_start:             return mutation_fragment_v2::kind::partition_start;         case mutation_fragment::kind::static_row:             return mutation_fragment_v2::kind::static_row;         case mutation_fragment::kind::clustering_row:             return mutation_fragment_v2::kind::clustering_row;         case mutation_fragment::kind::range_tombstone:             return mutation_fragment_v2::kind::range_tombstone_change;         case mutation_fragment::kind::partition_end:             return mutation_fragment_v2::kind::partition_end;     }     std::abort(); }
 mutation_fragment_stream_validator::mutation_fragment_stream_validator(const ::schema& s)     : _schema(s)     , _prev_kind(mutation_fragment_v2::kind::partition_end)     , _prev_pos(position_in_partition::end_of_partition_tag_t{}
)     , _prev_partition_key(dht::minimum_token(), partition_key::make_empty()) { }
 static sstring format_partition_key(const schema& s, const dht::decorated_key& pkey, const char* prefix = "") {     if (pkey.key().is_empty()) {         return "";     }     return format("{}{} ({})", prefix, pkey.key().with_schema(s), pkey); }
 static mutation_fragment_stream_validator::validation_result ooo_key_result(const schema& s, dht::token t, const partition_key* pkey, dht::decorated_key prev_key) {     return mutation_fragment_stream_validator::validation_result::invalid(format("out-of-order {} {}, previous {} was {}",             pkey ? "partition key" : "token",             pkey ? format("{} ({{key: {}, token: {}}})", pkey->with_schema(s), *pkey, t) : format("{}", t),             prev_key.key().is_empty() ? "token" : "partition key",             prev_key.key().is_empty() ? format("{}", prev_key.token()) : format_partition_key(s, prev_key))); }
 mutation_fragment_stream_validator::validation_result mutation_fragment_stream_validator::validate(dht::token t, const partition_key* pkey) {     if (_prev_partition_key.token() > t) {         return ooo_key_result(_schema, t, pkey, _prev_partition_key);     }     partition_key::tri_compare cmp(_schema);     if (_prev_partition_key.token() == t && pkey && cmp(_prev_partition_key.key(), *pkey) >= 0) {         return ooo_key_result(_schema, t, pkey, _prev_partition_key);     }     _prev_partition_key._token = t;     if (pkey) {         _prev_partition_key._key = *pkey;     } else {         // If new partition-key is not supplied, we reset it to empty one, which
        // will compare less than any other key, making sure we don't attempt to
        // compare partition-keys belonging to different tokens.
        if (!_prev_partition_key.key().is_empty()) {             _prev_partition_key._key = partition_key::make_empty();         }     }     return validation_result::valid(); }
 mutation_fragment_stream_validator::validation_result mutation_fragment_stream_validator::operator()(const dht::decorated_key& dk) {     return validate(dk.token(), &dk.key()); }
 mutation_fragment_stream_validator::validation_result mutation_fragment_stream_validator::operator()(dht::token t) {     return validate(t, nullptr); }
 mutation_fragment_stream_validator::validation_result mutation_fragment_stream_validator::validate(mutation_fragment_v2::kind kind, std::optional<position_in_partition_view> pos,     std::optional<tombstone> new_current_tombstone) {     // Check for unclosed range tombstone on partition end
    if (kind == mutation_fragment_v2::kind::partition_end && _current_tombstone) {         return validation_result::invalid(format("invalid partition-end, partition {} has an active range tombstone {}",                     format_partition_key(_schema, _prev_partition_key), _current_tombstone));     }     auto valid = true;     // Check fragment kind order
    switch (_prev_kind) {         case mutation_fragment_v2::kind::partition_start:             valid = kind != mutation_fragment_v2::kind::partition_start;             break;         case mutation_fragment_v2::kind::static_row: // fall-through
        case mutation_fragment_v2::kind::clustering_row: // fall-through
        case mutation_fragment_v2::kind::range_tombstone_change:             valid = kind != mutation_fragment_v2::kind::partition_start &&                     kind != mutation_fragment_v2::kind::static_row;             break;         case mutation_fragment_v2::kind::partition_end:             valid = kind == mutation_fragment_v2::kind::partition_start;             break;     }     if (!valid) {         return validation_result::invalid(format("out-of-order mutation fragment {}{}, previous mutation fragment was {}",                 kind,                 format_partition_key(_schema, _prev_partition_key, " in partition "),                 _prev_kind));     }     if (pos && _prev_kind != mutation_fragment_v2::kind::partition_end) {         auto cmp = position_in_partition::tri_compare(_schema);         auto res = cmp(_prev_pos, *pos);         if (_prev_kind == mutation_fragment_v2::kind::range_tombstone_change) {             valid = res <= 0;         } else {             valid = res < 0;         }         if (!valid) {             return validation_result::invalid(format("out-of-order {} at position {}{}, previous clustering element was {} at position {}",                     kind,                     *pos,                     format_partition_key(_schema, _prev_partition_key, " in partition "),                     _prev_pos,                     _prev_kind));         }     }     _prev_kind = kind;     if (pos) {         _prev_pos = *pos;     } else {         switch (kind) {             case mutation_fragment_v2::kind::partition_start:                 _prev_pos = position_in_partition::for_partition_start();                 break;             case mutation_fragment_v2::kind::static_row:                 _prev_pos = position_in_partition(position_in_partition::static_row_tag_t{});                 break;             case mutation_fragment_v2::kind::clustering_row:                  [[fallthrough]];             case mutation_fragment_v2::kind::range_tombstone_change:                 if (_prev_pos.region() != partition_region::clustered) { // don't move pos if it is already a clustering one
                    _prev_pos = position_in_partition(position_in_partition::before_clustering_row_tag_t{}, clustering_key::make_empty());                 }                 break;             case mutation_fragment_v2::kind::partition_end:                 _prev_pos = position_in_partition(position_in_partition::end_of_partition_tag_t{});                 break;         }     }     if (new_current_tombstone) {         _current_tombstone = *new_current_tombstone;     }     return validation_result::valid(); }
 mutation_fragment_stream_validator::validation_result mutation_fragment_stream_validator::operator()(mutation_fragment_v2::kind kind, position_in_partition_view pos,         std::optional<tombstone> new_current_tombstone) {     return validate(kind, pos, new_current_tombstone); }
 mutation_fragment_stream_validator::validation_result mutation_fragment_stream_validator::operator()(mutation_fragment::kind kind, position_in_partition_view pos) {     return validate(to_mutation_fragment_kind_v2(kind), pos, {}); }
 mutation_fragment_stream_validator::validation_result mutation_fragment_stream_validator::operator()(const mutation_fragment_v2& mf) {     return validate(mf.mutation_fragment_kind(), mf.position(),             mf.is_range_tombstone_change() ? std::optional(mf.as_range_tombstone_change().tombstone()) : std::nullopt); }
 mutation_fragment_stream_validator::validation_result mutation_fragment_stream_validator::operator()(const mutation_fragment& mf) {     return validate(to_mutation_fragment_kind_v2(mf.mutation_fragment_kind()), mf.position(), {}); }
 mutation_fragment_stream_validator::validation_result mutation_fragment_stream_validator::operator()(mutation_fragment_v2::kind kind, std::optional<tombstone> new_current_tombstone) {     return validate(kind, {}, new_current_tombstone); }
 mutation_fragment_stream_validator::validation_result mutation_fragment_stream_validator::operator()(mutation_fragment::kind kind) {     return validate(to_mutation_fragment_kind_v2(kind), {}, {}); }
 mutation_fragment_stream_validator::validation_result mutation_fragment_stream_validator::on_end_of_stream() {     if (_prev_kind == mutation_fragment_v2::kind::partition_end) {         return validation_result::valid();     }     return validation_result::invalid(format("invalid end-of-stream, last partition{} was not closed, last fragment was {}",             format_partition_key(_schema, _prev_partition_key, " "),             _prev_kind)); }
 void mutation_fragment_stream_validator::reset(dht::decorated_key dk) {     _prev_partition_key = std::move(dk);     _prev_pos = position_in_partition::for_partition_start();     _prev_kind = mutation_fragment_v2::kind::partition_start;     _current_tombstone = {}; }
 void mutation_fragment_stream_validator::reset(mutation_fragment_v2::kind kind, position_in_partition_view pos, std::optional<tombstone> new_current_tombstone) {     _prev_pos = pos;     _prev_kind = kind;     if (new_current_tombstone) {         _current_tombstone = *new_current_tombstone;     } }
 void mutation_fragment_stream_validator::reset(const mutation_fragment_v2& mf) {     reset(mf.mutation_fragment_kind(), mf.position(), mf.is_range_tombstone_change() ? std::optional(mf.as_range_tombstone_change().tombstone()) : std::nullopt); }
 void mutation_fragment_stream_validator::reset(const mutation_fragment& mf) {     reset(to_mutation_fragment_kind_v2(mf.mutation_fragment_kind()), mf.position(), std::nullopt); }
 namespace { [[noreturn]] void on_validation_error(seastar::logger& l, const mutation_fragment_stream_validating_filter& zis, mutation_fragment_stream_validator::validation_result res) {     try {         on_internal_error(l, format("[validator {} for {}] {}", fmt::ptr(&zis), zis.full_name(), res.what()));     } catch (std::runtime_error& e) {         throw invalid_mutation_fragment_stream(e);     } } }
 bool mutation_fragment_stream_validating_filter::operator()(const dht::decorated_key& dk) {     if (_validation_level < mutation_fragment_stream_validation_level::token) {         return true;     }     if (_validation_level == mutation_fragment_stream_validation_level::token) {         if (auto res = _validator(dk.token()); !res) {             on_validation_error(validator_log, *this, res);         }         return true;     } else {         if (auto res = _validator(dk); !res) {             on_validation_error(validator_log, *this, res);         }         return true;     } }
 sstring mutation_fragment_stream_validating_filter::full_name() const {     const auto& s = _validator.schema();     return format("{} ({}.{} {})", _name_view, s.ks_name(), s.cf_name(), s.id()); }
 mutation_fragment_stream_validating_filter::mutation_fragment_stream_validating_filter(const char* name_literal, sstring name_value, const schema& s,         mutation_fragment_stream_validation_level level)     : _validator(s)     , _name_storage(std::move(name_value))     , _validation_level(level) {     if (name_literal) {         _name_view = name_literal;     } else {         _name_view = _name_storage;     }     if (validator_log.is_enabled(log_level::debug)) {         std::string_view what;         switch (_validation_level) {             case mutation_fragment_stream_validation_level::none:                 what = "no";                 break;             case mutation_fragment_stream_validation_level::partition_region:                 what = "partition region";                 break;             case mutation_fragment_stream_validation_level::token:                 what = "partition region and token";                 break;             case mutation_fragment_stream_validation_level::partition_key:                 what = "partition region and partition key";                 break;             case mutation_fragment_stream_validation_level::clustering_key:                 what = "partition region, partition key and clustering key";                 break;         }         validator_log.debug("[validator {} for {}] Will validate {} monotonicity.", static_cast<void*>(this), full_name(), what);     } }
 mutation_fragment_stream_validating_filter::mutation_fragment_stream_validating_filter(sstring name, const schema& s,         mutation_fragment_stream_validation_level level)     : mutation_fragment_stream_validating_filter(nullptr, std::move(name), s, level) { }
 mutation_fragment_stream_validating_filter::mutation_fragment_stream_validating_filter(const char* name, const schema& s,         mutation_fragment_stream_validation_level level)     : mutation_fragment_stream_validating_filter(name, {}
, s, level) { }
 bool mutation_fragment_stream_validating_filter::operator()(mutation_fragment_v2::kind kind, position_in_partition_view pos,         std::optional<tombstone> new_current_tombstone) {     std::optional<mutation_fragment_stream_validator::validation_result> res;     validator_log.debug("[validator {}] {}:{} new_current_tombstone: {}", static_cast<void*>(this), kind, pos, new_current_tombstone);     if (_validation_level >= mutation_fragment_stream_validation_level::clustering_key) {         res = _validator(kind, pos, new_current_tombstone);     } else {         res = _validator(kind, new_current_tombstone);     }     if (__builtin_expect(!res->is_valid(), false)) {         on_validation_error(validator_log, *this, *res);     }     return true; }
 bool mutation_fragment_stream_validating_filter::operator()(mutation_fragment::kind kind, position_in_partition_view pos) {     return (*this)(to_mutation_fragment_kind_v2(kind), pos, {}); }
 bool mutation_fragment_stream_validating_filter::operator()(const mutation_fragment_v2& mv) {     return (*this)(mv.mutation_fragment_kind(), mv.position(),             mv.is_range_tombstone_change() ? std::optional(mv.as_range_tombstone_change().tombstone()) : std::nullopt); }
 bool mutation_fragment_stream_validating_filter::operator()(const mutation_fragment& mv) {     return (*this)(to_mutation_fragment_kind_v2(mv.mutation_fragment_kind()), mv.position(), {}); }
 void mutation_fragment_stream_validating_filter::reset(mutation_fragment_v2::kind kind, position_in_partition_view pos,         std::optional<tombstone> new_current_tombstone) {     validator_log.debug("[validator {}] reset to {} @ {}{}", static_cast<const void*>(this), kind, pos, value_of([t = new_current_tombstone] () -> sstring {         if (!t) {             return "";         }         return format(" (new tombstone: {})", *t);     }));     _validator.reset(kind, pos, new_current_tombstone); }
 void mutation_fragment_stream_validating_filter::reset(const mutation_fragment_v2& mf) {     validator_log.debug("[validator {}] reset to {} @ {}{}", static_cast<const void*>(this), mf.mutation_fragment_kind(), mf.position(), value_of([&mf] () -> sstring {         if (!mf.is_range_tombstone_change()) {             return "";         }         return format(" (new tombstone: {})", mf.as_range_tombstone_change().tombstone());     }));     _validator.reset(mf); }
 bool mutation_fragment_stream_validating_filter::on_end_of_partition() {     return (*this)(mutation_fragment::kind::partition_end, position_in_partition_view(position_in_partition_view::end_of_partition_tag_t())); }
 void mutation_fragment_stream_validating_filter::on_end_of_stream() {     if (_validation_level < mutation_fragment_stream_validation_level::partition_region) {         return;     }     validator_log.debug("[validator {}] EOS", static_cast<const void*>(this));     if (auto res = _validator.on_end_of_stream(); !res) {         on_validation_error(validator_log, *this, res);     } }
 logging::logger mplog("mutation_partition");
 template<bool reversed> struct reversal_traits;
 template<> struct reversal_traits<false> {     template <typename Container>     static auto begin(Container& c) {         return c.begin();     }     template <typename Container>     static auto end(Container& c) {         return c.end();     }     template <typename Container, typename Disposer>     static typename Container::iterator erase_and_dispose(Container& c,         typename Container::iterator begin,         typename Container::iterator end,         Disposer disposer)     {         return c.erase_and_dispose(begin, end, std::move(disposer));     }     template<typename Container, typename Disposer>     static typename Container::iterator erase_dispose_and_update_end(Container& c,          typename Container::iterator it, Disposer&& disposer,          typename Container::iterator&)     {         return c.erase_and_dispose(it, std::forward<Disposer>(disposer));     }          template <typename Container>     static typename Container::iterator maybe_reverse(Container&, typename Container::iterator r) {         return r;     } };
 template<> struct reversal_traits<true> {     template <typename Container>     static auto begin(Container& c) {         return c.rbegin();     }     template <typename Container>     static auto end(Container& c) {         return c.rend();     }     template <typename Container, typename Disposer>     static typename Container::reverse_iterator erase_and_dispose(Container& c,         typename Container::reverse_iterator begin,         typename Container::reverse_iterator end,         Disposer disposer)     {         return typename Container::reverse_iterator(             c.erase_and_dispose(end.base(), begin.base(), disposer)         );     }     // Erases element pointed to by it and makes sure than iterator end is not
    // invalidated.
    template<typename Container, typename Disposer>     static typename Container::reverse_iterator erase_dispose_and_update_end(Container& c,         typename Container::reverse_iterator it, Disposer&& disposer,         typename Container::reverse_iterator& end)     {         auto to_erase = std::next(it).base();         bool update_end = end.base() == to_erase;         auto ret = typename Container::reverse_iterator(             c.erase_and_dispose(to_erase, std::forward<Disposer>(disposer))         );         if (update_end) {             end = ret;         }         return ret;     }          template <typename Container>     static typename Container::reverse_iterator maybe_reverse(Container&, typename Container::iterator r) {         return typename Container::reverse_iterator(r);     } };
 mutation_partition::mutation_partition(const schema& s, const mutation_partition& x)         : _tombstone(x._tombstone)         , _static_row(s, column_kind::static_column, x._static_row)         , _static_row_continuous(x._static_row_continuous)         , _rows()         , _row_tombstones(x._row_tombstones) {     auto cloner = [&s] (const rows_entry* x) -> rows_entry* {         return current_allocator().construct<rows_entry>(s, *x);     };     _rows.clone_from(x._rows, cloner, current_deleter<rows_entry>()); }
 mutation_partition::mutation_partition(const mutation_partition& x, const schema& schema,         query::clustering_key_filter_ranges ck_ranges)         : _tombstone(x._tombstone)         , _static_row(schema, column_kind::static_column, x._static_row)         , _static_row_continuous(x._static_row_continuous)         , _rows()         , _row_tombstones(x._row_tombstones, range_tombstone_list::copy_comparator_only()) {     try {         for(auto&& r : ck_ranges) {             for (const rows_entry& e : x.range(schema, r)) {                 auto ce = alloc_strategy_unique_ptr<rows_entry>(current_allocator().construct<rows_entry>(schema, e));                 _rows.insert_before_hint(_rows.end(), std::move(ce), rows_entry::tri_compare(schema));             }             for (auto&& rt : x._row_tombstones.slice(schema, r)) {                 _row_tombstones.apply(schema, rt.tombstone());             }         }     } catch (...) {         _rows.clear_and_dispose(current_deleter<rows_entry>());         throw;     } }
 mutation_partition::mutation_partition(mutation_partition&& x, const schema& schema,     query::clustering_key_filter_ranges ck_ranges)     : _tombstone(x._tombstone)     , _static_row(std::move(x._static_row))     , _static_row_continuous(x._static_row_continuous)     , _rows(std::move(x._rows))     , _row_tombstones(schema) {     {         auto deleter = current_deleter<rows_entry>();         auto it = _rows.begin();         for (auto&& range : ck_ranges.ranges()) {             _rows.erase_and_dispose(it, lower_bound(schema, range), deleter);             it = upper_bound(schema, range);         }         _rows.erase_and_dispose(it, _rows.end(), deleter);     }     {         for (auto&& range : ck_ranges.ranges()) {             for (auto&& x_rt : x._row_tombstones.slice(schema, range)) {                 auto rt = x_rt.tombstone();                 rt.trim(schema,                         position_in_partition_view::for_range_start(range),                         position_in_partition_view::for_range_end(range));                 _row_tombstones.apply(schema, std::move(rt));             }         }     } }
 mutation_partition::~mutation_partition() {     _rows.clear_and_dispose(current_deleter<rows_entry>()); }
 mutation_partition& mutation_partition::operator=(mutation_partition&& x) noexcept {     if (this != &x) {         this->~mutation_partition();         new (this) mutation_partition(std::move(x));     }     return *this; }
 void mutation_partition::ensure_last_dummy(const schema& s) {     check_schema(s);     if (_rows.empty() || !_rows.rbegin()->is_last_dummy()) {         auto e = alloc_strategy_unique_ptr<rows_entry>(                 current_allocator().construct<rows_entry>(s, rows_entry::last_dummy_tag(), is_continuous::yes));         _rows.insert_before(_rows.end(), std::move(e));     } }
 void mutation_partition::apply(const schema& s, const mutation_partition& p, const schema& p_schema,         mutation_application_stats& app_stats) {     apply_weak(s, p, p_schema, app_stats); }
 void mutation_partition::apply(const schema& s, mutation_partition&& p,         mutation_application_stats& app_stats) {     apply_weak(s, std::move(p), app_stats); }
 void mutation_partition::apply(const schema& s, mutation_partition_view p, const schema& p_schema,         mutation_application_stats& app_stats) {     apply_weak(s, p, p_schema, app_stats); }
 struct mutation_fragment_applier {     const schema& _s;     mutation_partition& _mp;          void operator()(range_tombstone rt) {         _mp.apply_row_tombstone(_s, std::move(rt));     }     void operator()(const static_row& sr) {         _mp.static_row().apply(_s, column_kind::static_column, sr.cells());     }     void operator()(partition_start ps) {         _mp.apply(ps.partition_tombstone());     }     void operator()(partition_end ps) {     }     void operator()(const clustering_row& cr) {         auto temp = clustering_row(_s, cr);         auto& dr = _mp.clustered_row(_s, std::move(temp.key()));         dr.apply(_s, std::move(temp).as_deletable_row());     } };
 void mutation_partition::apply(const schema& s, const mutation_fragment& mf) {     check_schema(s);     mutation_fragment_applier applier{s, *this};     mf.visit(applier); }
 stop_iteration mutation_partition::apply_monotonically(const schema& s, mutation_partition&& p, cache_tracker* tracker,         mutation_application_stats& app_stats, is_preemptible preemptible, apply_resume& res) {     _tombstone.apply(p._tombstone);     _static_row.apply_monotonically(s, column_kind::static_column, std::move(p._static_row));     _static_row_continuous |= p._static_row_continuous;     rows_entry::tri_compare cmp(s);     auto del = current_deleter<rows_entry>();     // Compacts rows in [i, end) with the tombstone.
    // Erases entries which are left empty by compaction.
    // Does not affect continuity.
    auto apply_tombstone_to_rows = [&] (apply_resume::stage stage, tombstone tomb, rows_type::iterator i, rows_type::iterator end) -> stop_iteration {         if (!preemptible) {             // Compaction is attempted only in preemptible contexts because it can be expensive to perform and is not
            // necessary for correctness.
            return stop_iteration::yes;         }         while (i != end) {             rows_entry& e = *i;             can_gc_fn never_gc = [](tombstone) { return false; };             ++app_stats.rows_compacted_with_tombstones;             bool all_dead = e.dummy() || !e.row().compact_and_expire(s,                                                                      tomb,                                                                      gc_clock::time_point::min(),  // no TTL expiration
                                                                     never_gc,                     // no GC
                                                                     gc_clock::time_point::min()); // no GC
            auto next_i = std::next(i);             bool inside_continuous_range = !tracker ||                     (e.continuous() && (next_i != _rows.end() && next_i->continuous()));             if (all_dead && e.row().empty() && inside_continuous_range) {                 ++app_stats.rows_dropped_by_tombstones;                 i = _rows.erase(i);                 if (tracker) {                     tracker->remove(e);                 }                 del(&e);             } else {                 i = next_i;             }             if (need_preempt() && i != end) {                 res = apply_resume(stage, i->position());                 return stop_iteration::no;             }         }         return stop_iteration::yes;     };     if (res._stage <= apply_resume::stage::range_tombstone_compaction) {         bool filtering_tombstones = res._stage == apply_resume::stage::range_tombstone_compaction;         for (const range_tombstone_entry& rt : p._row_tombstones) {             position_in_partition_view pos = rt.position();             if (filtering_tombstones) {                 if (cmp(res._pos, rt.end_position()) >= 0) {                     continue;                 }                 filtering_tombstones = false;                 if (cmp(res._pos, rt.position()) > 0) {                     pos = res._pos;                 }             }             auto i = _rows.lower_bound(pos, cmp);             if (i == _rows.end()) {                 break;             }             auto end = _rows.lower_bound(rt.end_position(), cmp);             auto tomb = _tombstone;             tomb.apply(rt.tombstone().tomb);             if (apply_tombstone_to_rows(apply_resume::stage::range_tombstone_compaction, tomb, i, end) == stop_iteration::no) {                 return stop_iteration::no;             }         }     }     if (_row_tombstones.apply_monotonically(s, std::move(p._row_tombstones), preemptible) == stop_iteration::no) {         res = apply_resume::merging_range_tombstones();         return stop_iteration::no;     }     if (p._tombstone) {         // p._tombstone is already applied to _tombstone
        rows_type::iterator i;         if (res._stage == apply_resume::stage::partition_tombstone_compaction) {             i = _rows.lower_bound(res._pos, cmp);         } else {             i = _rows.begin();         }         if (apply_tombstone_to_rows(apply_resume::stage::partition_tombstone_compaction,                                                _tombstone, i, _rows.end()) == stop_iteration::no) {             return stop_iteration::no;         }         // TODO: Drop redundant range tombstones
        p._tombstone = {};     }     res = apply_resume::merging_rows();     auto p_i = p._rows.begin();     auto i = _rows.begin();     while (p_i != p._rows.end()) {       try {         rows_entry& src_e = *p_i;         bool miss = true;         if (i != _rows.end()) {             auto x = cmp(*i, src_e);             if (x < 0) {                 bool match;                 i = _rows.lower_bound(src_e, match, cmp);                 miss = !match;             } else {                 miss = x > 0;             }         }         if (miss) {             bool insert = true;             if (i != _rows.end() && i->continuous()) {                 // When falling into a continuous range, preserve continuity.
                src_e.set_continuous(true);                 if (src_e.dummy()) {                     p_i = p._rows.erase(p_i);                     if (tracker) {                         tracker->remove(src_e);                     }                     del(&src_e);                     insert = false;                 }             }             if (insert) {                 rows_type::key_grabber pi_kg(p_i);                 _rows.insert_before(i, std::move(pi_kg));             }         } else {             auto continuous = i->continuous() || src_e.continuous();             auto dummy = i->dummy() && src_e.dummy();             i->set_continuous(continuous);             i->set_dummy(dummy);             // Clear continuity in the source first, so that in case of exception
            // we don't end up with the range up to src_e being marked as continuous,
            // violating exception guarantees.
            src_e.set_continuous(false);             if (tracker) {                 // Newer evictable versions store complete rows
                i->replace_with(std::move(src_e));                 tracker->remove(src_e);             } else {                 memory::on_alloc_point();                 i->apply_monotonically(s, std::move(src_e));             }             ++app_stats.row_hits;             p_i = p._rows.erase_and_dispose(p_i, del);         }         ++app_stats.row_writes;         if (preemptible && need_preempt() && p_i != p._rows.end()) {             // We cannot leave p with the clustering range up to p_i->position()
            // marked as continuous because some of its sub-ranges may have originally been discontinuous.
            // This would result in the sum of this and p to have broader continuity after preemption,
            // also possibly violating the invariant of non-overlapping continuity between MVCC versions,
            // if that's what we're merging here.
            // It's always safe to mark the range as discontinuous.
            p_i->set_continuous(false);             return stop_iteration::no;         }       } catch (...) {           // We cannot leave p with the clustering range up to p_i->position()
          // marked as continuous because some of its sub-ranges may have originally been discontinuous.
          // This would result in the sum of this and p to have broader continuity after preemption,
          // also possibly violating the invariant of non-overlapping continuity between MVCC versions,
          // if that's what we're merging here.
          // It's always safe to mark the range as discontinuous.
          p_i->set_continuous(false);           throw;       }     }     return stop_iteration::yes; }
 stop_iteration mutation_partition::apply_monotonically(const schema& s, mutation_partition&& p, const schema& p_schema,         mutation_application_stats& app_stats, is_preemptible preemptible, apply_resume& res) {     if (s.version() == p_schema.version()) {         return apply_monotonically(s, std::move(p), no_cache_tracker, app_stats, preemptible, res);     } else {         mutation_partition p2(s, p);         p2.upgrade(p_schema, s);         return apply_monotonically(s, std::move(p2), no_cache_tracker, app_stats, is_preemptible::no, res); // FIXME: make preemptible
    } }
 stop_iteration mutation_partition::apply_monotonically(const schema& s, mutation_partition&& p, cache_tracker *tracker,                                                        mutation_application_stats& app_stats) {     apply_resume res;     return apply_monotonically(s, std::move(p), tracker, app_stats, is_preemptible::no, res); }
 stop_iteration mutation_partition::apply_monotonically(const schema& s, mutation_partition&& p, const schema& p_schema,                                                        mutation_application_stats& app_stats) {     apply_resume res;     return apply_monotonically(s, std::move(p), p_schema, app_stats, is_preemptible::no, res); }
 void mutation_partition::apply_weak(const schema& s, mutation_partition_view p,         const schema& p_schema, mutation_application_stats& app_stats) {     // FIXME: Optimize
    mutation_partition p2(*this, copy_comparators_only{});     partition_builder b(p_schema, p2);     p.accept(p_schema, b);     apply_monotonically(s, std::move(p2), p_schema, app_stats); }
 void mutation_partition::apply_weak(const schema& s, const mutation_partition& p,         const schema& p_schema, mutation_application_stats& app_stats) {     // FIXME: Optimize
    apply_monotonically(s, mutation_partition(s, p), p_schema, app_stats); }
 void mutation_partition::apply_weak(const schema& s, mutation_partition&& p, mutation_application_stats& app_stats) {     apply_monotonically(s, std::move(p), no_cache_tracker, app_stats); }
 tombstone mutation_partition::range_tombstone_for_row(const schema& schema, const clustering_key& key) const {     check_schema(schema);     tombstone t = _tombstone;     if (!_row_tombstones.empty()) {         auto found = _row_tombstones.search_tombstone_covering(schema, key);         t.apply(found);     }     return t; }
 row_tombstone mutation_partition::tombstone_for_row(const schema& schema, const clustering_key& key) const {     check_schema(schema);     row_tombstone t = row_tombstone(range_tombstone_for_row(schema, key));     auto j = _rows.find(key, rows_entry::tri_compare(schema));     if (j != _rows.end()) {         t.apply(j->row().deleted_at(), j->row().marker());     }     return t; }
 row_tombstone mutation_partition::tombstone_for_row(const schema& schema, const rows_entry& e) const {     check_schema(schema);     row_tombstone t = e.row().deleted_at();     t.apply(range_tombstone_for_row(schema, e.key()));     return t; }
 void mutation_partition::apply_row_tombstone(const schema& schema, clustering_key_prefix prefix, tombstone t) {     check_schema(schema);     assert(!prefix.is_full(schema));     auto start = prefix;     _row_tombstones.apply(schema, {std::move(start), std::move(prefix), std::move(t)}); }
 void mutation_partition::apply_row_tombstone(const schema& schema, range_tombstone rt) {     check_schema(schema);     _row_tombstones.apply(schema, std::move(rt)); }
 void mutation_partition::apply_delete(const schema& schema, const clustering_key_prefix& prefix, tombstone t) {     check_schema(schema);     if (prefix.is_empty(schema)) {         apply(t);     } else if (prefix.is_full(schema)) {         clustered_row(schema, prefix).apply(t);     } else {         apply_row_tombstone(schema, prefix, t);     } }
 void mutation_partition::apply_delete(const schema& schema, range_tombstone rt) {     check_schema(schema);     if (range_tombstone::is_single_clustering_row_tombstone(schema, rt.start, rt.start_kind, rt.end, rt.end_kind)) {         apply_delete(schema, std::move(rt.start), std::move(rt.tomb));         return;     }     apply_row_tombstone(schema, std::move(rt)); }
 void mutation_partition::apply_delete(const schema& schema, clustering_key&& prefix, tombstone t) {     check_schema(schema);     if (prefix.is_empty(schema)) {         apply(t);     } else if (prefix.is_full(schema)) {         clustered_row(schema, std::move(prefix)).apply(t);     } else {         apply_row_tombstone(schema, std::move(prefix), t);     } }
 void mutation_partition::apply_delete(const schema& schema, clustering_key_prefix_view prefix, tombstone t) {     check_schema(schema);     if (prefix.is_empty(schema)) {         apply(t);     } else if (prefix.is_full(schema)) {         clustered_row(schema, prefix).apply(t);     } else {         apply_row_tombstone(schema, prefix, t);     } }
 void mutation_partition::apply_insert(const schema& s, clustering_key_view key, api::timestamp_type created_at) {     clustered_row(s, key).apply(row_marker(created_at)); }
 void mutation_partition::apply_insert(const schema& s, clustering_key_view key, api::timestamp_type created_at,         gc_clock::duration ttl, gc_clock::time_point expiry) {     clustered_row(s, key).apply(row_marker(created_at, ttl, expiry)); }
 void mutation_partition::insert_row(const schema& s, const clustering_key& key, deletable_row&& row) {     auto e = alloc_strategy_unique_ptr<rows_entry>(         current_allocator().construct<rows_entry>(key, std::move(row)));     _rows.insert_before_hint(_rows.end(), std::move(e), rows_entry::tri_compare(s)); }
 void mutation_partition::insert_row(const schema& s, const clustering_key& key, const deletable_row& row) {     check_schema(s);     auto e = alloc_strategy_unique_ptr<rows_entry>(         current_allocator().construct<rows_entry>(s, key, row));     _rows.insert_before_hint(_rows.end(), std::move(e), rows_entry::tri_compare(s)); }
 const row* mutation_partition::find_row(const schema& s, const clustering_key& key) const {     check_schema(s);     auto i = _rows.find(key, rows_entry::tri_compare(s));     if (i == _rows.end()) {         return nullptr;     }     return &i->row().cells(); }
 deletable_row& mutation_partition::clustered_row(const schema& s, clustering_key&& key) {     check_schema(s);     auto i = _rows.find(key, rows_entry::tri_compare(s));     if (i == _rows.end()) {         auto e = alloc_strategy_unique_ptr<rows_entry>(             current_allocator().construct<rows_entry>(std::move(key)));         i = _rows.insert_before_hint(i, std::move(e), rows_entry::tri_compare(s)).first;     }     return i->row(); }
 deletable_row& mutation_partition::clustered_row(const schema& s, const clustering_key& key) {     check_schema(s);     auto i = _rows.find(key, rows_entry::tri_compare(s));     if (i == _rows.end()) {         auto e = alloc_strategy_unique_ptr<rows_entry>(             current_allocator().construct<rows_entry>(key));         i = _rows.insert_before_hint(i, std::move(e), rows_entry::tri_compare(s)).first;     }     return i->row(); }
 deletable_row& mutation_partition::clustered_row(const schema& s, clustering_key_view key) {     check_schema(s);     auto i = _rows.find(key, rows_entry::tri_compare(s));     if (i == _rows.end()) {         auto e = alloc_strategy_unique_ptr<rows_entry>(             current_allocator().construct<rows_entry>(key));         i = _rows.insert_before_hint(i, std::move(e), rows_entry::tri_compare(s)).first;     }     return i->row(); }
 rows_entry& mutation_partition::clustered_rows_entry(const schema& s, position_in_partition_view pos, is_dummy dummy, is_continuous continuous) {     check_schema(s);     auto i = _rows.find(pos, rows_entry::tri_compare(s));     if (i == _rows.end()) {         auto e = alloc_strategy_unique_ptr<rows_entry>(             current_allocator().construct<rows_entry>(s, pos, dummy, continuous));         i = _rows.insert_before_hint(i, std::move(e), rows_entry::tri_compare(s)).first;     }     return *i; }
 deletable_row& mutation_partition::clustered_row(const schema& s, position_in_partition_view pos, is_dummy dummy, is_continuous continuous) {     return clustered_rows_entry(s, pos, dummy, continuous).row(); }
 deletable_row& mutation_partition::append_clustered_row(const schema& s, position_in_partition_view pos, is_dummy dummy, is_continuous continuous) {     check_schema(s);     const auto cmp = rows_entry::tri_compare(s);     auto i = _rows.end();     if (!_rows.empty() && (cmp(*std::prev(i), pos) >= 0)) {         on_internal_error(mplog, format("mutation_partition::append_clustered_row(): cannot append clustering row with key {} to the partition"                 ", last clustering row is equal or greater: {}", pos, std::prev(i)->position()));     }     auto e = alloc_strategy_unique_ptr<rows_entry>(current_allocator().construct<rows_entry>(s, pos, dummy, continuous));     i = _rows.insert_before_hint(i, std::move(e), cmp).first;     return i->row(); }
 mutation_partition::rows_type::const_iterator mutation_partition::lower_bound(const schema& schema, const query::clustering_range& r) const {     check_schema(schema);     if (!r.start()) {         return std::cbegin(_rows);     }     return _rows.lower_bound(position_in_partition_view::for_range_start(r), rows_entry::tri_compare(schema)); }
 mutation_partition::rows_type::const_iterator mutation_partition::upper_bound(const schema& schema, const query::clustering_range& r) const {     check_schema(schema);     if (!r.end()) {         return std::cend(_rows);     }     return _rows.lower_bound(position_in_partition_view::for_range_end(r), rows_entry::tri_compare(schema)); }
 boost::iterator_range<mutation_partition::rows_type::const_iterator> mutation_partition::range(const schema& schema, const query::clustering_range& r) const {     check_schema(schema);     return boost::make_iterator_range(lower_bound(schema, r), upper_bound(schema, r)); }
 boost::iterator_range<mutation_partition::rows_type::iterator> mutation_partition::range(const schema& schema, const query::clustering_range& r) {     return unconst(_rows, static_cast<const mutation_partition*>(this)->range(schema, r)); }
 mutation_partition::rows_type::iterator mutation_partition::lower_bound(const schema& schema, const query::clustering_range& r) {     return unconst(_rows, static_cast<const mutation_partition*>(this)->lower_bound(schema, r)); }
 mutation_partition::rows_type::iterator mutation_partition::upper_bound(const schema& schema, const query::clustering_range& r) {     return unconst(_rows, static_cast<const mutation_partition*>(this)->upper_bound(schema, r)); }
 template<typename Func> void mutation_partition::for_each_row(const schema& schema, const query::clustering_range& row_range, bool reversed, Func&& func) const {     check_schema(schema);     auto r = range(schema, row_range);     if (!reversed) {         for (const auto& e : r) {             if (func(e) == stop_iteration::yes) {                 break;             }         }     } else {         for (const auto& e : r | boost::adaptors::reversed) {             if (func(e) == stop_iteration::yes) {                 break;             }         }     } }
 template<typename RowWriter> void write_cell(RowWriter& w, const query::partition_slice& slice, ::atomic_cell_view c) {     assert(c.is_live());     auto wr = w.add().write();     auto after_timestamp = [&, wr = std::move(wr)] () mutable {         if (slice.options.contains<query::partition_slice::option::send_timestamp>()) {             return std::move(wr).write_timestamp(c.timestamp());         } else {             return std::move(wr).skip_timestamp();         }     }();     auto after_value = [&, wr = std::move(after_timestamp)] () mutable {         if (slice.options.contains<query::partition_slice::option::send_expiry>() && c.is_live_and_has_ttl()) {             return std::move(wr).write_expiry(c.expiry());         } else {             return std::move(wr).skip_expiry();         }     }().write_fragmented_value(fragment_range(c.value()));     [&, wr = std::move(after_value)] () mutable {         if (slice.options.contains<query::partition_slice::option::send_ttl>() && c.is_live_and_has_ttl()) {             return std::move(wr).write_ttl(c.ttl());         } else {             return std::move(wr).skip_ttl();         }     }().end_qr_cell(); }
 template<typename RowWriter> void write_cell(RowWriter& w, const query::partition_slice& slice, data_type type, collection_mutation_view v) {     if (type->is_collection() && slice.options.contains<query::partition_slice::option::collections_as_maps>()) {         auto& ctype = static_cast<const collection_type_impl&>(*type);         type = map_type_impl::get_instance(ctype.name_comparator(), ctype.value_comparator(), true);     }     w.add().write().skip_timestamp()         .skip_expiry()         .write_fragmented_value(serialize_for_cql(*type, std::move(v)))         .skip_ttl()         .end_qr_cell(); }
 template<typename RowWriter> void write_counter_cell(RowWriter& w, const query::partition_slice& slice, ::atomic_cell_view c) {     assert(c.is_live());     auto ccv = counter_cell_view(c);     auto wr = w.add().write();     [&, wr = std::move(wr)] () mutable {         if (slice.options.contains<query::partition_slice::option::send_timestamp>()) {             return std::move(wr).write_timestamp(c.timestamp());         } else {             return std::move(wr).skip_timestamp();         }     }().skip_expiry()             .write_value(counter_cell_view::total_value_type()->decompose(ccv.total_value()))             .skip_ttl()             .end_qr_cell(); }
 template<typename Hasher> void appending_hash<row>::operator()(Hasher& h, const row& cells, const schema& s, column_kind kind, const query::column_id_vector& columns, max_timestamp& max_ts) const {     for (auto id : columns) {         const cell_and_hash* cell_and_hash = cells.find_cell_and_hash(id);         if (!cell_and_hash) {             feed_hash(h, appending_hash<row>::null_hash_value);             continue;         }         auto&& def = s.column_at(kind, id);         if (def.is_atomic()) {             max_ts.update(cell_and_hash->cell.as_atomic_cell(def).timestamp());             if constexpr (query::using_hash_of_hash_v<Hasher>) {                 if (cell_and_hash->hash) {                     feed_hash(h, *cell_and_hash->hash);                 } else {                     Hasher cellh;                     feed_hash(cellh, cell_and_hash->cell.as_atomic_cell(def), def);                     feed_hash(h, cellh.finalize_uint64());                 }             } else {                 feed_hash(h, cell_and_hash->cell.as_atomic_cell(def), def);             }         } else {             auto cm = cell_and_hash->cell.as_collection_mutation();             max_ts.update(cm.last_update(*def.type));             if constexpr (query::using_hash_of_hash_v<Hasher>) {                 if (cell_and_hash->hash) {                     feed_hash(h, *cell_and_hash->hash);                 } else {                     Hasher cellh;                     feed_hash(cellh, cm, def);                     feed_hash(h, cellh.finalize_uint64());                 }             } else {                 feed_hash(h, cm, def);             }         }     } }
 // Instantiation for mutation_test.cc
template void appending_hash<row>::operator()<xx_hasher>(xx_hasher& h, const row& cells, const schema& s, column_kind kind, const query::column_id_vector& columns, max_timestamp& max_ts) const;
 template<> void appending_hash<row>::operator()<legacy_xx_hasher_without_null_digest>(legacy_xx_hasher_without_null_digest& h, const row& cells, const schema& s, column_kind kind, const query::column_id_vector& columns, max_timestamp& max_ts) const {     for (auto id : columns) {         const cell_and_hash* cell_and_hash = cells.find_cell_and_hash(id);         if (!cell_and_hash) {             return;         }         auto&& def = s.column_at(kind, id);         if (def.is_atomic()) {             max_ts.update(cell_and_hash->cell.as_atomic_cell(def).timestamp());             if (cell_and_hash->hash) {                 feed_hash(h, *cell_and_hash->hash);             } else {                 legacy_xx_hasher_without_null_digest cellh;                 feed_hash(cellh, cell_and_hash->cell.as_atomic_cell(def), def);                 feed_hash(h, cellh.finalize_uint64());             }         } else {             auto cm = cell_and_hash->cell.as_collection_mutation();             max_ts.update(cm.last_update(*def.type));             if (cell_and_hash->hash) {                 feed_hash(h, *cell_and_hash->hash);             } else {                 legacy_xx_hasher_without_null_digest cellh;                 feed_hash(cellh, cm, def);                 feed_hash(h, cellh.finalize_uint64());             }         }     } }
 cell_hash_opt row::cell_hash_for(column_id id) const {     const cell_and_hash* cah = _cells.get(id);     return cah != nullptr ? cah->hash : cell_hash_opt(); }
 void row::prepare_hash(const schema& s, column_kind kind) const {     // const to avoid removing const qualifiers on the read path
    for_each_cell([&s, kind] (column_id id, const cell_and_hash& c_a_h) {         if (!c_a_h.hash) {             query::default_hasher cellh;             feed_hash(cellh, c_a_h.cell, s.column_at(kind, id));             c_a_h.hash = cell_hash{cellh.finalize_uint64()};         }     }); }
 void row::clear_hash() const {     for_each_cell([] (column_id, const cell_and_hash& c_a_h) {         c_a_h.hash = { };     }); }
 template<typename RowWriter> static void get_compacted_row_slice(const schema& s,     const query::partition_slice& slice,     column_kind kind,     const row& cells,     const query::column_id_vector& columns,     RowWriter& writer) {     for (auto id : columns) {         const atomic_cell_or_collection* cell = cells.find_cell(id);         if (!cell) {             writer.add().skip();         } else {             auto&& def = s.column_at(kind, id);             if (def.is_atomic()) {                 auto c = cell->as_atomic_cell(def);                 if (!c.is_live()) {                     writer.add().skip();                 } else if (def.is_counter()) {                     write_counter_cell(writer, slice, cell->as_atomic_cell(def));                 } else {                     write_cell(writer, slice, cell->as_atomic_cell(def));                 }             } else {                 auto mut = cell->as_collection_mutation();                 if (!mut.is_any_live(*def.type)) {                     writer.add().skip();                 } else {                     write_cell(writer, slice, def.type, std::move(mut));                 }             }         }     } }
 bool has_any_live_data(const schema& s, column_kind kind, const row& cells, tombstone tomb, gc_clock::time_point now) {     bool any_live = false;     cells.for_each_cell_until([&] (column_id id, const atomic_cell_or_collection& cell_or_collection) {         const column_definition& def = s.column_at(kind, id);         if (def.is_atomic()) {             auto&& c = cell_or_collection.as_atomic_cell(def);             if (c.is_live(tomb, now, def.is_counter())) {                 any_live = true;                 return stop_iteration::yes;             }         } else {             auto mut = cell_or_collection.as_collection_mutation();             if (mut.is_any_live(*def.type, tomb, now)) {                 any_live = true;                 return stop_iteration::yes;             }         }         return stop_iteration::no;     });     return any_live; }
 std::ostream& operator<<(std::ostream& os, const std::pair<column_id, const atomic_cell_or_collection::printer&>& c) {     fmt::print(os, "{{column: {} {}}}", c.first, c.second);     return os; }
 // Transforms given range of printable into a range of strings where each element
// in the original range is prefxied with given string.
template<typename RangeOfPrintable> static auto prefixed(const sstring& prefix, const RangeOfPrintable& r) {     return r | boost::adaptors::transformed([&] (auto&& e) { return format("{}{}", prefix, e); }); }
 std::ostream& operator<<(std::ostream& os, const row::printer& p) {     auto& cells = p._row._cells;     os << "{{row:";     cells.walk([&] (column_id id, const cell_and_hash& cah) {         auto& cdef = p._schema.column_at(p._kind, id);         os << "\n    " << cdef.name_as_text() << atomic_cell_or_collection::printer(cdef, cah.cell);         return true;     });     return os << "}}"; }
 std::ostream& operator<<(std::ostream& os, const row_marker& rm) {     if (rm.is_missing()) {         fmt::print(os, "{{row_marker: }}");     } else if (rm._ttl == row_marker::dead) {         fmt::print(os, "{{row_marker: dead {} {}}}", rm._timestamp, rm._expiry.time_since_epoch().count());     } else {         fmt::print(os, "{{row_marker: {} {} {}}}", rm._timestamp, rm._ttl.count(),             rm._ttl != row_marker::no_ttl ? rm._expiry.time_since_epoch().count() : 0);     }     return os; }
 std::ostream& operator<<(std::ostream& os, const deletable_row::printer& p) {     auto& dr = p._deletable_row;     os << "{deletable_row: ";     if (!dr._marker.is_missing()) {         os << dr._marker << " ";     }     if (dr._deleted_at) {         os << dr._deleted_at << " ";     }     return os << row::printer(p._schema, column_kind::regular_column, dr._cells) << "}"; }
 std::ostream& operator<<(std::ostream& os, const rows_entry::printer& p) {     auto& re = p._rows_entry;     fmt::print(os, "{{rows_entry: cont={} dummy={} {} {}}}", re.continuous(), re.dummy(),                   position_in_partition_view::printer(p._schema, re.position()),                   deletable_row::printer(p._schema, re._row));     return os; }
 std::ostream& operator<<(std::ostream& os, const mutation_partition::printer& p) {     const auto indent = "  ";     auto& mp = p._mutation_partition;     os << "mutation_partition: {\n";     if (mp._tombstone) {         fmt::print(os, "{}tombstone: {},\n", indent, mp._tombstone);     }     if (!mp._row_tombstones.empty()) {         fmt::print(os, "{}range_tombstones: {{{}}},\n", indent, fmt::join(prefixed("\n    ", mp._row_tombstones), ","));     }     if (!mp.static_row().empty()) {         os << indent << "static_row: {\n";         const auto& srow = mp.static_row().get();         srow.for_each_cell([&] (column_id& c_id, const atomic_cell_or_collection& cell) {             auto& column_def = p._schema.column_at(column_kind::static_column, c_id);             os << indent << indent <<  "'" << column_def.name_as_text()                 << "': " << atomic_cell_or_collection::printer(column_def, cell) << ",\n";         });          os << indent << "},\n";     }     os << indent << "rows: [\n";     for (const auto& re : mp.clustered_rows()) {         os << indent << indent << "{\n";         const auto& row = re.row();         os << indent << indent << indent << "cont: " << re.continuous() << ",\n";         os << indent << indent << indent << "dummy: " << re.dummy() << ",\n";         if (!row.marker().is_missing()) {             os << indent << indent << indent << "marker: " << row.marker() << ",\n";         }         if (row.deleted_at()) {             os << indent << indent << indent << "tombstone: " << row.deleted_at() << ",\n";         }         position_in_partition pip(re.position());         if (pip.get_clustering_key_prefix()) {             os << indent << indent << indent << "position: {\n";             auto ck = *pip.get_clustering_key_prefix();             auto type_iterator = ck.get_compound_type(p._schema)->types().begin();             auto column_iterator = p._schema.clustering_key_columns().begin();             os << indent << indent << indent << indent << "bound_weight: " << int32_t(pip.get_bound_weight()) << ",\n";             for (auto&& e : ck.components(p._schema)) {                 os << indent << indent << indent << indent << "'" << column_iterator->name_as_text()                     << "': " << (*type_iterator)->to_string(to_bytes(e)) << ",\n";                 ++type_iterator;                 ++column_iterator;             }             os << indent << indent << indent << "},\n";         }         row.cells().for_each_cell([&] (column_id& c_id, const atomic_cell_or_collection& cell) {             auto& column_def = p._schema.column_at(column_kind::regular_column, c_id);             os << indent << indent << indent <<  "'" << column_def.name_as_text()                 << "': " << atomic_cell_or_collection::printer(column_def, cell) << ",\n";         });         os << indent << indent << "},\n";     }     os << indent << "]\n}";     return os; }
 constexpr gc_clock::duration row_marker::no_ttl;
 constexpr gc_clock::duration row_marker::dead;
 int compare_row_marker_for_merge(const row_marker& left, const row_marker& right) noexcept {     if (left.timestamp() != right.timestamp()) {         return left.timestamp() > right.timestamp() ? 1 : -1;     }     if (left.is_live() != right.is_live()) {         return left.is_live() ? -1 : 1;     }     if (left.is_live()) {         if (left.is_expiring() != right.is_expiring()) {             // prefer expiring cells.
            return left.is_expiring() ? 1 : -1;         }         if (left.is_expiring() && left.expiry() != right.expiry()) {             return left.expiry() < right.expiry() ? -1 : 1;         }     } else {         // Both are either deleted or missing
        if (left.deletion_time() != right.deletion_time()) {             // Origin compares big-endian serialized deletion time. That's because it
            // delegates to AbstractCell.reconcile() which compares values after
            // comparing timestamps, which in case of deleted cells will hold
            // serialized expiry.
            return (uint64_t) left.deletion_time().time_since_epoch().count()                    < (uint64_t) right.deletion_time().time_since_epoch().count() ? -1 : 1;         }     }     return 0; }
 bool deletable_row::equal(column_kind kind, const schema& s, const deletable_row& other, const schema& other_schema) const {     if (_deleted_at != other._deleted_at || _marker != other._marker) {         return false;     }     return _cells.equal(kind, s, other._cells, other_schema); }
 void deletable_row::apply(const schema& s, const deletable_row& src) {     apply_monotonically(s, src); }
 void deletable_row::apply(const schema& s, deletable_row&& src) {     apply_monotonically(s, std::move(src)); }
 void deletable_row::apply_monotonically(const schema& s, const deletable_row& src) {     _cells.apply(s, column_kind::regular_column, src._cells);     _marker.apply(src._marker);     _deleted_at.apply(src._deleted_at, _marker); }
 void deletable_row::apply_monotonically(const schema& s, deletable_row&& src) {     _cells.apply(s, column_kind::regular_column, std::move(src._cells));     _marker.apply(src._marker);     _deleted_at.apply(src._deleted_at, _marker); }
 bool rows_entry::equal(const schema& s, const rows_entry& other) const {     return equal(s, other, s); }
 bool rows_entry::equal(const schema& s, const rows_entry& other, const schema& other_schema) const {     position_in_partition::equal_compare eq(s);     return eq(position(), other.position())            && _range_tombstone == other._range_tombstone            && row().equal(column_kind::regular_column, s, other.row(), other_schema); }
 bool mutation_partition::equal(const schema& s, const mutation_partition& p) const {     return equal(s, p, s); }
 bool mutation_partition::equal(const schema& this_schema, const mutation_partition& p, const schema& p_schema) const {     if (_tombstone != p._tombstone) {         return false;     }     if (!boost::equal(non_dummy_rows(), p.non_dummy_rows(),         [&] (const rows_entry& e1, const rows_entry& e2) {             return e1.equal(this_schema, e2, p_schema);         }     )) {         return false;     }     if (!std::equal(_row_tombstones.begin(), _row_tombstones.end(),         p._row_tombstones.begin(), p._row_tombstones.end(),         [&] (const auto& rt1, const auto& rt2) { return rt1.tombstone().equal(this_schema, rt2.tombstone()); }     )) {         return false;     }     return _static_row.equal(column_kind::static_column, this_schema, p._static_row, p_schema); }
 bool mutation_partition::equal_continuity(const schema& s, const mutation_partition& p) const {     return _static_row_continuous == p._static_row_continuous         && get_continuity(s).equals(s, p.get_continuity(s)); }
 mutation_partition mutation_partition::sliced(const schema& s, const query::clustering_row_ranges& ranges) const {     auto p = mutation_partition(*this, s, ranges);     p._row_tombstones.trim(s, ranges);     return p; }
 static void apply_monotonically(const column_definition& def, cell_and_hash& dst,                     atomic_cell_or_collection& src, cell_hash_opt src_hash) {     if (def.is_atomic()) {         if (def.is_counter()) {             counter_cell_view::apply(def, dst.cell, src); // FIXME: Optimize
            dst.hash = { };         } else if (compare_atomic_cell_for_merge(dst.cell.as_atomic_cell(def), src.as_atomic_cell(def)) < 0) {             using std::swap;             swap(dst.cell, src);             dst.hash = std::move(src_hash);         }     } else {         dst.cell = merge(*def.type, dst.cell.as_collection_mutation(), src.as_collection_mutation());         dst.hash = { };     } }
 void row::apply(const column_definition& column, const atomic_cell_or_collection& value, cell_hash_opt hash) {     auto tmp = value.copy(*column.type);     apply_monotonically(column, std::move(tmp), std::move(hash)); }
 void row::apply(const column_definition& column, atomic_cell_or_collection&& value, cell_hash_opt hash) {     apply_monotonically(column, std::move(value), std::move(hash)); }
 template<typename Func> void row::consume_with(Func&& func) {     _cells.weed([func, this] (column_id id, cell_and_hash& cah) {         func(id, cah);         _size--;         return true;     }); }
 void row::apply_monotonically(const column_definition& column, atomic_cell_or_collection&& value, cell_hash_opt hash) {     static_assert(std::is_nothrow_move_constructible<atomic_cell_or_collection>::value                   && std::is_nothrow_move_assignable<atomic_cell_or_collection>::value,                   "noexcept required for atomicity");     // our mutations are not yet immutable
    auto id = column.id;     cell_and_hash* cah = _cells.get(id);     if (cah == nullptr) {         // FIXME -- add .locate method to radix_tree to find or allocate a spot
        _cells.emplace(id, std::move(value), std::move(hash));         _size++;     } else {         ::apply_monotonically(column, *cah, value, std::move(hash));     } }
 void row::append_cell(column_id id, atomic_cell_or_collection value) {     _cells.emplace(id, std::move(value), cell_hash_opt());     _size++; }
 const cell_and_hash* row::find_cell_and_hash(column_id id) const {     return _cells.get(id); }
 const atomic_cell_or_collection* row::find_cell(column_id id) const {     auto c_a_h = find_cell_and_hash(id);     return c_a_h ? &c_a_h->cell : nullptr; }
 size_t row::external_memory_usage(const schema& s, column_kind kind) const {     return _cells.memory_usage([&] (column_id id, const cell_and_hash& cah) noexcept {             auto& cdef = s.column_at(kind, id);             return cah.cell.external_memory_usage(*cdef.type);     }); }
 size_t rows_entry::memory_usage(const schema& s) const {     size_t size = 0;     if (!dummy()) {         size += key().external_memory_usage();     }     return size +            row().cells().external_memory_usage(s, column_kind::regular_column) +            sizeof(rows_entry); }
 size_t mutation_partition::external_memory_usage(const schema& s) const {     check_schema(s);     size_t sum = 0;     sum += static_row().external_memory_usage(s, column_kind::static_column);     sum += clustered_rows().external_memory_usage();     for (auto& clr : clustered_rows()) {         sum += clr.memory_usage(s);     }     sum += row_tombstones().external_memory_usage(s);     return sum; }
 template<bool reversed, typename Func> requires std::is_invocable_r_v<stop_iteration, Func, rows_entry&> void mutation_partition::trim_rows(const schema& s,     const std::vector<query::clustering_range>& row_ranges,     Func&& func) {     check_schema(s);     stop_iteration stop = stop_iteration::no;     auto last = reversal_traits<reversed>::begin(_rows);     auto deleter = current_deleter<rows_entry>();     auto range_begin = [this, &s] (const query::clustering_range& range) {         return reversed ? upper_bound(s, range) : lower_bound(s, range);     };     auto range_end = [this, &s] (const query::clustering_range& range) {         return reversed ? lower_bound(s, range) : upper_bound(s, range);     };     for (auto&& row_range : row_ranges) {         if (stop) {             break;         }         last = reversal_traits<reversed>::erase_and_dispose(_rows, last,             reversal_traits<reversed>::maybe_reverse(_rows, range_begin(row_range)), deleter);         auto end = reversal_traits<reversed>::maybe_reverse(_rows, range_end(row_range));         while (last != end && !stop) {             rows_entry& e = *last;             stop = func(e);             if (e.empty()) {                 last = reversal_traits<reversed>::erase_dispose_and_update_end(_rows, last, deleter, end);             } else {                 ++last;             }         }     }     reversal_traits<reversed>::erase_and_dispose(_rows, last, reversal_traits<reversed>::end(_rows), deleter); }
 uint32_t mutation_partition::do_compact(const schema& s,     const dht::decorated_key& dk,     gc_clock::time_point query_time,     const std::vector<query::clustering_range>& row_ranges,     bool always_return_static_content,     bool reverse,     uint64_t row_limit,     can_gc_fn& can_gc,     bool drop_tombstones_unconditionally,     const tombstone_gc_state& gc_state) {     check_schema(s);     assert(row_limit > 0);     auto gc_before = drop_tombstones_unconditionally ? gc_clock::time_point::max() :         gc_state.get_gc_before_for_key(s.shared_from_this(), dk, query_time);     auto should_purge_tombstone = [&] (const tombstone& t) {         return t.deletion_time < gc_before && can_gc(t);     };     bool static_row_live = _static_row.compact_and_expire(s, column_kind::static_column, row_tombstone(_tombstone),         query_time, can_gc, gc_before);     uint64_t row_count = 0;     auto row_callback = [&] (rows_entry& e) {         if (e.dummy()) {             return stop_iteration::no;         }         deletable_row& row = e.row();         tombstone tomb = range_tombstone_for_row(s, e.key());         bool is_live = row.compact_and_expire(s, tomb, query_time, can_gc, gc_before, nullptr);         return stop_iteration(is_live && ++row_count == row_limit);     };     if (reverse) {         trim_rows<true>(s, row_ranges, row_callback);     } else {         trim_rows<false>(s, row_ranges, row_callback);     }     // #589 - Do not add extra row for statics unless we did a CK range-less query.
    // See comment in query
    bool return_static_content_on_partition_with_no_rows = always_return_static_content || !has_ck_selector(row_ranges);     if (row_count == 0 && static_row_live && return_static_content_on_partition_with_no_rows) {         ++row_count;     }     _row_tombstones.erase_where([&] (auto&& rt) {         return should_purge_tombstone(rt.tomb) || rt.tomb <= _tombstone;     });     if (should_purge_tombstone(_tombstone)) {         _tombstone = tombstone();     }     // FIXME: purge unneeded prefix tombstones based on row_ranges
    return row_count; }
 uint64_t mutation_partition::compact_for_query(     const schema& s,     const dht::decorated_key& dk,     gc_clock::time_point query_time,     const std::vector<query::clustering_range>& row_ranges,     bool always_return_static_content,     bool reverse,     uint64_t row_limit) {     check_schema(s);     bool drop_tombstones_unconditionally = false;     // Replicas should only send non-purgeable tombstones already,
    // so we can expect to not have to actually purge any tombstones here.
    return do_compact(s, dk, query_time, row_ranges, always_return_static_content, reverse, row_limit, always_gc, drop_tombstones_unconditionally, tombstone_gc_state(nullptr)); }
 void mutation_partition::compact_for_compaction(const schema& s,     can_gc_fn& can_gc, const dht::decorated_key& dk, gc_clock::time_point compaction_time,     const tombstone_gc_state& gc_state) {     check_schema(s);     static const std::vector<query::clustering_range> all_rows = {         query::clustering_range::make_open_ended_both_sides()     };     bool drop_tombstones_unconditionally = false;     do_compact(s, dk, compaction_time, all_rows, true, false, query::partition_max_rows, can_gc, drop_tombstones_unconditionally, gc_state); }
 void mutation_partition::compact_for_compaction_drop_tombstones_unconditionally(const schema& s, const dht::decorated_key& dk) {     check_schema(s);     static const std::vector<query::clustering_range> all_rows = {         query::clustering_range::make_open_ended_both_sides()     };     bool drop_tombstones_unconditionally = true;     auto compaction_time = gc_clock::time_point::max();     do_compact(s, dk, compaction_time, all_rows, true, false, query::partition_max_rows, always_gc, drop_tombstones_unconditionally, tombstone_gc_state(nullptr)); }
 // Returns true if the mutation_partition represents no writes.
bool mutation_partition::empty() const {     if (_tombstone.timestamp != api::missing_timestamp) {         return false;     }     return !_static_row.size() && _rows.empty() && _row_tombstones.empty(); }
 bool deletable_row::is_live(const schema& s, column_kind kind, tombstone base_tombstone, gc_clock::time_point query_time) const {     // _created_at corresponds to the row marker cell, present for rows
    // created with the 'insert' statement. If row marker is live, we know the
    // row is live. Otherwise, a row is considered live if it has any cell
    // which is live.
    base_tombstone.apply(_deleted_at.tomb());     return _marker.is_live(base_tombstone, query_time) || _cells.is_live(s, kind, base_tombstone, query_time); }
 bool row::is_live(const schema& s, column_kind kind, tombstone base_tombstone, gc_clock::time_point query_time) const {     return has_any_live_data(s, kind, *this, base_tombstone, query_time); }
 bool mutation_partition::is_static_row_live(const schema& s, gc_clock::time_point query_time) const {     check_schema(s);     return has_any_live_data(s, column_kind::static_column, static_row().get(), _tombstone, query_time); }
 uint64_t mutation_partition::live_row_count(const schema& s, gc_clock::time_point query_time) const {     check_schema(s);     uint64_t count = 0;     for (const rows_entry& e : non_dummy_rows()) {         tombstone base_tombstone = range_tombstone_for_row(s, e.key());         if (e.row().is_live(s, column_kind::regular_column, base_tombstone, query_time)) {             ++count;         }     }     if (count == 0 && is_static_row_live(s, query_time)) {         return 1;     }     return count; }
 uint64_t mutation_partition::row_count() const {     return _rows.calculate_size(); }
 rows_entry::rows_entry(rows_entry&& o) noexcept     : evictable(std::move(o))     , _link(std::move(o._link))     , _key(std::move(o._key))     , _row(std::move(o._row))     , _range_tombstone(std::move(o._range_tombstone))     , _flags(std::move(o._flags)) { }
 void rows_entry::compact(const schema& s, tombstone t) {     can_gc_fn never_gc = [] (tombstone) { return false; };     row().compact_and_expire(s,                              t + _range_tombstone,                              gc_clock::time_point::min(),  // no TTL expiration
                             never_gc,                     // no GC
                             gc_clock::time_point::min()); // no GC
    // FIXME: Purge redundant _range_tombstone
}
 void rows_entry::replace_with(rows_entry&& o) noexcept {     swap(o);     _range_tombstone = std::move(o._range_tombstone);     _row = std::move(o._row); }
 row::row(const schema& s, column_kind kind, const row& o) : _size(o._size) {     auto clone_cell_and_hash = [&s, &kind] (column_id id, const cell_and_hash& cah) {         auto& cdef = s.column_at(kind, id);         return cell_and_hash(cah.cell.copy(*cdef.type), cah.hash);     };     _cells.clone_from(o._cells, clone_cell_and_hash); }
 row::~row() { }
 const atomic_cell_or_collection& row::cell_at(column_id id) const {     auto&& cell = find_cell(id);     if (!cell) {         throw_with_backtrace<std::out_of_range>(format("Column not found for id = {:d}", id));     }     return *cell; }
 bool row::equal(column_kind kind, const schema& this_schema, const row& other, const schema& other_schema) const {     if (size() != other.size()) {         return false;     }     auto cells_equal = [&] (column_id id1, const atomic_cell_or_collection& c1,                             column_id id2, const atomic_cell_or_collection& c2) {         static_assert(schema::row_column_ids_are_ordered_by_name::value, "Relying on column ids being ordered by name");         auto& at1 = *this_schema.column_at(kind, id1).type;         auto& at2 = *other_schema.column_at(kind, id2).type;         return at1 == at2                && this_schema.column_at(kind, id1).name() == other_schema.column_at(kind, id2).name()                && c1.equals(at1, c2);     };     auto i1 = _cells.begin();     auto i1_end = _cells.end();     auto i2 = other._cells.begin();     auto i2_end = other._cells.end();     while (true) {         if (i1 == i1_end) {             return i2 == i2_end;         }         if (i2 == i2_end) {             return i1 == i1_end;         }         if (!cells_equal(i1.key(), i1->cell, i2.key(), i2->cell)) {             return false;         }         i1++;         i2++;     } }
 row::row() { }
 row::row(row&& other) noexcept     : _size(other._size), _cells(std::move(other._cells)) {     other._size = 0; }
 row& row::operator=(row&& other) noexcept {     if (this != &other) {         this->~row();         new (this) row(std::move(other));     }     return *this; }
 void row::apply(const schema& s, column_kind kind, const row& other) {     if (other.empty()) {         return;     }     other.for_each_cell([&] (column_id id, const cell_and_hash& c_a_h) {         apply(s.column_at(kind, id), c_a_h.cell, c_a_h.hash);     }); }
 void row::apply(const schema& s, column_kind kind, row&& other) {     apply_monotonically(s, kind, std::move(other)); }
 void row::apply_monotonically(const schema& s, column_kind kind, row&& other) {     if (other.empty()) {         return;     }     other.consume_with([&] (column_id id, cell_and_hash& c_a_h) {         apply_monotonically(s.column_at(kind, id), std::move(c_a_h.cell), std::move(c_a_h.hash));     }); }
 // When views contain a primary key column that is not part of the base table primary key,
// that column determines whether the row is live or not. We need to ensure that when that
// cell is dead, and thus the derived row marker, either by normal deletion of by TTL, so
// is the rest of the row. To ensure that none of the regular columns keep the row alive,
// we erase the live cells according to the shadowable_tombstone rules.
static bool dead_marker_shadows_row(const schema& s, column_kind kind, const row_marker& marker) {     return s.is_view()             && s.view_info()->has_base_non_pk_columns_in_view_pk()             && !marker.is_live()             && kind == column_kind::regular_column; // not applicable to static rows
}
 bool row::compact_and_expire(         const schema& s,         column_kind kind,         row_tombstone tomb,         gc_clock::time_point query_time,         can_gc_fn& can_gc,         gc_clock::time_point gc_before,         const row_marker& marker,         compaction_garbage_collector* collector) {     if (dead_marker_shadows_row(s, kind, marker)) {         tomb.apply(shadowable_tombstone(api::max_timestamp, gc_clock::time_point::max()), row_marker());     }     bool any_live = false;     remove_if([&] (column_id id, atomic_cell_or_collection& c) {         bool erase = false;         const column_definition& def = s.column_at(kind, id);         if (def.is_atomic()) {             atomic_cell_view cell = c.as_atomic_cell(def);             auto can_erase_cell = [&] {                 return cell.deletion_time() < gc_before && can_gc(tombstone(cell.timestamp(), cell.deletion_time()));             };             if (cell.is_covered_by(tomb.regular(), def.is_counter())) {                 erase = true;             } else if (cell.is_covered_by(tomb.shadowable().tomb(), def.is_counter())) {                 erase = true;             } else if (cell.has_expired(query_time)) {                 erase = can_erase_cell();                 if (!erase) {                     c = atomic_cell::make_dead(cell.timestamp(), cell.deletion_time());                 } else if (collector) {                     collector->collect(id, atomic_cell::make_dead(cell.timestamp(), cell.deletion_time()));                 }             } else if (!cell.is_live()) {                 erase = can_erase_cell();                 if (erase && collector) {                     collector->collect(id, atomic_cell::make_dead(cell.timestamp(), cell.deletion_time()));                 }             } else {                 any_live = true;             }         } else {             c.as_collection_mutation().with_deserialized(*def.type, [&] (collection_mutation_view_description m_view) {                 auto m = m_view.materialize(*def.type);                 any_live |= m.compact_and_expire(id, tomb, query_time, can_gc, gc_before, collector);                 if (m.cells.empty() && m.tomb <= tomb.tomb()) {                     erase = true;                 } else {                     c = m.serialize(*def.type);                 }             });         }         return erase;     });     return any_live; }
 bool row::compact_and_expire(         const schema& s,         column_kind kind,         row_tombstone tomb,         gc_clock::time_point query_time,         can_gc_fn& can_gc,         gc_clock::time_point gc_before,         compaction_garbage_collector* collector) {     row_marker m;     return compact_and_expire(s, kind, tomb, query_time, can_gc, gc_before, m, collector); }
 bool lazy_row::compact_and_expire(         const schema& s,         column_kind kind,         row_tombstone tomb,         gc_clock::time_point query_time,         can_gc_fn& can_gc,         gc_clock::time_point gc_before,         const row_marker& marker,         compaction_garbage_collector* collector) {     if (!_row) {         return false;     }     return _row->compact_and_expire(s, kind, tomb, query_time, can_gc, gc_before, marker, collector); }
 bool lazy_row::compact_and_expire(         const schema& s,         column_kind kind,         row_tombstone tomb,         gc_clock::time_point query_time,         can_gc_fn& can_gc,         gc_clock::time_point gc_before,         compaction_garbage_collector* collector) {     if (!_row) {         return false;     }     return _row->compact_and_expire(s, kind, tomb, query_time, can_gc, gc_before, collector); }
 std::ostream& operator<<(std::ostream& os, const lazy_row::printer& p) {     return os << row::printer(p._schema, p._kind, p._row.get()); }
 bool deletable_row::compact_and_expire(const schema& s,                                        tombstone tomb,                                        gc_clock::time_point query_time,                                        can_gc_fn& can_gc,                                        gc_clock::time_point gc_before,                                        compaction_garbage_collector* collector) {     auto should_purge_row_tombstone = [&] (const row_tombstone& t) {         return t.max_deletion_time() < gc_before && can_gc(t.tomb());     };     apply(tomb);     bool is_live = marker().compact_and_expire(deleted_at().tomb(), query_time, can_gc, gc_before);     is_live |= cells().compact_and_expire(s, column_kind::regular_column, deleted_at(), query_time, can_gc, gc_before, marker(), collector);     if (deleted_at().tomb() <= tomb || should_purge_row_tombstone(deleted_at())) {         remove_tombstone();     }     return is_live; }
 deletable_row deletable_row::difference(const schema& s, column_kind kind, const deletable_row& other) const {     deletable_row dr;     if (_deleted_at > other._deleted_at) {         dr.apply(_deleted_at);     }     if (compare_row_marker_for_merge(_marker, other._marker) > 0) {         dr.apply(_marker);     }     dr._cells = _cells.difference(s, kind, other._cells);     return dr; }
 row row::difference(const schema& s, column_kind kind, const row& other) const {     row r;     auto c = _cells.begin();     auto c_end = _cells.end();     auto it = other._cells.begin();     auto it_end = other._cells.end();     while (c != c_end) {         while (it != it_end && it.key() < c.key()) {             ++it;         }         auto& cdef = s.column_at(kind, c.key());         if (it == it_end || it.key() != c.key()) {             r.append_cell(c.key(), c->cell.copy(*cdef.type));         } else if (cdef.is_counter()) {             auto cell = counter_cell_view::difference(c->cell.as_atomic_cell(cdef), it->cell.as_atomic_cell(cdef));             if (cell) {                 r.append_cell(c.key(), std::move(*cell));             }         } else if (s.column_at(kind, c.key()).is_atomic()) {             if (compare_atomic_cell_for_merge(c->cell.as_atomic_cell(cdef), it->cell.as_atomic_cell(cdef)) > 0) {                 r.append_cell(c.key(), c->cell.copy(*cdef.type));             }         } else {             auto diff = ::difference(*s.column_at(kind, c.key()).type,                     c->cell.as_collection_mutation(), it->cell.as_collection_mutation());             if (!static_cast<collection_mutation_view>(diff).is_empty()) {                 r.append_cell(c.key(), std::move(diff));             }         }         c++;     }     return r; }
 bool row_marker::compact_and_expire(tombstone tomb, gc_clock::time_point now,         can_gc_fn& can_gc, gc_clock::time_point gc_before, compaction_garbage_collector* collector) {     if (is_missing()) {         return false;     }     if (_timestamp <= tomb.timestamp) {         _timestamp = api::missing_timestamp;         return false;     }     if (_ttl > no_ttl && _expiry <= now) {         _expiry -= _ttl;         _ttl = dead;     }     if (_ttl == dead && _expiry < gc_before && can_gc(tombstone(_timestamp, _expiry))) {         if (collector) {             collector->collect(*this);         }         _timestamp = api::missing_timestamp;     }     return !is_missing() && _ttl != dead; }
 mutation_partition mutation_partition::difference(schema_ptr s, const mutation_partition& other) const {     check_schema(*s);     mutation_partition mp(s);     if (_tombstone > other._tombstone) {         mp.apply(_tombstone);     }     mp._static_row = _static_row.difference(*s, column_kind::static_column, other._static_row);     mp._row_tombstones = _row_tombstones.difference(*s, other._row_tombstones);     auto it_r = other._rows.begin();     rows_entry::compare cmp_r(*s);     for (auto&& r : _rows) {         if (r.dummy()) {             continue;         }         while (it_r != other._rows.end() && (it_r->dummy() || cmp_r(*it_r, r))) {             ++it_r;         }         if (it_r == other._rows.end() || !it_r->key().equal(*s, r.key())) {             mp.insert_row(*s, r.key(), r.row());         } else {             auto dr = r.row().difference(*s, column_kind::regular_column, it_r->row());             if (!dr.empty()) {                 mp.insert_row(*s, r.key(), std::move(dr));             }         }     }     return mp; }
 void mutation_partition::accept(const schema& s, mutation_partition_visitor& v) const {     check_schema(s);     v.accept_partition_tombstone(_tombstone);     _static_row.for_each_cell([&] (column_id id, const atomic_cell_or_collection& cell) {         const column_definition& def = s.static_column_at(id);         if (def.is_atomic()) {             v.accept_static_cell(id, cell.as_atomic_cell(def));         } else {             v.accept_static_cell(id, cell.as_collection_mutation());         }     });     for (const auto& rt : _row_tombstones) {         v.accept_row_tombstone(rt.tombstone());     }     for (const rows_entry& e : _rows) {         const deletable_row& dr = e.row();         v.accept_row(e.position(), dr.deleted_at(), dr.marker(), e.dummy(), e.continuous());         dr.cells().for_each_cell([&] (column_id id, const atomic_cell_or_collection& cell) {             const column_definition& def = s.regular_column_at(id);             if (def.is_atomic()) {                 v.accept_row_cell(id, cell.as_atomic_cell(def));             } else {                 v.accept_row_cell(id, cell.as_collection_mutation());             }         });     } }
 void mutation_partition::upgrade(const schema& old_schema, const schema& new_schema) {     // We need to copy to provide strong exception guarantees.
    mutation_partition tmp(new_schema.shared_from_this());     tmp.set_static_row_continuous(_static_row_continuous);     converting_mutation_partition_applier v(old_schema.get_column_mapping(), new_schema, tmp);     accept(old_schema, v);     *this = std::move(tmp); }
 mutation_querier::mutation_querier(const schema& s, query::result::partition_writer pw,                                    query::result_memory_accounter& memory_accounter)     : _schema(s)     , _memory_accounter(memory_accounter)     , _pw(std::move(pw))     , _static_cells_wr(pw.start().start_static_row().start_cells()) { }
 void mutation_querier::query_static_row(const row& r, tombstone current_tombstone) {     const query::partition_slice& slice = _pw.slice();     if (!slice.static_columns.empty()) {         if (_pw.requested_result()) {             auto start = _static_cells_wr._out.size();             get_compacted_row_slice(_schema, slice, column_kind::static_column,                                     r, slice.static_columns, _static_cells_wr);             _memory_accounter.update(_static_cells_wr._out.size() - start);         } else {             seastar::measuring_output_stream stream;             ser::qr_partition__static_row__cells<seastar::measuring_output_stream> out(stream, { });             auto start = stream.size();             get_compacted_row_slice(_schema, slice, column_kind::static_column,                                     r, slice.static_columns, out);             _memory_accounter.update(stream.size() - start);         }         if (_pw.requested_digest()) {             max_timestamp max_ts{_pw.last_modified()};             _pw.digest().feed_hash(current_tombstone);             max_ts.update(current_tombstone.timestamp);             _pw.digest().feed_hash(r, _schema, column_kind::static_column, slice.static_columns, max_ts);             _pw.last_modified() = max_ts.max;         }     }     _rows_wr.emplace(std::move(_static_cells_wr).end_cells().end_static_row().start_rows()); }
 stop_iteration mutation_querier::consume(static_row&& sr, tombstone current_tombstone) {     query_static_row(sr.cells(), current_tombstone);     _live_data_in_static_row = true;     return stop_iteration::no; }
 void mutation_querier::prepare_writers() {     if (!_rows_wr) {         row empty_row;         query_static_row(empty_row, { });         _live_data_in_static_row = false;     } }
 stop_iteration mutation_querier::consume(clustering_row&& cr, row_tombstone current_tombstone) {     prepare_writers();     const query::partition_slice& slice = _pw.slice();     if (_pw.requested_digest()) {         _pw.digest().feed_hash(cr.key(), _schema);         _pw.digest().feed_hash(current_tombstone);         max_timestamp max_ts{_pw.last_modified()};         max_ts.update(current_tombstone.tomb().timestamp);         _pw.digest().feed_hash(cr.cells(), _schema, column_kind::regular_column, slice.regular_columns, max_ts);         _pw.last_modified() = max_ts.max;     }     auto write_row = [&] (auto& rows_writer) {         auto cells_wr = [&] {             if (slice.options.contains(query::partition_slice::option::send_clustering_key)) {                 return rows_writer.add().write_key(cr.key()).start_cells().start_cells();             } else {                 return rows_writer.add().skip_key().start_cells().start_cells();             }         }();         get_compacted_row_slice(_schema, slice, column_kind::regular_column, cr.cells(), slice.regular_columns, cells_wr);         std::move(cells_wr).end_cells().end_cells().end_qr_clustered_row();     };     auto stop = stop_iteration::no;     if (_pw.requested_result()) {         auto start = _rows_wr->_out.size();         write_row(*_rows_wr);         stop = _memory_accounter.update_and_check(_rows_wr->_out.size() - start);     } else {         seastar::measuring_output_stream stream;         ser::qr_partition__rows<seastar::measuring_output_stream> out(stream, { });         auto start = stream.size();         write_row(out);         stop = _memory_accounter.update_and_check(stream.size() - start);     }     _live_clustering_rows++;     return stop; }
 uint64_t mutation_querier::consume_end_of_stream() {     prepare_writers();     // If we got no rows, but have live static columns, we should only
    // give them back IFF we did not have any CK restrictions.
    // #589
    // If ck:s exist, and we do a restriction on them, we either have maching
    // rows, or return nothing, since cql does not allow "is null".
    bool return_static_content_on_partition_with_no_rows =         _pw.slice().options.contains(query::partition_slice::option::always_return_static_content) ||         !has_ck_selector(_pw.ranges());     if (!_live_clustering_rows && (!return_static_content_on_partition_with_no_rows || !_live_data_in_static_row)) {         _pw.retract();         return 0;     } else {         auto live_rows = std::max(_live_clustering_rows, uint64_t(1));         _pw.row_count() += live_rows;         _pw.partition_count() += 1;         std::move(*_rows_wr).end_rows().end_qr_partition();         return live_rows;     } }
 query_result_builder::query_result_builder(const schema& s, query::result::builder& rb) noexcept     : _schema(s), _rb(rb) { }
 void query_result_builder::consume_new_partition(const dht::decorated_key& dk) {     _mutation_consumer.emplace(mutation_querier(_schema, _rb.add_partition(_schema, dk.key()), _rb.memory_accounter())); }
 void query_result_builder::consume(tombstone t) {     _mutation_consumer->consume(t);     _stop = _rb.bump_and_check_tombstone_limit(); }
 stop_iteration query_result_builder::consume(static_row&& sr, tombstone t, bool is_live) {     if (!is_live) {         _stop = _rb.bump_and_check_tombstone_limit();         return _stop;     }     _stop = _mutation_consumer->consume(std::move(sr), t);     return _stop; }
 stop_iteration query_result_builder::consume(clustering_row&& cr, row_tombstone t,  bool is_live) {     if (!is_live) {         _stop = _rb.bump_and_check_tombstone_limit();         return _stop;     }     _stop = _mutation_consumer->consume(std::move(cr), t);     return _stop; }
 stop_iteration query_result_builder::consume(range_tombstone_change&& rtc) {     _stop = _rb.bump_and_check_tombstone_limit();     return _stop; }
 stop_iteration query_result_builder::consume_end_of_partition() {     auto live_rows_in_partition = _mutation_consumer->consume_end_of_stream();     if (live_rows_in_partition > 0 && !_stop) {         _stop = _rb.memory_accounter().check();     }     if (_stop) {         _rb.mark_as_short_read();     }     return _stop; }
 void query_result_builder::consume_end_of_stream() { }
 stop_iteration query::result_memory_accounter::check_local_limit() const {     if (_short_read_allowed) {         return stop_iteration(_total_used_memory > _maximum_result_size.get_page_size());     } else {         if (_total_used_memory > _maximum_result_size.hard_limit) {             throw std::runtime_error(fmt::format(                     "Memory usage of unpaged query exceeds hard limit of {} (configured via max_memory_for_unlimited_query_hard_limit)",                     _maximum_result_size.hard_limit));         }         if (_below_soft_limit && _total_used_memory > _maximum_result_size.soft_limit) {             mplog.warn(                     "Memory usage of unpaged query exceeds soft limit of {} (configured via max_memory_for_unlimited_query_soft_limit)",                     _maximum_result_size.soft_limit);             _below_soft_limit = false;         }     }     return stop_iteration::no; }
 void reconcilable_result_builder::consume_new_partition(const dht::decorated_key& dk) {     _rt_assembler.reset();     _return_static_content_on_partition_with_no_rows =         _slice.options.contains(query::partition_slice::option::always_return_static_content) ||         !has_ck_selector(_slice.row_ranges(_schema, dk.key()));     _static_row_is_alive = false;     _live_rows = 0;     _mutation_consumer.emplace(streamed_mutation_freezer(_schema, dk.key(), _reversed)); }
 void reconcilable_result_builder::consume(tombstone t) {     _mutation_consumer->consume(t); }
 stop_iteration reconcilable_result_builder::consume(static_row&& sr, tombstone, bool is_alive) {     _static_row_is_alive = is_alive;     _memory_accounter.update(sr.memory_usage(_schema));     return _mutation_consumer->consume(std::move(sr)); }
 stop_iteration reconcilable_result_builder::consume(clustering_row&& cr, row_tombstone, bool is_alive) {     if (_rt_assembler.needs_flush()) {         if (auto rt_opt = _rt_assembler.flush(_schema, position_in_partition::after_key(_schema, cr.key()))) {             consume(std::move(*rt_opt));         }     }     _live_rows += is_alive;     auto stop = _memory_accounter.update_and_check(cr.memory_usage(_schema));     if (is_alive) {         // We are considering finishing current read only after consuming a
        // live clustering row. While sending a single live row is enough to
        // guarantee progress, not ending the result on a live row would
        // mean that the next page fetch will read all tombstones after the
        // last live row again.
        _stop = stop;     }     return _mutation_consumer->consume(std::move(cr)) || _stop; }
 stop_iteration reconcilable_result_builder::consume(range_tombstone&& rt) {     _memory_accounter.update(rt.memory_usage(_schema));     if (_reversed) {         // undo reversing done for the native reversed format, coordinator still uses old reversing format
        rt.reverse();     }     return _mutation_consumer->consume(std::move(rt)); }
 stop_iteration reconcilable_result_builder::consume(range_tombstone_change&& rtc) {     if (auto rt_opt = _rt_assembler.consume(_schema, std::move(rtc))) {         return consume(std::move(*rt_opt));     }     return stop_iteration::no; }
 stop_iteration reconcilable_result_builder::consume_end_of_partition() {     _rt_assembler.on_end_of_stream();     if (_live_rows == 0 && _static_row_is_alive && _return_static_content_on_partition_with_no_rows) {         ++_live_rows;         // Normally we count only live clustering rows, to guarantee that
        // the next page fetch won't ask for the same range. However,
        // if we return just a single static row we can stop the result as
        // well. Next page fetch will ask for the next partition and if we
        // don't do that we could end up with an unbounded number of
        // partitions with only a static row.
        _stop = _stop || _memory_accounter.check();     }     _total_live_rows += _live_rows;     _result.emplace_back(partition { _live_rows, _mutation_consumer->consume_end_of_stream() });     return _stop; }
 reconcilable_result reconcilable_result_builder::consume_end_of_stream() {     return reconcilable_result(_total_live_rows, std::move(_result),                                query::short_read(bool(_stop)),                                std::move(_memory_accounter).done()); }
 future<query::result> to_data_query_result(const reconcilable_result& r, schema_ptr s, const query::partition_slice& slice, uint64_t max_rows, uint32_t max_partitions,         query::result_options opts) {     // This result was already built with a limit, don't apply another one.
    query::result::builder builder(slice, opts, query::result_memory_accounter{ query::result_memory_limiter::unlimited_result_size }, query::max_tombstones);     auto consumer = compact_for_query_v2<query_result_builder>(*s, gc_clock::time_point::min(), slice, max_rows,             max_partitions, query_result_builder(*s, builder));     auto compaction_state = consumer.get_state();     const auto reverse = slice.options.contains(query::partition_slice::option::reversed) ? consume_in_reverse::yes : consume_in_reverse::no;     // FIXME: frozen_mutation::consume supports only forward consumers
    if (reverse == consume_in_reverse::no) {         frozen_mutation_consumer_adaptor adaptor(s, consumer);         for (const partition& p : r.partitions()) {             const auto res = co_await p.mut().consume_gently(s, adaptor);             if (res.stop == stop_iteration::yes) {                 break;             }         }     } else {         for (const partition& p : r.partitions()) {             auto m = co_await p.mut().unfreeze_gently(s);             const auto res = co_await std::move(m).consume_gently(consumer, reverse);             if (res.stop == stop_iteration::yes) {                 break;             }         }     }     if (r.is_short_read()) {         builder.mark_as_short_read();     }     co_return builder.build(compaction_state->current_full_position()); }
 query::result query_mutation(mutation&& m, const query::partition_slice& slice, uint64_t row_limit, gc_clock::time_point now, query::result_options opts) {     query::result::builder builder(slice, opts, query::result_memory_accounter{ query::result_memory_limiter::unlimited_result_size }, query::max_tombstones);     auto consumer = compact_for_query_v2<query_result_builder>(*m.schema(), now, slice, row_limit,             query::max_partitions, query_result_builder(*m.schema(), builder));     auto compaction_state = consumer.get_state();     const auto reverse = slice.options.contains(query::partition_slice::option::reversed) ? consume_in_reverse::yes : consume_in_reverse::no;     std::move(m).consume(consumer, reverse);     return builder.build(compaction_state->current_full_position()); }
 class counter_write_query_result_builder {     const schema& _schema;     mutation_opt _mutation; public:     counter_write_query_result_builder(const schema& s) : _schema(s) { }     void consume_new_partition(const dht::decorated_key& dk) {         _mutation = mutation(_schema.shared_from_this(), dk);     }     void consume(tombstone) { }     stop_iteration consume(static_row&& sr, tombstone, bool is_live) {         if (!is_live) {             return stop_iteration::no;         }         _mutation->partition().static_row().maybe_create() = std::move(sr.cells());         return stop_iteration::no;     }     stop_iteration consume(clustering_row&& cr, row_tombstone,  bool is_live) {         if (!is_live) {             return stop_iteration::no;         }         _mutation->partition().insert_row(_schema, cr.key(), std::move(cr).as_deletable_row());         return stop_iteration::no;     }     stop_iteration consume(range_tombstone_change&& rtc) {         return stop_iteration::no;     }     stop_iteration consume_end_of_partition() {         return stop_iteration::no;     }     mutation_opt consume_end_of_stream() {         return std::move(_mutation);     } };
 mutation_partition::mutation_partition(mutation_partition::incomplete_tag, const schema& s, tombstone t)     : _tombstone(t)     , _static_row_continuous(!s.has_static_columns())     , _rows()     , _row_tombstones(s) {     auto e = alloc_strategy_unique_ptr<rows_entry>(             current_allocator().construct<rows_entry>(s, rows_entry::last_dummy_tag(), is_continuous::no));     _rows.insert_before(_rows.end(), std::move(e)); }
 bool mutation_partition::is_fully_continuous() const {     if (!_static_row_continuous) {         return false;     }     for (auto&& row : _rows) {         if (!row.continuous()) {             return false;         }     }     return true; }
 void mutation_partition::make_fully_continuous() {     _static_row_continuous = true;     auto i = _rows.begin();     while (i != _rows.end()) {         if (i->dummy()) {             i = _rows.erase_and_dispose(i, alloc_strategy_deleter<rows_entry>());         } else {             i->set_continuous(true);             ++i;         }     } }
 void mutation_partition::set_continuity(const schema& s, const position_range& pr, is_continuous cont) {     auto cmp = rows_entry::tri_compare(s);     if (cmp(pr.start(), pr.end()) >= 0) {         return; // empty range
    }     auto end = _rows.lower_bound(pr.end(), cmp);     if (end == _rows.end() || cmp(pr.end(), end->position()) < 0) {         auto e = alloc_strategy_unique_ptr<rows_entry>(                 current_allocator().construct<rows_entry>(s, pr.end(), is_dummy::yes,                     end == _rows.end() ? is_continuous::yes : end->continuous()));         end = _rows.insert_before(end, std::move(e));     }     auto i = _rows.lower_bound(pr.start(), cmp);     if (cmp(pr.start(), i->position()) < 0) {         auto e = alloc_strategy_unique_ptr<rows_entry>(                 current_allocator().construct<rows_entry>(s, pr.start(), is_dummy::yes, i->continuous()));         i = _rows.insert_before(i, std::move(e));     }     assert(i != end);     ++i;     while (1) {         i->set_continuous(cont);         if (i == end) {             break;         }         if (i->dummy()) {             i = _rows.erase_and_dispose(i, alloc_strategy_deleter<rows_entry>());         } else {             ++i;         }     } }
 clustering_interval_set mutation_partition::get_continuity(const schema& s, is_continuous cont) const {     check_schema(s);     clustering_interval_set result;     auto i = _rows.begin();     auto prev_pos = position_in_partition::before_all_clustered_rows();     while (i != _rows.end()) {         if (i->continuous() == cont) {             result.add(s, position_range(std::move(prev_pos), position_in_partition(i->position())));         }         if (i->position().is_clustering_row() && bool(i->dummy()) == !bool(cont)) {             result.add(s, position_range(position_in_partition(i->position()),                 position_in_partition::after_key(s, i->position().key())));         }         prev_pos = i->position().is_clustering_row()             ? position_in_partition::after_key(s, i->position().key())             : position_in_partition(i->position());         ++i;     }     if (cont) {         result.add(s, position_range(std::move(prev_pos), position_in_partition::after_all_clustered_rows()));     }     return result; }
 stop_iteration mutation_partition::clear_gently(cache_tracker* tracker) noexcept {     if (_row_tombstones.clear_gently() == stop_iteration::no) {         return stop_iteration::no;     }     auto del = current_deleter<rows_entry>();     auto i = _rows.begin();     auto end = _rows.end();     while (i != end) {         if (tracker) {             tracker->remove(*i);         }         i = _rows.erase_and_dispose(i, del);         // The iterator comparison below is to not defer destruction of now empty
        // mutation_partition objects. Not doing this would cause eviction to leave garbage
        // versions behind unnecessarily.
        if (need_preempt() && i != end) {             return stop_iteration::no;         }     }     return stop_iteration::yes; }
 bool mutation_partition::check_continuity(const schema& s, const position_range& r, is_continuous cont) const {     check_schema(s);     auto cmp = rows_entry::tri_compare(s);     auto i = _rows.lower_bound(r.start(), cmp);     auto end = _rows.lower_bound(r.end(), cmp);     if (cmp(r.start(), r.end()) >= 0) {         return bool(cont);     }     if (i != end) {         if (no_clustering_row_between(s, r.start(), i->position())) {             ++i;         }         while (i != end) {             if (i->continuous() != cont) {                 return false;             }             ++i;         }         if (end != _rows.begin() && no_clustering_row_between(s, std::prev(end)->position(), r.end())) {             return true;         }     }     return (end == _rows.end() ? is_continuous::yes : end->continuous()) == cont; }
 bool mutation_partition::fully_continuous(const schema& s, const position_range& r) {     return check_continuity(s, r, is_continuous::yes); }
 bool mutation_partition::fully_discontinuous(const schema& s, const position_range& r) {     return check_continuity(s, r, is_continuous::no); }
  mutation_cleaner_impl::~mutation_cleaner_impl() {     _worker_state->done = true;     _worker_state->cv.signal();     _worker_state->snapshots.clear_and_dispose(typename lw_shared_ptr<partition_snapshot>::disposer());     with_allocator(_region.allocator(), [this] {         clear();     }); }
 void mutation_cleaner_impl::clear() noexcept {     while (clear_gently() == stop_iteration::no) ; }
 stop_iteration mutation_cleaner_impl::clear_gently() noexcept {     while (clear_some() == memory::reclaiming_result::reclaimed_something) {         if (need_preempt()) {             return stop_iteration::no;         }     }     return stop_iteration::yes; }
 memory::reclaiming_result mutation_cleaner_impl::clear_some() noexcept {     if (_versions.empty()) {         return memory::reclaiming_result::reclaimed_nothing;     }     auto&& alloc = current_allocator();     partition_version& pv = _versions.front();     if (pv.clear_gently(_tracker) == stop_iteration::yes) {         _versions.pop_front();         alloc.destroy(&pv);     }     return memory::reclaiming_result::reclaimed_something; }
 void mutation_cleaner_impl::merge(mutation_cleaner_impl& r) noexcept {     _versions.splice(r._versions);     for (partition_snapshot& snp : r._worker_state->snapshots) {         snp.migrate(&_region, _cleaner);     }     _worker_state->snapshots.splice(_worker_state->snapshots.end(), r._worker_state->snapshots);     if (!_worker_state->snapshots.empty()) {         _worker_state->cv.signal();     } }
 void mutation_cleaner_impl::start_worker() {     auto f = repeat([w = _worker_state, this] () mutable noexcept {       if (w->done) {           return make_ready_future<stop_iteration>(stop_iteration::yes);       }       return with_scheduling_group(_scheduling_group, [w, this] {         return w->cv.wait([w] {             return w->done || !w->snapshots.empty();         }).then([this, w] () noexcept {             if (w->done) {                 return stop_iteration::yes;             }             merge_some();             return stop_iteration::no;         });       });     });     if (f.failed()) {         f.get();     } }
 stop_iteration mutation_cleaner_impl::merge_some(partition_snapshot& snp) noexcept {     auto&& region = snp.region();     return with_allocator(region.allocator(), [&] {         {             // Allocating sections require the region to be reclaimable
            // which means that they cannot be nested.
            // It is, however, possible, that if the snapshot is taken
            // inside an allocating section and then an exception is thrown
            // this function will be called to clean up even though we
            // still will be in the context of the allocating section.
            if (!region.reclaiming_enabled()) {                 return stop_iteration::no;             }             try {                 auto dirty_guard = make_region_space_guard();                 return _worker_state->alloc_section(region, [&] {                     return snp.merge_partition_versions(_app_stats);                 });             } catch (...) {                 // Merging failed, give up as there is no guarantee of forward progress.
                return stop_iteration::yes;             }         }     }); }
 stop_iteration mutation_cleaner_impl::merge_some() noexcept {     if (_worker_state->snapshots.empty()) {         return stop_iteration::yes;     }     partition_snapshot& snp = _worker_state->snapshots.front();     if (merge_some(snp) == stop_iteration::yes) {         _worker_state->snapshots.pop_front();         lw_shared_ptr<partition_snapshot>::dispose(&snp);     }     return stop_iteration::no; }
 future<> mutation_cleaner_impl::drain() {     return repeat([this] {         return merge_some();     }).then([this] {         return repeat([this] {             return with_allocator(_region.allocator(), [this] {                 return clear_gently();             });         });     }); }
 can_gc_fn always_gc = [] (tombstone) { return true; };
 logging::logger compound_logger("compound");
 extern logging::logger mplog;
 mutation_partition_v2::mutation_partition_v2(const schema& s, const mutation_partition_v2& x)         : _tombstone(x._tombstone)         , _static_row(s, column_kind::static_column, x._static_row)         , _static_row_continuous(x._static_row_continuous)         , _rows() {     auto cloner = [&s] (const rows_entry* x) -> rows_entry* {         return current_allocator().construct<rows_entry>(s, *x);     };     _rows.clone_from(x._rows, cloner, current_deleter<rows_entry>()); }
 mutation_partition_v2::mutation_partition_v2(const schema& s, mutation_partition&& x)     : _tombstone(x.partition_tombstone())     , _static_row(std::move(x.static_row()))     , _static_row_continuous(x.static_row_continuous())     , _rows(std::move(x.mutable_clustered_rows())) {     auto&& tombstones = x.mutable_row_tombstones();     if (!tombstones.empty()) {         try {             mutation_partition_v2 p(s.shared_from_this());             for (auto&& t: tombstones) {                 range_tombstone & rt = t.tombstone();                 p.clustered_rows_entry(s, rt.position(), is_dummy::yes, is_continuous::no);                 p.clustered_rows_entry(s, rt.end_position(), is_dummy::yes, is_continuous::yes)                         .set_range_tombstone(rt.tomb);             }             mutation_application_stats app_stats;             apply_monotonically(s, std::move(p), s, app_stats);         } catch (...) {             _rows.clear_and_dispose(current_deleter<rows_entry>());             throw;         }     } }
 mutation_partition_v2::mutation_partition_v2(const schema& s, const mutation_partition& x)     : mutation_partition_v2(s, mutation_partition(s, x)) { }
 mutation_partition_v2::~mutation_partition_v2() {     _rows.clear_and_dispose(current_deleter<rows_entry>()); }
 mutation_partition_v2& mutation_partition_v2::operator=(mutation_partition_v2&& x) noexcept {     if (this != &x) {         this->~mutation_partition_v2();         new (this) mutation_partition_v2(std::move(x));     }     return *this; }
 void mutation_partition_v2::ensure_last_dummy(const schema& s) {     check_schema(s);     if (_rows.empty() || !_rows.rbegin()->is_last_dummy()) {         auto e = alloc_strategy_unique_ptr<rows_entry>(                 current_allocator().construct<rows_entry>(s, rows_entry::last_dummy_tag(), is_continuous::yes));         _rows.insert_before(_rows.end(), std::move(e));     } }
 template <> struct fmt::formatter<apply_resume> : fmt::formatter<std::string_view> {     template <typename FormatContext>     auto format(const apply_resume& res, FormatContext& ctx) const {         return fmt::format_to(ctx.out(), "{{{}, {}}}", int(res._stage), res._pos);     } };
 stop_iteration mutation_partition_v2::apply_monotonically(const schema& s, mutation_partition_v2&& p, cache_tracker* tracker,         mutation_application_stats& app_stats, is_preemptible preemptible, apply_resume& res, is_evictable evictable) {     return apply_monotonically(s, std::move(p), tracker, app_stats,         preemptible ? default_preemption_check() : never_preempt(), res, evictable); }
 stop_iteration mutation_partition_v2::apply_monotonically(const schema& s, mutation_partition_v2&& p, cache_tracker* tracker,         mutation_application_stats& app_stats, preemption_check need_preempt, apply_resume& res, is_evictable evictable) {     _tombstone.apply(p._tombstone);     _static_row.apply_monotonically(s, column_kind::static_column, std::move(p._static_row));     _static_row_continuous |= p._static_row_continuous;     rows_entry::tri_compare cmp(s);     position_in_partition::equal_compare eq(s);     auto del = current_deleter<rows_entry>();     auto compact = [&] (rows_entry& e) {         ++app_stats.rows_compacted_with_tombstones;         e.compact(s, _tombstone);     };     if (p._tombstone) {         rows_type::iterator i;         if (res._stage == apply_resume::stage::partition_tombstone_compaction) {             i = _rows.upper_bound(res._pos, cmp);         } else {             i = _rows.begin();         }         auto prev_i = (i == _rows.begin()) ? rows_type::iterator() : std::prev(i);         while (i != _rows.end()) {             compact(*i);             if (prev_i) {                 maybe_drop(s, tracker, prev_i, app_stats);             }             if (need_preempt() && i != _rows.end()) {                 res = apply_resume(apply_resume::stage::partition_tombstone_compaction, i->position());                 return stop_iteration::no;             }             prev_i = i;             ++i;         }         if (prev_i != _rows.end()) {             maybe_drop(s, tracker, prev_i, app_stats);         }         // TODO: Drop redundant range tombstones
        p._tombstone = {};     }     // Inserting new entries into LRU here is generally unsafe because
    // it may violate the "older versions are evicted first" rule (see row_cache.md).
    // It could happen, that there are newer versions in the MVCC chain with the same
    // key, not involved in this merge. Inserting an entry here would put this
    // entry ahead in the LRU, and the newer entry could get evicted earlier leading
    // to apparent loss of writes.
    // To avoid this, when inserting sentinels we must use lru::add_before() so that
    // they are put right before in the same place in the LRU.
    // Note: This procedure is not violating the "older versions are evicted first" rule.
    // It may move some entries from the newer version into the old version,
    // so the older version may have entries while the new version is already experiencing
    // eviction. However, the original information which was there in the old version
    // is guaranteed to be evicted prior to that, so there is no way for old information
    // to be exposed by such eviction.
    auto p_i = p._rows.begin();     auto i = _rows.begin();     rows_type::iterator lb_i; // iterator into _rows for previously inserted entry.
    // When resuming, the predecessor of the sentinel may have been compacted.
    bool prev_compacted = true;     if (res._stage < apply_resume::stage::merging_rows) {         prev_compacted = false;         res = apply_resume::merging_rows();     }     bool made_progress = false;     // Engaged p_sentinel indicates that information in p up to sentinel->position() was
    // merged into this instance and that flags on the entry pointed to by p_i are
    // only valid for the key range up to sentinel->position().
    // We should insert the sentinel back before returning so that the sum of p and this instance
    // remains consistent, and attributes like continuity and range_tombstone do not
    // extend to before_all_clustering_keys() in p.
    // If this_sentinel is engaged then it will be inserted into this instance at
    // the same position as p_sentinel, and reflects information about the interval
    // preceding the sentinel.
    // We need two sentinels so that there is no gap in continuity in case there is no entry
    // in this instance at the position of p_sentinel.
    // The sentinel never has a clustering key position, so it carries no row information.
    alloc_strategy_unique_ptr<rows_entry> p_sentinel;     alloc_strategy_unique_ptr<rows_entry> this_sentinel;     auto insert_sentinel_back = defer([&] {         // Insert this_sentinel before sentinel so that the former lands before the latter in LRU.
        if (this_sentinel) {             assert(p_i != p._rows.end());             auto rt = this_sentinel->range_tombstone();             auto insert_result = _rows.insert_before_hint(i, std::move(this_sentinel), cmp);             auto i2 = insert_result.first;             if (insert_result.second) {                 mplog.trace("{}: inserting sentinel at {}", fmt::ptr(this), i2->position());                 if (tracker) {                     tracker->insert(*std::prev(i2), *i2);                 }             } else {                 mplog.trace("{}: merging sentinel at {}", fmt::ptr(this), i2->position());                 i2->set_continuous(true);                 i2->set_range_tombstone(rt);             }         }         if (p_sentinel) {             assert(p_i != p._rows.end());             if (cmp(p_i->position(), p_sentinel->position()) == 0) {                 mplog.trace("{}: clearing attributes on {}", fmt::ptr(&p), p_i->position());                 assert(p_i->dummy());                 p_i->set_continuous(false);                 p_i->set_range_tombstone({});             } else {                 mplog.trace("{}: inserting sentinel at {}", fmt::ptr(&p), p_sentinel->position());                 auto insert_result = p._rows.insert_before_hint(p_i, std::move(p_sentinel), cmp);                 if (tracker) {                     tracker->insert(*p_i, *insert_result.first);                 }             }         }     });     while (p_i != p._rows.end()) {         rows_entry& src_e = *p_i;         bool miss = true;         if (i != _rows.end()) {             auto x = cmp(*i, src_e);             if (x < 0) {                 bool match;                 i = _rows.lower_bound(src_e, match, cmp);                 miss = !match;             } else {                 miss = x > 0;             }         }         // Invariants:
        //   i->position() >= p_i->position()
        // The block below reflects the information from interval (lb_i->position(), p_i->position()) to _rows,
        // up to the last entry in _rows which has position() < p_i->position(). The remainder is reflected by the act of
        // moving p_i itself.
        bool prev_interval_loaded = (evictable && src_e.continuous()) || (!evictable && src_e.range_tombstone());         if (prev_interval_loaded) {             // lb_i is only valid if prev_interval_loaded.
            rows_type::iterator prev_lb_i;             if (lb_i) {                 // If there is lb_i, it means the interval starts exactly at lb_i->position() in p.
                // Increment is needed, we don't want to set attributes on the lower bound of the interval.
                prev_lb_i = lb_i;                 ++lb_i;             } else {                 lb_i = _rows.begin();             }             while (lb_i != i) {                 bool compaction_worthwhile = src_e.range_tombstone() > lb_i->range_tombstone();                 // This works for both evictable and non-evictable snapshots.
                // For evictable snapshots we could replace the tombstone with newer, but due to
                // the "information monotonicity" rule, adding tombstone works too.
                lb_i->set_range_tombstone(lb_i->range_tombstone() + src_e.range_tombstone());                 lb_i->set_continuous(true);                 if (prev_compacted && prev_lb_i) {                     maybe_drop(s, tracker, prev_lb_i, app_stats);                 }                 prev_compacted = false;                 if (lb_i->dummy()) {                     prev_compacted = true;                 } else if (compaction_worthwhile) {                     compact(*lb_i);                     prev_compacted = true;                 }                 if (need_preempt()) {                     auto s1 = alloc_strategy_unique_ptr<rows_entry>(                             current_allocator().construct<rows_entry>(s,                                  position_in_partition::after_key(s, lb_i->position()), is_dummy::yes, is_continuous::no));                     alloc_strategy_unique_ptr<rows_entry> s2;                     if (lb_i->position().is_clustering_row()) {                         s2 = alloc_strategy_unique_ptr<rows_entry>(                                 current_allocator().construct<rows_entry>(s, s1->position(), is_dummy::yes, is_continuous::yes));                         auto lb_i_next = std::next(lb_i);                         if (lb_i_next != _rows.end() && lb_i_next->continuous()) {                             s2->set_range_tombstone(lb_i_next->range_tombstone() + src_e.range_tombstone());                         } else {                             s2->set_range_tombstone(src_e.range_tombstone());                         }                     }                     p_sentinel = std::move(s1);                     this_sentinel = std::move(s2);                     mplog.trace("preempted, res={}", res);                     return stop_iteration::no;                 }                 prev_lb_i = lb_i;                 ++lb_i;             }         }         auto next_p_i = std::next(p_i);         // next_interval_loaded is true iff there are attributes on next_p_i which apply
        // to the interval (p_i->position(), next_p_i->position), and we
        // have to prepare a sentinel when removing p_i from p in case merging
        // needs to stop before next_p_i is moved.
        bool next_interval_loaded = next_p_i != p._rows.end()                 && ((evictable && next_p_i->continuous()) || (!evictable && next_p_i->range_tombstone()));         bool do_compact = false;         if (miss) {             alloc_strategy_unique_ptr<rows_entry> s1;             alloc_strategy_unique_ptr<rows_entry> s2;             if (next_interval_loaded) {                 // FIXME: Avoid reallocation
                s1 = alloc_strategy_unique_ptr<rows_entry>(                     current_allocator().construct<rows_entry>(s,                         position_in_partition::after_key(s, src_e.position()), is_dummy::yes, is_continuous::no));                 if (src_e.position().is_clustering_row()) {                     s2 = alloc_strategy_unique_ptr<rows_entry>(                             current_allocator().construct<rows_entry>(s,                                 s1->position(), is_dummy::yes, is_continuous::yes));                     if (i != _rows.end() && i->continuous()) {                         s2->set_range_tombstone(i->range_tombstone() + src_e.range_tombstone());                     } else {                         s2->set_range_tombstone(src_e.range_tombstone());                     }                 }             }             rows_type::key_grabber pi_kg(p_i);             lb_i = _rows.insert_before(i, std::move(pi_kg));             p_sentinel = std::move(s1);             this_sentinel = std::move(s2);             // Check if src_e falls into a continuous range.
            // The range past the last entry is also always implicitly continuous.
            if (i == _rows.end() || i->continuous()) {                 tombstone i_rt = i != _rows.end() ? i->range_tombstone() : tombstone();                 // Cannot apply only-row range tombstone falling into a continuous range without inserting extra entry.
                // Should not occur in practice due to the "older versions are evicted first" rule.
                // Never occurs in non-evictable snapshots because they are continuous.
                if (!src_e.continuous() && src_e.range_tombstone() > i_rt) {                     if (src_e.dummy()) {                         lb_i->set_range_tombstone(i_rt);                     } else {                         position_in_partition_view i_pos = i != _rows.end() ? i->position()                                 : position_in_partition_view::after_all_clustered_rows();                         // See the "no singular tombstones" rule.
                        mplog.error("Cannot merge entry {} with rt={}, cont=0 into continuous range before {} with rt={}",                                 src_e.position(), src_e.range_tombstone(), i_pos, i_rt);                         abort();                     }                 } else {                     lb_i->set_range_tombstone(src_e.range_tombstone() + i_rt);                 }                 lb_i->set_continuous(true);             }         } else {             assert(i->dummy() == src_e.dummy());             alloc_strategy_unique_ptr<rows_entry> s1;             alloc_strategy_unique_ptr<rows_entry> s2;             if (next_interval_loaded) {                 // FIXME: Avoid reallocation
                s1 = alloc_strategy_unique_ptr<rows_entry>(                         current_allocator().construct<rows_entry>(s,                             position_in_partition::after_key(s, src_e.position()), is_dummy::yes, is_continuous::no));                 if (src_e.position().is_clustering_row()) {                     s2 = alloc_strategy_unique_ptr<rows_entry>(                             current_allocator().construct<rows_entry>(s, s1->position(), is_dummy::yes, is_continuous::yes));                     auto next_i = std::next(i);                     if (next_i != _rows.end() && next_i->continuous()) {                         s2->set_range_tombstone(next_i->range_tombstone() + src_e.range_tombstone());                     } else {                         s2->set_range_tombstone(src_e.range_tombstone());                     }                 }             }             {                 // FIXME: This can be an evictable snapshot even if !tracker, see partition_entry::squashed()
                // So we need to handle continuity as if it was an evictable snapshot.
                if (i->continuous()) {                     if (src_e.range_tombstone() > i->range_tombstone()) {                         // Cannot apply range tombstone in such a case.
                        // Should not occur in practice due to the "older versions are evicted first" rule.
                        if (!src_e.continuous()) {                             // range tombstone on a discontinuous dummy does not matter
                            if (!src_e.dummy()) {                                 // See the "no singular tombstones" rule.
                                mplog.error("Cannot merge entry {} with rt={}, cont=0 into an entry which has rt={}, cont=1",                                         src_e.position(), src_e.range_tombstone(), i->range_tombstone());                                 abort();                             }                         } else {                             i->set_range_tombstone(i->range_tombstone() + src_e.range_tombstone());                         }                     }                 } else {                     i->set_continuous(src_e.continuous());                     i->set_range_tombstone(i->range_tombstone() + src_e.range_tombstone());                 }             }             if (tracker) {                 // Newer evictable versions store complete rows
                i->row() = std::move(src_e.row());                 // Need to preserve the LRU link of the later version in case it's
                // the last dummy entry which holds the partition entry linked in LRU.
                i->swap(src_e);                 tracker->remove(src_e);             } else {                 // Avoid row compaction if no newer range tombstone.
                do_compact = (src_e.range_tombstone() + src_e.row().deleted_at().regular()) >                             (i->range_tombstone() + i->row().deleted_at().regular());                 memory::on_alloc_point();                 i->apply_monotonically(s, std::move(src_e));             }             ++app_stats.row_hits;             p_i = p._rows.erase_and_dispose(p_i, del);             lb_i = i;             ++i;             p_sentinel = std::move(s1);             this_sentinel = std::move(s2);         }         // All operations above up to each insert_before() must be noexcept.
        if (prev_compacted && lb_i != _rows.begin()) {             maybe_drop(s, tracker, std::prev(lb_i), app_stats);         }         if (lb_i->dummy()) {             prev_compacted = true;         } else if (do_compact) {             compact(*lb_i);             prev_compacted = true;         } else {             prev_compacted = false;         }         if (prev_compacted && !next_interval_loaded) {             // next_p_i will not see prev_interval_loaded so will not attempt to drop predecessors.
            // We have to do it now.
            maybe_drop(s, tracker, lb_i, app_stats);             lb_i = {};         }         ++app_stats.row_writes;         // We must not return stop_iteration::no if we removed the last element from p._rows.
        // Otherwise, p_i will be left empty, and thus fully continuous, violating the
        // invariant that the sum of this and p has the same continuity as before merging.
        if (made_progress && need_preempt() && p_i != p._rows.end()) {             return stop_iteration::no;         }         made_progress = true;     }     if (prev_compacted && lb_i != _rows.end()) {         maybe_drop(s, tracker, lb_i, app_stats);     }     return stop_iteration::yes; }
 stop_iteration mutation_partition_v2::apply_monotonically(const schema& s, mutation_partition_v2&& p, const schema& p_schema,         mutation_application_stats& app_stats, is_preemptible preemptible, apply_resume& res, is_evictable evictable) {     if (s.version() == p_schema.version()) {         return apply_monotonically(s, std::move(p), no_cache_tracker, app_stats,                                    preemptible ? default_preemption_check() : never_preempt(), res, evictable);     } else {         mutation_partition_v2 p2(s, p);         p2.upgrade(p_schema, s);         return apply_monotonically(s, std::move(p2), no_cache_tracker, app_stats, never_preempt(), res, evictable); // FIXME: make preemptible
    } }
 stop_iteration mutation_partition_v2::apply_monotonically(const schema& s, mutation_partition_v2&& p, cache_tracker *tracker,                                                        mutation_application_stats& app_stats, is_evictable evictable) {     apply_resume res;     return apply_monotonically(s, std::move(p), tracker, app_stats, is_preemptible::no, res, evictable); }
 stop_iteration mutation_partition_v2::apply_monotonically(const schema& s, mutation_partition_v2&& p, const schema& p_schema,                                                        mutation_application_stats& app_stats) {     apply_resume res;     return apply_monotonically(s, std::move(p), p_schema, app_stats, is_preemptible::no, res, is_evictable::no); }
 void mutation_partition_v2::apply(const schema& s, const mutation_partition_v2& p, const schema& p_schema,                                mutation_application_stats& app_stats) {     apply_monotonically(s, mutation_partition_v2(p_schema, std::move(p)), p_schema, app_stats); }
 void mutation_partition_v2::apply(const schema& s, mutation_partition_v2&& p, mutation_application_stats& app_stats) {     apply_monotonically(s, mutation_partition_v2(s, std::move(p)), no_cache_tracker, app_stats, is_evictable::no); }
 void mutation_partition_v2::apply_weak(const schema& s, mutation_partition_view p,                                   const schema& p_schema, mutation_application_stats& app_stats) {     // FIXME: Optimize
    mutation_partition p2(p_schema.shared_from_this());     partition_builder b(p_schema, p2);     p.accept(p_schema, b);     apply_monotonically(s, mutation_partition_v2(p_schema, std::move(p2)), p_schema, app_stats); }
 void mutation_partition_v2::apply_weak(const schema& s, const mutation_partition& p,                                        const schema& p_schema, mutation_application_stats& app_stats) {     // FIXME: Optimize
    apply_monotonically(s, mutation_partition_v2(s, p), p_schema, app_stats); }
 void mutation_partition_v2::apply_weak(const schema& s, mutation_partition&& p, mutation_application_stats& app_stats) {     apply_monotonically(s, mutation_partition_v2(s, std::move(p)), no_cache_tracker, app_stats, is_evictable::no); }
 void mutation_partition_v2::apply_row_tombstone(const schema& schema, clustering_key_prefix prefix, tombstone t) {     check_schema(schema);     assert(!prefix.is_full(schema));     auto start = prefix;     apply_row_tombstone(schema, range_tombstone{std::move(start), std::move(prefix), std::move(t)}); }
 void mutation_partition_v2::apply_row_tombstone(const schema& schema, range_tombstone rt) {     check_schema(schema);     mutation_partition mp(schema.shared_from_this());     mp.apply_row_tombstone(schema, std::move(rt));     mutation_application_stats stats;     apply_weak(schema, std::move(mp), stats); }
 void mutation_partition_v2::apply_delete(const schema& schema, const clustering_key_prefix& prefix, tombstone t) {     check_schema(schema);     if (prefix.is_empty(schema)) {         apply(t);     } else if (prefix.is_full(schema)) {         clustered_row(schema, prefix).apply(t);     } else {         apply_row_tombstone(schema, prefix, t);     } }
 void mutation_partition_v2::apply_delete(const schema& schema, range_tombstone rt) {     check_schema(schema);     if (range_tombstone::is_single_clustering_row_tombstone(schema, rt.start, rt.start_kind, rt.end, rt.end_kind)) {         apply_delete(schema, std::move(rt.start), std::move(rt.tomb));         return;     }     apply_row_tombstone(schema, std::move(rt)); }
 void mutation_partition_v2::apply_delete(const schema& schema, clustering_key&& prefix, tombstone t) {     check_schema(schema);     if (prefix.is_empty(schema)) {         apply(t);     } else if (prefix.is_full(schema)) {         clustered_row(schema, std::move(prefix)).apply(t);     } else {         apply_row_tombstone(schema, std::move(prefix), t);     } }
 void mutation_partition_v2::apply_delete(const schema& schema, clustering_key_prefix_view prefix, tombstone t) {     check_schema(schema);     if (prefix.is_empty(schema)) {         apply(t);     } else if (prefix.is_full(schema)) {         clustered_row(schema, prefix).apply(t);     } else {         apply_row_tombstone(schema, prefix, t);     } }
 void mutation_partition_v2::apply_insert(const schema& s, clustering_key_view key, api::timestamp_type created_at) {     clustered_row(s, key).apply(row_marker(created_at)); }
 void mutation_partition_v2::apply_insert(const schema& s, clustering_key_view key, api::timestamp_type created_at,         gc_clock::duration ttl, gc_clock::time_point expiry) {     clustered_row(s, key).apply(row_marker(created_at, ttl, expiry)); }
 void mutation_partition_v2::insert_row(const schema& s, const clustering_key& key, deletable_row&& row) {     auto e = alloc_strategy_unique_ptr<rows_entry>(         current_allocator().construct<rows_entry>(key, std::move(row)));     _rows.insert_before_hint(_rows.end(), std::move(e), rows_entry::tri_compare(s)); }
 void mutation_partition_v2::insert_row(const schema& s, const clustering_key& key, const deletable_row& row) {     check_schema(s);     auto e = alloc_strategy_unique_ptr<rows_entry>(         current_allocator().construct<rows_entry>(s, key, row));     _rows.insert_before_hint(_rows.end(), std::move(e), rows_entry::tri_compare(s)); }
 const row* mutation_partition_v2::find_row(const schema& s, const clustering_key& key) const {     check_schema(s);     auto i = _rows.find(key, rows_entry::tri_compare(s));     if (i == _rows.end()) {         return nullptr;     }     return &i->row().cells(); }
 deletable_row& mutation_partition_v2::clustered_row(const schema& s, clustering_key&& key) {     check_schema(s);     auto i = _rows.find(key, rows_entry::tri_compare(s));     if (i == _rows.end()) {         auto e = alloc_strategy_unique_ptr<rows_entry>(             current_allocator().construct<rows_entry>(std::move(key)));         i = _rows.insert_before_hint(i, std::move(e), rows_entry::tri_compare(s)).first;     }     return i->row(); }
 deletable_row& mutation_partition_v2::clustered_row(const schema& s, const clustering_key& key) {     check_schema(s);     auto i = _rows.find(key, rows_entry::tri_compare(s));     if (i == _rows.end()) {         auto e = alloc_strategy_unique_ptr<rows_entry>(             current_allocator().construct<rows_entry>(key));         i = _rows.insert_before_hint(i, std::move(e), rows_entry::tri_compare(s)).first;     }     return i->row(); }
 deletable_row& mutation_partition_v2::clustered_row(const schema& s, clustering_key_view key) {     check_schema(s);     auto i = _rows.find(key, rows_entry::tri_compare(s));     if (i == _rows.end()) {         auto e = alloc_strategy_unique_ptr<rows_entry>(             current_allocator().construct<rows_entry>(key));         i = _rows.insert_before_hint(i, std::move(e), rows_entry::tri_compare(s)).first;     }     return i->row(); }
 rows_entry& mutation_partition_v2::clustered_rows_entry(const schema& s, position_in_partition_view pos, is_dummy dummy, is_continuous continuous) {     check_schema(s);     auto i = _rows.find(pos, rows_entry::tri_compare(s));     if (i == _rows.end()) {         auto e = alloc_strategy_unique_ptr<rows_entry>(             current_allocator().construct<rows_entry>(s, pos, dummy, continuous));         i = _rows.insert_before_hint(i, std::move(e), rows_entry::tri_compare(s)).first;     }     return *i; }
 deletable_row& mutation_partition_v2::clustered_row(const schema& s, position_in_partition_view pos, is_dummy dummy, is_continuous continuous) {     return clustered_rows_entry(s, pos, dummy, continuous).row(); }
 rows_entry& mutation_partition_v2::clustered_row(const schema& s, position_in_partition_view pos, is_dummy dummy) {     check_schema(s);     auto cmp = rows_entry::tri_compare(s);     auto i = _rows.lower_bound(pos, cmp);     if (i == _rows.end() || cmp(i->position(), pos) != 0) {         auto e = alloc_strategy_unique_ptr<rows_entry>(             current_allocator().construct<rows_entry>(s, pos, dummy, is_continuous::no));         if (i != _rows.end()) {             e->set_continuous(i->continuous());             e->set_range_tombstone(i->range_tombstone());         }         i = _rows.insert_before_hint(i, std::move(e), rows_entry::tri_compare(s)).first;     }     return *i; }
 deletable_row& mutation_partition_v2::append_clustered_row(const schema& s, position_in_partition_view pos, is_dummy dummy, is_continuous continuous) {     check_schema(s);     const auto cmp = rows_entry::tri_compare(s);     auto i = _rows.end();     if (!_rows.empty() && (cmp(*std::prev(i), pos) >= 0)) {         throw std::runtime_error(format("mutation_partition_v2::append_clustered_row(): cannot append clustering row with key {} to the partition"                 ", last clustering row is equal or greater: {}", pos, std::prev(i)->key()));     }     auto e = alloc_strategy_unique_ptr<rows_entry>(current_allocator().construct<rows_entry>(s, pos, dummy, continuous));     i = _rows.insert_before_hint(i, std::move(e), cmp).first;     return i->row(); }
 mutation_partition_v2::rows_type::const_iterator mutation_partition_v2::lower_bound(const schema& schema, const query::clustering_range& r) const {     check_schema(schema);     if (!r.start()) {         return std::cbegin(_rows);     }     return _rows.lower_bound(position_in_partition_view::for_range_start(r), rows_entry::tri_compare(schema)); }
 mutation_partition_v2::rows_type::const_iterator mutation_partition_v2::upper_bound(const schema& schema, const query::clustering_range& r) const {     check_schema(schema);     if (!r.end()) {         return std::cend(_rows);     }     return _rows.lower_bound(position_in_partition_view::for_range_end(r), rows_entry::tri_compare(schema)); }
 boost::iterator_range<mutation_partition_v2::rows_type::const_iterator> mutation_partition_v2::range(const schema& schema, const query::clustering_range& r) const {     check_schema(schema);     return boost::make_iterator_range(lower_bound(schema, r), upper_bound(schema, r)); }
 boost::iterator_range<mutation_partition_v2::rows_type::iterator> mutation_partition_v2::range(const schema& schema, const query::clustering_range& r) {     return unconst(_rows, static_cast<const mutation_partition_v2*>(this)->range(schema, r)); }
 mutation_partition_v2::rows_type::iterator mutation_partition_v2::lower_bound(const schema& schema, const query::clustering_range& r) {     return unconst(_rows, static_cast<const mutation_partition_v2*>(this)->lower_bound(schema, r)); }
 mutation_partition_v2::rows_type::iterator mutation_partition_v2::upper_bound(const schema& schema, const query::clustering_range& r) {     return unconst(_rows, static_cast<const mutation_partition_v2*>(this)->upper_bound(schema, r)); }
 template<typename Func> void mutation_partition_v2::for_each_row(const schema& schema, const query::clustering_range& row_range, bool reversed, Func&& func) const {     check_schema(schema);     auto r = range(schema, row_range);     if (!reversed) {         for (const auto& e : r) {             if (func(e) == stop_iteration::yes) {                 break;             }         }     } else {         for (const auto& e : r | boost::adaptors::reversed) {             if (func(e) == stop_iteration::yes) {                 break;             }         }     } }
 std::ostream& operator<<(std::ostream& os, const mutation_partition_v2::printer& p) {     const auto indent = "  ";     auto& mp = p._mutation_partition;     os << "mutation_partition_v2: {\n";     if (mp._tombstone) {         fmt::print(os, "{:2}tombstone: {},\n", "", mp._tombstone);     }     if (!mp.static_row().empty()) {         os << indent << "static_row: {\n";         const auto& srow = mp.static_row().get();         srow.for_each_cell([&] (column_id& c_id, const atomic_cell_or_collection& cell) {             auto& column_def = p._schema.column_at(column_kind::static_column, c_id);             os << indent << indent <<  "'" << column_def.name_as_text()                 << "': " << atomic_cell_or_collection::printer(column_def, cell) << ",\n";         });          os << indent << "},\n";     }     os << indent << "rows: [\n";     for (const auto& re : mp.clustered_rows()) {         os << indent << indent << "{\n";         const auto& row = re.row();         os << indent << indent << indent << "cont: " << re.continuous() << ",\n";         os << indent << indent << indent << "dummy: " << re.dummy() << ",\n";         if (!row.marker().is_missing()) {             os << indent << indent << indent << "marker: " << row.marker() << ",\n";         }         if (row.deleted_at()) {             os << indent << indent << indent << "tombstone: " << row.deleted_at() << ",\n";         }         if (re.range_tombstone()) {             fmt::print(os, "{:6}rt: {},\n", "", re.range_tombstone());         }         position_in_partition pip(re.position());         if (pip.get_clustering_key_prefix()) {             os << indent << indent << indent << "position: {\n";             auto ck = *pip.get_clustering_key_prefix();             auto type_iterator = ck.get_compound_type(p._schema)->types().begin();             auto column_iterator = p._schema.clustering_key_columns().begin();             os << indent << indent << indent << indent << "bound_weight: " << int32_t(pip.get_bound_weight()) << ",\n";             for (auto&& e : ck.components(p._schema)) {                 os << indent << indent << indent << indent << "'" << column_iterator->name_as_text()                     << "': " << (*type_iterator)->to_string(to_bytes(e)) << ",\n";                 ++type_iterator;                 ++column_iterator;             }             os << indent << indent << indent << "},\n";         }         row.cells().for_each_cell([&] (column_id& c_id, const atomic_cell_or_collection& cell) {             auto& column_def = p._schema.column_at(column_kind::regular_column, c_id);             os << indent << indent << indent <<  "'" << column_def.name_as_text()                 << "': " << atomic_cell_or_collection::printer(column_def, cell) << ",\n";         });         os << indent << indent << "},\n";     }     os << indent << "]\n}";     return os; }
 bool mutation_partition_v2::equal(const schema& s, const mutation_partition_v2& p) const {     return equal(s, p, s); }
 bool mutation_partition_v2::equal(const schema& this_schema, const mutation_partition_v2& p, const schema& p_schema) const {     if (_tombstone != p._tombstone) {         return false;     }     if (!boost::equal(non_dummy_rows(), p.non_dummy_rows(),         [&] (const rows_entry& e1, const rows_entry& e2) {             return e1.equal(this_schema, e2, p_schema);         }     )) {         return false;     }     return _static_row.equal(column_kind::static_column, this_schema, p._static_row, p_schema); }
 bool mutation_partition_v2::equal_continuity(const schema& s, const mutation_partition_v2& p) const {     return _static_row_continuous == p._static_row_continuous         && get_continuity(s).equals(s, p.get_continuity(s)); }
 size_t mutation_partition_v2::external_memory_usage(const schema& s) const {     check_schema(s);     size_t sum = 0;     sum += static_row().external_memory_usage(s, column_kind::static_column);     sum += clustered_rows().external_memory_usage();     for (auto& clr : clustered_rows()) {         sum += clr.memory_usage(s);     }     return sum; }
 // Returns true if the mutation_partition_v2 represents no writes.
bool mutation_partition_v2::empty() const {     if (_tombstone.timestamp != api::missing_timestamp) {         return false;     }     return !_static_row.size() && _rows.empty(); }
 bool mutation_partition_v2::is_static_row_live(const schema& s, gc_clock::time_point query_time) const {     check_schema(s);     return has_any_live_data(s, column_kind::static_column, static_row().get(), _tombstone, query_time); }
 uint64_t mutation_partition_v2::row_count() const {     return _rows.calculate_size(); }
 void mutation_partition_v2::accept(const schema& s, mutation_partition_visitor& v) const {     check_schema(s);     v.accept_partition_tombstone(_tombstone);     _static_row.for_each_cell([&] (column_id id, const atomic_cell_or_collection& cell) {         const column_definition& def = s.static_column_at(id);         if (def.is_atomic()) {             v.accept_static_cell(id, cell.as_atomic_cell(def));         } else {             v.accept_static_cell(id, cell.as_collection_mutation());         }     });     std::optional<position_in_partition> prev_pos;     for (const rows_entry& e : _rows) {         const deletable_row& dr = e.row();         if (e.range_tombstone()) {             if (!e.continuous()) {                 v.accept_row_tombstone(range_tombstone(position_in_partition::before_key(e.position()),                                                        position_in_partition::after_key(s, e.position()),                                                        e.range_tombstone()));             } else {                 v.accept_row_tombstone(range_tombstone(prev_pos ? position_in_partition::after_key(s, *prev_pos)                                                                 : position_in_partition::before_all_clustered_rows(),                                                        position_in_partition::after_key(s, e.position()),                                                        e.range_tombstone()));             }         }         v.accept_row(e.position(), dr.deleted_at(), dr.marker(), e.dummy(), e.continuous());         dr.cells().for_each_cell([&] (column_id id, const atomic_cell_or_collection& cell) {             const column_definition& def = s.regular_column_at(id);             if (def.is_atomic()) {                 v.accept_row_cell(id, cell.as_atomic_cell(def));             } else {                 v.accept_row_cell(id, cell.as_collection_mutation());             }         });         prev_pos = e.position();     } }
 void mutation_partition_v2::upgrade(const schema& old_schema, const schema& new_schema) {     // We need to copy to provide strong exception guarantees.
    mutation_partition tmp(new_schema.shared_from_this());     tmp.set_static_row_continuous(_static_row_continuous);     converting_mutation_partition_applier v(old_schema.get_column_mapping(), new_schema, tmp);     accept(old_schema, v);     *this = mutation_partition_v2(new_schema, std::move(tmp)); }
 mutation_partition mutation_partition_v2::as_mutation_partition(const schema& s) const {     mutation_partition tmp(s.shared_from_this());     tmp.set_static_row_continuous(_static_row_continuous);     partition_builder v(s, tmp);     accept(s, v);     return tmp; }
 mutation_partition_v2::mutation_partition_v2(mutation_partition_v2::incomplete_tag, const schema& s, tombstone t)     : _tombstone(t)     , _static_row_continuous(!s.has_static_columns())     , _rows() {     auto e = alloc_strategy_unique_ptr<rows_entry>(             current_allocator().construct<rows_entry>(s, rows_entry::last_dummy_tag(), is_continuous::no));     _rows.insert_before(_rows.end(), std::move(e)); }
 bool mutation_partition_v2::is_fully_continuous() const {     if (!_static_row_continuous) {         return false;     }     for (auto&& row : _rows) {         if (!row.continuous()) {             return false;         }     }     return true; }
 void mutation_partition_v2::make_fully_continuous() {     _static_row_continuous = true;     auto i = _rows.begin();     while (i != _rows.end()) {         i->set_continuous(true);         ++i;     } }
 void mutation_partition_v2::set_continuity(const schema& s, const position_range& pr, is_continuous cont) {     auto cmp = rows_entry::tri_compare(s);     if (cmp(pr.start(), pr.end()) >= 0) {         return; // empty range
    }     auto end = _rows.lower_bound(pr.end(), cmp);     if (end == _rows.end() || cmp(pr.end(), end->position()) < 0) {         auto e = alloc_strategy_unique_ptr<rows_entry>(                 current_allocator().construct<rows_entry>(s, pr.end(), is_dummy::yes,                     end == _rows.end() ? is_continuous::yes : end->continuous()));         end = _rows.insert_before(end, std::move(e));     }     auto i = _rows.lower_bound(pr.start(), cmp);     if (cmp(pr.start(), i->position()) < 0) {         auto e = alloc_strategy_unique_ptr<rows_entry>(                 current_allocator().construct<rows_entry>(s, pr.start(), is_dummy::yes, i->continuous()));         i = _rows.insert_before(i, std::move(e));     }     assert(i != end);     ++i;     while (1) {         i->set_continuous(cont);         if (i == end) {             break;         }         ++i;     } }
 clustering_interval_set mutation_partition_v2::get_continuity(const schema& s, is_continuous cont) const {     check_schema(s);     clustering_interval_set result;     auto i = _rows.begin();     auto prev_pos = position_in_partition::before_all_clustered_rows();     while (i != _rows.end()) {         if (i->continuous() == cont) {             result.add(s, position_range(std::move(prev_pos), position_in_partition::before_key(i->position())));         }         if (i->position().is_clustering_row() && cont) {             result.add(s, position_range(position_in_partition::before_key(i->position()),                                          position_in_partition::after_key(s, i->position())));         }         prev_pos = position_in_partition::after_key(s, i->position());         ++i;     }     if (cont) {         result.add(s, position_range(std::move(prev_pos), position_in_partition::after_all_clustered_rows()));     }     return result; }
 stop_iteration mutation_partition_v2::clear_gently(cache_tracker* tracker) noexcept {     auto del = current_deleter<rows_entry>();     auto i = _rows.begin();     auto end = _rows.end();     while (i != end) {         if (tracker) {             tracker->remove(*i);         }         i = _rows.erase_and_dispose(i, del);         // The iterator comparison below is to not defer destruction of now empty
        // mutation_partition_v2 objects. Not doing this would cause eviction to leave garbage
        // versions behind unnecessarily.
        if (need_preempt() && i != end) {             return stop_iteration::no;         }     }     return stop_iteration::yes; }
 bool mutation_partition_v2::check_continuity(const schema& s, const position_range& r, is_continuous cont) const {     check_schema(s);     auto cmp = rows_entry::tri_compare(s);     auto i = _rows.lower_bound(r.start(), cmp);     auto end = _rows.lower_bound(r.end(), cmp);     if (cmp(r.start(), r.end()) >= 0) {         return bool(cont);     }     if (i != end) {         if (no_clustering_row_between(s, r.start(), i->position())) {             ++i;         }         while (i != end) {             if (i->continuous() != cont) {                 return false;             }             ++i;         }         if (end != _rows.begin() && no_clustering_row_between(s, std::prev(end)->position(), r.end())) {             return true;         }     }     return (end == _rows.end() ? is_continuous::yes : end->continuous()) == cont; }
 bool mutation_partition_v2::fully_continuous(const schema& s, const position_range& r) {     return check_continuity(s, r, is_continuous::yes); }
 bool mutation_partition_v2::fully_discontinuous(const schema& s, const position_range& r) {     return check_continuity(s, r, is_continuous::no); }
 mutation_partition_v2::rows_type::iterator mutation_partition_v2::maybe_drop(const schema& s,       cache_tracker* tracker,       mutation_partition_v2::rows_type::iterator i,       mutation_application_stats& app_stats) {     rows_entry& e = *i;     auto next_i = std::next(i);     if (!e.row().empty() || e.is_last_dummy()) {         return next_i;     }     // Pass only if continuity is the same on both sides and
    // range tombstones for the intervals are the same on both sides (if intervals are continuous).
    bool next_continuous = next_i == _rows.end() || next_i->continuous();     if (e.continuous() && next_continuous) {         tombstone next_range_tombstone = (next_i == _rows.end() ? tombstone{} : next_i->range_tombstone());         if (e.range_tombstone() != next_range_tombstone) {             return next_i;         }     } else if (!e.continuous() && !next_continuous) {         if (!e.dummy() && e.range_tombstone()) {             return next_i;         }     } else {         return next_i;     }     ++app_stats.rows_dropped_by_tombstones; // FIXME: it's more general than that now
    auto del = current_deleter<rows_entry>();     i = _rows.erase(i);     if (tracker) {         tracker->remove(e);     }     del(&e);     return next_i; }
 void mutation_partition_v2::compact(const schema& s, cache_tracker* tracker) {     mutation_application_stats stats;     auto i = _rows.begin();     rows_type::iterator prev_i;     while (i != _rows.end()) {         i->compact(s, _tombstone);         if (prev_i) {             // We cannot call maybe_drop() on i because the entry may become redundant
            // only after the next entry is compacted, e.g. when next entry's range tombstone is dropped.
            maybe_drop(s, tracker, prev_i, stats);         }         prev_i = i++;     }     if (prev_i) {         maybe_drop(s, tracker, prev_i, stats);     } }
 bool has_redundant_dummies(const mutation_partition_v2& p) {     bool last_dummy = false;     bool last_cont = false;     tombstone last_rt;     auto i = p.clustered_rows().begin();     while (i != p.clustered_rows().end()) {         const rows_entry& e = *i;         if (last_dummy) {             bool redundant = last_cont == bool(e.continuous()) && last_rt == e.range_tombstone();             if (redundant) {                 return true;             }         }         last_dummy = bool(e.dummy());         last_rt = e.range_tombstone();         last_cont = bool(e.continuous());         ++i;     }     return false; }
 using namespace db;
 static_assert(MutationViewVisitor<mutation_partition_view_virtual_visitor>);
 mutation_partition_view_virtual_visitor::~mutation_partition_view_virtual_visitor() = default;
 namespace { using atomic_cell_variant = boost::variant<ser::live_cell_view,                                            ser::expiring_cell_view,                                            ser::dead_cell_view,                                            ser::counter_cell_view,                                            ser::unknown_variant_type>; atomic_cell read_atomic_cell(const abstract_type& type, atomic_cell_variant cv, atomic_cell::collection_member cm = atomic_cell::collection_member::no) {     class atomic_cell_visitor : public boost::static_visitor<atomic_cell> {         const abstract_type& _type;         atomic_cell::collection_member _collection_member;     public:         explicit atomic_cell_visitor(const abstract_type& t, atomic_cell::collection_member cm)             : _type(t), _collection_member(cm) { }         atomic_cell operator()(ser::live_cell_view& lcv) const {             return atomic_cell::make_live(_type, lcv.created_at(), lcv.value().view(), _collection_member);         }         atomic_cell operator()(ser::expiring_cell_view& ecv) const {             return atomic_cell::make_live(_type, ecv.c().created_at(), ecv.c().value().view(), ecv.expiry(), ecv.ttl(), _collection_member);         }         atomic_cell operator()(ser::dead_cell_view& dcv) const {             return atomic_cell::make_dead(dcv.tomb().timestamp(), dcv.tomb().deletion_time());         }         atomic_cell operator()(ser::counter_cell_view& ccv) const {             class counter_cell_visitor : public boost::static_visitor<atomic_cell> {                 api::timestamp_type _created_at;             public:                 explicit counter_cell_visitor(api::timestamp_type ts)                     : _created_at(ts) { }                 atomic_cell operator()(ser::counter_cell_full_view& ccv) const {                     // TODO: a lot of copying for something called view
                    counter_cell_builder ccb; // we know the final number of shards
                    for (auto csv : ccv.shards()) {                         ccb.add_maybe_unsorted_shard(counter_shard(csv));                     }                     ccb.sort_and_remove_duplicates();                     return ccb.build(_created_at);                 }                 atomic_cell operator()(ser::counter_cell_update_view& ccv) const {                     return atomic_cell::make_live_counter_update(_created_at, ccv.delta());                 }                 atomic_cell operator()(ser::unknown_variant_type&) const {                     throw std::runtime_error("Trying to deserialize counter cell in unknown state");                 }             };             auto v = ccv.value();             return boost::apply_visitor(counter_cell_visitor(ccv.created_at()), v);         }         atomic_cell operator()(ser::unknown_variant_type&) const {             throw std::runtime_error("Trying to deserialize cell in unknown state");         }     };     return boost::apply_visitor(atomic_cell_visitor(type, cm), cv); } collection_mutation read_collection_cell(const abstract_type& type, ser::collection_cell_view cv) {     collection_mutation_description mut;     mut.tomb = cv.tomb();     auto&& elements = cv.elements();     mut.cells.reserve(elements.size());     visit(type, make_visitor(         [&] (const collection_type_impl& ctype) {             auto& value_type = *ctype.value_comparator();             for (auto&& e : elements) {                 mut.cells.emplace_back(e.key(), read_atomic_cell(value_type, e.value(), atomic_cell::collection_member::yes));             }         },         [&] (const user_type_impl& utype) {             for (auto&& e : elements) {                 bytes key = e.key();                 auto idx = deserialize_field_index(key);                 assert(idx < utype.size());                 mut.cells.emplace_back(key, read_atomic_cell(*utype.type(idx), e.value(), atomic_cell::collection_member::yes));             }         },         [&] (const abstract_type& o) {             throw std::runtime_error(format("attempted to read a collection cell with type: {}", o.name()));         }     ));     return mut.serialize(type); } template<typename Visitor> void read_and_visit_row(ser::row_view rv, const column_mapping& cm, column_kind kind, Visitor&& visitor) {     for (auto&& cv : rv.columns()) {         auto id = cv.id();         auto& col = cm.column_at(kind, id);         class atomic_cell_or_collection_visitor : public boost::static_visitor<> {             Visitor& _visitor;             column_id _id;             const column_mapping_entry& _col;         public:             explicit atomic_cell_or_collection_visitor(Visitor& v, column_id id, const column_mapping_entry& col)                 : _visitor(v), _id(id), _col(col) { }             void operator()(atomic_cell_variant& acv) const {                 if (!_col.is_atomic()) {                     throw std::runtime_error("A collection expected, got an atomic cell");                 }                 _visitor.accept_atomic_cell(_id, read_atomic_cell(*_col.type(), acv));             }             void operator()(ser::collection_cell_view& ccv) const {                 if (_col.is_atomic()) {                     throw std::runtime_error("An atomic cell expected, got a collection");                 }                 // FIXME: Pass view to cell to avoid copy
                auto&& outer = current_allocator();                 with_allocator(standard_allocator(), [&] {                     auto cell = read_collection_cell(*_col.type(), ccv);                     with_allocator(outer, [&] {                         _visitor.accept_collection(_id, cell);                     });                 });             }             void operator()(ser::unknown_variant_type&) const {                 throw std::runtime_error("Trying to deserialize unknown cell type");             }         };         auto&& cell = cv.c();         boost::apply_visitor(atomic_cell_or_collection_visitor(visitor, id, col), cell);     } } row_marker read_row_marker(boost::variant<ser::live_marker_view, ser::expiring_marker_view, ser::dead_marker_view, ser::no_marker_view, ser::unknown_variant_type> rmv) {     struct row_marker_visitor : boost::static_visitor<row_marker> {         row_marker operator()(ser::live_marker_view& lmv) const {             return row_marker(lmv.created_at());         }         row_marker operator()(ser::expiring_marker_view& emv) const {             return row_marker(emv.lm().created_at(), emv.ttl(), emv.expiry());         }         row_marker operator()(ser::dead_marker_view& dmv) const {             return row_marker(dmv.tomb());         }         row_marker operator()(ser::no_marker_view&) const {             return row_marker();         }         row_marker operator()(ser::unknown_variant_type&) const {             throw std::runtime_error("Trying to deserialize unknown row marker type");         }     };     return boost::apply_visitor(row_marker_visitor(), rmv); } }
 template<typename Visitor> requires MutationViewVisitor<Visitor> void mutation_partition_view::do_accept(const column_mapping& cm, Visitor& visitor) const {     auto in = _in;     auto mpv = ser::deserialize(in, boost::type<ser::mutation_partition_view>());     visitor.accept_partition_tombstone(mpv.tomb());     struct static_row_cell_visitor {         Visitor& _visitor;         void accept_atomic_cell(column_id id, atomic_cell ac) const {            _visitor.accept_static_cell(id, std::move(ac));         }         void accept_collection(column_id id, const collection_mutation& cm) const {            _visitor.accept_static_cell(id, cm);         }     };     read_and_visit_row(mpv.static_row(), cm, column_kind::static_column, static_row_cell_visitor{visitor});     for (auto&& rt : mpv.range_tombstones()) {         visitor.accept_row_tombstone(rt);     }     for (auto&& cr : mpv.rows()) {         auto t = row_tombstone(cr.deleted_at(), shadowable_tombstone(cr.shadowable_deleted_at()));         visitor.accept_row(position_in_partition_view::for_key(cr.key()), t, read_row_marker(cr.marker()), is_dummy::no, is_continuous::yes);         struct cell_visitor {             Visitor& _visitor;             void accept_atomic_cell(column_id id, atomic_cell ac) const {                _visitor.accept_row_cell(id, std::move(ac));             }             void accept_collection(column_id id, const collection_mutation& cm) const {                _visitor.accept_row_cell(id, cm);             }         };         read_and_visit_row(cr.cells(), cm, column_kind::regular_column, cell_visitor{visitor});     } }
 template<typename Visitor> requires MutationViewVisitor<Visitor> future<> mutation_partition_view::do_accept_gently(const column_mapping& cm, Visitor& visitor) const {     auto in = _in;     auto mpv = ser::deserialize(in, boost::type<ser::mutation_partition_view>());     visitor.accept_partition_tombstone(mpv.tomb());     struct static_row_cell_visitor {         Visitor& _visitor;         void accept_atomic_cell(column_id id, atomic_cell ac) const {            _visitor.accept_static_cell(id, std::move(ac));         }         void accept_collection(column_id id, const collection_mutation& cm) const {            _visitor.accept_static_cell(id, cm);         }     };     read_and_visit_row(mpv.static_row(), cm, column_kind::static_column, static_row_cell_visitor{visitor});     co_await coroutine::maybe_yield();     for (auto rt : mpv.range_tombstones()) {         visitor.accept_row_tombstone(rt);         co_await coroutine::maybe_yield();     }     for (auto cr : mpv.rows()) {         auto t = row_tombstone(cr.deleted_at(), shadowable_tombstone(cr.shadowable_deleted_at()));         auto key = cr.key();         visitor.accept_row(position_in_partition_view::for_key(key), t, read_row_marker(cr.marker()), is_dummy::no, is_continuous::yes);         struct cell_visitor {             Visitor& _visitor;             void accept_atomic_cell(column_id id, atomic_cell ac) const {                _visitor.accept_row_cell(id, std::move(ac));             }             void accept_collection(column_id id, const collection_mutation& cm) const {                _visitor.accept_row_cell(id, cm);             }         };         read_and_visit_row(cr.cells(), cm, column_kind::regular_column, cell_visitor{visitor});         co_await coroutine::maybe_yield();     } }
 template <bool is_preemptible> mutation_partition_view::accept_ordered_result mutation_partition_view::do_accept_ordered(const schema& s, mutation_partition_view_virtual_visitor& visitor, accept_ordered_cookie cookie) const {     auto in = _in;     auto mpv = ser::deserialize(in, boost::type<ser::mutation_partition_view>());     const column_mapping& cm = s.get_column_mapping();     if (!cookie.accepted_partition_tombstone) {         visitor.accept_partition_tombstone(mpv.tomb());         cookie.accepted_partition_tombstone = true;     }     if (!cookie.accepted_static_row) {         struct static_row_cell_visitor {             mutation_partition_view_virtual_visitor& _visitor;             void accept_atomic_cell(column_id id, atomic_cell ac) const {                 _visitor.accept_static_cell(id, std::move(ac));             }             void accept_collection(column_id id, const collection_mutation& cm) const {                 _visitor.accept_static_cell(id, cm);             }         };         read_and_visit_row(mpv.static_row(), cm, column_kind::static_column, static_row_cell_visitor{visitor});         cookie.accepted_static_row = true;     }     if (!cookie.iterators) {         cookie.iterators.emplace(accept_ordered_cookie::rts_crs_iterators{             .rts_begin = mpv.range_tombstones().cbegin(),             .rts_end = mpv.range_tombstones().cend(),             .crs_begin = mpv.rows().cbegin(),             .crs_end = mpv.rows().cend(),         });     }     auto rt_it = cookie.iterators->rts_begin;     const auto& rt_e = cookie.iterators->rts_end;     auto cr_it = cookie.iterators->crs_begin;     const auto& cr_e = cookie.iterators->crs_end;     auto consume_rt = [&] (range_tombstone&& rt) {         cookie.iterators->rts_begin = rt_it;         return visitor.accept_row_tombstone(std::move(rt));     };     auto consume_cr = [&] (ser::deletable_row_view&& cr, clustering_key_prefix&& cr_key) {         cookie.iterators->crs_begin = cr_it;         auto t = row_tombstone(cr.deleted_at(), shadowable_tombstone(cr.shadowable_deleted_at()));         if (visitor.accept_row(position_in_partition_view::for_key(cr_key), t, read_row_marker(cr.marker()), is_dummy::no, is_continuous::yes)) {             return stop_iteration::yes;         }         struct cell_visitor {             mutation_partition_view_virtual_visitor& _visitor;             void accept_atomic_cell(column_id id, atomic_cell ac) const {                 _visitor.accept_row_cell(id, std::move(ac));             }             void accept_collection(column_id id, const collection_mutation& cm) const {                 _visitor.accept_row_cell(id, cm);             }         };         read_and_visit_row(cr.cells(), cm, column_kind::regular_column, cell_visitor{visitor});         return stop_iteration::no;     };     std::optional<range_tombstone> rt;     auto next_rt = [&] {         if (rt || rt_it == rt_e) {             return;         }         rt = *rt_it;         ++rt_it;     };     std::optional<ser::deletable_row_view> cr;     std::optional<clustering_key_prefix> cr_key;     auto next_cr = [&] {         if (cr || cr_it == cr_e) {             return;         }         cr = *cr_it;         cr_key = cr->key();         ++cr_it;     };     position_in_partition::tri_compare cmp{s};     for (;;) {         next_rt();         next_cr();         bool emit_rt = bool(rt);         stop_iteration stop;         if (rt && cr) {             auto rt_pos = rt->position();             auto cr_pos = position_in_partition_view::for_key(*cr_key);             emit_rt = (cmp(rt_pos, cr_pos) < 0);         }         if (emit_rt) {             stop = consume_rt(std::move(*std::exchange(rt, std::nullopt)));         } else if (cr) {             stop = consume_cr(std::move(*std::exchange(cr, std::nullopt)), std::move(*cr_key));         } else {             return accept_ordered_result{stop_iteration::yes, accept_ordered_cookie{}};         }         if (stop || (is_preemptible && need_preempt())) {             return accept_ordered_result{stop, std::move(cookie)};         }     } }
 void mutation_partition_view::accept(const schema& s, partition_builder& visitor) const {     do_accept(s.get_column_mapping(), visitor); }
 future<> mutation_partition_view::accept_gently(const schema& s, partition_builder& visitor) const {     return do_accept_gently(s.get_column_mapping(), visitor); }
 void mutation_partition_view::accept(const column_mapping& cm, converting_mutation_partition_applier& visitor) const {     do_accept(cm, visitor); }
 future<> mutation_partition_view::accept_gently(const column_mapping& cm, converting_mutation_partition_applier& visitor) const {     return do_accept_gently(cm, visitor); }
 void mutation_partition_view::accept(const column_mapping& cm, mutation_partition_view_virtual_visitor& visitor) const {     do_accept(cm, visitor); }
 void mutation_partition_view::accept_ordered(const schema& s, mutation_partition_view_virtual_visitor& visitor) const {     do_accept_ordered<false>(s, visitor, accept_ordered_cookie{}); }
 future<> mutation_partition_view::accept_gently_ordered(const schema& s, mutation_partition_view_virtual_visitor& visitor) const {     accept_ordered_result res;     do {         res = do_accept_ordered<true>(s, visitor, std::move(res.cookie));         co_await coroutine::maybe_yield();     } while (!res.stop); }
 std::optional<clustering_key> mutation_partition_view::first_row_key() const {     auto in = _in;     auto mpv = ser::deserialize(in, boost::type<ser::mutation_partition_view>());     auto rows = mpv.rows();     if (rows.empty()) {         return { };     }     return (*rows.begin()).key(); }
 std::optional<clustering_key> mutation_partition_view::last_row_key() const {     auto in = _in;     auto mpv = ser::deserialize(in, boost::type<ser::mutation_partition_view>());     auto rows = mpv.rows();     if (rows.empty()) {         return { };     }     auto it = rows.begin();     auto next = it;     while (++next != rows.end()) {         it = next;     }     return (*it).key(); }
 mutation_partition_view mutation_partition_view::from_view(ser::mutation_partition_view v) {     return { v.v }; }
 mutation_fragment frozen_mutation_fragment::unfreeze(const schema& s, reader_permit permit) {     auto in = ser::as_input_stream(_bytes);     auto view = ser::deserialize(in, boost::type<ser::mutation_fragment_view>());     return seastar::visit(view.fragment(),         [&] (ser::clustering_row_view crv) {             class clustering_row_builder {                 const schema& _s;                 mutation_fragment _mf;             public:                 clustering_row_builder(const schema& s, reader_permit permit, clustering_key key, row_tombstone t, row_marker m)                     : _s(s), _mf(mutation_fragment::clustering_row_tag_t(), s, std::move(permit), std::move(key), std::move(t), std::move(m), row()) { }                 void accept_atomic_cell(column_id id, atomic_cell ac) {                     _mf.mutate_as_clustering_row(_s, [&] (clustering_row& cr) mutable {                         cr.cells().append_cell(id, std::move(ac));                     });                 }                 void accept_collection(column_id id, const collection_mutation& cm) {                     _mf.mutate_as_clustering_row(_s, [&] (clustering_row& cr) mutable {                         cr.cells().append_cell(id, collection_mutation(*_s.regular_column_at(id).type, cm));                     });                 }                 mutation_fragment get_mutation_fragment() && { return std::move(_mf); }             };             auto cr = crv.row();             auto t = row_tombstone(cr.deleted_at(), shadowable_tombstone(cr.shadowable_deleted_at()));             clustering_row_builder builder(s, permit, cr.key(), std::move(t), read_row_marker(cr.marker()));             read_and_visit_row(cr.cells(), s.get_column_mapping(), column_kind::regular_column, builder);             return std::move(builder).get_mutation_fragment();         },         [&] (ser::static_row_view sr) {             class static_row_builder {                 const schema& _s;                 mutation_fragment _mf;             public:                 explicit static_row_builder(const schema& s, reader_permit permit) : _s(s), _mf(_s, std::move(permit), static_row()) { }                 void accept_atomic_cell(column_id id, atomic_cell ac) {                     _mf.mutate_as_static_row(_s, [&] (static_row& sr) mutable {                         sr.cells().append_cell(id, std::move(ac));                     });                 }                 void accept_collection(column_id id, const collection_mutation& cm) {                     _mf.mutate_as_static_row(_s, [&] (static_row& sr) mutable {                         sr.cells().append_cell(id, collection_mutation(*_s.static_column_at(id).type, cm));                     });                 }                 mutation_fragment get_mutation_fragment() && { return std::move(_mf); }             };             static_row_builder builder(s, permit);             read_and_visit_row(sr.cells(), s.get_column_mapping(), column_kind::static_column, builder);             return std::move(builder).get_mutation_fragment();         },         [&] (ser::range_tombstone_view rt) {             return mutation_fragment(s, permit, range_tombstone(rt));         },         [&] (ser::partition_start_view ps) {             auto dkey = dht::decorate_key(s, ps.key());             return mutation_fragment(s, permit, partition_start(std::move(dkey), ps.partition_tombstone()));         },         [&] (partition_end) {             return mutation_fragment(s, permit, partition_end());         },         [] (ser::unknown_variant_type) -> mutation_fragment {             throw std::runtime_error("Trying to deserialize unknown mutation fragment type");         }     ); }
 using namespace db;
 namespace { template<typename Writer> auto write_live_cell(Writer&& writer, atomic_cell_view c) {     return std::move(writer).write_created_at(c.timestamp())                             .write_fragmented_value(fragment_range(c.value()))                         .end_live_cell(); } template<typename Writer> auto write_counter_cell(Writer&& writer, atomic_cell_view c) {     auto value = std::move(writer).write_created_at(c.timestamp());     return [&c, value = std::move(value)] () mutable {         if (c.is_counter_update()) {             auto delta = c.counter_update_value();             return std::move(value).start_value_counter_cell_update()                                    .write_delta(delta)                                    .end_counter_cell_update();         } else {             auto ccv = counter_cell_view(c);             auto shards = std::move(value).start_value_counter_cell_full()                                           .start_shards();             for (auto csv : ccv.shards()) {                 shards.add_shards(counter_shard(csv));             }             return std::move(shards).end_shards().end_counter_cell_full();         }     }().end_counter_cell(); } template<typename Writer> auto write_expiring_cell(Writer&& writer, atomic_cell_view c) {     return std::move(writer).write_ttl(c.ttl())                             .write_expiry(c.expiry())                             .start_c()                                 .write_created_at(c.timestamp())                                 .write_fragmented_value(fragment_range(c.value()))                             .end_c()                         .end_expiring_cell(); } template<typename Writer> auto write_dead_cell(Writer&& writer, atomic_cell_view c) {     return std::move(writer).start_tomb()                                 .write_timestamp(c.timestamp())                                 .write_deletion_time(c.deletion_time())                             .end_tomb()                         .end_dead_cell(); } template<typename Writer> auto write_collection_cell(Writer&& collection_writer, collection_mutation_view cmv, const column_definition& def) {   return cmv.with_deserialized(*def.type, [&] (collection_mutation_view_description m_view) {     auto cells_writer = std::move(collection_writer).write_tomb(m_view.tomb).start_elements();     for (auto&& c : m_view.cells) {         auto cell_writer = cells_writer.add().write_key(c.first);         if (!c.second.is_live()) {             write_dead_cell(std::move(cell_writer).start_value_dead_cell(), c.second).end_collection_element();         } else if (c.second.is_live_and_has_ttl()) {             write_expiring_cell(std::move(cell_writer).start_value_expiring_cell(), c.second).end_collection_element();         } else {             write_live_cell(std::move(cell_writer).start_value_live_cell(), c.second).end_collection_element();         }     }     return std::move(cells_writer).end_elements().end_collection_cell();   }); } template<typename Writer> auto write_row_cells(Writer&& writer, const row& r, const schema& s, column_kind kind) {     auto column_writer = std::move(writer).start_columns();     r.for_each_cell([&] (column_id id, const atomic_cell_or_collection& cell) {         auto& def = s.column_at(kind, id);         auto cell_or_collection_writer = column_writer.add().write_id(id);         if (def.is_atomic()) {             auto&& c = cell.as_atomic_cell(def);             auto cell_writer = std::move(cell_or_collection_writer).start_c_variant();             if (!c.is_live()) {                 write_dead_cell(std::move(cell_writer).start_variant_dead_cell(), c).end_variant().end_column();             } else if (def.is_counter()) {                 write_counter_cell(std::move(cell_writer).start_variant_counter_cell(), c).end_variant().end_column();             } else if (c.is_live_and_has_ttl()) {                 write_expiring_cell(std::move(cell_writer).start_variant_expiring_cell(), c).end_variant().end_column();             } else {                 write_live_cell(std::move(cell_writer).start_variant_live_cell(), c).end_variant().end_column();             }         } else {             write_collection_cell(std::move(cell_or_collection_writer).start_c_collection_cell(), cell.as_collection_mutation(), def).end_column();         }     });     return std::move(column_writer).end_columns(); } template<typename Writer> auto write_row_marker(Writer&& writer, const row_marker& marker) {     if (marker.is_missing()) {         return std::move(writer).start_marker_no_marker().end_no_marker();     } else if (!marker.is_live()) {         return std::move(writer).start_marker_dead_marker()                                     .start_tomb()                                         .write_timestamp(marker.timestamp())                                         .write_deletion_time(marker.deletion_time())                                     .end_tomb()                                 .end_dead_marker();     } else if (marker.is_expiring()) {         return std::move(writer).start_marker_expiring_marker()                                     .start_lm()                                         .write_created_at(marker.timestamp())                                     .end_lm()                                     .write_ttl(marker.ttl())                                     .write_expiry(marker.expiry())                                 .end_expiring_marker();     } else {         return std::move(writer).start_marker_live_marker()                                     .write_created_at(marker.timestamp())                                 .end_live_marker();     } } }
 template <typename RowTombstones> static void write_tombstones(const schema& s, RowTombstones& row_tombstones, const range_tombstone_list& rt_list) {     for (auto&& rte : rt_list) {         auto& rt = rte.tombstone();         row_tombstones.add().write_start(rt.start).write_tomb(rt.tomb).write_start_kind(rt.start_kind)             .write_end(rt.end).write_end_kind(rt.end_kind).end_range_tombstone();     } }
 template<typename Writer> static auto write_tombstone(Writer&& writer, const tombstone& t) {     return std::move(writer).write_timestamp(t.timestamp).write_deletion_time(t.deletion_time); }
 template<typename Writer> static auto write_row(Writer&& writer, const schema& s, const clustering_key_prefix& key, const row& cells, const row_marker& m, const row_tombstone& t) {     auto marker_writer = std::move(writer).write_key(key);     auto deleted_at_writer = write_row_marker(std::move(marker_writer), m).start_deleted_at();     auto row_writer = write_tombstone(std::move(deleted_at_writer), t.regular()).end_deleted_at().start_cells();     auto shadowable_deleted_at_writer = write_row_cells(std::move(row_writer), cells, s, column_kind::regular_column).end_cells().start_shadowable_deleted_at();     return write_tombstone(std::move(shadowable_deleted_at_writer), t.shadowable().tomb()).end_shadowable_deleted_at(); }
 template<typename Writer> void mutation_partition_serializer::write_serialized(Writer&& writer, const schema& s, const mutation_partition& mp) {     auto srow_writer = std::move(writer).write_tomb(mp.partition_tombstone()).start_static_row();     auto row_tombstones = write_row_cells(std::move(srow_writer), mp.static_row().get(), s, column_kind::static_column).end_static_row().start_range_tombstones();     write_tombstones(s, row_tombstones, mp.row_tombstones());     auto clustering_rows = std::move(row_tombstones).end_range_tombstones().start_rows();     for (auto&& cr : mp.non_dummy_rows()) {         write_row(clustering_rows.add(), s, cr.key(), cr.row().cells(), cr.row().marker(), cr.row().deleted_at()).end_deletable_row();     }     std::move(clustering_rows).end_rows().end_mutation_partition(); }
 mutation_partition_serializer::mutation_partition_serializer(const schema& schema, const mutation_partition& p)     : _schema(schema), _p(p) { }
 void mutation_partition_serializer::write(bytes_ostream& out) const {     write(ser::writer_of_mutation_partition<bytes_ostream>(out)); }
 void mutation_partition_serializer::write(ser::writer_of_mutation_partition<bytes_ostream>&& wr) const {     write_serialized(std::move(wr), _schema, _p); }
 void serialize_mutation_fragments(const schema& s, tombstone partition_tombstone,     std::optional<static_row> sr,  range_tombstone_list rts,     std::deque<clustering_row> crs, ser::writer_of_mutation_partition<bytes_ostream>&& wr) {     auto srow_writer = std::move(wr).write_tomb(partition_tombstone).start_static_row();     auto row_tombstones = [&] {         if (sr) {             return write_row_cells(std::move(srow_writer), sr->cells(), s, column_kind::static_column).end_static_row().start_range_tombstones();         } else {             return std::move(srow_writer).start_columns().end_columns().end_static_row().start_range_tombstones();         }     }();     sr = { };     write_tombstones(s, row_tombstones, rts);     rts.clear();     auto clustering_rows = std::move(row_tombstones).end_range_tombstones().start_rows();     while (!crs.empty()) {         auto& cr = crs.front();         write_row(clustering_rows.add(), s, cr.key(), cr.cells(), cr.marker(), cr.tomb()).end_deletable_row();         crs.pop_front();     }     std::move(clustering_rows).end_rows().end_mutation_partition(); }
 frozen_mutation_fragment freeze(const schema& s, const mutation_fragment& mf) {     bytes_ostream out;     ser::writer_of_mutation_fragment<bytes_ostream> writer(out);     mf.visit(seastar::make_visitor(         [&] (const clustering_row& cr) {             return write_row(std::move(writer).start_fragment_clustering_row().start_row(), s, cr.key(), cr.cells(), cr.marker(), cr.tomb())                     .end_row()                 .end_clustering_row();         },         [&] (const static_row& sr) {             return write_row_cells(std::move(writer).start_fragment_static_row().start_cells(), sr.cells(), s, column_kind::static_column)                     .end_cells()                 .end_static_row();         },         [&] (const range_tombstone& rt) {             return std::move(writer).write_fragment_range_tombstone(rt);         },         [&] (const partition_start& ps) {             return std::move(writer).start_fragment_partition_start()                     .write_key(ps.key().key())                     .write_partition_tombstone(ps.partition_tombstone())                 .end_partition_start();         },         [&] (const partition_end& pe) {             return std::move(writer).write_fragment_partition_end(pe);         }     )).end_mutation_fragment();     return frozen_mutation_fragment(std::move(out)); }
 static void remove_or_mark_as_unique_owner(partition_version* current, mutation_cleaner* cleaner) {     while (current && !current->is_referenced()) {         auto next = current->next();         current->erase();         if (cleaner) {             cleaner->destroy_gently(*current);         } else {             current_allocator().destroy(current);         }         current = next;     }     if (current) {         current->back_reference().mark_as_unique_owner();     } }
 partition_version::partition_version(partition_version&& pv) noexcept     : anchorless_list_base_hook(std::move(pv))     , _backref(pv._backref)     , _partition(std::move(pv._partition)) {     if (_backref) {         _backref->_version = this;     }     pv._backref = nullptr; }
 partition_version& partition_version::operator=(partition_version&& pv) noexcept {     if (this != &pv) {         this->~partition_version();         new (this) partition_version(std::move(pv));     }     return *this; }
 partition_version::~partition_version() {     if (_backref) {         _backref->_version = nullptr;     } }
 stop_iteration partition_version::clear_gently(cache_tracker* tracker) noexcept {     return _partition.clear_gently(tracker); }
 size_t partition_version::size_in_allocator(const schema& s, allocation_strategy& allocator) const {     return allocator.object_memory_size_in_allocator(this) +            partition().external_memory_usage(s); }
 namespace { // A functor which transforms objects from Domain into objects from CoDomain
template<typename U, typename Domain, typename CoDomain> concept Mapper =     requires(U obj, const Domain& src) {         { obj(src) } -> std::convertible_to<const CoDomain&>;     }; // A functor which merges two objects from Domain into one. The result is stored in the first argument.
template<typename U, typename Domain> concept Reducer =     requires(U obj, Domain& dst, const Domain& src) {         { obj(dst, src) } -> std::same_as<void>;     }; // Calculates the value of particular part of mutation_partition represented by
// the version chain starting from v.
// |map| extracts the part from each version.
// |reduce| Combines parts from the two versions.
template <typename Result, typename Map, typename Initial, typename Reduce> requires Mapper<Map, mutation_partition_v2, Result> && Reducer<Reduce, Result> inline Result squashed(const partition_version_ref& v, Map&& map, Initial&& initial, Reduce&& reduce) {     const partition_version* this_v = &*v;     partition_version* it = v->last();     Result r = initial(map(it->partition()));     while (it != this_v) {         it = it->prev();         reduce(r, map(it->partition()));     }     return r; } template <typename Result, typename Map, typename Reduce> requires Mapper<Map, mutation_partition_v2, Result> && Reducer<Reduce, Result> inline Result squashed(const partition_version_ref& v, Map&& map, Reduce&& reduce) {     return squashed<Result>(v, map,                             [] (auto&& o) -> decltype(auto) { return std::forward<decltype(o)>(o); },                             reduce); } }
 ::static_row partition_snapshot::static_row(bool digest_requested) const {     return ::static_row(::squashed<row>(version(),                          [&] (const mutation_partition_v2& mp) -> const row& {                             if (digest_requested) {                                 mp.static_row().prepare_hash(*_schema, column_kind::static_column);                             }                             return mp.static_row().get();                          },                          [this] (const row& r) { return row(*_schema, column_kind::static_column, r); },                          [this] (row& a, const row& b) { a.apply(*_schema, column_kind::static_column, b); })); }
 bool partition_snapshot::static_row_continuous() const {     return version()->partition().static_row_continuous(); }
 tombstone partition_snapshot::partition_tombstone() const {     return ::squashed<tombstone>(version(),                                [] (const mutation_partition_v2& mp) { return mp.partition_tombstone(); },                                [] (tombstone& a, tombstone b) { a.apply(b); }); }
 mutation_partition partition_snapshot::squashed() const {     return ::squashed<mutation_partition>(version(),                                [this] (const mutation_partition_v2& mp) -> mutation_partition {                                    return mp.as_mutation_partition(*_schema);                                },                                [] (mutation_partition&& mp) { return std::move(mp); },                                [this] (mutation_partition& a, const mutation_partition& b) {                                    mutation_application_stats app_stats;                                    a.apply(*_schema, b, *_schema, app_stats);                                }); }
 tombstone partition_entry::partition_tombstone() const {     return ::squashed<tombstone>(_version,         [] (const mutation_partition_v2& mp) { return mp.partition_tombstone(); },         [] (tombstone& a, tombstone b) { a.apply(b); }); }
 partition_snapshot::~partition_snapshot() {     with_allocator(region().allocator(), [this] {         if (_locked) {             touch();         }         if (_version && _version.is_unique_owner()) {             auto v = &*_version;             _version = {};             remove_or_mark_as_unique_owner(v, _cleaner);         } else if (_entry) {             _entry->_snapshot = nullptr;         }     }); }
 void merge_versions(const schema& s, mutation_partition_v2& newer, mutation_partition_v2&& older, cache_tracker* tracker, is_evictable evictable) {     mutation_application_stats app_stats;     older.apply_monotonically(s, std::move(newer), tracker, app_stats, evictable);     newer = std::move(older); }
 stop_iteration partition_snapshot::merge_partition_versions(mutation_application_stats& app_stats) {     partition_version_ref& v = version();     if (!v.is_unique_owner()) {         // Shift _version to the oldest unreferenced version and then keep merging left hand side into it.
        // This is good for performance because in case we were at the latest version
        // we leave it for incoming writes and they don't have to create a new one.
        partition_version* current = &*v;         while (current->next() && !current->next()->is_referenced()) {             current = current->next();             _version = partition_version_ref(*current);             _version_merging_state.reset();         }         while (auto prev = current->prev()) {             region().allocator().invalidate_references();             // Here we count writes that overwrote rows from a previous version. Total number of writes does not change.
            mutation_application_stats local_app_stats;             if (!_version_merging_state) {                 _version_merging_state = apply_resume();             }             const auto do_stop_iteration = current->partition().apply_monotonically(*schema(),                 std::move(prev->partition()), _tracker, local_app_stats, is_preemptible::yes, *_version_merging_state,                 is_evictable(bool(_tracker)));             app_stats.row_hits += local_app_stats.row_hits;             if (do_stop_iteration == stop_iteration::no) {                 return stop_iteration::no;             }             // If do_stop_iteration is yes, we have to remove the previous version.
            // It now appears as fully continuous because it is empty.
            _version_merging_state.reset();             if (prev->is_referenced()) {                 _version.release();                 prev->back_reference() = partition_version_ref(*current, prev->back_reference().is_unique_owner());                 current_allocator().destroy(prev);                 return stop_iteration::yes;             }             current_allocator().destroy(prev);         }     }     return stop_iteration::yes; }
 stop_iteration partition_snapshot::slide_to_oldest() noexcept {     partition_version_ref& v = version();     if (v.is_unique_owner()) {         return stop_iteration::yes;     }     if (_entry) {         _entry->_snapshot = nullptr;         _entry = nullptr;     }     partition_version* current = &*v;     while (current->next() && !current->next()->is_referenced()) {         current = current->next();         _version = partition_version_ref(*current);     }     return current->prev() ? stop_iteration::no : stop_iteration::yes; }
 unsigned partition_snapshot::version_count() {     unsigned count = 0;     for (auto&& v : versions()) {         (void)v;         count++;     }     return count; }
 partition_entry::partition_entry(mutation_partition_v2 mp) {     auto new_version = current_allocator().construct<partition_version>(std::move(mp));     _version = partition_version_ref(*new_version); }
 partition_entry::partition_entry(const schema& s, mutation_partition mp)     : partition_entry(mutation_partition_v2(s, std::move(mp))) { }
 partition_entry::partition_entry(partition_entry::evictable_tag, const schema& s, mutation_partition&& mp)     : partition_entry([&] {         mp.ensure_last_dummy(s);         return mutation_partition_v2(s, std::move(mp));     }
()) { }
 partition_entry partition_entry::make_evictable(const schema& s, mutation_partition&& mp) {     return {evictable_tag(), s, std::move(mp)}; }
 partition_entry partition_entry::make_evictable(const schema& s, const mutation_partition& mp) {     return make_evictable(s, mutation_partition(s, mp)); }
 partition_entry::~partition_entry() {     if (!_version) {         return;     }     if (_snapshot) {         assert(!_snapshot->is_locked());         _snapshot->_version = std::move(_version);         _snapshot->_version.mark_as_unique_owner();         _snapshot->_entry = nullptr;     } else {         auto v = &*_version;         _version = { };         remove_or_mark_as_unique_owner(v, no_cleaner);     } }
 stop_iteration partition_entry::clear_gently(cache_tracker* tracker) noexcept {     if (!_version) {         return stop_iteration::yes;     }     if (_snapshot) {         assert(!_snapshot->is_locked());         _snapshot->_version = std::move(_version);         _snapshot->_version.mark_as_unique_owner();         _snapshot->_entry = nullptr;         return stop_iteration::yes;     }     partition_version* v = &*_version;     _version = {};     while (v) {         if (v->is_referenced()) {             v->back_reference().mark_as_unique_owner();             break;         }         auto next = v->next();         if (v->clear_gently(tracker) == stop_iteration::no) {             _version = partition_version_ref(*v);             return stop_iteration::no;         }         current_allocator().destroy(&*v);         v = next;     }     return stop_iteration::yes; }
 void partition_entry::set_version(partition_version* new_version) {     if (_snapshot) {         assert(!_snapshot->is_locked());         _snapshot->_version = std::move(_version);         _snapshot->_entry = nullptr;     }     _snapshot = nullptr;     _version = partition_version_ref(*new_version); }
 partition_version& partition_entry::add_version(const schema& s, cache_tracker* tracker) {     // Every evictable version must have a dummy entry at the end so that
    // it can be tracked in the LRU. It is also needed to allow old versions
    // to stay around (with tombstones and static rows) after fully evicted.
    // Such versions must be fully discontinuous, and thus have a dummy at the end.
    auto new_version = tracker                        ? current_allocator().construct<partition_version>(mutation_partition_v2::make_incomplete(s))                        : current_allocator().construct<partition_version>(mutation_partition_v2(s.shared_from_this()));     new_version->partition().set_static_row_continuous(_version->partition().static_row_continuous());     new_version->insert_before(*_version);     set_version(new_version);     if (tracker) {         tracker->insert(*new_version);     }     return *new_version; }
 void partition_entry::apply(logalloc::region& r, mutation_cleaner& cleaner, const schema& s, const mutation_partition_v2& mp, const schema& mp_schema,         mutation_application_stats& app_stats) {     apply(r, cleaner, s, mutation_partition_v2(mp_schema, mp), mp_schema, app_stats); }
 void partition_entry::apply(logalloc::region& r,            mutation_cleaner& c,            const schema& s,            const mutation_partition& mp,            const schema& mp_schema,            mutation_application_stats& app_stats) {     auto mp_v1 = mutation_partition(mp_schema, mp);     mp_v1.make_fully_continuous();     apply(r, c, s, mutation_partition_v2(mp_schema, std::move(mp_v1)), mp_schema, app_stats); }
 void partition_entry::apply(logalloc::region& r, mutation_cleaner& cleaner, const schema& s, mutation_partition_v2&& mp, const schema& mp_schema,         mutation_application_stats& app_stats) {     // A note about app_stats: it may happen that mp has rows that overwrite other rows
    // in older partition_version. Those overwrites will be counted when their versions get merged.
    if (s.version() != mp_schema.version()) {         mp.upgrade(mp_schema, s);     }     auto new_version = current_allocator().construct<partition_version>(std::move(mp));     partition_snapshot_ptr snp; // Should die after new_version is inserted
    if (!_snapshot) {         try {             apply_resume res;             auto notify = cleaner.make_region_space_guard();             if (_version->partition().apply_monotonically(s,                       std::move(new_version->partition()),                       no_cache_tracker,                       app_stats,                       is_preemptible::yes,                       res,                       is_evictable::no) == stop_iteration::yes) {                 current_allocator().destroy(new_version);                 return;             } else {                 // Apply was preempted. Let the cleaner finish the job when snapshot dies
                snp = read(r, cleaner, s.shared_from_this(), no_cache_tracker);                 // FIXME: Store res in the snapshot as an optimization to resume from where we left off.
            }         } catch (...) {             // fall through
        }     }     new_version->insert_before(*_version);     set_version(new_version);     app_stats.row_writes += new_version->partition().row_count(); }
 utils::coroutine partition_entry::apply_to_incomplete(const schema& s,     partition_entry&& pe,     mutation_cleaner& pe_cleaner,     logalloc::allocating_section& alloc,     logalloc::region& reg,     cache_tracker& tracker,     partition_snapshot::phase_type phase,     real_dirty_memory_accounter& acc) {     // This flag controls whether this operation may defer. It is more
    // expensive to apply with deferring due to construction of snapshots and
    // two-pass application, with the first pass filtering and moving data to
    // the new version and the second pass merging it back once all is done.
    // We cannot merge into current version because if we defer in the middle
    // that may publish partial writes. Also, snapshot construction results in
    // creation of garbage objects, partition_version and rows_entry. Garbage
    // will yield sparse segments and add overhead due to increased LSA
    // segment compaction. This becomes especially significant for small
    // partitions where I saw 40% slow down.
    const bool preemptible = s.clustering_key_size() > 0;     // When preemptible, later memtable reads could start using the snapshot before
    // snapshot's writes are made visible in cache, which would cause them to miss those writes.
    // So we cannot allow erasing when preemptible.
    bool can_move = !preemptible && !pe._snapshot;     auto src_snp = pe.read(reg, pe_cleaner, s.shared_from_this(), no_cache_tracker);     partition_snapshot_ptr prev_snp;     if (preemptible) {         // Reads must see prev_snp until whole update completes so that writes
        // are not partially visible.
        prev_snp = read(reg, tracker.cleaner(), s.shared_from_this(), &tracker, phase - 1);     }     auto dst_snp = read(reg, tracker.cleaner(), s.shared_from_this(), &tracker, phase);     dst_snp->lock();     // Once we start updating the partition, we must keep all snapshots until the update completes,
    // otherwise partial writes would be published. So the scope of snapshots must enclose the scope
    // of allocating sections, so we return here to get out of the current allocating section and
    // give the caller a chance to store the coroutine object. The code inside coroutine below
    // runs outside allocating section.
    return utils::coroutine([&tracker, &s, &alloc, &reg, &acc, can_move, preemptible,             cur = partition_snapshot_row_cursor(s, *dst_snp),             src_cur = partition_snapshot_row_cursor(s, *src_snp, can_move),             dst_snp = std::move(dst_snp),             prev_snp = std::move(prev_snp),             src_snp = std::move(src_snp),             lb = position_in_partition::before_all_clustered_rows(),             static_done = false] () mutable {         auto&& allocator = reg.allocator();         return alloc(reg, [&] {             size_t dirty_size = 0;             if (!static_done) {                 partition_version& dst = *dst_snp->version();                 bool static_row_continuous = dst_snp->static_row_continuous();                 auto current = &*src_snp->version();                 while (current) {                     dirty_size += allocator.object_memory_size_in_allocator(current)                         + current->partition().static_row().external_memory_usage(s, column_kind::static_column);                     dst.partition().apply(current->partition().partition_tombstone());                     if (static_row_continuous) {                         lazy_row& static_row = dst.partition().static_row();                         if (can_move) {                             static_row.apply(s, column_kind::static_column,                                 std::move(current->partition().static_row()));                         } else {                             static_row.apply(s, column_kind::static_column, current->partition().static_row());                         }                     }                     current = current->next();                     can_move &= current && !current->is_referenced();                 }                 acc.unpin_memory(dirty_size);                 static_done = true;             }             if (!src_cur.maybe_refresh_static()) {                 return stop_iteration::yes;             }             do {                 auto size = src_cur.memory_usage();                 // Range tombstones in memtables are bounded by dummy entries on both sides.
                assert(src_cur.range_tombstone_for_row() == src_cur.range_tombstone());                 if (src_cur.range_tombstone()) {                     // Apply the tombstone to (lb, src_cur.position())
                    // FIXME: Avoid if before all rows
                    auto ropt = cur.ensure_entry_if_complete(lb);                     cur.advance_to(lb); // ensure_entry_if_complete() leaves the cursor invalid. Bring back to valid.
                    // If !ropt, it means there is no entry at lb, so cur is guaranteed to be at a position
                    // greater than lb. No need to advance it.
                    if (ropt) {                         cur.next();                     }                     position_in_partition::less_compare less(s);                     assert(less(lb, cur.position()));                     while (less(cur.position(), src_cur.position())) {                         auto res = cur.ensure_entry_in_latest();                         if (cur.continuous()) {                             assert(cur.dummy() || cur.range_tombstone_for_row() == cur.range_tombstone());                             res.row.set_continuous(is_continuous::yes);                         }                         res.row.set_range_tombstone(cur.range_tombstone_for_row() + src_cur.range_tombstone());                         // FIXME: Compact the row
                        ++tracker.get_stats().rows_covered_by_range_tombstones_from_memtable;                         cur.next();                         // FIXME: preempt
                    }                 }                 {                     if (src_cur.dummy()) {                         ++tracker.get_stats().dummy_processed_from_memtable;                     } else {                         tracker.on_row_processed_from_memtable();                     }                     auto ropt = cur.ensure_entry_if_complete(src_cur.position());                     if (ropt) {                         if (!ropt->inserted) {                             tracker.on_row_merged_from_memtable();                         }                         rows_entry& e = ropt->row;                         if (!src_cur.dummy()) {                             src_cur.consume_row([&](deletable_row&& row) {                                 e.row().apply_monotonically(s, std::move(row));                             });                         }                         // We can set cont=1 only if there is a range tombstone because
                        // only then the lower bound of the range is ensured in the latest version earlier.
                        if (src_cur.range_tombstone()) {                             if (cur.continuous()) {                                 assert(cur.dummy() || cur.range_tombstone_for_row() == cur.range_tombstone());                                 e.set_continuous(is_continuous::yes);                             }                             e.set_range_tombstone(cur.range_tombstone_for_row() + src_cur.range_tombstone());                         }                     } else {                         tracker.on_row_dropped_from_memtable();                     }                 }                 // FIXME: Avoid storing lb if no range tombstones
                lb = position_in_partition(src_cur.position());                 auto has_next = src_cur.erase_and_advance();                 acc.unpin_memory(size);                 if (!has_next) {                     dst_snp->unlock();                     return stop_iteration::yes;                 }             } while (!preemptible || !need_preempt());             return stop_iteration::no;         });     }); }
 mutation_partition_v2 partition_entry::squashed(schema_ptr from, schema_ptr to, is_evictable evictable) {     mutation_partition_v2 mp(to);     mp.set_static_row_continuous(_version->partition().static_row_continuous());     for (auto&& v : _version->all_elements()) {         auto older = mutation_partition_v2(*from, v.partition());         if (from->version() != to->version()) {             older.upgrade(*from, *to);         }         merge_versions(*to, mp, std::move(older), no_cache_tracker, evictable);     }     return mp; }
 mutation_partition partition_entry::squashed(const schema& s, is_evictable evictable) {     return squashed(s.shared_from_this(), s.shared_from_this(), evictable)         .as_mutation_partition(s); }
 void partition_entry::upgrade(schema_ptr from, schema_ptr to, mutation_cleaner& cleaner, cache_tracker* tracker) {     auto new_version = current_allocator().construct<partition_version>(squashed(from, to, is_evictable(bool(tracker))));     auto old_version = &*_version;     set_version(new_version);     if (tracker) {         tracker->insert(*new_version);     }     remove_or_mark_as_unique_owner(old_version, &cleaner); }
 partition_snapshot_ptr partition_entry::read(logalloc::region& r,     mutation_cleaner& cleaner, schema_ptr entry_schema, cache_tracker* tracker, partition_snapshot::phase_type phase) {     if (_snapshot) {         if (_snapshot->_phase == phase) {             return _snapshot->shared_from_this();         } else if (phase < _snapshot->_phase) {             // If entry is being updated, we will get reads for non-latest phase, and
            // they must attach to the non-current version.
            partition_version* second = _version->next();             assert(second && second->is_referenced());             auto snp = partition_snapshot::container_of(second->_backref).shared_from_this();             assert(phase == snp->_phase);             return snp;         } else { // phase > _snapshot->_phase
            with_allocator(r.allocator(), [&] {                 add_version(*entry_schema, tracker);             });         }     }     auto snp = make_lw_shared<partition_snapshot>(entry_schema, r, cleaner, this, tracker, phase);     _snapshot = snp.get();     return partition_snapshot_ptr(std::move(snp)); }
 void partition_snapshot::touch() noexcept {     // Eviction assumes that older versions are evicted before newer so only the latest snapshot
    // can be touched.
    if (_tracker && at_latest_version()) {         auto&& rows = version()->partition().clustered_rows();         assert(!rows.empty());         rows_entry& last_dummy = *rows.rbegin();         assert(last_dummy.is_last_dummy());         _tracker->touch(last_dummy);     } }
 std::ostream& operator<<(std::ostream& out, const partition_entry::printer& p) {     auto& e = p._partition_entry;     out << "{";     bool first = true;     if (e._version) {         const partition_version* v = &*e._version;         while (v) {             if (!first) {                 out << ", ";             }             if (v->is_referenced()) {                 partition_snapshot* snp = nullptr;                 if (first) {                     snp = e._snapshot;                 } else {                     snp = &partition_snapshot::container_of(&v->back_reference());                 }                 out << "(*";                 if (snp) {                     out << " snp=" << snp << ", phase=" << snp->phase();                 }                 out << ") ";             }             out << fmt::ptr(v) << ": " << mutation_partition_v2::printer(p._schema, v->partition());             v = v->next();             first = false;         }     }     out << "}";     return out; }
 void partition_entry::evict(mutation_cleaner& cleaner) noexcept {     if (!_version) {         return;     }     if (_snapshot) {         assert(!_snapshot->is_locked());         _snapshot->_version = std::move(_version);         _snapshot->_version.mark_as_unique_owner();         _snapshot->_entry = nullptr;     } else {         auto v = &*_version;         _version = { };         remove_or_mark_as_unique_owner(v, &cleaner);     } }
 partition_snapshot_ptr::~partition_snapshot_ptr() {     if (_snp) {         auto&& cleaner = _snp->cleaner();         auto snp = _snp.release();         if (snp) {             cleaner.merge_and_destroy(*snp.release());         }     } }
 void partition_snapshot::lock() noexcept {     // partition_entry::is_locked() assumes that if there is a locked snapshot,
    // it can be found attached directly to it.
    assert(at_latest_version());     _locked = true; }
 void partition_snapshot::unlock() noexcept {     // Locked snapshots must always be latest, is_locked() assumes that.
    // Also, touch() is only effective when this snapshot is latest. 
    assert(at_latest_version());     _locked = false;     touch(); // Make the entry evictable again in case it was fully unlinked by eviction attempt.
}
 std::optional<range_tombstone> range_tombstone::apply(const schema& s, range_tombstone&& src) {     bound_view::compare cmp(s);     if (tomb == src.tomb) {         if (cmp(end_bound(), src.end_bound())) {             end = std::move(src.end);             end_kind = src.end_kind;         }         return { };     }     if (tomb < src.tomb) {         std::swap(*this, src);     }     if (cmp(end_bound(), src.end_bound())) {         return range_tombstone(end, invert_kind(end_kind), std::move(src.end), src.end_kind, src.tomb);     }     return { }; }
 position_in_partition_view range_tombstone::position() const {     return position_in_partition_view(position_in_partition_view::range_tombstone_tag_t(), start_bound()); }
 position_in_partition_view range_tombstone::end_position() const {     return position_in_partition_view(position_in_partition_view::range_tombstone_tag_t(), end_bound()); }
 void range_tombstone_accumulator::update_current_tombstone() {     _current_tombstone = boost::accumulate(_range_tombstones, _partition_tombstone, [] (tombstone t, const range_tombstone& rt) {         t.apply(rt.tomb);         return t;     }); }
 void range_tombstone_accumulator::drop_unneeded_tombstones(const clustering_key_prefix& ck, int w) {     auto cmp = [&] (const range_tombstone& rt, const clustering_key_prefix& ck, int w) {         auto bv = rt.end_bound();         return _cmp(bv.prefix(), weight(bv.kind()), ck, w);     };     bool dropped = false;     while (!_range_tombstones.empty() && cmp(*_range_tombstones.begin(), ck, w)) {         dropped = true;         _range_tombstones.pop_front();     }     if (dropped) {         update_current_tombstone();     } }
 void range_tombstone_accumulator::apply(range_tombstone rt) {     drop_unneeded_tombstones(rt.start, weight(rt.start_kind));     _current_tombstone.apply(rt.tomb);     auto cmp = [&] (const range_tombstone& rt1, const range_tombstone& rt2) {         return _cmp(rt1.end_bound(), rt2.end_bound());     };     _range_tombstones.insert(boost::upper_bound(_range_tombstones, rt, cmp), std::move(rt)); }
 void range_tombstone_accumulator::clear() {     _range_tombstones.clear();     _partition_tombstone = { };     _current_tombstone = { }; }
 range_tombstone_list::range_tombstone_list(const range_tombstone_list& x)         : _tombstones(x._tombstones.value_comp()) {     auto cloner = [] (const range_tombstone_entry& x) {         return current_allocator().construct<range_tombstone_entry>(x);     };     _tombstones.clone_from(x._tombstones, cloner, current_deleter<range_tombstone_entry>()); }
 range_tombstone_list::~range_tombstone_list() {     _tombstones.clear_and_dispose(current_deleter<range_tombstone_entry>()); }
 template <typename... Args> static auto construct_range_tombstone_entry(Args&&... args) {     return alloc_strategy_unique_ptr<range_tombstone_entry>(current_allocator().construct<range_tombstone_entry>(range_tombstone(std::forward<Args>(args)...))); }
 void range_tombstone_list::apply_reversibly(const schema& s,         clustering_key_prefix start_key, bound_kind start_kind,         clustering_key_prefix end_key,         bound_kind end_kind,         tombstone tomb,         reverter& rev) {     position_in_partition::less_compare less(s);     position_in_partition start(position_in_partition::range_tag_t(), bound_view(std::move(start_key), start_kind));     position_in_partition end(position_in_partition::range_tag_t(), bound_view(std::move(end_key), end_kind));     if (!less(start, end)) {         return;     }     if (!_tombstones.empty()) {         auto last = --_tombstones.end();         range_tombstones_type::iterator it;         if (less(start, last->end_position())) {             it = _tombstones.upper_bound(start, [less](auto&& sb, auto&& rt) {                 return less(sb, rt.end_position());             });         } else {             it = _tombstones.end();         }         insert_from(s, std::move(it), std::move(start), std::move(end), std::move(tomb), rev);         return;     }     auto rt = construct_range_tombstone_entry(std::move(start), std::move(end), std::move(tomb));     rev.insert(_tombstones.end(), *rt);     rt.release(); }
 void range_tombstone_list::insert_from(const schema& s,     range_tombstones_type::iterator it,     position_in_partition start,     position_in_partition end,     tombstone tomb,     reverter& rev) {     position_in_partition::tri_compare cmp(s);     if (it != _tombstones.begin()) {         auto prev = std::prev(it);         if (prev->tombstone().tomb == tomb && cmp(prev->end_position(), start) == 0) {             start = prev->position();             rev.erase(prev);         }     }     while (it != _tombstones.end()) {         if (cmp(end, start) <= 0) {             return;         }         if (cmp(end, it->position()) < 0) {             // not overlapping
            if (it->tombstone().tomb == tomb && cmp(end, it->position()) == 0) {                 rev.update(it, {std::move(start), std::move(end), tomb});             } else {                 auto rt = construct_range_tombstone_entry(std::move(start), std::move(end), tomb);                 rev.insert(it, *rt);                 rt.release();             }             return;         }         auto c = tomb <=> it->tombstone().tomb;         if (c == 0) {             // same timestamp, overlapping or adjacent, so merge.
            if (cmp(it->position(), start) < 0) {                 start = it->position();             }             if (cmp(end, it->end_position()) < 0) {                 end = it->end_position();             }             it = rev.erase(it);         } else if (c > 0) {             // We overwrite the current tombstone.
            if (cmp(it->position(), start) < 0) {                 {                     auto rt = construct_range_tombstone_entry(it->position(), start, it->tombstone().tomb);                     rev.update(it, {start, it->end_position(), it->tombstone().tomb});                     rev.insert(it, *rt);                     rt.release();                 }             }             if (cmp(end, it->end_position()) < 0) {                 // Here start <= it->start and end < it->end.
                auto rt = construct_range_tombstone_entry(std::move(start), end, std::move(tomb));                 rev.update(it, {std::move(end), it->end_position(), it->tombstone().tomb});                 rev.insert(it, *rt);                 rt.release();                 return;             }             // Here start <= it->start and end >= it->end.
            it = rev.erase(it);         } else {             // We don't overwrite the current tombstone.
            if (cmp(start, it->position()) < 0) {                 // The new tombstone starts before the current one.
                if (cmp(it->position(), end) < 0) {                     // Here start < it->start and it->start < end.
                    {                         auto rt = construct_range_tombstone_entry(std::move(start), it->position(), tomb);                         it = rev.insert(it, *rt);                         rt.release();                         ++it;                     }                 } else {                     // Here start < it->start and end <= it->start, so just insert the new tombstone.
                    auto rt = construct_range_tombstone_entry(std::move(start), std::move(end), std::move(tomb));                     rev.insert(it, *rt);                     rt.release();                     return;                 }             }             if (cmp(it->end_position(), end) < 0) {                 // Here the current tombstone overwrites a range of the new one.
                start = it->end_position();                 ++it;             } else {                 // Here the current tombstone completely overwrites the new one.
                return;             }         }     }     // If we got here, then just insert the remainder at the end.
    auto rt = construct_range_tombstone_entry(std::move(start), std::move(end), std::move(tomb));     rev.insert(it, *rt);     rt.release(); }
 range_tombstone_list::range_tombstones_type::iterator range_tombstone_list::find(const schema& s, const range_tombstone_entry& rt) {     bound_view::compare less(s);     auto it = _tombstones.find(rt, [less](auto&& rt1, auto&& rt2) {         return less(rt1.end_bound(), rt2.end_bound());     });     if (it != _tombstones.end() && it->tombstone().equal(s, rt.tombstone())) {         return it;     }     return _tombstones.end(); }
 tombstone range_tombstone_list::search_tombstone_covering(const schema& s, const clustering_key_prefix& key) const {     bound_view::compare less(s);     auto it = _tombstones.upper_bound(key, [less](auto&& k, auto&& rt) {         return less(k, rt.end_bound());     });     if (it == _tombstones.end() || less(key, it->start_bound())) {         return {};     }     return it->tombstone().tomb; }
 range_tombstone_list range_tombstone_list::difference(const schema& s, const range_tombstone_list& other) const {     range_tombstone_list diff(s);     bound_view::compare cmp_rt(s);     auto other_rt = other.begin();     auto this_rt = begin();     if (this_rt == end()) {         return diff;     }     bound_view cur_start = this_rt->start_bound();     bound_view cur_end = this_rt->end_bound();     auto advance_this_rt = [&] () {         if (++this_rt != end()) {             cur_start = this_rt->start_bound();             cur_end = this_rt->end_bound();         }     };     while (this_rt != end() && other_rt != other.end()) {         if (cmp_rt(cur_end, other_rt->start_bound())) {             diff.apply(s, cur_start, cur_end, this_rt->tombstone().tomb);             advance_this_rt();             continue;         }         if (cmp_rt(other_rt->end_bound(), cur_start)) {             ++other_rt;             continue;         }         auto new_end = bound_view(other_rt->start_bound().prefix(), invert_kind(other_rt->start_bound().kind()));         if (cmp_rt(cur_start, new_end)) {             diff.apply(s, cur_start, new_end, this_rt->tombstone().tomb);             cur_start = other_rt->start_bound();         }         if (cmp_rt(cur_end, other_rt->end_bound())) {             if (this_rt->tombstone().tomb > other_rt->tombstone().tomb) {                 diff.apply(s, cur_start, cur_end, this_rt->tombstone().tomb);             }             advance_this_rt();         } else {             auto end = other_rt->end_bound();             if (this_rt->tombstone().tomb > other_rt->tombstone().tomb) {                 diff.apply(s, cur_start, end, this_rt->tombstone().tomb);             }             cur_start = bound_view(end.prefix(), invert_kind(end.kind()));             ++other_rt;             if (cmp_rt(cur_end, cur_start)) {                 advance_this_rt();             }         }     }     while (this_rt != end()) {         diff.apply(s, cur_start, cur_end, this_rt->tombstone().tomb);         advance_this_rt();     }     return diff; }
 stop_iteration range_tombstone_list::clear_gently() noexcept {     auto del = current_deleter<range_tombstone_entry>();     auto i = _tombstones.begin();     auto end = _tombstones.end();     while (i != end) {         i = _tombstones.erase_and_dispose(i, del);         if (need_preempt()) {             return stop_iteration::no;         }     }     return stop_iteration::yes; }
 void range_tombstone_list::apply(const schema& s, const range_tombstone_list& rt_list) {     for (auto&& rt : rt_list) {         apply(s, rt.tombstone());     } }
 // See reversibly_mergeable.hh
range_tombstone_list::reverter range_tombstone_list::apply_reversibly(const schema& s, range_tombstone_list& rt_list) {     reverter rev(s, *this);     for (auto&& rt : rt_list) {         apply_reversibly(s, rt.tombstone().start, rt.tombstone().start_kind, rt.tombstone().end, rt.tombstone().end_kind, rt.tombstone().tomb, rev);     }     return rev; }
 namespace { struct bv_order_by_end {     bound_view::compare less;     bv_order_by_end(const schema& s) : less(s) {}     bool operator()(bound_view v, const range_tombstone_entry& rt) const { return less(v, rt.end_bound()); }     bool operator()(const range_tombstone_entry& rt, bound_view v) const { return less(rt.end_bound(), v); } }; struct bv_order_by_start {     bound_view::compare less;     bv_order_by_start(const schema& s) : less(s) {}     bool operator()(bound_view v, const range_tombstone_entry& rt) const { return less(v, rt.start_bound()); }     bool operator()(const range_tombstone_entry& rt, bound_view v) const { return less(rt.start_bound(), v); } }; struct pos_order_by_end {     position_in_partition::less_compare less;     pos_order_by_end(const schema& s) : less(s) {}     bool operator()(position_in_partition_view v, const range_tombstone_entry& rt) const { return less(v, rt.end_position()); }     bool operator()(const range_tombstone_entry& rt, position_in_partition_view v) const { return less(rt.end_position(), v); } }; struct pos_order_by_start {     position_in_partition::less_compare less;     pos_order_by_start(const schema& s) : less(s) {}     bool operator()(position_in_partition_view v, const range_tombstone_entry& rt) const { return less(v, rt.position()); }     bool operator()(const range_tombstone_entry& rt, position_in_partition_view v) const { return less(rt.position(), v); } }; }
 // namespace
range_tombstone_list::iterator_range range_tombstone_list::slice(const schema& s, const query::clustering_range& r) const {     auto bv_range = bound_view::from_range(r);     return boost::make_iterator_range(         _tombstones.lower_bound(bv_range.first, bv_order_by_end{s}),         _tombstones.upper_bound(bv_range.second, bv_order_by_start{s})); }
 range_tombstone_list::iterator_range range_tombstone_list::slice(const schema& s, position_in_partition_view start, position_in_partition_view end) const {     return boost::make_iterator_range(         _tombstones.upper_bound(start, pos_order_by_end{s}), // end_position() is exclusive, hence upper_bound()
        _tombstones.lower_bound(end, pos_order_by_start{s})); }
 range_tombstone_list::iterator_range range_tombstone_list::lower_slice(const schema& s, bound_view start, position_in_partition_view before) const {     return boost::make_iterator_range(         _tombstones.lower_bound(start, bv_order_by_end{s}),         _tombstones.lower_bound(before, pos_order_by_end{s})); }
 range_tombstone_list::iterator_range range_tombstone_list::upper_slice(const schema& s, position_in_partition_view after, bound_view end) const {     return boost::make_iterator_range(         _tombstones.upper_bound(after, pos_order_by_start{s}),         _tombstones.upper_bound(end, bv_order_by_start{s})); }
 range_tombstone_list::iterator range_tombstone_list::erase(const_iterator a, const_iterator b) {     return _tombstones.erase_and_dispose(a, b, current_deleter<range_tombstone_entry>()); }
 void range_tombstone_list::trim(const schema& s, const query::clustering_row_ranges& ranges) {     range_tombstone_list list(s);     bound_view::compare less(s);     for (auto&& range : ranges) {         auto start = bound_view::from_range_start(range);         auto end = bound_view::from_range_end(range);         for (const auto& rt : slice(s, range)) {             list.apply(s, range_tombstone(                 std::max(rt.start_bound(), start, less),                 std::min(rt.end_bound(), end, less),                 rt.tombstone().tomb));         }     }     *this = std::move(list); }
 range_tombstone_list::range_tombstones_type::iterator range_tombstone_list::reverter::insert(range_tombstones_type::iterator it, range_tombstone_entry& new_rt) {     _ops.emplace_back(insert_undo_op(new_rt));     return _dst._tombstones.insert_before(it, new_rt); }
 range_tombstone_list::range_tombstones_type::iterator range_tombstone_list::reverter::erase(range_tombstones_type::iterator it) {     _ops.emplace_back(std::in_place_type<erase_undo_op>, *it);     return _dst._tombstones.erase(it); }
 void range_tombstone_list::reverter::update(range_tombstones_type::iterator it, range_tombstone&& new_rt) {     _ops.emplace_back(std::in_place_type<update_undo_op>, std::move(it->tombstone()), *it);     it->tombstone() = std::move(new_rt); }
 void range_tombstone_list::reverter::revert() noexcept {     for (auto&& rt : _ops | boost::adaptors::reversed) {         seastar::visit(rt, [this] (auto& op) {             op.undo(_s, _dst);         });     }     cancel(); }
 range_tombstone_list::range_tombstones_type::iterator range_tombstone_list::nop_reverter::insert(range_tombstones_type::iterator it, range_tombstone_entry& new_rt) {     return _dst._tombstones.insert_before(it, new_rt); }
 range_tombstone_list::range_tombstones_type::iterator range_tombstone_list::nop_reverter::erase(range_tombstones_type::iterator it) {     return _dst._tombstones.erase_and_dispose(it, alloc_strategy_deleter<range_tombstone_entry>()); }
 void range_tombstone_list::nop_reverter::update(range_tombstones_type::iterator it, range_tombstone&& new_rt) {     *it = std::move(new_rt); }
 void range_tombstone_list::insert_undo_op::undo(const schema& s, range_tombstone_list& rt_list) noexcept {     auto it = rt_list.find(s, _new_rt);     assert (it != rt_list.end());     rt_list._tombstones.erase_and_dispose(it, current_deleter<range_tombstone_entry>()); }
 void range_tombstone_list::erase_undo_op::undo(const schema& s, range_tombstone_list& rt_list) noexcept {     rt_list._tombstones.insert(*_rt.release()); }
 void range_tombstone_list::update_undo_op::undo(const schema& s, range_tombstone_list& rt_list) noexcept {     auto it = rt_list.find(s, _new_rt);     assert (it != rt_list.end());     *it = std::move(_old_rt); }
 bool range_tombstone_list::equal(const schema& s, const range_tombstone_list& other) const {     return boost::equal(_tombstones, other._tombstones, [&s] (auto&& rt1, auto&& rt2) {         return rt1.tombstone().equal(s, rt2.tombstone());     }); }
 stop_iteration range_tombstone_list::apply_monotonically(const schema& s, range_tombstone_list&& list, is_preemptible preemptible) {     auto del = current_deleter<range_tombstone_entry>();     auto it = list.begin();     while (it != list.end()) {         // FIXME: Optimize by stealing the entry
        apply_monotonically(s, it->tombstone());         it = list._tombstones.erase_and_dispose(it, del);         if (preemptible && need_preempt()) {             return stop_iteration::no;         }     }     return stop_iteration::yes; }
 void range_tombstone_list::apply_monotonically(const schema& s, const range_tombstone_list& list) {     for (auto&& rt : list) {         apply_monotonically(s, rt.tombstone());     } }
 void range_tombstone_list::apply_monotonically(const schema& s, const range_tombstone& rt) {     // FIXME: Optimize given this has relaxed exception guarantees.
    // Note that apply() doesn't have monotonic guarantee because it doesn't restore erased entries.
    reverter rev(s, *this);     apply_reversibly(s, rt.start, rt.start_kind, rt.end, rt.end_kind, rt.tomb, rev);     rev.cancel(); }
 size_t sstring_hash::operator()(std::string_view v) const noexcept {     return absl::Hash<std::string_view>{}(v); }
 bytes_view collection_mutation_input_stream::read_linearized(size_t n) {     managed_bytes_view mbv = ::read_simple_bytes(_src, n);     if (mbv.is_linearized()) {         return mbv.current_fragment();     } else {         return _linearized.emplace_front(linearized(mbv));     } }
 managed_bytes_view collection_mutation_input_stream::read_fragmented(size_t n) {     return ::read_simple_bytes(_src, n); }
 bool collection_mutation_input_stream::empty() const {     return _src.empty(); }
 collection_mutation::collection_mutation(const abstract_type& type, collection_mutation_view v)     : _data(v.data) {}
 collection_mutation::collection_mutation(const abstract_type& type, managed_bytes data)     : _data(std::move(data)) {}
 collection_mutation::operator collection_mutation_view() const {     return collection_mutation_view{managed_bytes_view(_data)}; }
 collection_mutation_view atomic_cell_or_collection::as_collection_mutation() const {     return collection_mutation_view{managed_bytes_view(_data)}; }
 bool collection_mutation_view::is_empty() const {     auto in = collection_mutation_input_stream(data);     auto has_tomb = in.read_trivial<uint8_t>();     return !has_tomb && in.read_trivial<uint32_t>() == 0; }
 bool collection_mutation_view::is_any_live(const abstract_type& type, tombstone tomb, gc_clock::time_point now) const {     auto in = collection_mutation_input_stream(data);     auto has_tomb = in.read_trivial<uint8_t>();     if (has_tomb) {         auto ts = in.read_trivial<api::timestamp_type>();         auto ttl = in.read_trivial<gc_clock::duration::rep>();         tomb.apply(tombstone{ts, gc_clock::time_point(gc_clock::duration(ttl))});     }     auto nr = in.read_trivial<uint32_t>();     for (uint32_t i = 0; i != nr; ++i) {         auto key_size = in.read_trivial<uint32_t>();         in.read_fragmented(key_size); // Skip
        auto vsize = in.read_trivial<uint32_t>();         auto value = atomic_cell_view::from_bytes(type, in.read_fragmented(vsize));         if (value.is_live(tomb, now, false)) {             return true;         }     }     return false; }
 api::timestamp_type collection_mutation_view::last_update(const abstract_type& type) const {     auto in = collection_mutation_input_stream(data);     api::timestamp_type max = api::missing_timestamp;     auto has_tomb = in.read_trivial<uint8_t>();     if (has_tomb) {         max = std::max(max, in.read_trivial<api::timestamp_type>());         (void)in.read_trivial<gc_clock::duration::rep>();     }     auto nr = in.read_trivial<uint32_t>();     for (uint32_t i = 0; i != nr; ++i) {         const auto key_size = in.read_trivial<uint32_t>();         in.read_fragmented(key_size); // Skip
        auto vsize = in.read_trivial<uint32_t>();         auto value = atomic_cell_view::from_bytes(type, in.read_fragmented(vsize));         max = std::max(value.timestamp(), max);     }     return max; }
 std::ostream& operator<<(std::ostream& os, const collection_mutation_view::printer& cmvp) {     fmt::print(os, "{{collection_mutation_view ");     cmvp._cmv.with_deserialized(cmvp._type, [&os, &type = cmvp._type] (const collection_mutation_view_description& cmvd) {         bool first = true;         fmt::print(os, "tombstone {}", cmvd.tomb);         visit(type, make_visitor(         [&] (const collection_type_impl& ctype) {             auto&& key_type = ctype.name_comparator();             auto&& value_type = ctype.value_comparator();             for (auto&& [key, value] : cmvd.cells) {                 if (!first) {                     fmt::print(os, ", ");                 }                 fmt::print(os, "{}: {}", key_type->to_string(key), atomic_cell_view::printer(*value_type, value));                 first = false;             }         },         [&] (const user_type_impl& utype) {             for (auto&& [raw_idx, value] : cmvd.cells) {                 if (!first) {                     fmt::print(os, ", ");                 }                 auto idx = deserialize_field_index(raw_idx);                 fmt::print(os, "{}: {}", utype.field_name_as_string(idx), atomic_cell_view::printer(*utype.type(idx), value));                 first = false;             }         },         [&] (const abstract_type& o) {             // Not throwing exception in this likely-to-be debug context
            fmt::print(os, "attempted to pretty-print collection_mutation_view_description with type {}", o.name());         }         ));     });     fmt::print(os, "}}");     return os; }
 collection_mutation_description collection_mutation_view_description::materialize(const abstract_type& type) const {     collection_mutation_description m;     m.tomb = tomb;     m.cells.reserve(cells.size());     visit(type, make_visitor(     [&] (const collection_type_impl& ctype) {         auto& value_type = *ctype.value_comparator();         for (auto&& e : cells) {             m.cells.emplace_back(to_bytes(e.first), atomic_cell(value_type, e.second));         }     },     [&] (const user_type_impl& utype) {         for (auto&& e : cells) {             m.cells.emplace_back(to_bytes(e.first), atomic_cell(*utype.type(deserialize_field_index(e.first)), e.second));         }     },     [&] (const abstract_type& o) {         throw std::runtime_error(format("attempted to materialize collection_mutation_view_description with type {}", o.name()));     }     ));     return m; }
 bool collection_mutation_description::compact_and_expire(column_id id, row_tombstone base_tomb, gc_clock::time_point query_time,     can_gc_fn& can_gc, gc_clock::time_point gc_before, compaction_garbage_collector* collector) {     bool any_live = false;     auto t = tomb;     tombstone purged_tomb;     if (tomb <= base_tomb.regular()) {         tomb = tombstone();     } else if (tomb.deletion_time < gc_before && can_gc(tomb)) {         purged_tomb = tomb;         tomb = tombstone();     }     t.apply(base_tomb.regular());     utils::chunked_vector<std::pair<bytes, atomic_cell>> survivors;     utils::chunked_vector<std::pair<bytes, atomic_cell>> losers;     for (auto&& name_and_cell : cells) {         atomic_cell& cell = name_and_cell.second;         auto cannot_erase_cell = [&] {             return cell.deletion_time() >= gc_before || !can_gc(tombstone(cell.timestamp(), cell.deletion_time()));         };         if (cell.is_covered_by(t, false) || cell.is_covered_by(base_tomb.shadowable().tomb(), false)) {             continue;         }         if (cell.has_expired(query_time)) {             if (cannot_erase_cell()) {                 survivors.emplace_back(std::make_pair(                     std::move(name_and_cell.first), atomic_cell::make_dead(cell.timestamp(), cell.deletion_time())));             } else if (collector) {                 losers.emplace_back(std::pair(                         std::move(name_and_cell.first), atomic_cell::make_dead(cell.timestamp(), cell.deletion_time())));             }         } else if (!cell.is_live()) {             if (cannot_erase_cell()) {                 survivors.emplace_back(std::move(name_and_cell));             } else if (collector) {                 losers.emplace_back(std::move(name_and_cell));             }         } else {             any_live |= true;             survivors.emplace_back(std::move(name_and_cell));         }     }     if (collector) {         collector->collect(id, collection_mutation_description{purged_tomb, std::move(losers)});     }     cells = std::move(survivors);     return any_live; }
 template <typename Iterator> static collection_mutation serialize_collection_mutation(         const abstract_type& type,         const tombstone& tomb,         boost::iterator_range<Iterator> cells) {     auto element_size = [] (size_t c, auto&& e) -> size_t {         return c + 8 + e.first.size() + e.second.serialize().size();     };     auto size = accumulate(cells, (size_t)4, element_size);     size += 1;     if (tomb) {         size += sizeof(int64_t) + sizeof(int64_t);     }     managed_bytes ret(managed_bytes::initialized_later(), size);     managed_bytes_mutable_view out(ret);     write<uint8_t>(out, uint8_t(bool(tomb)));     if (tomb) {         write<int64_t>(out, tomb.timestamp);         write<int64_t>(out, tomb.deletion_time.time_since_epoch().count());     }     auto writek = [&out] (bytes_view v) {         write<int32_t>(out, v.size());         write_fragmented(out, single_fragmented_view(v));     };     auto writev = [&out] (managed_bytes_view v) {         write<int32_t>(out, v.size());         write_fragmented(out, v);     };     // FIXME: overflow?
    write<int32_t>(out, boost::distance(cells));     for (auto&& kv : cells) {         auto&& k = kv.first;         auto&& v = kv.second;         writek(k);         writev(v.serialize());     }     return collection_mutation(type, ret); }
 collection_mutation collection_mutation_description::serialize(const abstract_type& type) const {     return serialize_collection_mutation(type, tomb, boost::make_iterator_range(cells.begin(), cells.end())); }
 collection_mutation collection_mutation_view_description::serialize(const abstract_type& type) const {     return serialize_collection_mutation(type, tomb, boost::make_iterator_range(cells.begin(), cells.end())); }
 template <typename C> requires std::is_base_of_v<abstract_type, std::remove_reference_t<C>> static collection_mutation_view_description merge(collection_mutation_view_description a, collection_mutation_view_description b, C&& key_type) {     using element_type = std::pair<bytes_view, atomic_cell_view>;     auto compare = [&] (const element_type& e1, const element_type& e2) {         return key_type.less(e1.first, e2.first);     };     auto merge = [] (const element_type& e1, const element_type& e2) {         // FIXME: use std::max()?
        return std::make_pair(e1.first, compare_atomic_cell_for_merge(e1.second, e2.second) > 0 ? e1.second : e2.second);     };     // applied to a tombstone, returns a predicate checking whether a cell is killed by
    // the tombstone
    auto cell_killed = [] (const std::optional<tombstone>& t) {         return [&t] (const element_type& e) {             if (!t) {                 return false;             }             // tombstone wins if timestamps equal here, unlike row tombstones
            if (t->timestamp < e.second.timestamp()) {                 return false;             }             return true;             // FIXME: should we consider TTLs too?
        };     };     collection_mutation_view_description merged;     merged.cells.reserve(a.cells.size() + b.cells.size());     combine(a.cells.begin(), std::remove_if(a.cells.begin(), a.cells.end(), cell_killed(b.tomb)),             b.cells.begin(), std::remove_if(b.cells.begin(), b.cells.end(), cell_killed(a.tomb)),             std::back_inserter(merged.cells),             compare,             merge);     merged.tomb = std::max(a.tomb, b.tomb);     return merged; }
 collection_mutation merge(const abstract_type& type, collection_mutation_view a, collection_mutation_view b) {     return a.with_deserialized(type, [&] (collection_mutation_view_description a_view) {         return b.with_deserialized(type, [&] (collection_mutation_view_description b_view) {             return visit(type, make_visitor(             [&] (const collection_type_impl& ctype) {                 return merge(std::move(a_view), std::move(b_view), *ctype.name_comparator());             },             [&] (const user_type_impl& utype) {                 return merge(std::move(a_view), std::move(b_view), *short_type);             },             [] (const abstract_type& o) -> collection_mutation_view_description {                 throw std::runtime_error(format("collection_mutation merge: unknown type: {}", o.name()));             }             )).serialize(type);         });     }); }
 template <typename C> requires std::is_base_of_v<abstract_type, std::remove_reference_t<C>> static collection_mutation_view_description difference(collection_mutation_view_description a, collection_mutation_view_description b, C&& key_type) {     collection_mutation_view_description diff;     diff.cells.reserve(std::max(a.cells.size(), b.cells.size()));     auto it = b.cells.begin();     for (auto&& c : a.cells) {         while (it != b.cells.end() && key_type.less(it->first, c.first)) {             ++it;         }         if (it == b.cells.end() || !key_type.equal(it->first, c.first)             || compare_atomic_cell_for_merge(c.second, it->second) > 0) {             auto cell = std::make_pair(c.first, c.second);             diff.cells.emplace_back(std::move(cell));         }     }     if (a.tomb > b.tomb) {         diff.tomb = a.tomb;     }     return diff; }
 collection_mutation difference(const abstract_type& type, collection_mutation_view a, collection_mutation_view b) {     return a.with_deserialized(type, [&] (collection_mutation_view_description a_view) {         return b.with_deserialized(type, [&] (collection_mutation_view_description b_view) {             return visit(type, make_visitor(             [&] (const collection_type_impl& ctype) {                 return difference(std::move(a_view), std::move(b_view), *ctype.name_comparator());             },             [&] (const user_type_impl& utype) {                 return difference(std::move(a_view), std::move(b_view), *short_type);             },             [] (const abstract_type& o) -> collection_mutation_view_description {                 throw std::runtime_error(format("collection_mutation difference: unknown type: {}", o.name()));             }             )).serialize(type);         });     }); }
 template <typename F> requires std::is_invocable_r_v<std::pair<bytes_view, atomic_cell_view>, F, collection_mutation_input_stream&> static collection_mutation_view_description deserialize_collection_mutation(collection_mutation_input_stream& in, F&& read_kv) {     collection_mutation_view_description ret;     auto has_tomb = in.read_trivial<uint8_t>();     if (has_tomb) {         auto ts = in.read_trivial<api::timestamp_type>();         auto ttl = in.read_trivial<gc_clock::duration::rep>();         ret.tomb = tombstone{ts, gc_clock::time_point(gc_clock::duration(ttl))};     }     auto nr = in.read_trivial<uint32_t>();     ret.cells.reserve(nr);     for (uint32_t i = 0; i != nr; ++i) {         ret.cells.push_back(read_kv(in));     }     assert(in.empty());     return ret; }
 collection_mutation_view_description deserialize_collection_mutation(const abstract_type& type, collection_mutation_input_stream& in) {     return visit(type, make_visitor(     [&] (const collection_type_impl& ctype) {         // value_comparator(), ugh
        return deserialize_collection_mutation(in, [&ctype] (collection_mutation_input_stream& in) {             // FIXME: we could probably avoid the need for size
            auto ksize = in.read_trivial<uint32_t>();             auto key = in.read_linearized(ksize);             auto vsize = in.read_trivial<uint32_t>();             auto value = atomic_cell_view::from_bytes(*ctype.value_comparator(), in.read_fragmented(vsize));             return std::make_pair(key, value);         });     },     [&] (const user_type_impl& utype) {         return deserialize_collection_mutation(in, [&utype] (collection_mutation_input_stream& in) {             // FIXME: we could probably avoid the need for size
            auto ksize = in.read_trivial<uint32_t>();             auto key = in.read_linearized(ksize);             auto vsize = in.read_trivial<uint32_t>();             auto value = atomic_cell_view::from_bytes(*utype.type(deserialize_field_index(key)), in.read_fragmented(vsize));             return std::make_pair(key, value);         });     },     [&] (const abstract_type& o) -> collection_mutation_view_description {         throw std::runtime_error(format("deserialize_collection_mutation: unknown type {}", o.name()));     }     )); }
 caching_options::caching_options(sstring k, sstring r, bool enabled)         : _key_cache(k), _row_cache(r), _enabled(enabled) {     if ((k != "ALL") && (k != "NONE")) {         throw exceptions::configuration_exception("Invalid key value: " + k);      }     if ((r == "ALL") || (r == "NONE")) {         return;     } else {         try {             boost::lexical_cast<unsigned long>(r);         } catch (boost::bad_lexical_cast& e) {             throw exceptions::configuration_exception("Invalid key value: " + r);         }     } }
 caching_options::caching_options()         : _key_cache(default_key), _row_cache(default_row) { }
 std::map<sstring, sstring> caching_options::to_map() const {     std::map<sstring, sstring> res = {{ "keys", _key_cache },             { "rows_per_partition", _row_cache }};     if (!_enabled) {         res.insert({"enabled", "false"});     }     return res; }
 sstring caching_options::to_sstring() const {     return rjson::print(rjson::from_string_map(to_map())); }
 caching_options caching_options::get_disabled_caching_options() {     return caching_options("NONE", "NONE", false); }
 caching_options caching_options::from_map(const std::map<sstring, sstring>& map) {     sstring k = default_key;     sstring r = default_row;     bool e = true;     for (auto& p : map) {         if (p.first == "keys") {             k = p.second;         } else if (p.first == "rows_per_partition") {             r = p.second;         } else if (p.first == "enabled") {             e = p.second == "true";         } else {             throw exceptions::configuration_exception(format("Invalid caching option: {}", p.first));         }     }     return caching_options(k, r, e); }
 caching_options caching_options::from_sstring(const sstring& str) {     return from_map(rjson::parse_to_map<std::map<sstring, sstring>>(str)); }
 constexpr int32_t schema::NAME_LENGTH;
 extern logging::logger dblog;
   column_mapping_entry::column_mapping_entry(bytes name, sstring type_name)     : column_mapping_entry(std::move(name), db::marshal::type_parser::parse(type_name)) { }
 column_mapping_entry::column_mapping_entry(const column_mapping_entry& o)     : column_mapping_entry(o._name, o._type->name()) { }
 column_mapping_entry& column_mapping_entry::operator=(const column_mapping_entry& o) {     auto copy = o;     return operator=(std::move(copy)); }
   template<typename Sequence> std::vector<data_type> get_column_types(const Sequence& column_definitions) {     std::vector<data_type> result;     for (auto&& col : column_definitions) {         result.push_back(col.type);     }     return result; }
   thread_local std::map<sstring, std::unique_ptr<dht::i_partitioner>> partitioners;
 thread_local std::map<std::pair<unsigned, unsigned>, std::unique_ptr<dht::sharder>> sharders;
 sstring default_partitioner_name = "org.apache.cassandra.dht.Murmur3Partitioner";
 unsigned default_partitioner_ignore_msb = 12;
 static const dht::i_partitioner& get_partitioner(const sstring& name) {     auto it = partitioners.find(name);     if (it == partitioners.end()) {         auto p = dht::make_partitioner(name);         it = partitioners.insert({name, std::move(p)}).first;     }     return *it->second; }
 void schema::set_default_partitioner(const sstring& class_name, unsigned ignore_msb) {     default_partitioner_name = class_name;     default_partitioner_ignore_msb = ignore_msb; }
 static const dht::sharder& get_sharder(unsigned shard_count, unsigned ignore_msb) {     auto it = sharders.find({shard_count, ignore_msb});     if (it == sharders.end()) {         auto sharder = std::make_unique<dht::sharder>(shard_count, ignore_msb);         it = sharders.emplace(std::make_pair(shard_count, ignore_msb), std::move(sharder)).first;     }     return *it->second; }
 const dht::i_partitioner& schema::get_partitioner() const {     return _raw._partitioner.get(); }
 const dht::sharder& schema::get_sharder() const {     return _raw._sharder.get(); }
 bool schema::has_custom_partitioner() const {     return _raw._partitioner.get().name() != default_partitioner_name; }
 lw_shared_ptr<cql3::column_specification> schema::make_column_specification(const column_definition& def) const {     auto id = ::make_shared<cql3::column_identifier>(def.name(), column_name_type(def));     return make_lw_shared<cql3::column_specification>(_raw._ks_name, _raw._cf_name, std::move(id), def.type); }
 v3_columns::v3_columns(std::vector<column_definition> cols, bool is_dense, bool is_compound)     : _is_dense(is_dense)     , _is_compound(is_compound)     , _columns(std::move(cols)) {     for (column_definition& def : _columns) {         _columns_by_name[def.name()] = &def;     } }
 v3_columns v3_columns::from_v2_schema(const schema& s) {     data_type static_column_name_type = utf8_type;     std::vector<column_definition> cols;     if (s.is_static_compact_table()) {         if (s.has_static_columns()) {             throw std::runtime_error(                 format("v2 static compact table should not have static columns: {}.{}", s.ks_name(), s.cf_name()));         }         if (s.clustering_key_size()) {             throw std::runtime_error(                 format("v2 static compact table should not have clustering columns: {}.{}", s.ks_name(), s.cf_name()));         }         static_column_name_type = s.regular_column_name_type();         for (auto& c : s.all_columns()) {             // Note that for "static" no-clustering compact storage we use static for the defined columns
            if (c.kind == column_kind::regular_column) {                 auto new_def = c;                 new_def.kind = column_kind::static_column;                 cols.push_back(new_def);             } else {                 cols.push_back(c);             }         }         schema_builder::default_names names(s._raw);         cols.emplace_back(to_bytes(names.clustering_name()), static_column_name_type, column_kind::clustering_key, 0);         cols.emplace_back(to_bytes(names.compact_value_name()), s.make_legacy_default_validator(), column_kind::regular_column, 0);     } else {         cols = s.all_columns();     }     for (column_definition& def : cols) {         data_type name_type = def.is_static() ? static_column_name_type : utf8_type;         auto id = ::make_shared<cql3::column_identifier>(def.name(), name_type);         def.column_specification = make_lw_shared<cql3::column_specification>(s.ks_name(), s.cf_name(), std::move(id), def.type);     }     return v3_columns(std::move(cols), s.is_dense(), s.is_compound()); }
 void v3_columns::apply_to(schema_builder& builder) const {     if (is_static_compact()) {         for (auto& c : _columns) {             if (c.kind == column_kind::regular_column) {                 builder.set_default_validation_class(c.type);             } else if (c.kind == column_kind::static_column) {                 auto new_def = c;                 new_def.kind = column_kind::regular_column;                 builder.with_column_ordered(new_def);             } else if (c.kind == column_kind::clustering_key) {                 builder.set_regular_column_name_type(c.type);             } else {                 builder.with_column_ordered(c);             }         }     } else {         for (auto& c : _columns) {             if (is_compact() && c.kind == column_kind::regular_column) {                 builder.set_default_validation_class(c.type);             }             builder.with_column_ordered(c);         }     } }
 bool v3_columns::is_static_compact() const {     return !_is_dense && !_is_compound; }
 bool v3_columns::is_compact() const {     return _is_dense || !_is_compound; }
 const std::unordered_map<bytes, const column_definition*>& v3_columns::columns_by_name() const {     return _columns_by_name; }
 const std::vector<column_definition>& v3_columns::all_columns() const {     return _columns; }
 void schema::rebuild() {     _partition_key_type = make_lw_shared<compound_type<>>(get_column_types(partition_key_columns()));     _clustering_key_type = make_lw_shared<compound_prefix>(get_column_types(clustering_key_columns()));     _clustering_key_size = column_offset(column_kind::static_column) - column_offset(column_kind::clustering_key);     _regular_column_count = _raw._columns.size() - column_offset(column_kind::regular_column);     _static_column_count = column_offset(column_kind::regular_column) - column_offset(column_kind::static_column);     _columns_by_name.clear();     for (const column_definition& def : all_columns()) {         _columns_by_name[def.name()] = &def;     }     static_assert(row_column_ids_are_ordered_by_name::value, "row columns don't need to be ordered by name");     if (!std::is_sorted(regular_columns().begin(), regular_columns().end(), column_definition::name_comparator(regular_column_name_type()))) {         throw std::runtime_error("Regular columns should be sorted by name");     }     if (!std::is_sorted(static_columns().begin(), static_columns().end(), column_definition::name_comparator(static_column_name_type()))) {         throw std::runtime_error("Static columns should be sorted by name");     }     {         std::vector<column_mapping_entry> cm_columns;         for (const column_definition& def : boost::range::join(static_columns(), regular_columns())) {             cm_columns.emplace_back(column_mapping_entry{def.name(), def.type});         }         _column_mapping = column_mapping(std::move(cm_columns), static_columns_count());     }     thrift()._compound = is_compound();     thrift()._is_dynamic = clustering_key_size() > 0;     if (is_counter()) {         for (auto&& cdef : boost::range::join(static_columns(), regular_columns())) {             if (!cdef.type->is_counter()) {                 throw exceptions::configuration_exception(format("Cannot add a non counter column ({}) in a counter column family", cdef.name_as_text()));             }         }     } else {         for (auto&& cdef : all_columns()) {             if (cdef.type->is_counter()) {                 throw exceptions::configuration_exception(format("Cannot add a counter column ({}) in a non counter column family", cdef.name_as_text()));             }         }     }     _v3_columns = v3_columns::from_v2_schema(*this);     _full_slice = make_shared<query::partition_slice>(partition_slice_builder(*this).build()); }
 const column_mapping& schema::get_column_mapping() const {     return _column_mapping; }
 schema::raw_schema::raw_schema(table_id id)     : _id(id)     , _partitioner(::get_partitioner(default_partitioner_name))     , _sharder(::get_sharder(smp::count, default_partitioner_ignore_msb)) { }
 schema::schema(private_tag, const raw_schema& raw, std::optional<raw_view_info> raw_view_info, const schema_static_props& props)     : _raw(raw)     , _static_props(props)     , _offsets([this] {         if (_raw._columns.size() > std::numeric_limits<column_count_type>::max()) {             throw std::runtime_error(format("Column count limit ({:d}) overflowed: {:d}",                                             std::numeric_limits<column_count_type>::max(), _raw._columns.size()));         }         auto& cols = _raw._columns;         std::array<column_count_type, 4> count = { 0, 0, 0, 0 };         auto i = cols.begin();         auto e = cols.end();         for (auto k : { column_kind::partition_key, column_kind::clustering_key, column_kind::static_column, column_kind::regular_column }) {             auto j = std::stable_partition(i, e, [k](const auto& c) {                 return c.kind == k;             });             count[column_count_type(k)] = std::distance(i, j);             i = j;         }         return std::array<column_count_type, 3> {                 count[0],                 count[0] + count[1],                 count[0] + count[1] + count[2],         };     }
()) {     std::sort(             _raw._columns.begin() + column_offset(column_kind::static_column),             _raw._columns.begin()                     + column_offset(column_kind::regular_column),             column_definition::name_comparator(static_column_name_type()));     std::sort(             _raw._columns.begin()                     + column_offset(column_kind::regular_column),             _raw._columns.end(), column_definition::name_comparator(regular_column_name_type()));     std::stable_sort(_raw._columns.begin(),               _raw._columns.begin() + column_offset(column_kind::clustering_key),               [] (auto x, auto y) { return x.id < y.id; });     std::stable_sort(_raw._columns.begin() + column_offset(column_kind::clustering_key),               _raw._columns.begin() + column_offset(column_kind::static_column),               [] (auto x, auto y) { return x.id < y.id; });     column_id id = 0;     for (auto& def : _raw._columns) {         def.column_specification = make_column_specification(def);         assert(!def.id || def.id == id - column_offset(def.kind));         def.ordinal_id = static_cast<ordinal_column_id>(id);         def.id = id - column_offset(def.kind);         auto dropped_at_it = _raw._dropped_columns.find(def.name_as_text());         if (dropped_at_it != _raw._dropped_columns.end()) {             def._dropped_at = std::max(def._dropped_at, dropped_at_it->second.timestamp);         }         def._thrift_bits = column_definition::thrift_bits();         {             // is_on_all_components
            // TODO : In origin, this predicate is "componentIndex == null", which is true in
            // a number of cases, some of which I've most likely missed...
            switch (def.kind) {             case column_kind::partition_key:                 // In origin, ci == null is true for a PK column where CFMetaData "keyValidator" is non-composite.
                // Which is true of #pk == 1
                def._thrift_bits.is_on_all_components = partition_key_size() == 1;                 break;             case column_kind::regular_column:                 if (_raw._is_dense) {                     // regular values in dense tables are alone, so they have no index
                    def._thrift_bits.is_on_all_components = true;                     break;                 }             default:                 // Or any other column where "comparator" is not compound
                def._thrift_bits.is_on_all_components = !thrift().has_compound_comparator();                 break;             }         }         ++id;     }     rebuild(); }
 schema::schema(const schema& o, const std::function<void(schema&)>& transform)     : _raw(o._raw)     , _static_props(o._static_props)     , _offsets(o._offsets) {     // Do the transformation after all the raw fields are initialized, but
    // *before* the derived fields are generated (from the raw ones).
    if (transform) {         transform(*this);     }     rebuild(); }
 schema::schema(const schema& o)     : schema(o, {}
) { }
 schema::schema(reversed_tag, const schema& o)     : schema(o, [] (schema& s) {         s._raw._version = reversed(s._raw._version);         for (auto& col : s._raw._columns) {             if (col.kind == column_kind::clustering_key) {                 col.type = reversed(col.type);             }         }     }
) { }
 lw_shared_ptr<const schema> make_shared_schema(std::optional<table_id> id, std::string_view ks_name,     std::string_view cf_name, std::vector<schema::column> partition_key, std::vector<schema::column> clustering_key,     std::vector<schema::column> regular_columns, std::vector<schema::column> static_columns,     data_type regular_column_name_type, sstring comment) {     schema_builder builder(std::move(ks_name), std::move(cf_name), std::move(id), std::move(regular_column_name_type));     for (auto&& column : partition_key) {         builder.with_column(std::move(column.name), std::move(column.type), column_kind::partition_key);     }     for (auto&& column : clustering_key) {         builder.with_column(std::move(column.name), std::move(column.type), column_kind::clustering_key);     }     for (auto&& column : regular_columns) {         builder.with_column(std::move(column.name), std::move(column.type));     }     for (auto&& column : static_columns) {         builder.with_column(std::move(column.name), std::move(column.type), column_kind::static_column);     }     builder.set_comment(comment);     return builder.build(); }
 schema::~schema() {     if (_registry_entry) {         _registry_entry->detach_schema();     } }
 schema_registry_entry* schema::registry_entry() const noexcept {     return _registry_entry; }
 sstring schema::thrift_key_validator() const {     if (partition_key_size() == 1) {         return partition_key_columns().begin()->type->name();     } else {         auto type_params = fmt::join(partition_key_columns()                             | boost::adaptors::transformed(std::mem_fn(&column_definition::type))                             | boost::adaptors::transformed(std::mem_fn(&abstract_type::name)),                                         ", ");         return format("org.apache.cassandra.db.marshal.CompositeType({})", type_params);     } }
 bool schema::has_multi_cell_collections() const {     return boost::algorithm::any_of(all_columns(), [] (const column_definition& cdef) {         return cdef.type->is_collection() && cdef.type->is_multi_cell();     }); }
 bool operator==(const schema& x, const schema& y) {     return x._raw._id == y._raw._id         && x._raw._ks_name == y._raw._ks_name         && x._raw._cf_name == y._raw._cf_name         && x._raw._columns == y._raw._columns         && x._raw._comment == y._raw._comment         && x._raw._default_time_to_live == y._raw._default_time_to_live         && x._raw._regular_column_name_type == y._raw._regular_column_name_type         && x._raw._bloom_filter_fp_chance == y._raw._bloom_filter_fp_chance         && x._raw._compressor_params == y._raw._compressor_params         && x._raw._is_dense == y._raw._is_dense         && x._raw._is_compound == y._raw._is_compound         && x._raw._type == y._raw._type         && x._raw._gc_grace_seconds == y._raw._gc_grace_seconds         && x.paxos_grace_seconds() == y.paxos_grace_seconds()         && x._raw._dc_local_read_repair_chance == y._raw._dc_local_read_repair_chance         && x._raw._read_repair_chance == y._raw._read_repair_chance         && x._raw._min_compaction_threshold == y._raw._min_compaction_threshold         && x._raw._max_compaction_threshold == y._raw._max_compaction_threshold         && x._raw._min_index_interval == y._raw._min_index_interval         && x._raw._max_index_interval == y._raw._max_index_interval         && x._raw._memtable_flush_period == y._raw._memtable_flush_period         && x._raw._speculative_retry == y._raw._speculative_retry         && x._raw._compaction_strategy == y._raw._compaction_strategy         && x._raw._compaction_strategy_options == y._raw._compaction_strategy_options         && x._raw._compaction_enabled == y._raw._compaction_enabled         && x.cdc_options() == y.cdc_options()         && x.tombstone_gc_options() == y.tombstone_gc_options()         && x._raw._caching_options == y._raw._caching_options         && x._raw._dropped_columns == y._raw._dropped_columns         && x._raw._collections == y._raw._collections         && x._raw._indices_by_name == y._raw._indices_by_name         && x._raw._is_counter == y._raw._is_counter         ; }
 index_metadata::index_metadata(const sstring& name,                                const index_options_map& options,                                index_metadata_kind kind,                                is_local_index local)     : _id{utils::UUID_gen::get_name_UUID(name)}
     , _name{name}
     , _kind{kind}
     , _options{options}
     , _local{bool(local)}
 {}
 bool index_metadata::operator==(const index_metadata& other) const {     return _id == other._id            && _name == other._name            && _kind == other._kind            && _options == other._options; }
 bool index_metadata::equals_noname(const index_metadata& other) const {     return _kind == other._kind && _options == other._options; }
 const table_id& index_metadata::id() const {     return _id; }
 const sstring& index_metadata::name() const {     return _name; }
 const index_metadata_kind index_metadata::kind() const {     return _kind; }
 const index_options_map& index_metadata::options() const {     return _options; }
 bool index_metadata::local() const {     return _local; }
 sstring index_metadata::get_default_index_name(const sstring& cf_name,                                                std::optional<sstring> root) {     if (root) {         // As noted in issue #3403, because table names in CQL only use word
        // characters [A-Za-z0-9_], the default index name should drop other
        // characters from the column name ("root").
        sstring name = root.value();         name.erase(std::remove_if(name.begin(), name.end(), [](char c) {             return !((c >= 'A' && c <= 'Z') ||                      (c >= 'a' && c <= 'z') ||                      (c >= '0' && c <= '9') ||                      (c == '_')); }), name.end());         return cf_name + "_" + name + "_idx";     }     return cf_name + "_idx"; }
 column_definition::column_definition(bytes name, data_type type, column_kind kind, column_id component_index, column_view_virtual is_view_virtual, column_computation_ptr computation, api::timestamp_type dropped_at)         : _name(std::move(name))         , _dropped_at(dropped_at)         , _is_atomic(type->is_atomic())         , _is_counter(type->is_counter())         , _is_view_virtual(is_view_virtual)         , _computation(std::move(computation))         , type(std::move(type))         , id(component_index)         , kind(kind) {}
 std::ostream& operator<<(std::ostream& os, const column_definition& cd) {     os << "ColumnDefinition{";     os << "name=" << cd.name_as_text();     os << ", type=" << cd.type->name();     os << ", kind=" << to_sstring(cd.kind);     if (cd.is_view_virtual()) {         os << ", view_virtual";     }     if (cd.is_computed()) {         os << ", computed:" << cd.get_computation().serialize();     }     os << ", componentIndex=" << (cd.has_component_index() ? std::to_string(cd.component_index()) : "null");     os << ", droppedAt=" << cd._dropped_at;     os << "}";     return os; }
 const column_definition* schema::get_column_definition(const bytes& name) const {     auto i = _columns_by_name.find(name);     if (i == _columns_by_name.end()) {         return nullptr;     }     return i->second; }
 const column_definition& schema::column_at(column_kind kind, column_id id) const {     return column_at(static_cast<ordinal_column_id>(column_offset(kind) + id)); }
 const column_definition& schema::column_at(ordinal_column_id ordinal_id) const {     if (size_t(ordinal_id) >= _raw._columns.size()) [[unlikely]] {         on_internal_error(dblog, format("{}.{}@{}: column id {:d} >= {:d}",             ks_name(), cf_name(), version(), size_t(ordinal_id), _raw._columns.size()));     }     return _raw._columns.at(static_cast<column_count_type>(ordinal_id)); }
 std::ostream& operator<<(std::ostream& os, const schema& s) {     os << "org.apache.cassandra.config.CFMetaData@" << &s << "[";     os << "cfId=" << s._raw._id;     os << ",ksName=" << s._raw._ks_name;     os << ",cfName=" << s._raw._cf_name;     os << ",cfType=" << cf_type_to_sstring(s._raw._type);     os << ",comparator=" << cell_comparator::to_sstring(s);     os << ",comment=" << s._raw._comment;     os << ",readRepairChance=" << s._raw._read_repair_chance;     os << ",dcLocalReadRepairChance=" << s._raw._dc_local_read_repair_chance;     os << ",tombstoneGcOptions=" << s.tombstone_gc_options().to_sstring();     os << ",gcGraceSeconds=" << s._raw._gc_grace_seconds;     os << ",keyValidator=" << s.thrift_key_validator();     os << ",minCompactionThreshold=" << s._raw._min_compaction_threshold;     os << ",maxCompactionThreshold=" << s._raw._max_compaction_threshold;     os << ",columnMetadata=[";     int n = 0;     for (auto& cdef : s._raw._columns) {         if (n++ != 0) {             os << ", ";         }         os << cdef;     }     os << "]";     os << ",compactionStrategyClass=class org.apache.cassandra.db.compaction." << sstables::compaction_strategy::name(s._raw._compaction_strategy);     os << ",compactionStrategyOptions={";     n = 0;     for (auto& p : s._raw._compaction_strategy_options) {         os << p.first << "=" << p.second;         os << ", ";     }     os << "enabled=" << std::boolalpha << s._raw._compaction_enabled;     os << "}";     os << ",compressionParameters={";     n = 0;     for (auto& p : s._raw._compressor_params.get_options() ) {         if (n++ != 0) {             os << ", ";         }         os << p.first << "=" << p.second;     }     os << "}";     os << ",bloomFilterFpChance=" << s._raw._bloom_filter_fp_chance;     os << ",memtableFlushPeriod=" << s._raw._memtable_flush_period;     os << ",caching=" << s._raw._caching_options.to_sstring();     os << ",cdc=" << s.cdc_options().to_sstring();     os << ",defaultTimeToLive=" << s._raw._default_time_to_live.count();     os << ",minIndexInterval=" << s._raw._min_index_interval;     os << ",maxIndexInterval=" << s._raw._max_index_interval;     os << ",speculativeRetry=" << s._raw._speculative_retry.to_sstring();     os << ",triggers=[]";     os << ",isDense=" << std::boolalpha << s._raw._is_dense;     os << ",version=" << s.version();     os << ",droppedColumns={";     n = 0;     for (auto& dc : s._raw._dropped_columns) {         if (n++ != 0) {             os << ", ";         }         os << dc.first << " : { " << dc.second.type->name() << ", " << dc.second.timestamp << " }";     }     os << "}";     os << ",collections={";     n = 0;     for (auto& c : s._raw._collections) {         if (n++ != 0) {             os << ", ";         }         os << c.first << " : " << c.second->name();     }     os << "}";     os << ",indices={";     n = 0;     for (auto& c : s._raw._indices_by_name) {         if (n++ != 0) {             os << ", ";         }         os << c.first << " : " << c.second.id();     }     os << "}";     os << "]";     return os; }
 static std::ostream& map_as_cql_param(std::ostream& os, const std::map<sstring, sstring>& map, bool first = true) {     for (auto i: map) {         if (first) {             first = false;         } else {             os << ",";         }         os << "'" << i.first << "': '" << i.second << "'";     }     return os; }
 static std::ostream& column_definition_as_cql_key(std::ostream& os, const column_definition & cd) {     os << cd.name_as_cql_string();     os << " " << cd.type->cql3_type_name();     if (cd.kind == column_kind::static_column) {         os << " STATIC";     }     return os; }
 static bool is_global_index(replica::database& db, const table_id& id, const schema& s) {     return false; }
 static bool is_index(replica::database& db, const table_id& id, const schema& s) {     return false; }
 sstring schema::element_type(replica::database& db) const {     return "table"; }
 std::ostream& schema::describe(replica::database& db, std::ostream& os, bool with_internals) const {     return os; }
 const sstring& column_definition::name_as_text() const {     return column_specification->name->text(); }
 const bytes& column_definition::name() const {     return _name; }
 sstring column_definition::name_as_cql_string() const {     return cql3::util::maybe_quote(name_as_text()); }
 bool column_definition::is_on_all_components() const {     return _thrift_bits.is_on_all_components; }
 bool operator==(const column_definition& x, const column_definition& y) {     return x._name == y._name         && x.type == y.type         && x.id == y.id         && x.kind == y.kind         && x._dropped_at == y._dropped_at; }
 // Based on org.apache.cassandra.config.CFMetaData#generateLegacyCfId
table_id generate_legacy_id(const sstring& ks_name, const sstring& cf_name) {     return table_id(utils::UUID_gen::get_name_UUID(ks_name + cf_name)); }
 bool thrift_schema::has_compound_comparator() const {     return _compound; }
 bool thrift_schema::is_dynamic() const {     return _is_dynamic; }
 schema_builder& schema_builder::set_compaction_strategy_options(std::map<sstring, sstring>&& options) {     _raw._compaction_strategy_options = std::move(options);     return *this; }
 schema_builder& schema_builder::with_partitioner(sstring name) {     _raw._partitioner = get_partitioner(name);     return *this; }
 schema_builder& schema_builder::with_sharder(unsigned shard_count, unsigned sharding_ignore_msb_bits) {     _raw._sharder = get_sharder(shard_count, sharding_ignore_msb_bits);     return *this; }
 schema_builder::schema_builder(std::string_view ks_name, std::string_view cf_name,         std::optional<table_id> id, data_type rct)         : _raw(id ? *id : table_id(utils::UUID_gen::get_time_UUID())) {     // Various schema-creation commands (creating tables, indexes, etc.)
    // usually place limits on which characters are allowed in keyspace or
    // table names. But in case we have a hole in those defences (see issue
    // #3403, for example), let's prevent at least the characters "/" and
    // null from being in the keyspace or table name, because those will
    // surely cause serious problems when materialized to directory names.
    // We throw a logic_error because we expect earlier defences to have
    // avoided this case in the first place.
    if (ks_name.find_first_of('/') != std::string_view::npos ||         ks_name.find_first_of('\0') != std::string_view::npos) {         throw std::logic_error(format("Tried to create a schema with illegal characters in keyspace name: {}", ks_name));     }     if (cf_name.find_first_of('/') != std::string_view::npos ||         cf_name.find_first_of('\0') != std::string_view::npos) {         throw std::logic_error(format("Tried to create a schema with illegal characters in table name: {}", cf_name));     }     _raw._ks_name = sstring(ks_name);     _raw._cf_name = sstring(cf_name);     _raw._regular_column_name_type = rct; }
 schema_builder::schema_builder(const schema_ptr s)     : schema_builder(s->_raw) { }
 schema_builder::schema_builder(const schema::raw_schema& raw)     : _raw(raw) {     static_assert(schema::row_column_ids_are_ordered_by_name::value, "row columns don't need to be ordered by name");     // Schema builder may add or remove columns and their ids need to be
    // recomputed in build().
    for (auto& def : _raw._columns | boost::adaptors::filtered([] (auto& def) { return !def.is_primary_key(); })) {             def.id = 0;             def.ordinal_id = static_cast<ordinal_column_id>(0);     } }
 schema_builder::schema_builder(         std::optional<table_id> id,         std::string_view ks_name,         std::string_view cf_name,         std::vector<schema::column> partition_key,         std::vector<schema::column> clustering_key,         std::vector<schema::column> regular_columns,         std::vector<schema::column> static_columns,         data_type regular_column_name_type,         sstring comment)     : schema_builder(ks_name, cf_name, std::move(id), std::move(regular_column_name_type)) {     for (auto&& column : partition_key) {         with_column(std::move(column.name), std::move(column.type), column_kind::partition_key);     }     for (auto&& column : clustering_key) {         with_column(std::move(column.name), std::move(column.type), column_kind::clustering_key);     }     for (auto&& column : regular_columns) {         with_column(std::move(column.name), std::move(column.type));     }     for (auto&& column : static_columns) {         with_column(std::move(column.name), std::move(column.type), column_kind::static_column);     }     set_comment(comment); }
 column_definition& schema_builder::find_column(const cql3::column_identifier& c) {     auto i = std::find_if(_raw._columns.begin(), _raw._columns.end(), [c](auto& p) {         return p.name() == c.name();      });     if (i != _raw._columns.end()) {         return *i;     }     throw std::invalid_argument(format("No such column {}", c.name())); }
 bool schema_builder::has_column(const cql3::column_identifier& c) {     auto i = std::find_if(_raw._columns.begin(), _raw._columns.end(), [c](auto& p) {         return p.name() == c.name();      });     return i != _raw._columns.end(); }
 schema_builder& schema_builder::with_column_ordered(const column_definition& c) {     return with_column(bytes(c.name()), data_type(c.type), column_kind(c.kind), c.position(), c.view_virtual(), c.get_computation_ptr()); }
 schema_builder& schema_builder::with_column(bytes name, data_type type, column_kind kind, column_view_virtual is_view_virtual) {     // component_index will be determined by schema cosntructor
    return with_column(name, type, kind, 0, is_view_virtual); }
 schema_builder& schema_builder::with_column(bytes name, data_type type, column_kind kind, column_id component_index, column_view_virtual is_view_virtual, column_computation_ptr computation) {     _raw._columns.emplace_back(name, type, kind, component_index, is_view_virtual, std::move(computation));     if (type->is_multi_cell()) {         with_collection(name, type);     } else if (type->is_counter()) { 	    set_is_counter(true); 	}     return *this; }
 schema_builder& schema_builder::with_computed_column(bytes name, data_type type, column_kind kind, column_computation_ptr computation) {     return with_column(name, type, kind, 0, column_view_virtual::no, std::move(computation)); }
 schema_builder& schema_builder::remove_column(bytes name) {     auto it = boost::range::find_if(_raw._columns, [&] (auto& column) {         return column.name() == name;     });     if(it == _raw._columns.end()) {         throw std::out_of_range(format("Cannot remove: column {} not found.", name));     }     auto name_as_text = it->column_specification ? it->name_as_text() : schema::column_name_type(*it, _raw._regular_column_name_type)->get_string(it->name());     without_column(name_as_text, it->type, api::new_timestamp());     _raw._columns.erase(it);     return *this; }
 schema_builder& schema_builder::without_column(sstring name, api::timestamp_type timestamp) {     return without_column(std::move(name), bytes_type, timestamp); }
 schema_builder& schema_builder::without_column(sstring name, data_type type, api::timestamp_type timestamp) {     auto ret = _raw._dropped_columns.emplace(name, schema::dropped_column{type, timestamp});     if (!ret.second && ret.first->second.timestamp < timestamp) {         ret.first->second.type = type;         ret.first->second.timestamp = timestamp;     }     return *this; }
 schema_builder& schema_builder::rename_column(bytes from, bytes to) {     auto it = std::find_if(_raw._columns.begin(), _raw._columns.end(), [&] (auto& col) {         return col.name() == from;     });     assert(it != _raw._columns.end());     auto& def = *it;     column_definition new_def(to, def.type, def.kind, def.component_index());     _raw._columns.erase(it);     return with_column_ordered(new_def); }
 schema_builder& schema_builder::alter_column_type(bytes name, data_type new_type) {     auto it = boost::find_if(_raw._columns, [&name] (auto& c) { return c.name() == name; });     assert(it != _raw._columns.end());     it->type = new_type;     if (new_type->is_multi_cell()) {         auto c_it = _raw._collections.find(name);         assert(c_it != _raw._collections.end());         c_it->second = new_type;     }     return *this; }
 schema_builder& schema_builder::mark_column_computed(bytes name, column_computation_ptr computation) {     auto it = boost::find_if(_raw._columns, [&name] (const column_definition& c) { return c.name() == name; });     assert(it != _raw._columns.end());     it->set_computed(std::move(computation));     return *this; }
 schema_builder& schema_builder::with_collection(bytes name, data_type type) {     _raw._collections.emplace(name, type);     return *this; }
 schema_builder& schema_builder::with(compact_storage cs) {     _compact_storage = cs;     return *this; }
 schema_builder& schema_builder::with_version(table_schema_version v) {     _version = v;     return *this; }
 static const sstring default_partition_key_name = "key";
 static const sstring default_clustering_name = "column";
 static const sstring default_compact_value_name = "value";
 schema_builder::default_names::default_names(const schema_builder& builder)     : default_names(builder._raw) {}
 schema_builder::default_names::default_names(const schema::raw_schema& raw)     : _raw(raw)     , _partition_index(0)     , _clustering_index(1)     , _compact_index(0) {}
 sstring schema_builder::default_names::unique_name(const sstring& base, size_t& idx, size_t off) const {     for (;;) {         auto candidate = idx == 0 ? base : base + std::to_string(idx + off);         ++idx;         auto i = std::find_if(_raw._columns.begin(), _raw._columns.end(), [b = to_bytes(candidate)](const column_definition& c) {             return c.name() == b;         });         if (i == _raw._columns.end()) {             return candidate;         }     } }
 sstring schema_builder::default_names::partition_key_name() {     // For compatibility sake, we call the first alias 'key' rather than 'key1'. This
    // is inconsistent with column alias, but it's probably not worth risking breaking compatibility now.
    return unique_name(default_partition_key_name, _partition_index, 1); }
 sstring schema_builder::default_names::clustering_name() {     return unique_name(default_clustering_name, _clustering_index, 0); }
 sstring schema_builder::default_names::compact_value_name() {     return unique_name(default_compact_value_name, _compact_index, 0); }
 void schema_builder::prepare_dense_schema(schema::raw_schema& raw) {     auto is_dense = raw._is_dense;     auto is_compound = raw._is_compound;     auto is_compact_table = is_dense || !is_compound;     if (is_compact_table) {         auto count_kind = [&raw](column_kind kind) {             return std::count_if(raw._columns.begin(), raw._columns.end(), [kind](const column_definition& c) {                 return c.kind == kind;             });         };         default_names names(raw);         if (is_dense) {             auto regular_cols = count_kind(column_kind::regular_column);             // In Origin, dense CFs always have at least one regular column
            if (regular_cols == 0) {                 raw._columns.emplace_back(to_bytes(names.compact_value_name()),                                 empty_type,                                 column_kind::regular_column, 0);             } else if (regular_cols > 1) {                 throw exceptions::configuration_exception(                                 format("Expecting exactly one regular column. Found {:d}",                                                 regular_cols));             }         }     } }
 schema_builder& schema_builder::with_view_info(table_id base_id, sstring base_name, bool include_all_columns, sstring where_clause) {     return *this; }
 schema_builder& schema_builder::with_index(const index_metadata& im) {     _raw._indices_by_name.emplace(im.name(), im);     return *this; }
 schema_builder& schema_builder::without_index(const sstring& name) {     if (_raw._indices_by_name.contains(name)) {         _raw._indices_by_name.erase(name);     }     return *this; }
 schema_builder& schema_builder::without_indexes() {     _raw._indices_by_name.clear();     return *this; }
 schema_ptr schema_builder::build() {     schema::raw_schema new_raw = _raw; // Copy so that build() remains idempotent.
    schema_static_props static_props{};     for (const auto& c: static_configurators()) {         c(new_raw._ks_name, new_raw._cf_name, static_props);     }     if (_version) {         new_raw._version = *_version;     } else {         new_raw._version = table_schema_version(utils::UUID_gen::get_time_UUID());     }     if (new_raw._is_counter) {         new_raw._default_validation_class = counter_type;     }     if (_compact_storage) {         // Dense means that no part of the comparator stores a CQL column name. This means
        // COMPACT STORAGE with at least one columnAliases (otherwise it's a thrift "static" CF).
        auto clustering_key_size = std::count_if(new_raw._columns.begin(), new_raw._columns.end(), [](auto&& col) {             return col.kind == column_kind::clustering_key;         });         new_raw._is_dense = (*_compact_storage == compact_storage::yes) && (clustering_key_size > 0);         if (clustering_key_size == 0) {             if (*_compact_storage == compact_storage::yes) {                 new_raw._is_compound = false;             } else {                 new_raw._is_compound = true;             }         } else {             if ((*_compact_storage == compact_storage::yes) && clustering_key_size == 1) {                 new_raw._is_compound = false;             } else {                 new_raw._is_compound = true;             }         }     }     prepare_dense_schema(new_raw);     // cache `paxos_grace_seconds` value for fast access through the schema object, which is immutable
    if (auto it = new_raw._extensions.find(db::paxos_grace_seconds_extension::NAME); it != new_raw._extensions.end()) {         new_raw._paxos_grace_seconds =             dynamic_pointer_cast<db::paxos_grace_seconds_extension>(it->second)->get_paxos_grace_seconds();     }     // cache the `per_partition_rate_limit` parameters for fast access through the schema object.
    if (auto it = new_raw._extensions.find(db::per_partition_rate_limit_extension::NAME); it != new_raw._extensions.end()) {         new_raw._per_partition_rate_limit_options =             dynamic_pointer_cast<db::per_partition_rate_limit_extension>(it->second)->get_options();     }     if (static_props.use_null_sharder) {         new_raw._sharder = get_sharder(1, 0);     }     return make_lw_shared<schema>(schema::private_tag{}, new_raw, _view_info, static_props); }
 auto schema_builder::static_configurators() -> std::vector<static_configurator>& {     static std::vector<static_configurator> result{};     return result; }
 int schema_builder::register_static_configurator(static_configurator&& configurator) {     static_configurators().push_back(std::move(configurator));     return 0; }
 const cdc::options& schema::cdc_options() const {     static const cdc::options default_cdc_options;     const auto& schema_extensions = _raw._extensions;     if (auto it = schema_extensions.find(cdc::cdc_extension::NAME); it != schema_extensions.end()) {         return dynamic_pointer_cast<cdc::cdc_extension>(it->second)->get_options();     }     return default_cdc_options; }
 const ::tombstone_gc_options& schema::tombstone_gc_options() const {     static const ::tombstone_gc_options default_tombstone_gc_options;     const auto& schema_extensions = _raw._extensions;     if (auto it = schema_extensions.find(tombstone_gc_extension::NAME); it != schema_extensions.end()) {         return dynamic_pointer_cast<tombstone_gc_extension>(it->second)->get_options();     }     return default_tombstone_gc_options; }
 schema_builder& schema_builder::with_cdc_options(const cdc::options& opts) {     add_extension(cdc::cdc_extension::NAME, ::make_shared<cdc::cdc_extension>(opts));     return *this; }
 schema_builder& schema_builder::with_tombstone_gc_options(const tombstone_gc_options& opts) {     add_extension(tombstone_gc_extension::NAME, ::make_shared<tombstone_gc_extension>(opts));     return *this; }
 schema_builder& schema_builder::with_per_partition_rate_limit_options(const db::per_partition_rate_limit_options& opts) {     add_extension(db::per_partition_rate_limit_extension::NAME, ::make_shared<db::per_partition_rate_limit_extension>(opts));     return *this; }
 schema_builder& schema_builder::set_paxos_grace_seconds(int32_t seconds) {     add_extension(db::paxos_grace_seconds_extension::NAME, ::make_shared<db::paxos_grace_seconds_extension>(seconds));     return *this; }
 gc_clock::duration schema::paxos_grace_seconds() const {     return std::chrono::duration_cast<gc_clock::duration>(         std::chrono::seconds(             _raw._paxos_grace_seconds ? *_raw._paxos_grace_seconds : DEFAULT_GC_GRACE_SECONDS         )     ); }
 schema_ptr schema_builder::build(compact_storage cp) {     return with(cp).build(); }
 // Useful functions to manipulate the schema's comparator field
namespace cell_comparator { static constexpr auto _composite_str = "org.apache.cassandra.db.marshal.CompositeType"; static constexpr auto _collection_str = "org.apache.cassandra.db.marshal.ColumnToCollectionType";     }
 schema::const_iterator schema::regular_begin() const {     return regular_columns().begin(); }
 schema::const_iterator schema::regular_end() const {     return regular_columns().end(); }
 struct column_less_comparator {     bool operator()(const column_definition& def, const bytes& name) {         return def.name() < name;     }     bool operator()(const bytes& name, const column_definition& def) {         return name < def.name();     } };
 schema::const_iterator schema::regular_lower_bound(const bytes& name) const {     return boost::lower_bound(regular_columns(), name, column_less_comparator()); }
 schema::const_iterator schema::regular_upper_bound(const bytes& name) const {     return boost::upper_bound(regular_columns(), name, column_less_comparator()); }
 schema::const_iterator schema::static_begin() const {     return static_columns().begin(); }
 schema::const_iterator schema::static_end() const {     return static_columns().end(); }
 schema::const_iterator schema::static_lower_bound(const bytes& name) const {     return boost::lower_bound(static_columns(), name, column_less_comparator()); }
 schema::const_iterator schema::static_upper_bound(const bytes& name) const {     return boost::upper_bound(static_columns(), name, column_less_comparator()); }
 data_type schema::column_name_type(const column_definition& def, const data_type& regular_column_name_type) {     if (def.kind == column_kind::regular_column) {         return regular_column_name_type;     }     return utf8_type; }
 data_type schema::column_name_type(const column_definition& def) const {     return column_name_type(def, _raw._regular_column_name_type); }
 const column_definition& schema::regular_column_at(column_id id) const {     if (id >= regular_columns_count()) {         on_internal_error(dblog, format("{}.{}@{}: regular column id {:d} >= {:d}",             ks_name(), cf_name(), version(), id, regular_columns_count()));     }     return _raw._columns.at(column_offset(column_kind::regular_column) + id); }
 const column_definition& schema::clustering_column_at(column_id id) const {     if (id >= clustering_key_size()) {         on_internal_error(dblog, format("{}.{}@{}: clustering column id {:d} >= {:d}",             ks_name(), cf_name(), version(), id, clustering_key_size()));     }     return _raw._columns.at(column_offset(column_kind::clustering_key) + id); }
 const column_definition& schema::static_column_at(column_id id) const {     if (id >= static_columns_count()) {         on_internal_error(dblog, format("{}.{}@{}: static column id {:d} >= {:d}",             ks_name(), cf_name(), version(), id, static_columns_count()));     }     return _raw._columns.at(column_offset(column_kind::static_column) + id); }
 bool schema::is_last_partition_key(const column_definition& def) const {     return &_raw._columns.at(partition_key_size() - 1) == &def; }
 bool schema::has_static_columns() const {     return !static_columns().empty(); }
 column_count_type schema::columns_count(column_kind kind) const {     switch (kind) {     case column_kind::partition_key:         return partition_key_size();     case column_kind::clustering_key:         return clustering_key_size();     case column_kind::static_column:         return static_columns_count();     case column_kind::regular_column:         return regular_columns_count();     default:         std::abort();     } }
 column_count_type schema::partition_key_size() const {     return column_offset(column_kind::clustering_key); }
 schema::const_iterator_range_type schema::partition_key_columns() const {     return boost::make_iterator_range(_raw._columns.begin() + column_offset(column_kind::partition_key)             , _raw._columns.begin() + column_offset(column_kind::clustering_key)); }
 schema::const_iterator_range_type schema::clustering_key_columns() const {     return boost::make_iterator_range(_raw._columns.begin() + column_offset(column_kind::clustering_key)             , _raw._columns.begin() + column_offset(column_kind::static_column)); }
 schema::const_iterator_range_type schema::static_columns() const {     return boost::make_iterator_range(_raw._columns.begin() + column_offset(column_kind::static_column)             , _raw._columns.begin() + column_offset(column_kind::regular_column)); }
 schema::const_iterator_range_type schema::regular_columns() const {     return boost::make_iterator_range(_raw._columns.begin() + column_offset(column_kind::regular_column)             , _raw._columns.end()); }
 schema::const_iterator_range_type schema::columns(column_kind kind) const {     switch (kind) {     case column_kind::partition_key:         return partition_key_columns();     case column_kind::clustering_key:         return clustering_key_columns();     case column_kind::static_column:         return static_columns();     case column_kind::regular_column:         return regular_columns();     }     throw std::invalid_argument(std::to_string(int(kind))); }
 schema::select_order_range schema::all_columns_in_select_order() const {     auto is_static_compact_table = this->is_static_compact_table();     auto no_non_pk_columns = is_compact_table()                     // Origin: && CompactTables.hasEmptyCompactValue(this);
                    && regular_columns_count() == 1                     && [](const column_definition& c) {         // We use empty_type now to match origin, but earlier incarnations
        // set name empty instead. check either.
        return c.type == empty_type || c.name().empty();     }(regular_column_at(0));     auto pk_range = const_iterator_range_type(_raw._columns.begin(),                     _raw._columns.begin() + (is_static_compact_table ?                                     column_offset(column_kind::clustering_key) :                                     column_offset(column_kind::static_column)));     auto ck_v_range = no_non_pk_columns ? static_columns()                                         : const_iterator_range_type(static_columns().begin(), all_columns().end());     return boost::range::join(pk_range, ck_v_range); }
 uint32_t schema::position(const column_definition& column) const {     if (column.is_primary_key()) {         return column.id;     }     return clustering_key_size(); }
 std::optional<index_metadata> schema::find_index_noname(const index_metadata& target) const {     const auto& it = boost::find_if(_raw._indices_by_name, [&] (auto&& e) {         return e.second.equals_noname(target);     });     if (it != _raw._indices_by_name.end()) {         return it->second;     }     return {}; }
 std::vector<index_metadata> schema::indices() const {     return boost::copy_range<std::vector<index_metadata>>(_raw._indices_by_name | boost::adaptors::map_values); }
 const std::unordered_map<sstring, index_metadata>& schema::all_indices() const {     return _raw._indices_by_name; }
 bool schema::has_index(const sstring& index_name) const {     return _raw._indices_by_name.contains(index_name); }
 std::vector<sstring> schema::index_names() const {     return boost::copy_range<std::vector<sstring>>(_raw._indices_by_name | boost::adaptors::map_keys); }
 data_type schema::make_legacy_default_validator() const {     return _raw._default_validation_class; }
 bool schema::is_synced() const {     return _registry_entry && _registry_entry->is_synced(); }
 bool schema::equal_columns(const schema& other) const {     return boost::equal(all_columns(), other.all_columns()); }
 schema_ptr schema::make_reversed() const {     return make_lw_shared<schema>(schema::reversed_tag{}, *this); }
 schema_ptr schema::get_reversed() const {     return local_schema_registry().get_or_load(reversed(_raw._version), [this] (table_schema_version) {         return frozen_schema(make_reversed());     }); }
 raw_view_info::raw_view_info(table_id base_id, sstring base_name, bool include_all_columns, sstring where_clause)         : _base_id(std::move(base_id))         , _base_name(std::move(base_name))         , _include_all_columns(include_all_columns)         , _where_clause(where_clause) { }
 column_computation_ptr column_computation::deserialize(bytes_view raw) {     rjson::value parsed = rjson::parse(std::string_view(reinterpret_cast<const char*>(raw.begin()), reinterpret_cast<const char*>(raw.end())));     if (!parsed.IsObject()) {         throw std::runtime_error(format("Invalid column computation value: {}", parsed));     }     const rjson::value* type_json = rjson::find(parsed, "type");     if (!type_json || !type_json->IsString()) {         throw std::runtime_error(format("Type {} is not convertible to string", *type_json));     }     const std::string_view type = rjson::to_string_view(*type_json);     if (type == "token") {         return std::make_unique<legacy_token_column_computation>();     }     if (type == "token_v2") {         return std::make_unique<token_column_computation>();     }     if (type.starts_with("collection_")) {         const rjson::value* collection_name = rjson::find(parsed, "collection_name");         if (collection_name && collection_name->IsString()) {             auto collection = rjson::to_string_view(*collection_name);             auto collection_as_bytes = bytes(collection.begin(), collection.end());             if (auto collection = collection_column_computation::for_target_type(type, collection_as_bytes)) {                 return collection->clone();             }         }     }     throw std::runtime_error(format("Incorrect column computation type {} found when parsing {}", *type_json, parsed)); }
 bytes legacy_token_column_computation::serialize() const {     rjson::value serialized = rjson::empty_object();     rjson::add(serialized, "type", rjson::from_string("token"));     return to_bytes(rjson::print(serialized)); }
 bytes legacy_token_column_computation::compute_value(const schema& schema, const partition_key& key) const {     return {dht::get_token(schema, key).data()}; }
 bytes token_column_computation::serialize() const {     rjson::value serialized = rjson::empty_object();     rjson::add(serialized, "type", rjson::from_string("token_v2"));     return to_bytes(rjson::print(serialized)); }
 bytes token_column_computation::compute_value(const schema& schema, const partition_key& key) const {     auto long_value = dht::token::to_int64(dht::get_token(schema, key));     return long_type->decompose(long_value); }
 bytes collection_column_computation::serialize() const {     rjson::value serialized = rjson::empty_object();     const char* type = nullptr;     switch (_kind) {         case kind::keys:             type = "collection_keys";             break;         case kind::values:             type = "collection_values";             break;         case kind::entries:             type = "collection_entries";             break;     }     rjson::add(serialized, "type", rjson::from_string(type));     rjson::add(serialized, "collection_name", rjson::from_string(to_sstring_view(_collection_name)));     return to_bytes(rjson::print(serialized)); }
 column_computation_ptr collection_column_computation::for_target_type(std::string_view type, const bytes& collection_name) {     if (type == "collection_keys") {         return collection_column_computation::for_keys(collection_name).clone();     }     if (type == "collection_values") {         return collection_column_computation::for_values(collection_name).clone();     }     if (type == "collection_entries") {         return collection_column_computation::for_entries(collection_name).clone();     }     return {}; }
 void collection_column_computation::operate_on_collection_entries(         std::invocable<collection_kv*, collection_kv*, tombstone> auto&& old_and_new_row_func, const schema& schema,         const partition_key& key, const db::view::clustering_or_static_row& update, const std::optional<db::view::clustering_or_static_row>& existing) const {     const column_definition* cdef = schema.get_column_definition(_collection_name);     decltype(collection_mutation_view_description::cells) update_cells, existing_cells;     const auto* update_cell = update.cells().find_cell(cdef->id);     tombstone update_tombstone = update.tomb().tomb();     if (update_cell) {         collection_mutation_view update_col_view = update_cell->as_collection_mutation();         update_col_view.with_deserialized(*(cdef->type), [&update_cells, &update_tombstone] (collection_mutation_view_description descr) {             update_tombstone.apply(descr.tomb);             update_cells = descr.cells;         });     }     if (existing) {         const auto* existing_cell = existing->cells().find_cell(cdef->id);         if (existing_cell) {             collection_mutation_view existing_col_view = existing_cell->as_collection_mutation();             existing_col_view.with_deserialized(*(cdef->type), [&existing_cells] (collection_mutation_view_description descr) {                 existing_cells = descr.cells;             });         }     }     auto compare = [](const collection_kv& p1, const collection_kv& p2) {         return p1.first <=> p2.first;     };     // Both collections are assumed to be sorted by the keys.
    auto existing_it = existing_cells.begin();     auto update_it = update_cells.begin();     auto is_existing_end = [&] {         return existing_it == existing_cells.end();     };     auto is_update_end = [&] {         return update_it == update_cells.end();     };     while (!(is_existing_end() && is_update_end())) {         std::strong_ordering cmp = [&] {             if (is_existing_end()) {                 return std::strong_ordering::greater;             } else if (is_update_end()) {                 return std::strong_ordering::less;             }             return compare(*existing_it, *update_it);         }();         auto existing_ptr = [&] () -> collection_kv* {             return (!is_existing_end() && cmp <= 0) ? &*existing_it : nullptr;         };         auto update_ptr = [&] () -> collection_kv* {             return (!is_update_end() && cmp >= 0) ? &*update_it : nullptr;         };         old_and_new_row_func(existing_ptr(), update_ptr(), update_tombstone);         if (cmp <= 0) {             ++existing_it;         }         if (cmp >= 0) {             ++update_it;         }     } }
 bytes collection_column_computation::compute_value(const schema&, const partition_key&) const {     throw std::runtime_error(fmt::format("{}: not supported", __PRETTY_FUNCTION__)); }
 std::vector<db::view::view_key_and_action> collection_column_computation::compute_values_with_action(const schema& schema, const partition_key& key,         const db::view::clustering_or_static_row& update, const std::optional<db::view::clustering_or_static_row>& existing) const {     using collection_kv = std::pair<bytes_view, atomic_cell_view>;     auto serialize_cell = [_kind = _kind](const collection_kv& kv) -> bytes {         using kind = collection_column_computation::kind;         auto& [key, value] = kv;         switch (_kind) {             case kind::keys:                 return bytes(key);             case kind::values:                 return value.value().linearize();             case kind::entries:                 bytes_opt elements[] = {bytes(key), value.value().linearize()};                 return tuple_type_impl::build_value(elements);         }         std::abort(); // compiler will error
    };     std::vector<db::view::view_key_and_action> ret;     auto compute_row_marker = [] (auto&& cell) -> row_marker {         return cell.is_live_and_has_ttl() ? row_marker(cell.timestamp(), cell.ttl(), cell.expiry()) : row_marker(cell.timestamp());     };     auto fn = [&ret, &compute_row_marker, &serialize_cell] (collection_kv* existing, collection_kv* update, tombstone tomb) {         api::timestamp_type operation_ts = tomb.timestamp;         if (existing && update && compare_atomic_cell_for_merge(existing->second, update->second) == 0) {             return;         }         if (update) {             operation_ts = update->second.timestamp();             if (update->second.is_live()) {                 row_marker rm = compute_row_marker(update->second);                 ret.push_back({serialize_cell(*update), {rm}});             }         }         operation_ts -= 1;         if (existing && existing->second.is_live()) {             db::view::view_key_and_action::shadowable_tombstone_tag tag{operation_ts};             ret.push_back({serialize_cell(*existing), {tag}});         }     };     operate_on_collection_entries(fn, schema, key, update, existing);     return ret; }
    schema_mismatch_error::schema_mismatch_error(table_schema_version expected, const schema& access)     : std::runtime_error(fmt::format("Attempted to deserialize schema-dependent object of version {} using {}.{} {}",         expected, access.ks_name(), access.cf_name(), access.version())) { }
 static logging::logger slogger("schema_registry");
 static thread_local schema_registry registry;
 schema_version_not_found::schema_version_not_found(table_schema_version v)         : std::runtime_error{format("Schema version {} not found", v)}
 { }
 schema_version_loading_failed::schema_version_loading_failed(table_schema_version v)         : std::runtime_error{format("Failed to load schema version {}", v)}
 { }
 schema_registry_entry::~schema_registry_entry() {     if (_schema) {         _schema->_registry_entry = nullptr;     } }
 schema_registry_entry::schema_registry_entry(table_schema_version v, schema_registry& r)     : _state(state::INITIAL)     , _version(v)     , _registry(r)     , _sync_state(sync_state::NOT_SYNCED) {     _erase_timer.set_callback([this] {         slogger.debug("Dropping {}", _version);         assert(!_schema);         try {             _registry._entries.erase(_version);         } catch (...) {             slogger.error("Failed to erase schema version {}: {}", _version, std::current_exception());         }     }); }
 schema_registry::~schema_registry() = default;
 void schema_registry::init(const db::schema_ctxt& ctxt) {     _ctxt = std::make_unique<db::schema_ctxt>(ctxt); }
 schema_ptr schema_registry::learn(const schema_ptr& s) {     if (s->registry_entry()) {         return std::move(s);     }     auto i = _entries.find(s->version());     if (i != _entries.end()) {         return i->second->get_schema();     }     slogger.debug("Learning about version {} of {}.{}", s->version(), s->ks_name(), s->cf_name());     auto e_ptr = make_lw_shared<schema_registry_entry>(s->version(), *this);     auto loaded_s = e_ptr->load(frozen_schema(s));     _entries.emplace(s->version(), e_ptr);     return loaded_s; }
 schema_registry_entry& schema_registry::get_entry(table_schema_version v) const {     auto i = _entries.find(v);     if (i == _entries.end()) {         throw schema_version_not_found(v);     }     schema_registry_entry& e = *i->second;     if (e._state != schema_registry_entry::state::LOADED) {         throw schema_version_not_found(v);     }     return e; }
 schema_registry_entry::erase_clock::duration schema_registry::grace_period() const {     return std::chrono::seconds(_ctxt->schema_registry_grace_period()); }
 schema_ptr schema_registry::get(table_schema_version v) const {     return get_entry(v).get_schema(); }
 frozen_schema schema_registry::get_frozen(table_schema_version v) const {     return get_entry(v).frozen(); }
 future<schema_ptr> schema_registry::get_or_load(table_schema_version v, const async_schema_loader& loader) {     auto i = _entries.find(v);     if (i == _entries.end()) {         auto e_ptr = make_lw_shared<schema_registry_entry>(v, *this);         auto f = e_ptr->start_loading(loader);         _entries.emplace(v, e_ptr);         return f;     }     schema_registry_entry& e = *i->second;     if (e._state == schema_registry_entry::state::LOADING) {         return e._schema_promise.get_shared_future();     }     return make_ready_future<schema_ptr>(e.get_schema()); }
 schema_ptr schema_registry::get_or_null(table_schema_version v) const {     auto i = _entries.find(v);     if (i == _entries.end()) {         return nullptr;     }     schema_registry_entry& e = *i->second;     if (e._state != schema_registry_entry::state::LOADED) {         return nullptr;     }     return e.get_schema(); }
 schema_ptr schema_registry::get_or_load(table_schema_version v, const schema_loader& loader) {     auto i = _entries.find(v);     if (i == _entries.end()) {         auto e_ptr = make_lw_shared<schema_registry_entry>(v, *this);         auto s = e_ptr->load(loader(v));         _entries.emplace(v, e_ptr);         return s;     }     schema_registry_entry& e = *i->second;     if (e._state == schema_registry_entry::state::LOADING) {         return e.load(loader(v));     }     return e.get_schema(); }
 schema_ptr schema_registry_entry::load(frozen_schema fs) {     _frozen_schema = std::move(fs);     auto s = get_schema();     if (_state == state::LOADING) {         _schema_promise.set_value(s);         _schema_promise = {};     }     _state = state::LOADED;     slogger.trace("Loaded {} = {}", _version, *s);     return s; }
 future<schema_ptr> schema_registry_entry::start_loading(async_schema_loader loader) {     _loader = std::move(loader);     auto f = _loader(_version);     auto sf = _schema_promise.get_shared_future();     _state = state::LOADING;     slogger.trace("Loading {}", _version);     // Move to background.
    (void)f.then_wrapped([self = shared_from_this(), this] (future<frozen_schema>&& f) {         _loader = {};         if (_state != state::LOADING) {             slogger.trace("Loading of {} aborted", _version);             return;         }         try {             try {                 load(f.get0());             } catch (...) {                 std::throw_with_nested(schema_version_loading_failed(_version));             }         } catch (...) {             slogger.debug("Loading of {} failed: {}", _version, std::current_exception());             _schema_promise.set_exception(std::current_exception());             _registry._entries.erase(_version);         }     });     return sf; }
 schema_ptr schema_registry_entry::get_schema() {     if (!_schema) {         slogger.trace("Activating {}", _version);         auto s = _frozen_schema->unfreeze(*_registry._ctxt);         if (s->version() != _version) {             throw std::runtime_error(format("Unfrozen schema version doesn't match entry version ({}): {}", _version, *s));         }         _erase_timer.cancel();         s->_registry_entry = this;         _schema = &*s;         return s;     } else {         return _schema->shared_from_this();     } }
 void schema_registry_entry::detach_schema() noexcept {     slogger.trace("Deactivating {}", _version);     _schema = nullptr;     _erase_timer.arm(_registry.grace_period()); }
 frozen_schema schema_registry_entry::frozen() const {     assert(_state >= state::LOADED);     return *_frozen_schema; }
 future<> schema_registry_entry::maybe_sync(std::function<future<>()> syncer) {     switch (_sync_state) {         case schema_registry_entry::sync_state::SYNCED:             return make_ready_future<>();         case schema_registry_entry::sync_state::SYNCING:             return _synced_promise.get_shared_future();         case schema_registry_entry::sync_state::NOT_SYNCED: {             slogger.debug("Syncing {}", _version);             _synced_promise = {};             auto f = do_with(std::move(syncer), [] (auto& syncer) {                 return syncer();             });             auto sf = _synced_promise.get_shared_future();             _sync_state = schema_registry_entry::sync_state::SYNCING;             // Move to background.
            (void)f.then_wrapped([this, self = shared_from_this()] (auto&& f) {                 if (_sync_state != sync_state::SYNCING) {                     f.ignore_ready_future();                     return;                 }                 if (f.failed()) {                     slogger.debug("Syncing of {} failed", _version);                     _sync_state = schema_registry_entry::sync_state::NOT_SYNCED;                     _synced_promise.set_exception(f.get_exception());                 } else {                     slogger.debug("Synced {}", _version);                     _sync_state = schema_registry_entry::sync_state::SYNCED;                     _synced_promise.set_value();                 }             });             return sf;         }     }     abort(); }
 bool schema_registry_entry::is_synced() const {     return _sync_state == sync_state::SYNCED; }
 void schema_registry_entry::mark_synced() {     if (_sync_state == sync_state::SYNCING) {         _synced_promise.set_value();     }     _sync_state = sync_state::SYNCED;     slogger.debug("Marked {} as synced", _version); }
  global_schema_ptr::global_schema_ptr(const global_schema_ptr& o)     : global_schema_ptr(o.get()) { }
 global_schema_ptr::global_schema_ptr(global_schema_ptr&& o) noexcept {     auto current = this_shard_id();     assert(o._cpu_of_origin == current);     _ptr = std::move(o._ptr);     _cpu_of_origin = current;     _base_schema = std::move(o._base_schema); }
 schema_ptr global_schema_ptr::get() const {     if (this_shard_id() == _cpu_of_origin) {         return _ptr;     } else {         auto registered_schema = [](const schema_registry_entry& e) {             schema_ptr ret = local_schema_registry().get_or_null(e.version());             if (!ret) {                 ret = local_schema_registry().get_or_load(e.version(), [&e](table_schema_version) {                     return e.frozen();                 });             }             return ret;         };         schema_ptr registered_bs;         // the following code contains registry entry dereference of a foreign shard
        // however, it is guarantied to succeed since we made sure in the constructor
        // that _bs_schema and _ptr will have a registry on the foreign shard where this
        // object originated so as long as this object lives the registry entries lives too
        // and it is safe to reference them on foreign shards.
        if (_base_schema) {             registered_bs = registered_schema(*_base_schema->registry_entry());             if (_base_schema->registry_entry()->is_synced()) {                 registered_bs->registry_entry()->mark_synced();             }         }         schema_ptr s = registered_schema(*_ptr->registry_entry());         if (s->is_view()) {             if (!s->view_info()->base_info()) {                 // we know that registered_bs is valid here because we make sure of it in the constructors.
                s->view_info()->set_base_info(s->view_info()->make_base_dependent_view_info(*registered_bs));             }         }         if (_ptr->registry_entry()->is_synced()) {             s->registry_entry()->mark_synced();         }         return s;     } }
 global_schema_ptr::global_schema_ptr(const schema_ptr& ptr)         : _cpu_of_origin(this_shard_id()) {     // _ptr must always have an associated registry entry,
    // if ptr doesn't, we need to load it into the registry.
    auto ensure_registry_entry = [] (const schema_ptr& s) {         schema_registry_entry* e = s->registry_entry();         if (e) {             return s;         } else {             return local_schema_registry().get_or_load(s->version(), [&s] (table_schema_version) {                 return frozen_schema(s);             });         }     };     schema_ptr s = ensure_registry_entry(ptr);     if (s->is_view()) {         if (s->view_info()->base_info()) {             _base_schema = ensure_registry_entry(s->view_info()->base_info()->base_schema());         } else if (ptr->view_info()->base_info()) {             _base_schema = ensure_registry_entry(ptr->view_info()->base_info()->base_schema());         } else {             on_internal_error(slogger, format("Tried to build a global schema for view {}.{} with an uninitialized base info", s->ks_name(), s->cf_name()));         }         if (!s->view_info()->base_info() || !s->view_info()->base_info()->base_schema()->registry_entry()) {             s->view_info()->set_base_info(s->view_info()->make_base_dependent_view_info(*_base_schema));         }     }     _ptr = s; }
 frozen_schema::frozen_schema(const schema_ptr& s)     : _data([&s] {         schema_mutations sm = db::schema_tables::make_schema_mutations(s, api::new_timestamp(), true);         bytes_ostream out;         ser::writer_of_schema<bytes_ostream> wr(out);         std::move(wr).write_version(s->version())                      .write_mutations(sm)                      .end_schema();         return out;     }
()) { }
 schema_ptr frozen_schema::unfreeze(const db::schema_ctxt& ctxt) const {     auto in = ser::as_input_stream(_data);     auto sv = ser::deserialize(in, boost::type<ser::schema_view>());     return sv.mutations().is_view()          ? db::schema_tables::create_view_from_mutations(ctxt, sv.mutations(), sv.version())          : db::schema_tables::create_table_from_mutations(ctxt, sv.mutations(), sv.version()); }
 frozen_schema::frozen_schema(bytes_ostream b)     : _data(std::move(b)) { }
 const bytes_ostream& frozen_schema::representation() const {     return _data; }
#define arch_target(name) [[gnu::target(name)]]
namespace utils { arch_target("default") int array_search_gt_impl(int64_t val, const int64_t* array, const int capacity, const int size) {     int i;     for (i = 0; i < size; i++) {         if (val < array[i])             break;     }     return i; } static inline unsigned array_search_eq_impl(uint8_t val, const uint8_t* arr, unsigned len) {     unsigned i;     for (i = 0; i < len; i++) {         if (arr[i] == val) {             break;         }     }     return i; } arch_target("default") unsigned array_search_16_eq_impl(uint8_t val, const uint8_t* arr) {     return array_search_eq_impl(val, arr, 16); } arch_target("default") unsigned array_search_32_eq_impl(uint8_t val, const uint8_t* arr) {     return array_search_eq_impl(val, arr, 32); } arch_target("default") unsigned array_search_x32_eq_impl(uint8_t val, const uint8_t* arr, int nr) {     return array_search_eq_impl(val, arr, 32 * nr); } arch_target("avx2") int array_search_gt_impl(int64_t val, const int64_t* array, const int capacity, const int size) {     int cnt = 0;     // 0. Load key into 256-bit ymm
    __m256i k = _mm256_set1_epi64x(val);     for (int i = 0; i < capacity; i += 4) {         // 4. Count the number of 1-s, each gt match gives 8 bits
        cnt += _mm_popcnt_u32(                     // 3. Pack result into 4 bytes -- 1 byte from each comparison
                    _mm256_movemask_epi8(                         // 2. Compare array[i] > key, 4 elements in one go
                        _mm256_cmpgt_epi64(                             // 1. Load next 4 elements into ymm
                            _mm256_lddqu_si256((__m256i*)&array[i]), k                         )                     )                 ) / 8;     }     return size - cnt; } arch_target("sse") unsigned array_search_16_eq_impl(uint8_t val, const uint8_t* arr) { 	auto a = _mm_set1_epi8(val); 	auto b = _mm_lddqu_si128((__m128i*)arr); 	auto c = _mm_cmpeq_epi8(a, b); 	unsigned int m = _mm_movemask_epi8(c); 	return __builtin_ctz(m | 0x10000); } arch_target("avx2") unsigned array_search_32_eq_impl(uint8_t val, const uint8_t* arr) {     auto a = _mm256_set1_epi8(val);     auto b = _mm256_lddqu_si256((__m256i*)arr);     auto c = _mm256_cmpeq_epi8(a, b);     unsigned long long m = _mm256_movemask_epi8(c);     return __builtin_ctzll(m | 0x100000000ull); } arch_target("avx2") unsigned array_search_x32_eq_impl(uint8_t val, const uint8_t* arr, int nr) {     unsigned len = 32 * nr;     auto a = _mm256_set1_epi8(val);     for (unsigned off = 0; off < len; off += 32) {         auto b = _mm256_lddqu_si256((__m256i*)arr);         auto c = _mm256_cmpeq_epi8(a, b);         unsigned m = _mm256_movemask_epi8(c);         if (m != 0) {             return __builtin_ctz(m) + off;         }     }     return len; }  unsigned array_search_16_eq(uint8_t val, const uint8_t* arr) {     return array_search_16_eq_impl(val, arr); } unsigned array_search_32_eq(uint8_t val, const uint8_t* array) {     return array_search_32_eq_impl(val, array); }  }
 // Arrays for quickly converting to and from an integer between 0 and 63,
// and the character used in base64 encoding to represent it.
static class base64_chars { public:     static constexpr const char to[] =             "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/";     static constexpr uint8_t invalid_char = 255;     uint8_t from[255];     base64_chars() {         static_assert(sizeof(to) == 64 + 1);         for (int i = 0; i < 255; i++) {             from[i] = invalid_char; // signal invalid character
        }         for (int i = 0; i < 64; i++) {             from[(unsigned) to[i]] = i;         }     } }
 base64_chars;
        using namespace std::chrono_literals;
 // For each aligned 8 byte segment, the algorithm used by address
// sanitizer can represent any addressable prefix followd by a
// poisoned suffix. The details are at:
// https://github.com/google/sanitizers/wiki/AddressSanitizerAlgorithm
// For us this means that:
// * The descriptor must be 8 byte aligned. If it was not, making the
//   descriptor addressable would also make the end of the previous
//   value addressable.
// * Each value must be at least 8 byte aligned. If it was not, making
//   the value addressable would also make the end of the descriptor
//   addressable.
template<typename T> [[nodiscard]] static T align_up_for_asan(T val) {     return align_up(val, size_t(8)); }
 template<typename T> void poison(const T* addr, size_t size) {     // Both values and descriptors must be aligned.
    assert(uintptr_t(addr) % 8 == 0);     // This can be followed by
    // * 8 byte aligned descriptor (this is a value)
    // * 8 byte aligned value
    // * dead value
    // * end of segment
    // In all cases, we can align up the size to guarantee that asan
    // is able to poison this.
    ASAN_POISON_MEMORY_REGION(addr, align_up_for_asan(size)); }
 void unpoison(const char *addr, size_t size) {     ASAN_UNPOISON_MEMORY_REGION(addr, size); }
 namespace bi = boost::intrusive;
 standard_allocation_strategy standard_allocation_strategy_instance;
 namespace { class migrators_base { protected:     std::vector<const migrate_fn_type*> _migrators; }; class migrators : public migrators_base, public enable_lw_shared_from_this<migrators> { private:     struct backtrace_entry {         saved_backtrace _registration;         saved_backtrace _deregistration;     };     std::vector<std::unique_ptr<backtrace_entry>> _backtraces;     static logging::logger _logger; private:     void on_error() { abort(); } public:     uint32_t add(const migrate_fn_type* m) {         _migrators.push_back(m);         _backtraces.push_back(std::make_unique<backtrace_entry>(backtrace_entry{current_backtrace(), {}}));         return _migrators.size() - 1;     }     void remove(uint32_t idx) {         if (idx >= _migrators.size()) {             _logger.error("Attempting to deregister migrator id {} which was never registered:\n{}",                           idx, current_backtrace());             on_error();         }         if (!_migrators[idx]) {             _logger.error("Attempting to double deregister migrator id {}:\n{}\n"                           "Previously deregistered at:\n{}\nRegistered at:\n{}",                           idx, current_backtrace(), _backtraces[idx]->_deregistration,                           _backtraces[idx]->_registration);             on_error();         }         _migrators[idx] = nullptr;         _backtraces[idx]->_deregistration = current_backtrace();     }     const migrate_fn_type*& operator[](uint32_t idx) {         if (idx >= _migrators.size()) {             _logger.error("Attempting to use migrator id {} that was never registered:\n{}",                           idx, current_backtrace());             on_error();         }         if (!_migrators[idx]) {             _logger.error("Attempting to use deregistered migrator id {}:\n{}\n"                           "Deregistered at:\n{}\nRegistered at:\n{}",                           idx, current_backtrace(), _backtraces[idx]->_deregistration,                           _backtraces[idx]->_registration);             on_error();         }         return _migrators[idx];     } }; logging::logger migrators::_logger("lsa-migrator-sanitizer"); static migrators& static_migrators() noexcept {     memory::scoped_critical_alloc_section dfg;     static thread_local lw_shared_ptr<migrators> obj = make_lw_shared<migrators>();     return *obj; } }
 namespace debug { thread_local migrators* static_migrators = &::static_migrators(); }
 uint32_t migrate_fn_type::register_migrator(migrate_fn_type* m) {     auto& migrators = *debug::static_migrators;     auto idx = migrators.add(m);     // object_descriptor encodes 2 * index() + 1
    assert(idx * 2 + 1 < utils::uleb64_express_supreme);     m->_migrators = migrators.shared_from_this();     return idx; }
 void migrate_fn_type::unregister_migrator(uint32_t index) {     static_migrators().remove(index); }
 namespace logalloc { class region_sanitizer {     struct allocation {         size_t size;         saved_backtrace backtrace;     }; private:     static logging::logger logger;     const bool* _report_backtrace = nullptr;     bool _broken = false;     std::unordered_map<const void*, allocation> _allocations; private:     template<typename Function>     void run_and_handle_errors(Function&& fn) noexcept {         memory::scoped_critical_alloc_section dfg;         if (_broken) {             return;         }         try {             fn();         } catch (...) {             logger.error("Internal error, disabling the sanitizer: {}", std::current_exception());             _broken = true;             _allocations.clear();         }     } private:     void on_error() { abort(); } public:     region_sanitizer(const bool& report_backtrace) : _report_backtrace(&report_backtrace) { }     void on_region_destruction() noexcept {         run_and_handle_errors([&] {             if (_allocations.empty()) {                 return;             }             for (auto [ptr, alloc] : _allocations) {                 logger.error("Leaked {} byte object at {} allocated from:\n{}",                              alloc.size, ptr, alloc.backtrace);             }             on_error();         });     }     void on_allocation(const void* ptr, size_t size) noexcept {         run_and_handle_errors([&] {             auto backtrace = *_report_backtrace ? current_backtrace() : saved_backtrace();             auto [ it, success ] = _allocations.emplace(ptr, allocation { size, std::move(backtrace) });             if (!success) {                 logger.error("Attempting to allocate an {} byte object at an already occupied address {}:\n{}\n"                              "Previous allocation of {} bytes:\n{}",                              ptr, size, current_backtrace(), it->second.size, it->second.backtrace);                 on_error();             }         });     }     void on_free(const void* ptr, size_t size) noexcept {         run_and_handle_errors([&] {             auto it = _allocations.find(ptr);             if (it == _allocations.end()) {                 logger.error("Attempting to free an object at {} (size: {}) that does not exist\n{}",                              ptr, size, current_backtrace());                 on_error();             }             if (it->second.size != size) {                 logger.error("Mismatch between allocation and deallocation size of object at {}: {} vs. {}:\n{}\n"                              "Allocated at:\n{}",                              ptr, it->second.size, size, current_backtrace(), it->second.backtrace);                 on_error();             }             _allocations.erase(it);         });     }     void on_migrate(const void* src, size_t size, const void* dst) noexcept {         run_and_handle_errors([&] {             auto it_src = _allocations.find(src);             if (it_src == _allocations.end()) {                 logger.error("Attempting to migrate an object at {} (size: {}) that does not exist",                              src, size);                 on_error();             }             if (it_src->second.size != size) {                 logger.error("Mismatch between allocation and migration size of object at {}: {} vs. {}\n"                              "Allocated at:\n{}",                              src, it_src->second.size, size, it_src->second.backtrace);                 on_error();             }             auto [ it_dst, success ] = _allocations.emplace(dst, std::move(it_src->second));             if (!success) {                 logger.error("Attempting to migrate an {} byte object to an already occupied address {}:\n"                              "Migrated object allocated from:\n{}\n"                              "Previous allocation of {} bytes at the destination:\n{}",                              size, dst, it_src->second.backtrace, it_dst->second.size, it_dst->second.backtrace);                 on_error();             }             _allocations.erase(it_src);         });     }     void merge(region_sanitizer& other) noexcept {         run_and_handle_errors([&] {             _broken = other._broken;             if (_broken) {                 _allocations.clear();             } else {                 _allocations.merge(other._allocations);                 if (!other._allocations.empty()) {                     for (auto [ptr, o_alloc] : other._allocations) {                         auto& alloc = _allocations.at(ptr);                         logger.error("Conflicting allocations at address {} in merged regions\n"                                      "{} bytes allocated from:\n{}\n"                                      "{} bytes allocated from:\n{}",                                      ptr, alloc.size, alloc.backtrace, o_alloc.size, o_alloc.backtrace);                     }                     on_error();                 }             }         });     } }; logging::logger region_sanitizer::logger("lsa-sanitizer"); struct segment; static logging::logger llogger("lsa"); static logging::logger timing_logger("lsa-timing"); static tracker& get_tracker_instance() noexcept {     memory::scoped_critical_alloc_section dfg;     static thread_local tracker obj;     return obj; } static thread_local tracker& tracker_instance = get_tracker_instance(); using clock = std::chrono::steady_clock; class background_reclaimer {     scheduling_group _sg;     noncopyable_function<void (size_t target)> _reclaim;     timer<lowres_clock> _adjust_shares_timer;     // If engaged, main loop is not running, set_value() to wake it.
    promise<>* _main_loop_wait = nullptr;     future<> _done;     bool _stopping = false;     static constexpr size_t free_memory_threshold = 60'000'000; private:     bool have_work() const {         return memory::free_memory() < free_memory_threshold;     }     void main_loop_wake() {         llogger.debug("background_reclaimer::main_loop_wake: waking {}", bool(_main_loop_wait));         if (_main_loop_wait) {             _main_loop_wait->set_value();             _main_loop_wait = nullptr;         }     }     future<> main_loop() {         llogger.debug("background_reclaimer::main_loop: entry");         while (true) {             while (!_stopping && !have_work()) {                 promise<> wait;                 _main_loop_wait = &wait;                 llogger.trace("background_reclaimer::main_loop: sleep");                 co_await wait.get_future();                 llogger.trace("background_reclaimer::main_loop: awakened");                 _main_loop_wait = nullptr;             }             if (_stopping) {                 break;             }             _reclaim(free_memory_threshold - memory::free_memory());             co_await coroutine::maybe_yield();         }         llogger.debug("background_reclaimer::main_loop: exit");     }     void adjust_shares() {         if (have_work()) {             auto shares = 1 + (1000 * (free_memory_threshold - memory::free_memory())) / free_memory_threshold;             _sg.set_shares(shares);             llogger.trace("background_reclaimer::adjust_shares: {}", shares);             if (_main_loop_wait) {                 main_loop_wake();             }         }     } public:     explicit background_reclaimer(scheduling_group sg, noncopyable_function<void (size_t target)> reclaim)             : _sg(sg)             , _reclaim(std::move(reclaim))             , _adjust_shares_timer(default_scheduling_group(), [this] { adjust_shares(); })             , _done(with_scheduling_group(_sg, [this] { return main_loop(); })) {         if (sg != default_scheduling_group()) {             _adjust_shares_timer.arm_periodic(50ms);         }     }     future<> stop() {         _stopping = true;         main_loop_wake();         return std::move(_done);     } }; class segment_pool; struct reclaim_timer; class tracker::impl {     std::unique_ptr<logalloc::segment_pool> _segment_pool;     std::optional<background_reclaimer> _background_reclaimer;     std::vector<region::impl*> _regions;     seastar::metrics::metric_groups _metrics;     unsigned _reclaiming_disabled_depth = 0;     size_t _reclamation_step = 1;     bool _abort_on_bad_alloc = false;     bool _sanitizer_report_backtrace = false;     reclaim_timer* _active_timer = nullptr; private:     // Prevents tracker's reclaimer from running while live. Reclaimer may be
    // invoked synchronously with allocator. This guard ensures that this
    // object is not re-entered while inside one of the tracker's methods.
    struct reclaiming_lock {         impl& _ref;         reclaiming_lock(impl& ref) noexcept             : _ref(ref)         {             _ref.disable_reclaim();         }         ~reclaiming_lock() {             _ref.enable_reclaim();         }     };     friend class tracker_reclaimer_lock; public:     impl();     ~impl();     future<> stop() {         if (_background_reclaimer) {             return _background_reclaimer->stop();         } else {             return make_ready_future<>();         }     }     void disable_reclaim() noexcept {         ++_reclaiming_disabled_depth;     }     void enable_reclaim() noexcept {         --_reclaiming_disabled_depth;     }     logalloc::segment_pool& segment_pool() {         return *_segment_pool;     }     void register_region(region::impl*);     void unregister_region(region::impl*) noexcept;     size_t reclaim(size_t bytes, is_preemptible p);     // Compacts one segment at a time from sparsest segment to least sparse until work_waiting_on_reactor returns true
    // or there are no more segments to compact.
    idle_cpu_handler_result compact_on_idle(work_waiting_on_reactor check_for_work);     // Releases whole segments back to the segment pool.
    // After the call, if there is enough evictable memory, the amount of free segments in the pool
    // will be at least reserve_segments + div_ceil(bytes, segment::size).
    // Returns the amount by which segment_pool.total_memory_in_use() has decreased.
    size_t compact_and_evict(size_t reserve_segments, size_t bytes, is_preemptible p);     void full_compaction();     void reclaim_all_free_segments();     occupancy_stats global_occupancy() const noexcept;     occupancy_stats region_occupancy() const noexcept;     occupancy_stats occupancy() const noexcept;     size_t non_lsa_used_space() const noexcept;     // Set the minimum number of segments reclaimed during single reclamation cycle.
    void set_reclamation_step(size_t step_in_segments) noexcept { _reclamation_step = step_in_segments; }     size_t reclamation_step() const noexcept { return _reclamation_step; }     // Abort on allocation failure from LSA
    void enable_abort_on_bad_alloc() noexcept { _abort_on_bad_alloc = true; }     bool should_abort_on_bad_alloc() const noexcept { return _abort_on_bad_alloc; }     void setup_background_reclaim(scheduling_group sg) {         assert(!_background_reclaimer);         _background_reclaimer.emplace(sg, [this] (size_t target) {             reclaim(target, is_preemptible::yes);         });     }     // const bool&, so interested parties can save a reference and see updates.
    const bool& sanitizer_report_backtrace() const { return _sanitizer_report_backtrace; }     void set_sanitizer_report_backtrace(bool rb) { _sanitizer_report_backtrace = rb; }     bool try_set_active_timer(reclaim_timer& timer) {         if (_active_timer) {             return false;         }         _active_timer = &timer;         return true;     }     bool try_reset_active_timer(reclaim_timer& timer) {         if (_active_timer == &timer) {             _active_timer = nullptr;             return true;         }         return false;     } private:     // Like compact_and_evict() but assumes that reclaim_lock is held around the operation.
    size_t compact_and_evict_locked(size_t reserve_segments, size_t bytes, is_preemptible preempt);     // Like reclaim() but assumes that reclaim_lock is held around the operation.
    size_t reclaim_locked(size_t bytes, is_preemptible p); }; tracker_reclaimer_lock::tracker_reclaimer_lock(tracker::impl& impl) noexcept : _tracker_impl(impl) {     _tracker_impl.disable_reclaim(); } tracker_reclaimer_lock::~tracker_reclaimer_lock() {     _tracker_impl.enable_reclaim(); } tracker::tracker()     : _impl(std::make_unique<impl>())     , _reclaimer([this] (seastar::memory::reclaimer::request r) { return reclaim(r); }, memory::reclaimer_scope::sync) { } tracker::~tracker() { } future<> tracker::stop() {     return _impl->stop(); } size_t tracker::reclaim(size_t bytes) {     return _impl->reclaim(bytes, is_preemptible::no); } occupancy_stats tracker::global_occupancy() const noexcept {     return _impl->global_occupancy(); } occupancy_stats tracker::region_occupancy() const noexcept {     return _impl->region_occupancy(); } occupancy_stats tracker::occupancy() const noexcept {     return _impl->occupancy(); } size_t tracker::non_lsa_used_space() const noexcept {     return _impl->non_lsa_used_space(); } void tracker::full_compaction() {     return _impl->full_compaction(); } void tracker::reclaim_all_free_segments() {     return _impl->reclaim_all_free_segments(); }  struct alignas(segment_size) segment {     static constexpr int size_shift = segment_size_shift;     static constexpr int size_mask = segment_size | (segment_size - 1);     using size_type = std::conditional_t<(size_shift < 16), uint16_t, uint32_t>;     static constexpr size_t size = segment_size;     uint8_t data[size];     segment() noexcept { }     template<typename T = void>     const T* at(size_t offset) const noexcept {         return reinterpret_cast<const T*>(data + offset);     }     template<typename T = void>     T* at(size_t offset) noexcept {         return reinterpret_cast<T*>(data + offset);     }     static void* operator new(size_t size) = delete;     static void* operator new(size_t, void* ptr) noexcept { return ptr; }     static void operator delete(void* ptr) = delete; }; static constexpr size_t max_managed_object_size = segment_size * 0.1; static constexpr auto max_used_space_ratio_for_compaction = 0.85; static constexpr size_t max_used_space_for_compaction = segment_size * max_used_space_ratio_for_compaction; static constexpr size_t min_free_space_for_compaction = segment_size - max_used_space_for_compaction; struct [[gnu::packed]] non_lsa_object_cookie {     uint64_t value = 0xbadcaffe; }; static_assert(min_free_space_for_compaction >= max_managed_object_size,     "Segments which cannot fit max_managed_object_size must not be considered compactible for the sake of forward progress of compaction"); // Since we only compact if there's >= min_free_space_for_compaction of free space,
// we use min_free_space_for_compaction as the histogram's minimum size and put
// everything below that value in the same bucket.
extern constexpr log_heap_options segment_descriptor_hist_options(min_free_space_for_compaction, 3, segment_size); enum segment_kind : int {     regular = 0, // Holds objects allocated with region_impl::alloc_small()
    bufs = 1     // Holds objects allocated with region_impl::alloc_buf()
}; struct segment_descriptor : public log_heap_hook<segment_descriptor_hist_options> {     static constexpr segment::size_type free_space_mask = segment::size_mask;     static constexpr unsigned bits_for_free_space = segment::size_shift + 1;     static constexpr segment::size_type segment_kind_mask = 1 << bits_for_free_space;     static constexpr unsigned bits_for_segment_kind = 1;     static constexpr unsigned shift_for_segment_kind = bits_for_free_space;     static_assert(sizeof(segment::size_type) * 8 >= bits_for_free_space + bits_for_segment_kind);     segment::size_type _free_space;     region::impl* _region;     segment::size_type free_space() const noexcept {         return _free_space & free_space_mask;     }     void set_free_space(segment::size_type free_space) noexcept {         _free_space = (_free_space & ~free_space_mask) | free_space;     }     segment_kind kind() const noexcept {         return static_cast<segment_kind>((_free_space & segment_kind_mask) >> shift_for_segment_kind);     }     void set_kind(segment_kind kind) noexcept {         _free_space = (_free_space & ~segment_kind_mask)                 | static_cast<segment::size_type>(kind) << shift_for_segment_kind;     }     // Valid if kind() == segment_kind::bufs.
    //
    // _buf_pointers holds links to lsa_buffer objects (paired with lsa_buffer::_link)
    // of live objects in the segment. The purpose of this is so that segment compaction
    // can update the pointers when it moves the objects.
    // The order of entangled objects in the vector is irrelevant.
    // Also, not all entangled objects may be engaged.
    std::vector<entangled> _buf_pointers;          bool is_empty() const noexcept {         return free_space() == segment::size;     }     occupancy_stats occupancy() const noexcept {         return { free_space(), segment::size };     }     void record_alloc(segment::size_type size) noexcept {         _free_space -= size;     }     void record_free(segment::size_type size) noexcept {         _free_space += size;     } }; using segment_descriptor_hist = log_heap<segment_descriptor, segment_descriptor_hist_options>; class segment_store_backend { protected:     memory::memory_layout _layout;     // Whether freeing segments actually increases availability of non-lsa memory.
    bool _freed_segment_increases_general_memory_availability;     // Aligned (to segment::size) address of the first segment.
    uintptr_t _segments_base; public:     explicit segment_store_backend(memory::memory_layout layout, bool freed_segment_increases_general_memory_availability) noexcept         : _layout(layout)         , _freed_segment_increases_general_memory_availability(freed_segment_increases_general_memory_availability)         , _segments_base(align_up(_layout.start, static_cast<uintptr_t>(segment::size)))     { }          memory::memory_layout memory_layout() const noexcept { return _layout; }     uintptr_t segments_base() const noexcept { return _segments_base; }     virtual void* alloc_segment_memory() noexcept = 0;     virtual void free_segment_memory(void* seg) noexcept = 0;     virtual size_t free_memory() const noexcept = 0;     bool can_allocate_more_segments(size_t non_lsa_reserve) const noexcept {         if (_freed_segment_increases_general_memory_availability) {             return free_memory() >= non_lsa_reserve + segment::size;         } else {             return free_memory() >= segment::size;         }     } }; // Segments are allocated from the seastar allocator.
// The entire memory area of the local shard is used as a segment store, i.e.
// segments are allocated from the same memory area regular objeces are.
class seastar_memory_segment_store_backend : public segment_store_backend { public:     seastar_memory_segment_store_backend()         : segment_store_backend(memory::get_memory_layout(), true)     { }     virtual void* alloc_segment_memory() noexcept override {         return aligned_alloc(segment::size, segment::size);     }     virtual void free_segment_memory(void* seg) noexcept override {         ::free(seg);     }     virtual size_t free_memory() const noexcept override {         return memory::free_memory();     } }; // Segments storage is allocated via `mmap()`.
// This area cannot be shrunk or enlarged, so freeing segments doesn't increase
// memory availability.
class standard_memory_segment_store_backend : public segment_store_backend {     struct free_segment {         free_segment* next = nullptr;     }; private:     uintptr_t _segments_offset = 0;     free_segment* _freelist = nullptr;     size_t _available_segments; // for fast free_memory()
private:     static memory::memory_layout allocate_memory(size_t segments) {         const auto size = segments * segment_size;         auto p = mmap(nullptr, size,                 PROT_READ | PROT_WRITE,                 MAP_PRIVATE | MAP_ANONYMOUS,                 -1, 0);         if (p == MAP_FAILED) {             std::abort();         }         madvise(p, size, MADV_HUGEPAGE);         auto start = reinterpret_cast<uintptr_t>(p);         return {start, start + size};     } public:     standard_memory_segment_store_backend(size_t segments)         : segment_store_backend(allocate_memory(segments), false)         , _available_segments((_layout.end - _segments_base) / segment_size)     { }          virtual void* alloc_segment_memory() noexcept override {         if (_freelist) {             --_available_segments;             return std::exchange(_freelist, _freelist->next);         }         auto seg = _segments_base + _segments_offset * segment_size;         if (seg + segment_size > _layout.end) {             return nullptr;         }         ++_segments_offset;         --_available_segments;         return reinterpret_cast<void*>(seg);     }     virtual void free_segment_memory(void* seg) noexcept override {         unpoison(reinterpret_cast<char*>(seg), sizeof(free_segment));         auto fs = new (seg) free_segment;         fs->next = _freelist;         _freelist = fs;         ++_available_segments;     }     virtual size_t free_memory() const noexcept override {         return _available_segments * segment_size;     } }; static constexpr size_t segment_npos = size_t(-1); // Segments are allocated from a large contiguous memory area.
class contiguous_memory_segment_store {     std::unique_ptr<segment_store_backend> _backend; public:     size_t non_lsa_reserve = 0;     contiguous_memory_segment_store()         : _backend(std::make_unique<seastar_memory_segment_store_backend>())     { }     struct with_standard_memory_backend {};          void use_standard_allocator_segment_pool_backend(size_t available_memory) {         _backend = std::make_unique<standard_memory_segment_store_backend>(available_memory / segment::size);         llogger.debug("using the standard allocator segment pool backend with {} available memory", available_memory);     }     const segment* segment_from_idx(size_t idx) const noexcept {         return reinterpret_cast<segment*>(_backend->segments_base()) + idx;     }     segment* segment_from_idx(size_t idx) noexcept {         return reinterpret_cast<segment*>(_backend->segments_base()) + idx;     }     size_t idx_from_segment(const segment* seg) const noexcept {         const auto seg_uint = reinterpret_cast<uintptr_t>(seg);         if (seg_uint < _backend->memory_layout().start || seg_uint > _backend->memory_layout().end) [[unlikely]] {             return segment_npos;         }         return seg - reinterpret_cast<segment*>(_backend->segments_base());     }     std::pair<segment*, size_t> allocate_segment() noexcept {         auto p = _backend->alloc_segment_memory();         if (!p) {             return {nullptr, 0};         }         auto seg = new (p) segment;         poison(seg, sizeof(segment));         return {seg, idx_from_segment(seg)};     }     void free_segment(segment *seg) noexcept {         seg->~segment();         _backend->free_segment_memory(seg);     }     size_t max_segments() const noexcept {         return (_backend->memory_layout().end - _backend->segments_base()) / segment::size;     }     bool can_allocate_more_segments() const noexcept {         return _backend->can_allocate_more_segments(non_lsa_reserve);     } }; using segment_store = contiguous_memory_segment_store; // Segment pool implementation for the seastar allocator.
// Stores segment descriptors in a vector which is indexed using most significant
// bits of segment address.
//
// We prefer using high-address segments, and returning low-address segments to the seastar
// allocator in order to segregate lsa and non-lsa memory, to reduce fragmentation.
class segment_pool {     logalloc::tracker::impl& _tracker;     segment_store _store;     std::vector<segment_descriptor> _segments;     size_t _segments_in_use{};     utils::dynamic_bitset _lsa_owned_segments_bitmap; // owned by this
    utils::dynamic_bitset _lsa_free_segments_bitmap;  // owned by this, but not in use
    size_t _free_segments = 0;     size_t _current_emergency_reserve_goal = 1;     size_t _emergency_reserve_max = 30;     bool _allocation_failure_flag = false;     bool _allocation_enabled = true;     struct allocation_lock {         segment_pool& _pool;         bool _prev;         allocation_lock(segment_pool& p) noexcept             : _pool(p)             , _prev(p._allocation_enabled)         {             _pool._allocation_enabled = false;         }         ~allocation_lock() {             _pool._allocation_enabled = _prev;         }     };     size_t _non_lsa_memory_in_use = 0;     // Invariants - a segment is in one of the following states:
    //   In use by some region
    //     - set in _lsa_owned_segments_bitmap
    //     - clear in _lsa_free_segments_bitmap
    //     - counted in _segments_in_use
    //   Free:
    //     - set in _lsa_owned_segments_bitmap
    //     - set in _lsa_free_segments_bitmap
    //     - counted in _unreserved_free_segments
    //   Non-lsa:
    //     - clear everywhere
private:     segment* allocate_segment(size_t reserve);     void deallocate_segment(segment* seg) noexcept;     friend void* segment::operator new(size_t);     friend void segment::operator delete(void*);     segment* allocate_or_fallback_to_reserve();          segment* segment_from_idx(size_t idx) noexcept {         return _store.segment_from_idx(idx);     }     size_t idx_from_segment(const segment* seg) const noexcept {         return _store.idx_from_segment(seg);     }     size_t max_segments() const noexcept {         return _store.max_segments();     }     bool can_allocate_more_segments() const noexcept {         return _allocation_enabled && _store.can_allocate_more_segments();     }     bool compact_segment(segment* seg); public:     explicit segment_pool(logalloc::tracker::impl& tracker);     logalloc::tracker::impl& tracker() { return _tracker; }     void prime(size_t available_memory, size_t min_free_memory);     void use_standard_allocator_segment_pool_backend(size_t available_memory);     segment* new_segment(region::impl* r);          segment_descriptor& descriptor(segment* seg) noexcept {         uintptr_t index = idx_from_segment(seg);         return _segments[index];     }     // Returns segment containing given object or nullptr.
    segment* containing_segment(const void* obj) noexcept;     segment* segment_from(const segment_descriptor& desc) noexcept;     void free_segment(segment*) noexcept;     void free_segment(segment*, segment_descriptor&) noexcept;          size_t current_emergency_reserve_goal() const noexcept { return _current_emergency_reserve_goal; }     void set_emergency_reserve_max(size_t new_size) noexcept { _emergency_reserve_max = new_size; }     size_t emergency_reserve_max() const noexcept { return _emergency_reserve_max; }     void set_current_emergency_reserve_goal(size_t goal) noexcept { _current_emergency_reserve_goal = goal; }     void clear_allocation_failure_flag() noexcept { _allocation_failure_flag = false; }     bool allocation_failure_flag() const noexcept { return _allocation_failure_flag; }     void refill_emergency_reserve();     void add_non_lsa_memory_in_use(size_t n) noexcept {         _non_lsa_memory_in_use += n;     }     void subtract_non_lsa_memory_in_use(size_t n) noexcept {         assert(_non_lsa_memory_in_use >= n);         _non_lsa_memory_in_use -= n;     }     size_t non_lsa_memory_in_use() const noexcept {         return _non_lsa_memory_in_use;     }     size_t total_memory_in_use() const noexcept {         return _non_lsa_memory_in_use + _segments_in_use * segment::size;     }     size_t total_free_memory() const noexcept {         return _free_segments * segment::size;     }     struct reservation_goal;     void set_region(segment* seg, region::impl* r) noexcept {         set_region(descriptor(seg), r);     }     void set_region(segment_descriptor& desc, region::impl* r) noexcept {         desc._region = r;     }     size_t reclaim_segments(size_t target, is_preemptible preempt);     void reclaim_all_free_segments() {         reclaim_segments(std::numeric_limits<size_t>::max(), is_preemptible::no);     } private:     tracker::stats _stats{}; public:     const tracker::stats& statistics() const noexcept { return _stats; }     inline void on_segment_compaction(size_t used_size) noexcept;     inline void on_memory_allocation(size_t size) noexcept;     inline void on_memory_deallocation(size_t size) noexcept;     inline void on_memory_eviction(size_t size) noexcept;     size_t unreserved_free_segments() const noexcept { return _free_segments - std::min(_free_segments, _emergency_reserve_max); }     size_t free_segments() const noexcept { return _free_segments; } }; struct reclaim_timer {     using extra_logger = noncopyable_function<void(log_level)>; private:     // CLOCK_MONOTONIC_COARSE is not quite what we want -- to look for stalls,
    // we want thread time, not wall time. Wall time will give false positives
    // if the process is descheduled.
    // For this reason Seastar uses CLOCK_THREAD_CPUTIME_ID in its stall detector.
    // Unfortunately, CLOCK_THREAD_CPUTIME_ID_COARSE does not exist.
    // It's not an important problem, though.
    using clock = utils::coarse_steady_clock;     struct stats {         occupancy_stats region_occupancy;         tracker::stats pool_stats;                  friend stats operator-(const stats& s1, const stats& s2) {             stats result(s1);             result -= s2;             return result;         }                  stats& operator-=(const stats& other) {             region_occupancy -= other.region_occupancy;             pool_stats -= other.pool_stats;             return *this;         }     };     clock::duration _duration_threshold;     const char* _name;     const is_preemptible _preemptible;     const size_t _memory_to_release;     const size_t _segments_to_release;     const size_t _reserve_goal, _reserve_max;     tracker::impl& _tracker;     segment_pool& _segment_pool;     extra_logger _extra_logs;     const bool _debug_enabled;     bool _stall_detected = false;     size_t _memory_released = 0;     clock::time_point _start;     stats _start_stats, _end_stats, _stat_diff;     clock::duration _duration;     inline reclaim_timer(const char* name, is_preemptible preemptible, size_t memory_to_release, size_t segments_to_release, tracker::impl& tracker, segment_pool& segment_pool, extra_logger extra_logs); public:     inline reclaim_timer(const char* name, is_preemptible preemptible, size_t memory_to_release, size_t segments_to_release, tracker::impl& tracker, extra_logger extra_logs = [](log_level){})         : reclaim_timer(name, preemptible, memory_to_release, segments_to_release, tracker, tracker.segment_pool(), std::move(extra_logs))     {}     inline reclaim_timer(const char* name, is_preemptible preemptible, size_t memory_to_release, size_t segments_to_release, segment_pool& segment_pool, extra_logger extra_logs = [](log_level){})         : reclaim_timer(name, preemptible, memory_to_release, segments_to_release, segment_pool.tracker(), segment_pool, std::move(extra_logs))     {}     ;     size_t set_memory_released(size_t memory_released) noexcept {         return this->_memory_released = memory_released;     } private:     void sample_stats(stats& data);     template <typename T>     void log_if_changed(log_level level, const char* name, T before, T now) const noexcept {         if (now != before) {             timing_logger.log(level, "- {}: {:.3f} -> {:.3f}", name, before, now);         }     }     template <typename T>     void log_if_any(log_level level, const char* name, T value) const noexcept {         if (value != 0) {             timing_logger.log(level, "- {}: {}", name, value);         }     }     template <typename T>     void log_if_any_mem(log_level level, const char* name, T value) const noexcept {         if (value != 0) {             timing_logger.log(level, "- {}: {:.3f} MiB", name, (float)value / (1024*1024));         }     }     ; }; tracker::stats tracker::statistics() const {     return _impl->segment_pool().statistics(); } size_t segment_pool::reclaim_segments(size_t target, is_preemptible preempt) {     // Reclaimer tries to release segments occupying lower parts of the address
    // space.
    llogger.debug("Trying to reclaim {} segments", target);     // Reclamation. Migrate segments to higher addresses and shrink segment pool.
    size_t reclaimed_segments = 0;     reclaim_timer timing_guard("reclaim_segments", preempt, target * segment::size, target, *this, [&] (log_level level) {         timing_logger.log(level, "- reclaimed {} out of requested {} segments", reclaimed_segments, target);     });     // We may fail to reclaim because a region has reclaim disabled (usually because
    // it is in an allocating_section. Failed reclaims can cause high CPU usage
    // if all of the lower addresses happen to be in a reclaim-disabled region (this
    // is somewhat mitigated by the fact that checking for reclaim disabled is very
    // cheap), but worse, failing a segment reclaim can lead to reclaimed memory
    // being fragmented.  This results in the original allocation continuing to fail.
    //
    // To combat that, we limit the number of failed reclaims. If we reach the limit,
    // we fail the reclaim.  The surrounding allocating_section will release the
    // reclaim_lock, and increase reserves, which will result in reclaim being
    // retried with all regions being reclaimable, and succeed in allocating
    // contiguous memory.
    size_t failed_reclaims_allowance = 10;     for (size_t src_idx = _lsa_owned_segments_bitmap.find_first_set();             reclaimed_segments != target && src_idx != utils::dynamic_bitset::npos                     && _free_segments > _current_emergency_reserve_goal;             src_idx = _lsa_owned_segments_bitmap.find_next_set(src_idx)) {         auto src = segment_from_idx(src_idx);         if (!_lsa_free_segments_bitmap.test(src_idx)) {             if (!compact_segment(src)) {                 if (--failed_reclaims_allowance == 0) {                     break;                 }                 continue;             }         }         _lsa_free_segments_bitmap.clear(src_idx);         _lsa_owned_segments_bitmap.clear(src_idx);         _store.free_segment(src);         ++reclaimed_segments;         --_free_segments;         if (preempt && need_preempt()) {             break;         }     }     llogger.debug("Reclaimed {} segments (requested {})", reclaimed_segments, target);     timing_guard.set_memory_released(reclaimed_segments * segment::size);     return reclaimed_segments; } segment* segment_pool::allocate_segment(size_t reserve) {     //
    // When allocating a segment we want to avoid:
    //  - LSA and general-purpose allocator shouldn't constantly fight each
    //    other for every last bit of memory
    //
    // allocate_segment() always works with LSA reclaimer disabled.
    // 1. Firstly, the algorithm tries to allocate an lsa-owned but free segment
    // 2. If no free segmented is available, a new segment is allocated from the
    //    system allocator. However, if the free memory is below set threshold
    //    this step is skipped.
    // 3. Finally, the algorithm ties to compact and evict data stored in LSA
    //    memory in order to reclaim enough segments.
    //
    do {         tracker_reclaimer_lock rl(_tracker);         if (_free_segments > reserve) {             auto free_idx = _lsa_free_segments_bitmap.find_last_set();             _lsa_free_segments_bitmap.clear(free_idx);             auto seg = segment_from_idx(free_idx);             --_free_segments;             return seg;         }         if (can_allocate_more_segments()) {             memory::disable_abort_on_alloc_failure_temporarily dfg;             auto [seg, idx] = _store.allocate_segment();             if (!seg) {                 continue;             }             _lsa_owned_segments_bitmap.set(idx);             return seg;         }     } while (_tracker.compact_and_evict(reserve, _tracker.reclamation_step() * segment::size, is_preemptible::no));     return nullptr; } void segment_pool::deallocate_segment(segment* seg) noexcept {     assert(_lsa_owned_segments_bitmap.test(idx_from_segment(seg)));     _lsa_free_segments_bitmap.set(idx_from_segment(seg));     _free_segments++; } void segment_pool::refill_emergency_reserve() {     while (_free_segments < _emergency_reserve_max) {         auto seg = allocate_segment(_emergency_reserve_max);         if (!seg) {             throw std::bad_alloc();         }         ++_segments_in_use;         free_segment(seg);     } } segment* segment_pool::containing_segment(const void* obj) noexcept {     auto addr = reinterpret_cast<uintptr_t>(obj);     auto offset = addr & (segment::size - 1);     auto seg = reinterpret_cast<segment*>(addr - offset);     auto index = idx_from_segment(seg);     if (index == segment_npos) {         return nullptr;     }     auto& desc = _segments[index];     if (desc._region) {         return seg;     } else {         return nullptr;     } } segment* segment_pool::segment_from(const segment_descriptor& desc) noexcept {     assert(desc._region);     auto index = &desc - &_segments[0];     return segment_from_idx(index); } segment* segment_pool::allocate_or_fallback_to_reserve() {     auto seg = allocate_segment(_current_emergency_reserve_goal);     if (!seg) {         _allocation_failure_flag = true;         throw std::bad_alloc();     }     return seg; } segment* segment_pool::new_segment(region::impl* r) {     auto seg = allocate_or_fallback_to_reserve();     ++_segments_in_use;     segment_descriptor& desc = descriptor(seg);     desc.set_free_space(segment::size);     desc.set_kind(segment_kind::regular);     desc._region = r;     return seg; } void segment_pool::free_segment(segment* seg) noexcept {     free_segment(seg, descriptor(seg)); } void segment_pool::free_segment(segment* seg, segment_descriptor& desc) noexcept {     llogger.trace("Releasing segment {}", fmt::ptr(seg));     desc._region = nullptr;     deallocate_segment(seg);     --_segments_in_use; } segment_pool::segment_pool(tracker::impl& tracker)     : _tracker(tracker)     , _segments(max_segments())     , _lsa_owned_segments_bitmap(max_segments())     , _lsa_free_segments_bitmap(max_segments()) { }   inline void segment_pool::on_segment_compaction(size_t used_size) noexcept {     _stats.segments_compacted++;     _stats.memory_compacted += used_size; }   inline void segment_pool::on_memory_eviction(size_t size) noexcept {     _stats.memory_evicted += size; } // RAII wrapper to maintain segment_pool::current_emergency_reserve_goal()
class segment_pool::reservation_goal {     segment_pool& _sp;     size_t _old_goal; public:     reservation_goal(segment_pool& sp, size_t goal) noexcept             : _sp(sp), _old_goal(_sp.current_emergency_reserve_goal()) {         _sp.set_current_emergency_reserve_goal(goal);     }      };  reclaim_timer::reclaim_timer(const char* name, is_preemptible preemptible, size_t memory_to_release, size_t segments_to_release, tracker::impl& tracker, segment_pool& segment_pool, extra_logger extra_logs)     : _duration_threshold(             // We only report reclaim stalls when their measured duration is
            // bigger than the threshold by at least one measurement error
            // (clock resolution). This prevents false positives.
            //
            // Explanation for the 10us: The clock value is not always an
            // integral multiply of its resolution. In the case of coarse
            // clocks, resolution only describes the frequency of syncs with
            // the hardware clock -- no effort is made to round the values to
            // resolution. Therefore, tick durations vary slightly in both
            // directions. We subtract something slightly bigger than these
            // variations, to accomodate blocked-reactor-notify-ms values which
            // are multiplies of resolution.
            // E.g. with kernel CONFIG_HZ=250, coarse clock resolution is 4ms.
            // If also we also have blocked-reactor-notify-ms=4, then we would
            // like to report two-tick stalls, since they have durations of
            // 4ms-8ms. But two-tick durations can be just slightly smaller
            // than 8ms (e.g. 7999us) due to the inaccuracy. So we set the
            // threshold not to (blocked_reactor_notify_ms + resolution) = 8000us,
            // but to (blocked_reactor_notify_ms + resolution - 10us) = 7990us,
            // to account for this.
            engine().get_blocked_reactor_notify_ms() + std::max(0ns, clock::get_resolution() - 10us))     , _name(name)     , _preemptible(preemptible)     , _memory_to_release(memory_to_release)     , _segments_to_release(segments_to_release)     , _reserve_goal(segment_pool.current_emergency_reserve_goal())     , _reserve_max(segment_pool.emergency_reserve_max())     , _tracker(tracker)     , _segment_pool(segment_pool)     , _extra_logs(std::move(extra_logs))     , _debug_enabled(timing_logger.is_enabled(logging::log_level::debug)) {     if (!_tracker.try_set_active_timer(*this)) {         return;     }     _start = clock::now();     sample_stats(_start_stats); }  void reclaim_timer::sample_stats(stats& data) {     if (_debug_enabled) {         data.region_occupancy = _tracker.region_occupancy();     }     data.pool_stats = _segment_pool.statistics(); }  region_listener::~region_listener() = default; //
// For interface documentation see logalloc::region and allocation_strategy.
//
// Allocation dynamics.
//
// Objects are allocated inside fixed-size segments. Objects don't cross
// segment boundary. Active allocations are served from a single segment using
// bump-the-pointer method. That segment is called the active segment. When
// active segment fills up, it is closed. Closed segments are kept in a heap
// which orders them by occupancy. As objects are freed, the segment become
// sparser and are eventually released. Objects which are too large are
// allocated using standard allocator.
//
// Segment layout.
//
// Objects in a segment are laid out sequentially. Each object is preceded by
// a descriptor (see object_descriptor). Object alignment is respected, so if
// there is a gap between the end of current object and the next object's
// descriptor, a trunk of the object descriptor is left right after the
// current object with the flags byte indicating the amount of padding.
//
// Per-segment metadata is kept in a separate array, managed by segment_pool
// object.
//
class region_impl final : public basic_region_impl {     // Serialized object descriptor format:
    //  byte0 byte1 ... byte[n-1]
    //  bit0-bit5: ULEB64 significand
    //  bit6: 1 iff first byte
    //  bit7: 1 iff last byte
    // This format allows decoding both forwards and backwards (by scanning for bit7/bit6 respectively);
    // backward decoding is needed to recover the descriptor from the object pointer when freeing.
    //
    // Significand interpretation (value = n):
    //     even:  dead object, size n/2 (including descriptor)
    //     odd:   migrate_fn_type at index n/2, from static_migrators()
    class object_descriptor {     private:         uint32_t _n;     private:         explicit object_descriptor(uint32_t n) noexcept : _n(n) {}     public:         object_descriptor(allocation_strategy::migrate_fn migrator) noexcept                 : _n(migrator->index() * 2 + 1)         { }         static object_descriptor make_dead(size_t size) noexcept {             return object_descriptor(size * 2);         }         allocation_strategy::migrate_fn migrator() const noexcept {             return static_migrators()[_n / 2];         }         uint8_t alignment() const noexcept {             return migrator()->align();         }         // excluding descriptor
        segment::size_type live_size(const void* obj) const noexcept {             return migrator()->size(obj);         }         // including descriptor
        segment::size_type dead_size() const noexcept {             return _n / 2;         }         bool is_live() const noexcept {             return (_n & 1) == 1;         }         segment::size_type encoded_size() const noexcept {             return utils::uleb64_encoded_size(_n); // 0 is illegal
        }         void encode(char*& pos) const noexcept {             utils::uleb64_encode(pos, _n, poison<char>, unpoison);         }         // non-canonical encoding to allow padding (for alignment); encoded_size must be
        // sufficient (greater than this->encoded_size()), _n must be the migrator's
        // index() (i.e. -- suitable for express encoding)
        void encode(char*& pos, size_t encoded_size, size_t size) const noexcept {             utils::uleb64_express_encode(pos, _n, encoded_size, size, poison<char>, unpoison);         }         static object_descriptor decode_forwards(const char*& pos) noexcept {             return object_descriptor(utils::uleb64_decode_forwards(pos, poison<char>, unpoison));         }         static object_descriptor decode_backwards(const char*& pos) noexcept {             return object_descriptor(utils::uleb64_decode_bacwards(pos, poison<char>, unpoison));         }              }; private: // lsa_buffer allocator
    segment* _buf_active = nullptr;     size_t _buf_active_offset;     static constexpr size_t buf_align = 4096; // All lsa_buffer:s will have addresses aligned to this value.
    // Emergency storage to ensure forward progress during segment compaction,
    // by ensuring that _buf_pointers allocation inside new_buf_active() does not fail.
    std::vector<entangled> _buf_ptrs_for_compact_segment; private:     region* _region = nullptr;     region_listener* _listener = nullptr;     segment* _active = nullptr;     size_t _active_offset;     segment_descriptor_hist _segment_descs; // Contains only closed segments
    occupancy_stats _closed_occupancy;     occupancy_stats _non_lsa_occupancy;     // This helps us updating out region_listener*. That's because we call update before
    // we have a chance to update the occupancy stats - mainly because at this point we don't know
    // what will we do with the new segment. Also, because we are not ever interested in the
    // fraction used, we'll keep it as a scalar and convert when we need to present it as an
    // occupancy. We could actually just present this as a scalar as well and never use occupancies,
    // but consistency is good.
    size_t _evictable_space = 0;     // This is a mask applied to _evictable_space with bitwise-and before it's returned from evictable_space().
    // Used for forcing the result to zero without using conditionals.
    size_t _evictable_space_mask = std::numeric_limits<size_t>::max();     bool _evictable = false;     region_sanitizer _sanitizer;     uint64_t _id;     eviction_fn _eviction_fn; private:     struct compaction_lock {         region_impl& _region;         bool _prev;         compaction_lock(region_impl& r) noexcept             : _region(r)             , _prev(r._reclaiming_enabled)         {             _region._reclaiming_enabled = false;         }         ~compaction_lock() {             _region._reclaiming_enabled = _prev;         }     };     void* alloc_small(const object_descriptor& desc, segment::size_type size, size_t alignment) {         if (!_active) {             _active = new_segment();             _active_offset = 0;         }         auto desc_encoded_size = desc.encoded_size();         size_t obj_offset = align_up_for_asan(align_up(_active_offset + desc_encoded_size, alignment));         if (obj_offset + size > segment::size) {             close_and_open();             return alloc_small(desc, size, alignment);         }         auto old_active_offset = _active_offset;         auto pos = _active->at<char>(_active_offset);         // Use non-canonical encoding to allow for alignment pad
        desc.encode(pos, obj_offset - _active_offset, size);         unpoison(pos, size);         _active_offset = obj_offset + size;         // Align the end of the value so that the next descriptor is aligned
        _active_offset = align_up_for_asan(_active_offset);         segment_pool().descriptor(_active).record_alloc(_active_offset - old_active_offset);         return pos;     }     template<typename Func>     requires std::is_invocable_r_v<void, Func, const object_descriptor*, void*, size_t>     void for_each_live(segment* seg, Func&& func) {         // scylla-gdb.py:scylla_lsa_segment is coupled with this implementation.
        auto pos = align_up_for_asan(seg->at<const char>(0));         while (pos < seg->at<const char>(segment::size)) {             auto old_pos = pos;             const auto desc = object_descriptor::decode_forwards(pos);             if (desc.is_live()) {                 auto size = desc.live_size(pos);                 func(&desc, const_cast<char*>(pos), size);                 pos += size;             } else {                 pos = old_pos + desc.dead_size();             }             pos = align_up_for_asan(pos);         }     }     void close_active() {         if (!_active) {             return;         }         if (_active_offset < segment::size) {             auto desc = object_descriptor::make_dead(segment::size - _active_offset);             auto pos =_active->at<char>(_active_offset);             desc.encode(pos);         }         auto& desc = segment_pool().descriptor(_active);         llogger.trace("Closing segment {}, used={}, waste={} [B]", fmt::ptr(_active), desc.occupancy(), segment::size - _active_offset);         _closed_occupancy += desc.occupancy();         _segment_descs.push(desc);         _active = nullptr;     }     void close_buf_active() {         if (!_buf_active) {             return;         }         auto& desc = segment_pool().descriptor(_buf_active);         llogger.trace("Closing buf segment {}, used={}, waste={} [B]", fmt::ptr(_buf_active), desc.occupancy(), segment::size - _buf_active_offset);         _closed_occupancy += desc.occupancy();         _segment_descs.push(desc);         _buf_active = nullptr;     }     void free_segment(segment_descriptor& desc) noexcept {         free_segment(segment_pool().segment_from(desc), desc);     }     void free_segment(segment* seg) noexcept {         free_segment(seg, segment_pool().descriptor(seg));     }     void free_segment(segment* seg, segment_descriptor& desc) noexcept {         segment_pool().free_segment(seg, desc);         if (_listener) {             _evictable_space -= segment_size;             _listener->decrease_usage(_region, -segment::size);         }     }     segment* new_segment() {         segment* seg = segment_pool().new_segment(this);         if (_listener) {             _evictable_space += segment_size;             _listener->increase_usage(_region, segment::size);         }         return seg;     }     lsa_buffer alloc_buf(size_t buf_size) {         // Note: Can be re-entered from allocation sites below due to memory reclamation which
        // invokes segment compaction.
        static_assert(segment::size % buf_align == 0);         if (buf_size > segment::size) {             throw_with_backtrace<std::runtime_error>(format("Buffer size {} too large", buf_size));         }         if (_buf_active_offset + buf_size > segment::size) {             close_buf_active();         }         if (!_buf_active) {             new_buf_active();         }         lsa_buffer ptr;         ptr._buf = _buf_active->at<char>(_buf_active_offset);         ptr._size = buf_size;         unpoison(ptr._buf, buf_size);         segment_descriptor& desc = segment_pool().descriptor(_buf_active);         ptr._desc = &desc;         desc._buf_pointers.emplace_back(entangled::make_paired_with(ptr._link));         auto alloc_size = align_up(buf_size, buf_align);         desc.record_alloc(alloc_size);         _buf_active_offset += alloc_size;         return ptr;     }     void free_buf(lsa_buffer& buf) noexcept {         segment_descriptor &desc = *buf._desc;         segment *seg = segment_pool().segment_from(desc);         if (seg != _buf_active) {             _closed_occupancy -= desc.occupancy();         }         auto alloc_size = align_up(buf._size, buf_align);         desc.record_free(alloc_size);         poison(buf._buf, buf._size);         // Pack links so that segment compaction only has to walk live objects.
        // This procedure also ensures that the link for buf is destroyed, either
        // by replacing it with the last entangled, or by popping it from the back
        // if it is the last element.
        // Moving entangled links around is fine so we can move last_link.
        entangled& last_link = desc._buf_pointers.back();         entangled& buf_link = *buf._link.get();         std::swap(last_link, buf_link);         desc._buf_pointers.pop_back();         if (seg != _buf_active) {             if (desc.is_empty()) {                 assert(desc._buf_pointers.empty());                 _segment_descs.erase(desc);                 desc._buf_pointers = std::vector<entangled>();                 free_segment(seg, desc);             } else {                 _segment_descs.adjust_up(desc);                 _closed_occupancy += desc.occupancy();             }         }     }     void compact_segment_locked(segment* seg, segment_descriptor& desc) noexcept {         auto seg_occupancy = desc.occupancy();         llogger.debug("Compacting segment {} from region {}, {}", fmt::ptr(seg), id(), seg_occupancy);         ++_invalidate_counter;         if (desc.kind() == segment_kind::bufs) {             // This will free the storage of _buf_ptrs_for_compact_segment
            // making sure that alloc_buf() makes progress.
            // Also, empties desc._buf_pointers, making it back a generic segment, which
            // we need to do before freeing it.
            _buf_ptrs_for_compact_segment = std::move(desc._buf_pointers);             for (entangled& e : _buf_ptrs_for_compact_segment) {                 if (e) {                     lsa_buffer* old_ptr = e.get(&lsa_buffer::_link);                     assert(&desc == old_ptr->_desc);                     lsa_buffer dst = alloc_buf(old_ptr->_size);                     memcpy(dst._buf, old_ptr->_buf, dst._size);                     old_ptr->_link = std::move(dst._link);                     old_ptr->_buf = dst._buf;                     old_ptr->_desc = dst._desc;                 }             }         } else {             for_each_live(seg, [this](const object_descriptor *desc, void *obj, size_t size) {                 auto dst = alloc_small(*desc, size, desc->alignment());                 _sanitizer.on_migrate(obj, size, dst);                 desc->migrator()->migrate(obj, dst, size);             });         }         free_segment(seg, desc);         segment_pool().on_segment_compaction(seg_occupancy.used_space());     }     void close_and_open() {         segment* new_active = new_segment();         close_active();         _active = new_active;         _active_offset = 0;     }     void new_buf_active() {         std::vector<entangled> ptrs;         ptrs.reserve(segment::size / buf_align);         segment* new_active = new_segment();         if (_buf_active) [[unlikely]] {             // Memory allocation above could allocate active buffer during segment compaction.
            close_buf_active();         }         assert((uintptr_t)new_active->at(0) % buf_align == 0);         segment_descriptor& desc = segment_pool().descriptor(new_active);         desc._buf_pointers = std::move(ptrs);         desc.set_kind(segment_kind::bufs);         _buf_active = new_active;         _buf_active_offset = 0;     }     static uint64_t next_id() noexcept {         static std::atomic<uint64_t> id{0};         return id.fetch_add(1);     }     struct unlisten_temporarily {         region_impl* impl;         region_listener* listener;         explicit unlisten_temporarily(region_impl* impl)                 : impl(impl), listener(impl->_listener) {             if (listener) {                 listener->del(impl->_region);             }         }         ~unlisten_temporarily() {             if (listener) {                 listener->add(impl->_region);             }         }     }; public:     explicit region_impl(tracker& tracker, region* region)         : basic_region_impl(tracker), _region(region), _sanitizer(tracker.get_impl().sanitizer_report_backtrace()), _id(next_id())     {         _buf_ptrs_for_compact_segment.reserve(segment::size / buf_align);         _preferred_max_contiguous_allocation = max_managed_object_size;         tracker_instance._impl->register_region(this);     }     virtual ~region_impl() {         _sanitizer.on_region_destruction();         _tracker.get_impl().unregister_region(this);         while (!_segment_descs.empty()) {             auto& desc = _segment_descs.one_of_largest();             _segment_descs.pop_one_of_largest();             assert(desc.is_empty());             free_segment(desc);         }         _closed_occupancy = {};         if (_active) {             assert(segment_pool().descriptor(_active).is_empty());             free_segment(_active);             _active = nullptr;         }         if (_buf_active) {             assert(segment_pool().descriptor(_buf_active).is_empty());             free_segment(_buf_active);             _buf_active = nullptr;         }     }               logalloc::segment_pool& segment_pool() const {         return _tracker.get_impl().segment_pool();     }          void listen(region_listener* listener) {         _listener = listener;         _listener->add(_region);     }     void unlisten() {         // _listener may have been removed be merge(), so check for that.
        // Yes, it's awkward, we should have the caller unlisten before merge
        // to remove implicit behavior.
        if (_listener) {             _listener->del(_region);             _listener = nullptr;         }     }     void moved(region* new_region) {         if (_listener) {             _listener->moved(_region, new_region);         }         _region = new_region;     }     // Note: allocation is disallowed in this path
    // since we don't instantiate reclaiming_lock
    // while traversing _regions
    occupancy_stats occupancy() const noexcept {         occupancy_stats total = _non_lsa_occupancy;         total += _closed_occupancy;         if (_active) {             total += segment_pool().descriptor(_active).occupancy();         }         if (_buf_active) {             total += segment_pool().descriptor(_buf_active).occupancy();         }         return total;     }     occupancy_stats compactible_occupancy() const noexcept {         return _closed_occupancy;     }     occupancy_stats evictable_occupancy() const noexcept {         return occupancy_stats(0, _evictable_space & _evictable_space_mask);     }     void ground_evictable_occupancy() {         _evictable_space_mask = 0;         if (_listener) {             _listener->decrease_evictable_usage(_region);         }     }     //
    // Returns true if this region can be compacted and compact() will make forward progress,
    // so that this will eventually stop:
    //
    //    while (is_compactible()) { compact(); }
    //
    bool is_compactible() const noexcept {         return _reclaiming_enabled             // We require 2 segments per allocation segregation group to ensure forward progress during compaction.
            // There are currently two fixed groups, one for the allocation_strategy implementation and one for lsa_buffer:s.
            && (_closed_occupancy.free_space() >= 4 * segment::size)             && _segment_descs.contains_above_min();     }     bool is_idle_compactible() const noexcept {         return is_compactible();     }     virtual void* alloc(allocation_strategy::migrate_fn migrator, size_t size, size_t alignment) override {         compaction_lock _(*this);         memory::on_alloc_point();         auto& pool = segment_pool();         pool.on_memory_allocation(size);         if (size > max_managed_object_size) {             auto ptr = standard_allocator().alloc(migrator, size + sizeof(non_lsa_object_cookie), alignment);             // This isn't very acurrate, the correct free_space value would be
            // malloc_usable_size(ptr) - size, but there is no way to get
            // the exact object size at free.
            auto allocated_size = malloc_usable_size(ptr);             new ((char*)ptr + allocated_size - sizeof(non_lsa_object_cookie)) non_lsa_object_cookie();             _non_lsa_occupancy += occupancy_stats(0, allocated_size);             if (_listener) {                  _evictable_space += allocated_size;                 _listener->increase_usage(_region, allocated_size);             }             pool.add_non_lsa_memory_in_use(allocated_size);             return ptr;         } else {             auto ptr = alloc_small(object_descriptor(migrator), (segment::size_type) size, alignment);             _sanitizer.on_allocation(ptr, size);             return ptr;         }     } private:     void on_non_lsa_free(void* obj) noexcept {         auto allocated_size = malloc_usable_size(obj);         auto cookie = (non_lsa_object_cookie*)((char*)obj + allocated_size) - 1;         assert(cookie->value == non_lsa_object_cookie().value);         _non_lsa_occupancy -= occupancy_stats(0, allocated_size);         if (_listener) {             _evictable_space -= allocated_size;             _listener->decrease_usage(_region, allocated_size);         }         segment_pool().subtract_non_lsa_memory_in_use(allocated_size);     } public:     virtual void free(void* obj) noexcept override {         compaction_lock _(*this);         segment* seg = segment_pool().containing_segment(obj);         if (!seg) {             on_non_lsa_free(obj);             standard_allocator().free(obj);             return;         }         auto pos = reinterpret_cast<const char*>(obj);         auto desc = object_descriptor::decode_backwards(pos);         free(obj, desc.live_size(obj));     }     virtual void free(void* obj, size_t size) noexcept override {         compaction_lock _(*this);         auto& pool = segment_pool();         segment* seg = pool.containing_segment(obj);         if (!seg) {             on_non_lsa_free(obj);             standard_allocator().free(obj, size);             return;         }         _sanitizer.on_free(obj, size);         segment_descriptor& seg_desc = pool.descriptor(seg);         auto pos = reinterpret_cast<const char*>(obj);         auto old_pos = pos;         auto desc = object_descriptor::decode_backwards(pos);         auto dead_size = align_up_for_asan(size + (old_pos - pos));         desc = object_descriptor::make_dead(dead_size);         auto npos = const_cast<char*>(pos);         desc.encode(npos);         poison(pos, dead_size);         if (seg != _active) {             _closed_occupancy -= seg_desc.occupancy();         }         seg_desc.record_free(dead_size);         pool.on_memory_deallocation(dead_size);         if (seg != _active) {             if (seg_desc.is_empty()) {                 _segment_descs.erase(seg_desc);                 free_segment(seg, seg_desc);             } else {                 _segment_descs.adjust_up(seg_desc);                 _closed_occupancy += seg_desc.occupancy();             }         }     }     virtual size_t object_memory_size_in_allocator(const void* obj) const noexcept override {         segment* seg = segment_pool().containing_segment(obj);         if (!seg) {             return standard_allocator().object_memory_size_in_allocator(obj);         } else {             auto pos = reinterpret_cast<const char*>(obj);             auto desc = object_descriptor::decode_backwards(pos);             return desc.encoded_size() + desc.live_size(obj);         }     }     // Merges another region into this region. The other region is made
    // to refer to this region.
    // Doesn't invalidate references to allocated objects.
    void merge(region_impl& other) noexcept {         // unlisten_temporarily may allocate via region_listener callbacks, which should not
        // fail, because we have a matching deallocation before that and we don't
        // allocate between them.
        memory::scoped_critical_alloc_section dfg;         compaction_lock dct1(*this);         compaction_lock dct2(other);         unlisten_temporarily ult1(this);         unlisten_temporarily ult2(&other);         auto& pool = segment_pool();         if (_active && pool.descriptor(_active).is_empty()) {             pool.free_segment(_active);             _active = nullptr;         }         if (!_active) {             _active = other._active;             other._active = nullptr;             _active_offset = other._active_offset;             if (_active) {                 pool.set_region(_active, this);             }         } else {             other.close_active();         }         other.close_buf_active();         for (auto& desc : other._segment_descs) {             pool.set_region(desc, this);         }         _segment_descs.merge(other._segment_descs);         _closed_occupancy += other._closed_occupancy;         _non_lsa_occupancy += other._non_lsa_occupancy;         other._closed_occupancy = {};         other._non_lsa_occupancy = {};         // Make sure both regions will notice a future increment
        // to the reclaim counter
        _invalidate_counter = std::max(_invalidate_counter, other._invalidate_counter);         _sanitizer.merge(other._sanitizer);         other._sanitizer = region_sanitizer(_tracker.get_impl().sanitizer_report_backtrace());     }     // Returns occupancy of the sparsest compactible segment.
    occupancy_stats min_occupancy() const noexcept {         if (_segment_descs.empty()) {             return {};         }         return _segment_descs.one_of_largest().occupancy();     }     // Compacts a single segment, most appropriate for it
    void compact() noexcept {         compaction_lock _(*this);         auto& desc = _segment_descs.one_of_largest();         _segment_descs.pop_one_of_largest();         _closed_occupancy -= desc.occupancy();         segment* seg = segment_pool().segment_from(desc);         compact_segment_locked(seg, desc);     }     // Compacts everything. Mainly for testing.
    // Invalidates references to allocated objects.
    void full_compaction() {         compaction_lock _(*this);         llogger.debug("Full compaction, {}", occupancy());         close_and_open();         close_buf_active();         segment_descriptor_hist all;         std::swap(all, _segment_descs);         _closed_occupancy = {};         while (!all.empty()) {             auto& desc = all.one_of_largest();             all.pop_one_of_largest();             compact_segment_locked(segment_pool().segment_from(desc), desc);         }         llogger.debug("Done, {}", occupancy());     }     void compact_segment(segment* seg, segment_descriptor& desc) {         compaction_lock _(*this);         if (_active == seg) {             close_active();         } else if (_buf_active == seg) {             close_buf_active();         }         _segment_descs.erase(desc);         _closed_occupancy -= desc.occupancy();         compact_segment_locked(seg, desc);     }          uint64_t id() const noexcept {         return _id;     }     // Returns true if this pool is evictable, so that evict_some() can be called.
    bool is_evictable() const noexcept {         return _evictable && _reclaiming_enabled;     }     memory::reclaiming_result evict_some() {         ++_invalidate_counter;         auto& pool = segment_pool();         auto freed = pool.statistics().memory_freed;         auto ret = _eviction_fn();         pool.on_memory_eviction(pool.statistics().memory_freed - freed);         return ret;     }          void make_evictable(eviction_fn fn) noexcept {         _evictable = true;         _eviction_fn = std::move(fn);     }     const eviction_fn& evictor() const noexcept {         return _eviction_fn;     }     friend class region;     friend class lsa_buffer;     friend class region_evictable_occupancy_ascending_less_comparator; }; lsa_buffer::~lsa_buffer() {     if (_link) {         _desc->_region->free_buf(*this);     } } size_t tracker::reclamation_step() const noexcept {     return _impl->reclamation_step(); } bool tracker::should_abort_on_bad_alloc() const noexcept {     return _impl->should_abort_on_bad_alloc(); } void tracker::configure(const config& cfg) {     if (cfg.defragment_on_idle) {         engine().set_idle_cpu_handler([this] (reactor::work_waiting_on_reactor check_for_work) {             return _impl->compact_on_idle(check_for_work);         });     }     _impl->set_reclamation_step(cfg.lsa_reclamation_step);     if (cfg.abort_on_lsa_bad_alloc) {         _impl->enable_abort_on_bad_alloc();     }     _impl->setup_background_reclaim(cfg.background_reclaim_sched_group);     _impl->set_sanitizer_report_backtrace(cfg.sanitizer_report_backtrace); } memory::reclaiming_result tracker::reclaim(seastar::memory::reclaimer::request r) {     return reclaim(std::max(r.bytes_to_reclaim, _impl->reclamation_step() * segment::size))            ? memory::reclaiming_result::reclaimed_something            : memory::reclaiming_result::reclaimed_nothing; } region::region()     : _impl(make_shared<impl>(shard_tracker(), this)) { } void region::listen(region_listener* listener) {     get_impl().listen(listener); } void region::unlisten() {     get_impl().unlisten(); } region_impl& region::get_impl() noexcept {     return *static_cast<region_impl*>(_impl.get()); } const region_impl& region::get_impl() const noexcept {     return *static_cast<const region_impl*>(_impl.get()); } region::region(region&& other) noexcept     : _impl(std::move(other._impl)) {     if (_impl) {         auto r_impl = static_cast<region_impl*>(_impl.get());         r_impl->moved(this);     } } region& region::operator=(region&& other) noexcept {     if (this == &other || _impl == other._impl) {         return *this;     }     if (_impl) {         unlisten();     }     this->_impl = std::move(other._impl);     if (_impl) {         auto r_impl = static_cast<region_impl*>(_impl.get());         r_impl->moved(this);     }     return *this; } region::~region() {     if (_impl) {         unlisten();     } } occupancy_stats region::occupancy() const noexcept {     return get_impl().occupancy(); } lsa_buffer region::alloc_buf(size_t buffer_size) {     return get_impl().alloc_buf(buffer_size); } void region::merge(region& other) noexcept {     if (_impl != other._impl) {         auto& other_impl = other.get_impl();         // Not very generic, but we know that post-merge the caller
        // (row_cache) isn't interested in listening, and one region
        // can't have many listeners.
        other_impl.unlisten();         get_impl().merge(other_impl);         other._impl = _impl;     } } void region::full_compaction() {     get_impl().full_compaction(); } memory::reclaiming_result region::evict_some() {     if (get_impl().is_evictable()) {         return get_impl().evict_some();     }     return memory::reclaiming_result::reclaimed_nothing; } void region::make_evictable(eviction_fn fn) noexcept {     get_impl().make_evictable(std::move(fn)); } void region::ground_evictable_occupancy() {     get_impl().ground_evictable_occupancy(); } occupancy_stats region::evictable_occupancy() const noexcept {     return get_impl().evictable_occupancy(); } const eviction_fn& region::evictor() const noexcept {     return get_impl().evictor(); } uint64_t region::id() const noexcept {     return get_impl().id(); } std::ostream& operator<<(std::ostream& out, const occupancy_stats& stats) {     return out << format("{:.2f}%, {:d} / {:d} [B]",         stats.used_fraction() * 100, stats.used_space(), stats.total_space()); } occupancy_stats tracker::impl::global_occupancy() const noexcept {     return occupancy_stats(_segment_pool->total_free_memory(), _segment_pool->total_memory_in_use()); } // Note: allocation is disallowed in this path
// since we don't instantiate reclaiming_lock
// while traversing _regions
occupancy_stats tracker::impl::region_occupancy() const noexcept {     occupancy_stats total{};     for (auto&& r: _regions) {         total += r->occupancy();     }     return total; } occupancy_stats tracker::impl::occupancy() const noexcept {     auto occ = region_occupancy();     {         auto s = _segment_pool->free_segments() * segment::size;         occ += occupancy_stats(s, s);     }     return occ; } size_t tracker::impl::non_lsa_used_space() const noexcept {     auto free_space_in_lsa = _segment_pool->free_segments() * segment_size;     return memory::stats().allocated_memory() - region_occupancy().total_space() - free_space_in_lsa; } void tracker::impl::reclaim_all_free_segments() {     llogger.debug("Reclaiming all free segments");     _segment_pool->reclaim_all_free_segments();     llogger.debug("Reclamation done"); } void tracker::impl::full_compaction() {     reclaiming_lock _(*this);     llogger.debug("Full compaction on all regions, {}", region_occupancy());     for (region_impl* r : _regions) {         if (r->reclaiming_enabled()) {             r->full_compaction();         }     }     llogger.debug("Compaction done, {}", region_occupancy()); } static void reclaim_from_evictable(region::impl& r, size_t target_mem_in_use, is_preemptible preempt) {     llogger.debug("reclaim_from_evictable: total_memory_in_use={} target={}", r.segment_pool().total_memory_in_use(), target_mem_in_use);     // Before attempting segment compaction, try to evict at least deficit and one segment more so that
    // for workloads in which eviction order matches allocation order we will reclaim full segments
    // without needing to perform expensive compaction.
    auto deficit = r.segment_pool().total_memory_in_use() - target_mem_in_use;     auto used = r.occupancy().used_space();     auto used_target = used - std::min(used, deficit + segment::size);     while (r.segment_pool().total_memory_in_use() > target_mem_in_use) {         used = r.occupancy().used_space();         if (used > used_target) {             llogger.debug("Evicting {} bytes from region {}, occupancy={} in advance",                     used - used_target, r.id(), r.occupancy());         } else {             llogger.debug("Evicting from region {}, occupancy={} until it's compactible", r.id(), r.occupancy());         }         while (r.occupancy().used_space() > used_target || !r.is_compactible()) {             if (r.evict_some() == memory::reclaiming_result::reclaimed_nothing) {                 if (r.is_compactible()) { // Need to make forward progress in case there is nothing to evict.
                    break;                 }                 llogger.debug("Unable to evict more, evicted {} bytes", used - r.occupancy().used_space());                 return;             }             if (r.segment_pool().total_memory_in_use() <= target_mem_in_use) {                 llogger.debug("Target met after evicting {} bytes", used - r.occupancy().used_space());                 return;             }             if (preempt && need_preempt()) {                 llogger.debug("reclaim_from_evictable preempted");                 return;             }         }         // If there are many compactible segments, we will keep compacting without
        // entering the eviction loop above. So the preemption check there is not
        // sufficient and we also need to check here.
        //
        // Note that a preemptible reclaim_from_evictable may not do any real progress,
        // but it doesn't need to. Preemptible (background) reclaim is an optimization.
        // If the system is overwhelmed, and reclaim_from_evictable keeps getting
        // preempted without doing any useful work, then eventually memory will be
        // exhausted and reclaim will be called synchronously, without preemption.
        if (preempt && need_preempt()) {             llogger.debug("reclaim_from_evictable preempted");             return;         }         llogger.debug("Compacting after evicting {} bytes", used - r.occupancy().used_space());         r.compact();     } } idle_cpu_handler_result tracker::impl::compact_on_idle(work_waiting_on_reactor check_for_work) {     if (_reclaiming_disabled_depth) {         return idle_cpu_handler_result::no_more_work;     }     reclaiming_lock rl(*this);     if (_regions.empty()) {         return idle_cpu_handler_result::no_more_work;     }     segment_pool::reservation_goal open_emergency_pool(*_segment_pool, 0);     auto cmp = [] (region::impl* c1, region::impl* c2) {         if (c1->is_idle_compactible() != c2->is_idle_compactible()) {             return !c1->is_idle_compactible();         }         return c2->min_occupancy() < c1->min_occupancy();     };     boost::range::make_heap(_regions, cmp);     while (!check_for_work()) {         boost::range::pop_heap(_regions, cmp);         region::impl* r = _regions.back();         if (!r->is_idle_compactible()) {             return idle_cpu_handler_result::no_more_work;         }         r->compact();         boost::range::push_heap(_regions, cmp);     }     return idle_cpu_handler_result::interrupted_by_higher_priority_task; } size_t tracker::impl::reclaim(size_t memory_to_release, is_preemptible preempt) {     if (_reclaiming_disabled_depth) {         return 0;     }     reclaiming_lock rl(*this);     reclaim_timer timing_guard("reclaim", preempt, memory_to_release, 0, *this);     return timing_guard.set_memory_released(reclaim_locked(memory_to_release, preempt)); } size_t tracker::impl::reclaim_locked(size_t memory_to_release, is_preemptible preempt) {     llogger.debug("reclaim_locked({}, preempt={})", memory_to_release, int(bool(preempt)));     // Reclamation steps:
    // 1. Try to release free segments from segment pool and emergency reserve.
    // 2. Compact used segments and/or evict data.
    constexpr auto max_bytes = std::numeric_limits<size_t>::max() - segment::size;     auto segments_to_release = align_up(std::min(max_bytes, memory_to_release), segment::size) >> segment::size_shift;     auto nr_released = _segment_pool->reclaim_segments(segments_to_release, preempt);     size_t mem_released = nr_released * segment::size;     if (mem_released >= memory_to_release) {         llogger.debug("reclaim_locked() = {}", memory_to_release);         return memory_to_release;     }     if (preempt && need_preempt()) {         llogger.debug("reclaim_locked() = {}", mem_released);         return mem_released;     }     auto compacted = compact_and_evict_locked(_segment_pool->current_emergency_reserve_goal(), memory_to_release - mem_released, preempt);     if (compacted == 0) {         llogger.debug("reclaim_locked() = {}", mem_released);         return mem_released;     }     // compact_and_evict_locked() will not return segments to the standard allocator,
    // so do it here:
    nr_released = _segment_pool->reclaim_segments(compacted / segment::size, preempt);     mem_released += nr_released * segment::size;     llogger.debug("reclaim_locked() = {}", mem_released);     return mem_released; } size_t tracker::impl::compact_and_evict(size_t reserve_segments, size_t memory_to_release, is_preemptible preempt) {     if (_reclaiming_disabled_depth) {         return 0;     }     reclaiming_lock rl(*this);     return compact_and_evict_locked(reserve_segments, memory_to_release, preempt); } size_t tracker::impl::compact_and_evict_locked(size_t reserve_segments, size_t memory_to_release, is_preemptible preempt) {     llogger.debug("compact_and_evict_locked({}, {}, {})", reserve_segments, memory_to_release, int(bool(preempt)));     //
    // Algorithm outline.
    //
    // Regions are kept in a max-heap ordered so that regions with
    // sparser segments are picked first. Non-compactible regions will be
    // picked last. In each iteration we try to release one whole segment from
    // the region which has the sparsest segment. We do it until we released
    // enough segments or there are no more regions we can compact.
    //
    // When compaction is not sufficient to reclaim space, we evict data from
    // evictable regions.
    //
    // This may run synchronously with allocation, so we should not allocate
    // memory, otherwise we may get std::bad_alloc. Currently we only allocate
    // in the logger when debug level is enabled. It's disabled during normal
    // operation. Having it is still valuable during testing and in most cases
    // should work just fine even if allocates.
    size_t mem_released = 0;     size_t mem_in_use = _segment_pool->total_memory_in_use();     memory_to_release += (reserve_segments - std::min(reserve_segments, _segment_pool->free_segments())) * segment::size;     auto target_mem = mem_in_use - std::min(mem_in_use, memory_to_release - mem_released);     llogger.debug("Compacting, requested {} bytes, {} bytes in use, target is {}",         memory_to_release, mem_in_use, target_mem);     // Allow dipping into reserves while compacting
    segment_pool::reservation_goal open_emergency_pool(*_segment_pool, 0);     auto cmp = [] (region::impl* c1, region::impl* c2) {         if (c1->is_compactible() != c2->is_compactible()) {             return !c1->is_compactible();         }         return c2->min_occupancy() < c1->min_occupancy();     };     boost::range::make_heap(_regions, cmp);     if (llogger.is_enabled(logging::log_level::debug)) {         llogger.debug("Occupancy of regions:");         for (region::impl* r : _regions) {             llogger.debug(" - {}: min={}, avg={}", r->id(), r->min_occupancy(), r->compactible_occupancy());         }     }     {         int regions = 0, evictable_regions = 0;         reclaim_timer timing_guard("compact", preempt, memory_to_release, reserve_segments, *this, [&] (log_level level) {             timing_logger.log(level, "- processed {} regions: reclaimed from {}, compacted {}",                               regions, evictable_regions, regions - evictable_regions);         });         while (_segment_pool->total_memory_in_use() > target_mem) {             boost::range::pop_heap(_regions, cmp);             region::impl* r = _regions.back();             if (!r->is_compactible()) {                 llogger.trace("Unable to release segments, no compactible pools.");                 break;             }             ++regions;             // Prefer evicting if average occupancy ratio is above the compaction threshold to avoid
            // overhead of compaction in workloads where allocation order matches eviction order, where
            // we can reclaim memory by eviction only. In some cases the cost of compaction on allocation
            // would be higher than the cost of repopulating the region with evicted items.
            if (r->is_evictable() && r->occupancy().used_space() >= max_used_space_ratio_for_compaction * r->occupancy().total_space()) {                 reclaim_from_evictable(*r, target_mem, preempt);                 ++evictable_regions;             } else {                 r->compact();             }             boost::range::push_heap(_regions, cmp);             if (preempt && need_preempt()) {                 break;             }         }     }     auto released_during_compaction = mem_in_use - _segment_pool->total_memory_in_use();     if (_segment_pool->total_memory_in_use() > target_mem) {         int regions = 0, evictable_regions = 0;         reclaim_timer timing_guard("evict", preempt, memory_to_release, reserve_segments, *this, [&] (log_level level) {             timing_logger.log(level, "- processed {} regions, reclaimed from {}", regions, evictable_regions);         });         llogger.debug("Considering evictable regions.");         // FIXME: Fair eviction
        for (region::impl* r : _regions) {             if (preempt && need_preempt()) {                 break;             }             ++regions;             if (r->is_evictable()) {                 ++evictable_regions;                 reclaim_from_evictable(*r, target_mem, preempt);                 if (_segment_pool->total_memory_in_use() <= target_mem) {                     break;                 }             }         }     }     mem_released += mem_in_use - _segment_pool->total_memory_in_use();     llogger.debug("Released {} bytes (wanted {}), {} during compaction",         mem_released, memory_to_release, released_during_compaction);     return mem_released; } void tracker::impl::register_region(region::impl* r) {     // If needed, increase capacity of regions before taking the reclaim lock,
    // to avoid failing an allocation when push_back() tries to increase
    // capacity.
    //
    // The capacity increase is atomic (wrt _regions) so it cannot be
    // observed
    if (_regions.size() == _regions.capacity()) {         auto copy = _regions;         copy.reserve(copy.capacity() * 2);         _regions = std::move(copy);     }     reclaiming_lock _(*this);     _regions.push_back(r);     llogger.debug("Registered region @{} with id={}", fmt::ptr(r), r->id()); } void tracker::impl::unregister_region(region::impl* r) noexcept {     reclaiming_lock _(*this);     llogger.debug("Unregistering region, id={}", r->id());     _regions.erase(std::remove(_regions.begin(), _regions.end(), r), _regions.end()); } tracker::impl::impl() : _segment_pool(std::make_unique<logalloc::segment_pool>(*this)) {     namespace sm = seastar::metrics;     _metrics.add_group("lsa", {         sm::make_gauge("total_space_bytes", [this] { return region_occupancy().total_space(); },                        sm::description("Holds a current size of allocated memory in bytes.")),         sm::make_gauge("used_space_bytes", [this] { return region_occupancy().used_space(); },                        sm::description("Holds a current amount of used memory in bytes.")),         sm::make_gauge("small_objects_total_space_bytes", [this] { return region_occupancy().total_space() - _segment_pool->non_lsa_memory_in_use(); },                        sm::description("Holds a current size of \"small objects\" memory region in bytes.")),         sm::make_gauge("small_objects_used_space_bytes", [this] { return region_occupancy().used_space() - _segment_pool->non_lsa_memory_in_use(); },                        sm::description("Holds a current amount of used \"small objects\" memory in bytes.")),         sm::make_gauge("large_objects_total_space_bytes", [this] { return _segment_pool->non_lsa_memory_in_use(); },                        sm::description("Holds a current size of allocated non-LSA memory.")),         sm::make_gauge("non_lsa_used_space_bytes", [this] { return non_lsa_used_space(); },                        sm::description("Holds a current amount of used non-LSA memory.")),         sm::make_gauge("free_space", [this] { return _segment_pool->unreserved_free_segments() * segment_size; },                        sm::description("Holds a current amount of free memory that is under lsa control.")),         sm::make_gauge("occupancy", [this] { return region_occupancy().used_fraction() * 100; },                        sm::description("Holds a current portion (in percents) of the used memory.")),         sm::make_counter("segments_compacted", [this] { return _segment_pool->statistics().segments_compacted; },                         sm::description("Counts a number of compacted segments.")),         sm::make_counter("memory_compacted", [this] { return _segment_pool->statistics().memory_compacted; },                         sm::description("Counts number of bytes which were copied as part of segment compaction.")),         sm::make_counter("memory_allocated", [this] { return _segment_pool->statistics().memory_allocated; },                         sm::description("Counts number of bytes which were requested from LSA.")),         sm::make_counter("memory_evicted", [this] { return _segment_pool->statistics().memory_evicted; },                         sm::description("Counts number of bytes which were evicted.")),         sm::make_counter("memory_freed", [this] { return _segment_pool->statistics().memory_freed; },                         sm::description("Counts number of bytes which were requested to be freed in LSA.")),     }); } tracker::impl::~impl() {     if (!_regions.empty()) {         for (auto&& r : _regions) {             llogger.error("Region with id={} not unregistered!", r->id());         }         abort();     } } bool segment_pool::compact_segment(segment* seg) {     auto& desc = descriptor(seg);     if (!desc._region->reclaiming_enabled()) {         return false;     }     // Called with emergency reserve, open one for
    // region::alloc_small not to throw if it needs
    // one more segment
    reservation_goal open_emergency_pool(*this, 0);     allocation_lock no_alloc(*this);     tracker_reclaimer_lock no_reclaim(_tracker);     desc._region->compact_segment(seg, desc);     return true; } allocating_section::guard::guard(tracker::impl& tracker) noexcept     : _tracker(tracker), _prev(_tracker.segment_pool().emergency_reserve_max()) { } allocating_section::guard::~guard() {     _tracker.segment_pool().set_emergency_reserve_max(_prev); } void allocating_section::maybe_decay_reserve() noexcept {     // The decay rate is inversely proportional to the reserve
    // (every (s_segments_per_decay/_lsa_reserve) allocations).
    //
    // If the reserve is high, it is expensive since we may need to
    // evict a lot of memory to satisfy the reserve. Hence, we are
    // willing to risk a more frequent bad_alloc in order to decay it.
    // The cost of a bad_alloc is also lower compared to maintaining
    // the reserve.
    //
    // If the reserve is low, it is not expensive to maintain, so we
    // decay it at a lower rate.
    _remaining_lsa_segments_until_decay -= _lsa_reserve;     if (_remaining_lsa_segments_until_decay < 0) {         _remaining_lsa_segments_until_decay = s_segments_per_decay;         _lsa_reserve = std::max(s_min_lsa_reserve, _lsa_reserve / 2);         llogger.debug("Decaying LSA reserve in section {} to {} segments", static_cast<void*>(this), _lsa_reserve);     }     _remaining_std_bytes_until_decay -= _std_reserve;     if (_remaining_std_bytes_until_decay < 0) {         _remaining_std_bytes_until_decay = s_bytes_per_decay;         _std_reserve = std::max(s_min_std_reserve, _std_reserve / 2);         llogger.debug("Decaying standard allocator head-room in section {} to {} [B]", static_cast<void*>(this), _std_reserve);     } } void allocating_section::reserve(tracker::impl& tracker) {     auto& pool = tracker.segment_pool();   try {     pool.set_emergency_reserve_max(std::max(_lsa_reserve, _minimum_lsa_emergency_reserve));     pool.refill_emergency_reserve();     while (true) {         size_t free = memory::free_memory();         if (free >= _std_reserve) {             break;         }         if (!tracker.reclaim(_std_reserve - free, is_preemptible::no)) {             throw std::bad_alloc();         }     }     pool.clear_allocation_failure_flag();   } catch (const std::bad_alloc&) {         if (tracker.should_abort_on_bad_alloc()) {             llogger.error("Aborting due to allocation failure");             abort();         }         throw;   } } void allocating_section::on_alloc_failure(logalloc::region& r) {     r.allocator().invalidate_references();     if (r.get_tracker().get_impl().segment_pool().allocation_failure_flag()) {         _lsa_reserve *= 2;         llogger.debug("LSA allocation failure, increasing reserve in section {} to {} segments", fmt::ptr(this), _lsa_reserve);     } else {         _std_reserve *= 2;         llogger.debug("Standard allocator failure, increasing head-room in section {} to {} [B]", fmt::ptr(this), _std_reserve);     }     reserve(r.get_tracker().get_impl()); } void allocating_section::set_lsa_reserve(size_t reserve) noexcept {     _lsa_reserve = reserve; } void allocating_section::set_std_reserve(size_t reserve) noexcept {     _std_reserve = reserve; }   }
 // Orders segments by free space, assuming all segments have the same size.
// This avoids using the occupancy, which entails extra division operations.
template<> size_t hist_key<logalloc::segment_descriptor>(const logalloc::segment_descriptor& desc) {     return desc.free_space(); }
 using namespace seastar;
 large_bitset::large_bitset(size_t nr_bits) : _nr_bits(nr_bits) {     assert(thread::running_in_thread());     const size_t orig_nr_ints = align_up(nr_bits, bits_per_int()) / bits_per_int();     auto nr_ints = orig_nr_ints;     while (nr_ints) {         nr_ints = _storage.reserve_partial(nr_ints);         if (need_preempt()) {             thread::yield();         }     }     nr_ints = orig_nr_ints;     while (nr_ints) {         _storage.push_back(0);         --nr_ints;         if (need_preempt()) {             thread::yield();         }     } }
 void large_bitset::clear() {     assert(thread::running_in_thread());     for (auto&& pos: _storage) {         pos = 0;         if (need_preempt()) {             thread::yield();         }     } }
 using namespace seastar;
 class buffer_data_source_impl : public data_source_impl { private:     temporary_buffer<char> _buf; public:     buffer_data_source_impl(temporary_buffer<char>&& buf)         : _buf(std::move(buf))     {}     buffer_data_source_impl(buffer_data_source_impl&&) noexcept = default;          virtual future<temporary_buffer<char>> get() override {         return make_ready_future<temporary_buffer<char>>(std::move(_buf));     }      };
  input_stream<char> make_buffer_input_stream(temporary_buffer<char>&& buf,                                             seastar::noncopyable_function<size_t()>&& limit_generator) {     auto res = data_source{std::make_unique<buffer_data_source_impl>(std::move(buf))};     return input_stream < char > { make_limiting_data_source(std::move(res), std::move(limit_generator)) }; }
 using namespace seastar;
 class limiting_data_source_impl final : public data_source_impl {     data_source _src;     seastar::noncopyable_function<size_t()> _limit_generator;     temporary_buffer<char> _buf;     future<temporary_buffer<char>> do_get() {         uint64_t size = std::min(_limit_generator(), _buf.size());         auto res = _buf.share(0, size);         _buf.trim_front(size);         return make_ready_future<temporary_buffer<char>>(std::move(res));     } public:     limiting_data_source_impl(data_source&& src, seastar::noncopyable_function<size_t()>&& limit_generator)         : _src(std::move(src))         , _limit_generator(std::move(limit_generator))     {}               virtual future<temporary_buffer<char>> get() override {         if (_buf.empty()) {             _buf.release();             return _src.get().then([this] (auto&& buf) {                 _buf = std::move(buf);                 return do_get();             });         }         return do_get();     }      };
 data_source make_limiting_data_source(data_source&& src, seastar::noncopyable_function<size_t()>&& limit_generator) {     return data_source{std::make_unique<limiting_data_source_impl>(std::move(src), std::move(limit_generator))}; }
 namespace utils { updateable_value_base::updateable_value_base(const updateable_value_source_base& source) {     source.add_ref(this);     _source = &source; } updateable_value_base::~updateable_value_base() {     if (_source) {         _source->del_ref(this);     } } updateable_value_base::updateable_value_base(const updateable_value_base& v) {     if (v._source) {         v._source->add_ref(this);         _source = v._source;     } } updateable_value_base& updateable_value_base::updateable_value_base::operator=(const updateable_value_base& v) {     if (this != &v) {         // If both sources are null, or non-null and equal, nothing needs to be done
        if (_source != v._source) {             if (v._source) {                 v._source->add_ref(this);             }             if (_source) {                 _source->del_ref(this);             }             _source = v._source;         }     }     return *this; } updateable_value_base::updateable_value_base(updateable_value_base&& v) noexcept         : _source(std::exchange(v._source, nullptr)) {     if (_source) {         _source->update_ref(&v, this);     } } updateable_value_base& updateable_value_base::operator=(updateable_value_base&& v) noexcept {     if (this != &v) {         if (_source) {             _source->del_ref(this);         }         _source = std::exchange(v._source, nullptr);         if (_source) {             _source->update_ref(&v, this);         }     }     return *this; } updateable_value_base& updateable_value_base::updateable_value_base::operator=(std::nullptr_t) {     if (_source) {         _source->del_ref(this);         _source = nullptr;     }     return *this; } void updateable_value_source_base::for_each_ref(std::function<void (updateable_value_base* ref)> func) {     for (auto ref : _refs) {         func(ref);     } } updateable_value_source_base::~updateable_value_source_base() {     for (auto ref : _refs) {         ref->_source = nullptr;     } } void updateable_value_source_base::add_ref(updateable_value_base* ref) const {     _refs.push_back(ref); } void updateable_value_source_base::del_ref(updateable_value_base* ref) const {     _refs.erase(std::remove(_refs.begin(), _refs.end(), ref), _refs.end()); } void updateable_value_source_base::update_ref(updateable_value_base* old_ref, updateable_value_base* new_ref) const {     std::replace(_refs.begin(), _refs.end(), old_ref, new_ref); } }
 namespace utils { static future<> disk_sanity(fs::path path, bool developer_mode) {     return check_direct_io_support(path.native()).then([] {         return make_ready_future<>();     }).handle_exception([path](auto ep) {         startlog.error("Could not access {}: {}", path, ep);         return make_exception_future<>(ep);     }); }; static future<file_lock> touch_and_lock(fs::path path) {     return io_check([path] { return recursive_touch_directory(path.native()); }).then_wrapped([path] (future<> f) {         try {             f.get();             return file_lock::acquire(path / ".lock").then_wrapped([path](future<file_lock> f) {                 // only do this because "normal" unhandled exception exit in seastar
                // _drops_ system_error message ("what()") and thus does not quite deliver
                // the relevant info to the user
                try {                     return make_ready_future<file_lock>(f.get());                 } catch (std::exception& e) {                     startlog.error("Could not initialize {}: {}", path, e.what());                     throw;                 } catch (...) {                     throw;                 }             });         } catch (...) {             startlog.error("Directory '{}' cannot be initialized. Tried to do it but failed with: {}", path, std::current_exception());             throw;         }     }); } void directories::set::add(fs::path path) {     _paths.insert(path); } void directories::set::add(sstring path) {     add(fs::path(path)); } void directories::set::add(std::vector<sstring> paths) {     for (auto& path : paths) {         add(path);     } } void directories::set::add_sharded(sstring p) {     fs::path path(p);     for (unsigned i = 0; i < smp::count; i++) {         add(path / seastar::to_sstring(i).c_str());     } } directories::directories(bool developer_mode)         : _developer_mode(developer_mode) { } future<> directories::create_and_verify(directories::set dir_set) {     return do_with(std::vector<file_lock>(), [this, dir_set = std::move(dir_set)] (std::vector<file_lock>& locks) {         return parallel_for_each(dir_set.get_paths(), [this, &locks] (fs::path path) {             return touch_and_lock(path).then([path = std::move(path), developer_mode = _developer_mode, &locks] (file_lock lock) {                 locks.emplace_back(std::move(lock));                 return disk_sanity(path, developer_mode).then([path = std::move(path)] {                     return directories::verify_owner_and_mode(path).handle_exception([](auto ep) {                         startlog.error("Failed owner and mode verification: {}", ep);                         return make_exception_future<>(ep);                     });                 });             });         }).then([this, &locks] {             std::move(locks.begin(), locks.end(), std::back_inserter(_locks));         });     }); } template <typename... Args> static inline future<> verification_error(fs::path path, const char* fstr, Args&&... args) {     auto emsg = fmt::format(fmt::runtime(fstr), std::forward<Args>(args)...);     startlog.error("{}: {}", path.string(), emsg);     return make_exception_future<>(std::runtime_error(emsg)); } // Verify that all files and directories are owned by current uid
// and that files can be read and directories can be read, written, and looked up (execute)
// No other file types may exist.
future<> directories::verify_owner_and_mode(fs::path path) {     return file_stat(path.string(), follow_symlink::no).then([path = std::move(path)] (stat_data sd) {         // Under docker, we run with euid 0 and there is no reasonable way to enforce that the
        // in-container uid will have the same uid as files mounted from outside the container. So
        // just allow euid 0 as a special case. It should survive the file_accessible() checks below.
        // See #4823.
        if (geteuid() != 0 && sd.uid != geteuid()) {             return verification_error(std::move(path), "File not owned by current euid: {}. Owner is: {}", geteuid(), sd.uid);         }         switch (sd.type) {         case directory_entry_type::regular: {             auto f = file_accessible(path.string(), access_flags::read);             return f.then([path = std::move(path)] (bool can_access) {                 if (!can_access) {                     return verification_error(std::move(path), "File cannot be accessed for read");                 }                 return make_ready_future<>();             });             break;         }         case directory_entry_type::directory: {             auto f = file_accessible(path.string(), access_flags::read | access_flags::write | access_flags::execute);             return f.then([path = std::move(path)] (bool can_access) {                 if (!can_access) {                     return verification_error(std::move(path), "Directory cannot be accessed for read, write, and execute");                 }                 return lister::scan_dir(path, {}, [] (fs::path dir, directory_entry de) {                     return verify_owner_and_mode(dir / de.name);                 });             });             break;         }         default:             return verification_error(std::move(path), "Must be either a regular file or a directory (type={})", static_cast<int>(sd.type));         }     }); }; }
 // namespace utils
namespace rjson { allocator the_allocator; // chunked_content_stream is a wrapper of a chunked_content which
// presents the Stream concept that the rapidjson library expects as input
// for its parser (https://rapidjson.org/classrapidjson_1_1_stream.html).
// This wrapper owns the chunked_content, so it can free each chunk as
// soon as it's parsed.
class chunked_content_stream { private:     chunked_content _content;     chunked_content::iterator _current_chunk;     // _count only needed for Tell(). 32 bits is enough, we don't allow
    // more than 16 MB requests anyway.
    unsigned _count; public:     typedef char Ch;     chunked_content_stream(chunked_content&& content)         : _content(std::move(content))         , _current_chunk(_content.begin())     {}     bool eof() const {         return _current_chunk == _content.end();     }     // Methods needed by rapidjson's Stream concept (see
    // https://rapidjson.org/classrapidjson_1_1_stream.html):
    char Peek() const {         if (eof()) {             // Rapidjson's Stream concept does not have the explicit notion of
            // an "end of file". Instead, reading after the end of stream will
            // return a null byte. This makes these streams appear like null-
            // terminated C strings. It is good enough for reading JSON, which
            // anyway can't include bare null characters.
            return '\0';         } else {             return *_current_chunk->begin();         }     }     char Take() {         if (eof()) {             return '\0';         } else {             char ret = *_current_chunk->begin();             _current_chunk->trim_front(1);             ++_count;             if (_current_chunk->empty()) {                 *_current_chunk = temporary_buffer<char>();                 ++_current_chunk;             }             return ret;         }     }     size_t Tell() const {         return _count;     }     // Not used in input streams, but unfortunately we still need to implement
    Ch* PutBegin() { RAPIDJSON_ASSERT(false); return 0; }     void Put(Ch) { RAPIDJSON_ASSERT(false); }     void Flush() { RAPIDJSON_ASSERT(false); }     size_t PutEnd(Ch*) { RAPIDJSON_ASSERT(false); return 0; } }; template<typename Handler, bool EnableYield, typename Buffer = string_buffer> struct guarded_yieldable_json_handler : public Handler {     size_t _nested_level = 0;     size_t _max_nested_level; public:     using handler_base = Handler;     explicit guarded_yieldable_json_handler(size_t max_nested_level) : _max_nested_level(max_nested_level) {}     guarded_yieldable_json_handler(Buffer& buf, size_t max_nested_level)             : handler_base(buf), _max_nested_level(max_nested_level) {}     // Parse any stream fitting https://rapidjson.org/classrapidjson_1_1_stream.html
    template<typename Stream>     void Parse(Stream& stream) {         rapidjson::GenericReader<encoding, encoding, allocator> reader(&the_allocator);         reader.Parse(stream, *this);         if (reader.HasParseError()) {             throw rjson::error(format("Parsing JSON failed: {}", rapidjson::GetParseError_En(reader.GetParseErrorCode())));         }         //NOTICE: The handler has parsed the string, but in case of rapidjson::GenericDocument
        // the data now resides in an internal stack_ variable, which is private instead of
        // protected... which means we cannot simply access its data. Fortunately, another
        // function for populating documents from SAX events can be abused to extract the data
        // from the stack via gadget-oriented programming - we use an empty event generator
        // which does nothing, and use it to call Populate(), which assumes that the generator
        // will fill the stack with something. It won't, but our stack is already filled with
        // data we want to steal, so once Populate() ends, our document will be properly parsed.
        // A proper solution could be programmed once rapidjson declares this stack_ variable
        // as protected instead of private, so that this class can access it.
        auto dummy_generator = [](handler_base&){return true;};         handler_base::Populate(dummy_generator);     }     void Parse(const char* str, size_t length) {         rapidjson::MemoryStream ms(static_cast<const char*>(str), length * sizeof(typename encoding::Ch));         rapidjson::EncodedInputStream<encoding, rapidjson::MemoryStream> is(ms);         Parse(is);     }     void Parse(chunked_content&& content) {         // Note that content was moved into this function. The intention is
        // that we free every chunk we are done with.
        chunked_content_stream is(std::move(content));         Parse(is);     }     bool StartObject() {         ++_nested_level;         check_nested_level();         maybe_yield();         return handler_base::StartObject();     }     bool EndObject(rapidjson::SizeType elements_count = 0) {         --_nested_level;         return handler_base::EndObject(elements_count);     }     bool StartArray() {         ++_nested_level;         check_nested_level();         maybe_yield();         return handler_base::StartArray();     }     bool EndArray(rapidjson::SizeType elements_count = 0) {         --_nested_level;         return handler_base::EndArray(elements_count);     }     bool Null()                 { maybe_yield(); return handler_base::Null(); }     bool Bool(bool b)           { maybe_yield(); return handler_base::Bool(b); }     bool Int(int i)             { maybe_yield(); return handler_base::Int(i); }     bool Uint(unsigned u)       { maybe_yield(); return handler_base::Uint(u); }     bool Int64(int64_t i64)     { maybe_yield(); return handler_base::Int64(i64); }     bool Uint64(uint64_t u64)   { maybe_yield(); return handler_base::Uint64(u64); }     bool Double(double d)       { maybe_yield(); return handler_base::Double(d); }     bool String(const value::Ch* str, size_t length, bool copy = false) { maybe_yield(); return handler_base::String(str, length, copy); }     bool Key(const value::Ch* str, size_t length, bool copy = false) { maybe_yield(); return handler_base::Key(str, length, copy); } protected:     static void maybe_yield() {         if constexpr (EnableYield) {             thread::maybe_yield();         }     }     void check_nested_level() const {         if (RAPIDJSON_UNLIKELY(_nested_level > _max_nested_level)) {             throw rjson::error(format("Max nested level reached: {}", _max_nested_level));         }     } }; void* internal::throwing_allocator::Malloc(size_t size) {     // For bypassing the address sanitizer failure in debug mode - allocating
    // too much memory results in an abort
    if (size > memory::stats().total_memory()) {         throw rjson::error(format("Failed to allocate {} bytes", size));     }     void* ret = base::Malloc(size);     if (size > 0 && !ret) {         throw rjson::error(format("Failed to allocate {} bytes", size));     }     return ret; } void* internal::throwing_allocator::Realloc(void* orig_ptr, size_t orig_size, size_t new_size) {     // For bypassing the address sanitizer failure in debug mode - allocating
    // too much memory results in an abort
    if (new_size > memory::stats().total_memory()) {         throw rjson::error(format("Failed to allocate {} bytes", new_size));     }     void* ret = base::Realloc(orig_ptr, orig_size, new_size);     if (new_size > 0 && !ret) {         throw rjson::error(format("Failed to reallocate {} bytes to {} bytes from {}", orig_size, new_size, orig_ptr));     }     return ret; } void internal::throwing_allocator::Free(void* ptr) {     base::Free(ptr); } std::string print(const rjson::value& value, size_t max_nested_level) {     string_buffer buffer;     guarded_yieldable_json_handler<writer, false> writer(buffer, max_nested_level);     value.Accept(writer);     return std::string(buffer.GetString()); } future<> print(const rjson::value& value, seastar::output_stream<char>& os, size_t max_nested_level) {     struct os_buffer {         seastar::output_stream<char>& _os;         temporary_buffer<char> _buf;         size_t _pos = 0;         future<> _f = make_ready_future<>();         
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wunused-local-typedefs"
        using Ch = char;         
#pragma GCC diagnostic pop
        void send(bool try_reuse = true) {             if (_f.failed()) {                 _f.get0();             }             if (!_buf.empty() && _pos > 0) {                 _buf.trim(_pos);                 _pos = 0;                 // Note: we're assuming we're writing to a buffered output_stream (hello http server).
                // If we were not, or if (http) output_stream supported mixed buffered/packed content
                // it might be a good idea to instead send our buffer as a packet directly. If so, the
                // buffer size should probably increase (at least after first send()).
                _f = _f.then([this, buf = std::move(_buf), &os = _os, try_reuse]() mutable -> future<> {                     return os.write(buf.get(), buf.size()).then([this, buf = std::move(buf), try_reuse]() mutable {                         // Chances are high we just copied this to output_stream buffer, and got here
                        // immediately. If so, reuse the buffer.
                        if (try_reuse && _buf.empty() && _pos == 0) {                             _buf = std::move(buf);                         }                     });                 });             }         }         void Put(char c) {             if (_pos == _buf.size()) {                 send();                 if (_buf.empty()) {                     _buf = temporary_buffer<char>(512);                 }             }             // Second note: Should consider writing directly to the buffer in output_stream
            // instead of double buffering. But output_stream for a single char has higher
            // overhead than the above check + once we hit a non-completed future, we'd have
            // to revert to this method anyway...
            *(_buf.get_write() + _pos) = c;             ++_pos;         }         void Flush() {             send();         }         future<> finish()&& {             send(false);             return std::move(_f);         }     };     os_buffer osb{ os };     using streamer = rapidjson::Writer<os_buffer, encoding, encoding, allocator>;     guarded_yieldable_json_handler<streamer, false, os_buffer> writer(osb, max_nested_level);     value.Accept(writer);     co_return co_await std::move(osb).finish(); } rjson::malformed_value::malformed_value(std::string_view name, const rjson::value& value)     : malformed_value(name, print(value)) {} rjson::malformed_value::malformed_value(std::string_view name, std::string_view value)     : error(format("Malformed value {} : {}", name, value)) {} rjson::missing_value::missing_value(std::string_view name)      // TODO: using old message here, but as pointed out. 
    // "parameter" is not really a JSON concept. It is a value
    // missing according to (implicit) schema. 
    : error(format("JSON parameter {} not found", name)) {} rjson::value copy(const rjson::value& value) {     return rjson::value(value, the_allocator); } rjson::value parse(std::string_view str, size_t max_nested_level) {     guarded_yieldable_json_handler<document, false> d(max_nested_level);     d.Parse(str.data(), str.size());     if (d.HasParseError()) {         throw rjson::error(format("Parsing JSON failed: {}", GetParseError_En(d.GetParseError())));     }     rjson::value& v = d;     return std::move(v); } rjson::value parse(chunked_content&& content, size_t max_nested_level) {     guarded_yieldable_json_handler<document, false> d(max_nested_level);     d.Parse(std::move(content));     if (d.HasParseError()) {         throw rjson::error(format("Parsing JSON failed: {}", GetParseError_En(d.GetParseError())));     }     rjson::value& v = d;     return std::move(v); } std::optional<rjson::value> try_parse(std::string_view str, size_t max_nested_level) {     guarded_yieldable_json_handler<document, false> d(max_nested_level);     try {         d.Parse(str.data(), str.size());     } catch (const rjson::error&) {         return std::nullopt;     }     if (d.HasParseError()) {         return std::nullopt;         }     rjson::value& v = d;     return std::move(v); } rjson::value parse_yieldable(std::string_view str, size_t max_nested_level) {     guarded_yieldable_json_handler<document, true> d(max_nested_level);     d.Parse(str.data(), str.size());     if (d.HasParseError()) {         throw rjson::error(format("Parsing JSON failed: {}", GetParseError_En(d.GetParseError())));     }     rjson::value& v = d;     return std::move(v); } rjson::value parse_yieldable(chunked_content&& content, size_t max_nested_level) {     guarded_yieldable_json_handler<document, true> d(max_nested_level);     d.Parse(std::move(content));     if (d.HasParseError()) {         throw rjson::error(format("Parsing JSON failed: {}", GetParseError_En(d.GetParseError())));     }     rjson::value& v = d;     return std::move(v); } rjson::value& get(rjson::value& value, std::string_view name) {     // Although FindMember() has a variant taking a StringRef, it ignores the
    // given length (see https://github.com/Tencent/rapidjson/issues/1649).
    // Luckily, the variant taking a GenericValue doesn't share this bug,
    // and we can create a string GenericValue without copying the string.
    auto member_it = value.FindMember(rjson::value(name.data(), name.size()));     if (member_it != value.MemberEnd()) {         return member_it->value;     }     throw missing_value(name); } const rjson::value& get(const rjson::value& value, std::string_view name) {     auto member_it = value.FindMember(rjson::value(name.data(), name.size()));     if (member_it != value.MemberEnd()) {         return member_it->value;     }     throw missing_value(name); } rjson::value from_string(const std::string& str) {     return rjson::value(str.c_str(), str.size(), the_allocator); } rjson::value from_string(const sstring& str) {     return rjson::value(str.c_str(), str.size(), the_allocator); } rjson::value from_string(const char* str, size_t size) {     return rjson::value(str, size, the_allocator); } rjson::value from_string(std::string_view view) {     return rjson::value(view.data(), view.size(), the_allocator); } const rjson::value* find(const rjson::value& value, std::string_view name) {     // Although FindMember() has a variant taking a StringRef, it ignores the
    // given length (see https://github.com/Tencent/rapidjson/issues/1649).
    // Luckily, the variant taking a GenericValue doesn't share this bug,
    // and we can create a string GenericValue without copying the string.
    auto member_it = value.FindMember(rjson::value(name.data(), name.size()));     return member_it != value.MemberEnd() ? &member_it->value : nullptr; } rjson::value* find(rjson::value& value, std::string_view name) {     auto member_it = value.FindMember(rjson::value(name.data(), name.size()));     return member_it != value.MemberEnd() ? &member_it->value : nullptr; } bool remove_member(rjson::value& value, std::string_view name) {     // Although RemoveMember() has a variant taking a StringRef, it ignores
    // given length (see https://github.com/Tencent/rapidjson/issues/1649).
    // Luckily, the variant taking a GenericValue doesn't share this bug,
    // and we can create a string GenericValue without copying the string.
    return value.RemoveMember(rjson::value(name.data(), name.size())); } void add_with_string_name(rjson::value& base, std::string_view name, rjson::value&& member) {     base.AddMember(rjson::value(name.data(), name.size(), the_allocator), std::move(member), the_allocator); } void add_with_string_name(rjson::value& base, std::string_view name, rjson::string_ref_type member) {     base.AddMember(rjson::value(name.data(), name.size(), the_allocator), rjson::value(member), the_allocator); } void add(rjson::value& base, rjson::string_ref_type name, rjson::value&& member) {     base.AddMember(name, std::move(member), the_allocator); } void add(rjson::value& base, rjson::string_ref_type name, rjson::string_ref_type member) {     base.AddMember(name, rjson::value(member), the_allocator); } void replace_with_string_name(rjson::value& base, const std::string_view name, rjson::value&& member) {     rjson::value *m = rjson::find(base, name);     if (m) {         *m = std::move(member);     } else {         add_with_string_name(base, name, std::move(member));     } } void push_back(rjson::value& base_array, rjson::value&& item) {     base_array.PushBack(std::move(item), the_allocator); } bool single_value_comp::operator()(const rjson::value& r1, const rjson::value& r2) const {    auto r1_type = r1.GetType();    auto r2_type = r2.GetType();    // null is the smallest type and compares with every other type, nothing is lesser than null
   if (r1_type == rjson::type::kNullType || r2_type == rjson::type::kNullType) {        return r1_type < r2_type;    }    // only null, true, and false are comparable with each other, other types are not compatible
   if (r1_type != r2_type) {        if (r1_type > rjson::type::kTrueType || r2_type > rjson::type::kTrueType) {            throw rjson::error(format("Types are not comparable: {} {}", r1, r2));        }    }    switch (r1_type) {    case rjson::type::kNullType:        // fall-through
   case rjson::type::kFalseType:        // fall-through
   case rjson::type::kTrueType:        return r1_type < r2_type;    case rjson::type::kObjectType:        throw rjson::error("Object type comparison is not supported");    case rjson::type::kArrayType:        throw rjson::error("Array type comparison is not supported");    case rjson::type::kStringType: {        const size_t r1_len = r1.GetStringLength();        const size_t r2_len = r2.GetStringLength();        size_t len = std::min(r1_len, r2_len);        int result = std::strncmp(r1.GetString(), r2.GetString(), len);        return result < 0 || (result == 0 && r1_len < r2_len);    }    case rjson::type::kNumberType: {        if (r1.IsInt() && r2.IsInt()) {            return r1.GetInt() < r2.GetInt();        } else if (r1.IsUint() && r2.IsUint()) {            return r1.GetUint() < r2.GetUint();        } else if (r1.IsInt64() && r2.IsInt64()) {            return r1.GetInt64() < r2.GetInt64();        } else if (r1.IsUint64() && r2.IsUint64()) {            return r1.GetUint64() < r2.GetUint64();        } else {            // it's safe to call GetDouble() on any number type
           return r1.GetDouble() < r2.GetDouble();        }    }    default:        return false;    } } rjson::value from_string_map(const std::map<sstring, sstring>& map) {     rjson::value v = rjson::empty_object();     for (auto& entry : map) {         rjson::add_with_string_name(v, std::string_view(entry.first), rjson::from_string(entry.second));     }     return v; } static inline bool is_control_char(char c) {     return c >= 0 && c <= 0x1F; } static inline bool needs_escaping(const sstring& s) {     return std::any_of(s.begin(), s.end(), [](char c) {return is_control_char(c) || c == '"' || c == '\\';}); } sstring quote_json_string(const sstring& value) {     if (!needs_escaping(value)) {         return format("\"{}\"", value);     }     std::ostringstream oss;     oss << std::hex << std::uppercase << std::setfill('0');     oss.put('"');     for (char c : value) {         switch (c) {         case '"':             oss.put('\\').put('"');             break;         case '\\':             oss.put('\\').put('\\');             break;         case '\b':             oss.put('\\').put('b');             break;         case '\f':             oss.put('\\').put('f');             break;         case '\n':             oss.put('\\').put('n');             break;         case '\r':             oss.put('\\').put('r');             break;         case '\t':             oss.put('\\').put('t');             break;         default:             if (is_control_char(c)) {                 oss.put('\\').put('u') << std::setw(4) << static_cast<int>(c);             } else {                 oss.put(c);             }             break;         }     }     oss.put('"');     return oss.str(); } }
 // end namespace rjson
std::ostream& std::operator<<(std::ostream& os, const rjson::value& v) {     return os << rjson::print(v); }
 namespace utils { std::ostream& operator<<(std::ostream& os, const human_readable_value& val) {     os << val.value;     if (val.suffix) {         os << val.suffix;     }     return os; } static human_readable_value to_human_readable_value(uint64_t value, uint64_t step, uint64_t precision, const std::array<char, 5>& suffixes) {     if (!value) {         return {0, suffixes[0]};     }     uint64_t result = value;     uint64_t remainder = 0;     unsigned i = 0;     // If there is no remainder we go below precision because we don't loose any.
    while (((!remainder && result >= step) || result >= precision)) {         remainder = result % step;         result /= step;         if (i == suffixes.size()) {             break;         } else {             ++i;         }     }     return {uint16_t(remainder < (step / 2) ? result : result + 1), suffixes[i]}; } human_readable_value to_hr_size(uint64_t size) {     const std::array<char, 5> suffixes = {'B', 'K', 'M', 'G', 'T'};     return to_human_readable_value(size, 1024, 8192, suffixes); } }
 // namespace utils
seastar::metrics::histogram to_metrics_summary(const utils::summary_calculator& summary) noexcept {     seastar::metrics::histogram res;     res.buckets.resize(summary.quantiles().size());     res.sample_count = summary.histogram().count();     for (size_t i = 0; i < summary.quantiles().size(); i++) {         res.buckets[i].count = summary.summary()[i];         res.buckets[i].upper_bound = summary.quantiles()[i];     }     return res; }
 bool converting_mutation_partition_applier::is_compatible(const column_definition& new_def, const abstract_type& old_type, column_kind kind) {     return ::is_compatible(new_def.kind, kind) && new_def.type->is_value_compatible_with(old_type); }
 atomic_cell converting_mutation_partition_applier::upgrade_cell(const abstract_type& new_type, const abstract_type& old_type, atomic_cell_view cell,                                 atomic_cell::collection_member cm) {     if (cell.is_live() && !old_type.is_counter()) {         if (cell.is_live_and_has_ttl()) {             return atomic_cell::make_live(new_type, cell.timestamp(), cell.value(), cell.expiry(), cell.ttl(), cm);         }         return atomic_cell::make_live(new_type, cell.timestamp(), cell.value(), cm);     } else {         return atomic_cell(new_type, cell);     } }
 void converting_mutation_partition_applier::accept_cell(row& dst, column_kind kind, const column_definition& new_def, const abstract_type& old_type, atomic_cell_view cell) {     if (!is_compatible(new_def, old_type, kind) || cell.timestamp() <= new_def.dropped_at()) {         return;     }     dst.apply(new_def, upgrade_cell(*new_def.type, old_type, cell)); }
 void converting_mutation_partition_applier::accept_cell(row& dst, column_kind kind, const column_definition& new_def, const abstract_type& old_type, collection_mutation_view cell) {     if (!is_compatible(new_def, old_type, kind)) {         return;     }   cell.with_deserialized(old_type, [&] (collection_mutation_view_description old_view) {     collection_mutation_description new_view;     if (old_view.tomb.timestamp > new_def.dropped_at()) {         new_view.tomb = old_view.tomb;     }     visit(old_type, make_visitor(         [&] (const collection_type_impl& old_ctype) {             assert(new_def.type->is_collection()); // because is_compatible
            auto& new_ctype = static_cast<const collection_type_impl&>(*new_def.type);             auto& new_value_type = *new_ctype.value_comparator();             auto& old_value_type = *old_ctype.value_comparator();             for (auto& c : old_view.cells) {                 if (c.second.timestamp() > new_def.dropped_at()) {                     new_view.cells.emplace_back(c.first, upgrade_cell(                             new_value_type, old_value_type, c.second, atomic_cell::collection_member::yes));                 }             }         },         [&] (const user_type_impl& old_utype) {             assert(new_def.type->is_user_type()); // because is_compatible
            auto& new_utype = static_cast<const user_type_impl&>(*new_def.type);             for (auto& c : old_view.cells) {                 if (c.second.timestamp() > new_def.dropped_at()) {                     auto idx = deserialize_field_index(c.first);                     assert(idx < new_utype.size() && idx < old_utype.size());                     new_view.cells.emplace_back(c.first, upgrade_cell(                             *new_utype.type(idx), *old_utype.type(idx), c.second, atomic_cell::collection_member::yes));                 }             }         },         [&] (const abstract_type& o) {             throw std::runtime_error(format("not a multi-cell type: {}", o.name()));         }     ));     if (new_view.tomb || !new_view.cells.empty()) {         dst.apply(new_def, new_view.serialize(*new_def.type));     }   }); }
 converting_mutation_partition_applier::converting_mutation_partition_applier(         const column_mapping& visited_column_mapping,         const schema& target_schema,         mutation_partition& target)     : _p_schema(target_schema)     , _p(target)     , _visited_column_mapping(visited_column_mapping) { }
 void converting_mutation_partition_applier::accept_partition_tombstone(tombstone t) {     _p.apply(t); }
 void converting_mutation_partition_applier::accept_static_cell(column_id id, atomic_cell cell) {     return accept_static_cell(id, atomic_cell_view(cell)); }
 void converting_mutation_partition_applier::accept_static_cell(column_id id, atomic_cell_view cell) {     const column_mapping_entry& col = _visited_column_mapping.static_column_at(id);     const column_definition* def = _p_schema.get_column_definition(col.name());     if (def) {         accept_cell(_p._static_row.maybe_create(), column_kind::static_column, *def, *col.type(), cell);     } }
 void converting_mutation_partition_applier::accept_static_cell(column_id id, collection_mutation_view collection) {     const column_mapping_entry& col = _visited_column_mapping.static_column_at(id);     const column_definition* def = _p_schema.get_column_definition(col.name());     if (def) {         accept_cell(_p._static_row.maybe_create(), column_kind::static_column, *def, *col.type(), collection);     } }
 void converting_mutation_partition_applier::accept_row_tombstone(const range_tombstone& rt) {     _p.apply_row_tombstone(_p_schema, rt); }
 void converting_mutation_partition_applier::accept_row(position_in_partition_view key, const row_tombstone& deleted_at, const row_marker& rm, is_dummy dummy, is_continuous continuous) {     deletable_row& r = _p.clustered_row(_p_schema, key, dummy, continuous);     r.apply(rm);     r.apply(deleted_at);     _current_row = &r; }
 void converting_mutation_partition_applier::accept_row_cell(column_id id, atomic_cell cell) {     return accept_row_cell(id, atomic_cell_view(cell)); }
 void converting_mutation_partition_applier::accept_row_cell(column_id id, atomic_cell_view cell) {     const column_mapping_entry& col = _visited_column_mapping.regular_column_at(id);     const column_definition* def = _p_schema.get_column_definition(col.name());     if (def) {         accept_cell(_current_row->cells(), column_kind::regular_column, *def, *col.type(), cell);     } }
 void converting_mutation_partition_applier::accept_row_cell(column_id id, collection_mutation_view collection) {     const column_mapping_entry& col = _visited_column_mapping.regular_column_at(id);     const column_definition* def = _p_schema.get_column_definition(col.name());     if (def) {         accept_cell(_current_row->cells(), column_kind::regular_column, *def, *col.type(), collection);     } }
 void converting_mutation_partition_applier::append_cell(row& dst, column_kind kind, const column_definition& new_def, const column_definition& old_def, const atomic_cell_or_collection& cell) {     if (new_def.is_atomic()) {         accept_cell(dst, kind, new_def, *old_def.type, cell.as_atomic_cell(old_def));     } else {         accept_cell(dst, kind, new_def, *old_def.type, cell.as_collection_mutation());     } }
 reconcilable_result::~reconcilable_result() {}
 reconcilable_result::reconcilable_result()     : _row_count_low_bits(0)     , _row_count_high_bits(0) { }
 reconcilable_result::reconcilable_result(uint32_t row_count_low_bits, utils::chunked_vector<partition> p, query::short_read short_read,                                          uint32_t row_count_high_bits, query::result_memory_tracker memory_tracker)     : _row_count_low_bits(row_count_low_bits)     , _short_read(short_read)     , _memory_tracker(std::move(memory_tracker))     , _partitions(std::move(p))     , _row_count_high_bits(row_count_high_bits) { }
 reconcilable_result::reconcilable_result(uint64_t row_count, utils::chunked_vector<partition> p, query::short_read short_read,                                          query::result_memory_tracker memory_tracker)     : reconcilable_result(static_cast<uint32_t>(row_count), std::move(p), short_read, static_cast<uint32_t>(row_count >> 32), std::move(memory_tracker)) { }
 const utils::chunked_vector<partition>& reconcilable_result::partitions() const {     return _partitions; }
 utils::chunked_vector<partition>& reconcilable_result::partitions() {     return _partitions; }
 bool reconcilable_result::operator==(const reconcilable_result& other) const {     return boost::equal(_partitions, other._partitions); }
 std::ostream& operator<<(std::ostream& out, const reconcilable_result::printer& pr) {     out << "{rows=" << pr.self.row_count() << ", short_read="         << pr.self.is_short_read() << ", [";     bool first = true;     for (const partition& p : pr.self.partitions()) {         if (!first) {             out << ", ";         }         first = false;         out << "{rows=" << p.row_count() << ", ";         out << p._m.pretty_printer(pr.schema);         out << "}";     }     out << "]}";     return out; }
 reconcilable_result::printer reconcilable_result::pretty_printer(schema_ptr s) const {     return { *this, std::move(s) }; }
 logging::logger klog("keys");
 const legacy_compound_view<partition_key_view::c_type> partition_key_view::legacy_form(const schema& s) const {     return { *get_compound_type(s), _bytes }; }
 std::strong_ordering partition_key_view::legacy_tri_compare(const schema& s, partition_key_view o) const {     auto cmp = legacy_compound_view<c_type>::tri_comparator(*get_compound_type(s));     return cmp(this->representation(), o.representation()); }
 std::strong_ordering partition_key_view::ring_order_tri_compare(const schema& s, partition_key_view k2) const {     auto t1 = dht::get_token(s, *this);     auto t2 = dht::get_token(s, k2);     if (t1 != t2) {         return t1 < t2 ? std::strong_ordering::less : std::strong_ordering::greater;     }     return legacy_tri_compare(s, k2); }
 partition_key partition_key::from_nodetool_style_string(const schema_ptr s, const sstring& key) {     std::vector<sstring> vec;     boost::split(vec, key, boost::is_any_of(":"));     auto it = std::begin(vec);     if (vec.size() != s->partition_key_type()->types().size()) {         throw std::invalid_argument("partition key '" + key + "' has mismatch number of components");     }     std::vector<bytes> r;     r.reserve(vec.size());     for (auto t : s->partition_key_type()->types()) {         r.emplace_back(t->from_string(*it++));     }     return partition_key::from_range(std::move(r)); }
    int32_t weight(bound_kind k) {     switch (k) {     case bound_kind::excl_end:         return -2;     case bound_kind::incl_start:         return -1;     case bound_kind::incl_end:         return 1;     case bound_kind::excl_start:         return 2;     }     abort(); }
 const thread_local clustering_key_prefix bound_view::_empty_prefix = clustering_key::make_empty();
   void counter_cell_builder::do_sort_and_remove_duplicates() {     boost::range::sort(_shards, [] (auto& a, auto& b) { return a.id() < b.id(); });     std::vector<counter_shard> new_shards;     new_shards.reserve(_shards.size());     for (auto& cs : _shards) {         if (new_shards.empty() || new_shards.back().id() != cs.id()) {             new_shards.emplace_back(cs);         } else {             new_shards.back().apply(cs);         }     }     _shards = std::move(new_shards);     _sorted = true; }
 static bool apply_in_place(const column_definition& cdef, atomic_cell_mutable_view dst, atomic_cell_mutable_view src) {     auto dst_ccmv = counter_cell_mutable_view(dst);     auto src_ccmv = counter_cell_mutable_view(src);     auto dst_shards = dst_ccmv.shards();     auto src_shards = src_ccmv.shards();     auto dst_it = dst_shards.begin();     auto src_it = src_shards.begin();     while (src_it != src_shards.end()) {         while (dst_it != dst_shards.end() && dst_it->id() < src_it->id()) {             ++dst_it;         }         if (dst_it == dst_shards.end() || dst_it->id() != src_it->id()) {             // Fast-path failed. Revert and fall back to the slow path.
            if (dst_it == dst_shards.end()) {                 --dst_it;             }             while (src_it != src_shards.begin()) {                 --src_it;                 while (dst_it->id() != src_it->id()) {                     --dst_it;                 }                 src_it->swap_value_and_clock(*dst_it);             }             return false;         }         if (dst_it->logical_clock() < src_it->logical_clock()) {             dst_it->swap_value_and_clock(*src_it);         } else {             src_it->set_value_and_clock(*dst_it);         }         ++src_it;     }     auto dst_ts = dst_ccmv.timestamp();     auto src_ts = src_ccmv.timestamp();     dst_ccmv.set_timestamp(std::max(dst_ts, src_ts));     src_ccmv.set_timestamp(dst_ts);     return true; }
 void counter_cell_view::apply(const column_definition& cdef, atomic_cell_or_collection& dst, atomic_cell_or_collection& src) {     auto dst_ac = dst.as_atomic_cell(cdef);     auto src_ac = src.as_atomic_cell(cdef);     if (!dst_ac.is_live() || !src_ac.is_live()) {         if (dst_ac.is_live() || (!src_ac.is_live() && compare_atomic_cell_for_merge(dst_ac, src_ac) < 0)) {             std::swap(dst, src);         }         return;     }     if (dst_ac.is_counter_update() && src_ac.is_counter_update()) {         auto src_v = src_ac.counter_update_value();         auto dst_v = dst_ac.counter_update_value();         dst = atomic_cell::make_live_counter_update(std::max(dst_ac.timestamp(), src_ac.timestamp()),                                                     src_v + dst_v);         return;     }     assert(!dst_ac.is_counter_update());     assert(!src_ac.is_counter_update());     auto src_ccv = counter_cell_view(src_ac);     auto dst_ccv = counter_cell_view(dst_ac);     if (dst_ccv.shard_count() >= src_ccv.shard_count()) {         auto dst_amc = dst.as_mutable_atomic_cell(cdef);         auto src_amc = src.as_mutable_atomic_cell(cdef);         if (apply_in_place(cdef, dst_amc, src_amc)) {             return;         }     }     auto dst_shards = dst_ccv.shards();     auto src_shards = src_ccv.shards();     counter_cell_builder result;     combine(dst_shards.begin(), dst_shards.end(), src_shards.begin(), src_shards.end(),             result.inserter(), counter_shard_view::less_compare_by_id(), [] (auto& x, auto& y) {                 return x.logical_clock() < y.logical_clock() ? y : x;             });     auto cell = result.build(std::max(dst_ac.timestamp(), src_ac.timestamp()));     src = std::exchange(dst, atomic_cell_or_collection(std::move(cell))); }
 std::optional<atomic_cell> counter_cell_view::difference(atomic_cell_view a, atomic_cell_view b) {     assert(!a.is_counter_update());     assert(!b.is_counter_update());     if (!b.is_live() || !a.is_live()) {         if (b.is_live() || (!a.is_live() && compare_atomic_cell_for_merge(b, a) < 0)) {             return atomic_cell(*counter_type, a);         }         return { };     }     auto a_ccv = counter_cell_view(a);     auto b_ccv = counter_cell_view(b);     auto a_shards = a_ccv.shards();     auto b_shards = b_ccv.shards();     auto a_it = a_shards.begin();     auto a_end = a_shards.end();     auto b_it = b_shards.begin();     auto b_end = b_shards.end();     counter_cell_builder result;     while (a_it != a_end) {         while (b_it != b_end && (*b_it).id() < (*a_it).id()) {             ++b_it;         }         if (b_it == b_end || (*a_it).id() != (*b_it).id() || (*a_it).logical_clock() > (*b_it).logical_clock()) {             result.add_shard(counter_shard(*a_it));         }         ++a_it;     }     std::optional<atomic_cell> diff;     if (!result.empty()) {         diff = result.build(std::max(a.timestamp(), b.timestamp()));     } else if (a.timestamp() > b.timestamp()) {         diff = atomic_cell::make_live(*counter_type, a.timestamp(), bytes_view());     }     return diff; }
  namespace runtime { static std::chrono::steady_clock::time_point boot_time;    }
 namespace utils { namespace murmur_hash {   static inline uint64_t getblock(bytes_view key, uint32_t index) {     uint32_t i_8 = index << 3;     auto p = reinterpret_cast<const uint8_t*>(key.data() + i_8);     return uint64_t(p[0])             | (uint64_t(p[1]) << 8)             | (uint64_t(p[2]) << 16)             | (uint64_t(p[3]) << 24)             | (uint64_t(p[4]) << 32)             | (uint64_t(p[5]) << 40)             | (uint64_t(p[6]) << 48)             | (uint64_t(p[7]) << 56); } void hash3_x64_128(bytes_view key, uint64_t seed, std::array<uint64_t,2> &result) {     uint32_t length = key.size();     const uint32_t nblocks = length >> 4; // Process as 128-bit blocks.
    uint64_t h1 = seed;     uint64_t h2 = seed;     uint64_t c1 = 0x87c37b91114253d5L;     uint64_t c2 = 0x4cf5ad432745937fL;     //----------
    // body
    for(uint32_t i = 0; i < nblocks; i++)     {         uint64_t k1 = getblock(key, i*2+0);         uint64_t k2 = getblock(key, i*2+1);         k1 *= c1; k1 = rotl64(k1,31); k1 *= c2; h1 ^= k1;         h1 = rotl64(h1,27); h1 += h2; h1 = h1*5+0x52dce729;         k2 *= c2; k2  = rotl64(k2,33); k2 *= c1; h2 ^= k2;         h2 = rotl64(h2,31); h2 += h1; h2 = h2*5+0x38495ab5;     }     //----------
    // tail
    // Advance offset to the unprocessed tail of the data.
    key.remove_prefix(nblocks * 16);     uint64_t k1 = 0;     uint64_t k2 = 0;     switch (length & 15)     {     case 15: k2 ^= ((uint64_t) key[14]) << 48;     case 14: k2 ^= ((uint64_t) key[13]) << 40;     case 13: k2 ^= ((uint64_t) key[12]) << 32;     case 12: k2 ^= ((uint64_t) key[11]) << 24;     case 11: k2 ^= ((uint64_t) key[10]) << 16;     case 10: k2 ^= ((uint64_t) key[9]) << 8;     case  9: k2 ^= ((uint64_t) key[8]) << 0;         k2 *= c2; k2  = rotl64(k2,33); k2 *= c1; h2 ^= k2;     case  8: k1 ^= ((uint64_t) key[7]) << 56;     case  7: k1 ^= ((uint64_t) key[6]) << 48;     case  6: k1 ^= ((uint64_t) key[5]) << 40;     case  5: k1 ^= ((uint64_t) key[4]) << 32;     case  4: k1 ^= ((uint64_t) key[3]) << 24;     case  3: k1 ^= ((uint64_t) key[2]) << 16;     case  2: k1 ^= ((uint64_t) key[1]) << 8;     case  1: k1 ^= ((uint64_t) key[0]);         k1 *= c1; k1  = rotl64(k1,31); k1 *= c2; h1 ^= k1;     };     //----------
    // finalization
    h1 ^= length; h2 ^= length;     h1 += h2;     h2 += h1;     h1 = fmix(h1);     h2 = fmix(h2);     h1 += h2;     h2 += h1;     result[0] = h1;     result[1] = h2; } } // namespace murmur_hash
}
 // namespace utils
namespace utils { UUID make_random_uuid() noexcept {     static thread_local std::mt19937_64 engine(std::random_device().operator()());     static thread_local std::uniform_int_distribution<uint64_t> dist;     uint64_t msb, lsb;     msb = dist(engine);     lsb = dist(engine);     msb &= ~uint64_t(0x0f << 12);     msb |= 0x4 << 12; // version 4
    lsb &= ~(uint64_t(0x3) << 62);     lsb |= uint64_t(0x2) << 62; // IETF variant
    return UUID(msb, lsb); }  UUID::UUID(sstring_view uuid) {     sstring uuid_string(uuid.begin(), uuid.end());     boost::erase_all(uuid_string, "-");     auto size = uuid_string.size() / 2;     if (size != 16) {         throw marshal_exception(format("UUID string size mismatch: '{}'", uuid));     }     sstring most = sstring(uuid_string.begin(), uuid_string.begin() + size);     sstring least = sstring(uuid_string.begin() + size, uuid_string.end());     int base = 16;     try {         std::size_t pos = 0;         this->most_sig_bits = std::stoull(most, &pos, base);         if (pos != most.size()) {             throw std::invalid_argument("");         }         this->least_sig_bits = std::stoull(least, &pos, base);         if (pos != least.size()) {             throw std::invalid_argument("");         }     } catch (const std::logic_error&) {         throw marshal_exception(format("invalid UUID: '{}'", uuid));     } } }
 // Clang or boost have a problem navigating the enable_if maze
// that is cpp_int's constructor. It ends up treating the
// string_view as binary and "0" ends up 48.
// Work around by casting to string.
using string_view_workaround = std::string;
  big_decimal::big_decimal() : big_decimal(0, 0) {}
 big_decimal::big_decimal(int32_t scale, boost::multiprecision::cpp_int unscaled_value)     : _scale(scale), _unscaled_value(std::move(unscaled_value)) {}
 big_decimal::big_decimal(sstring_view text) {     size_t e_pos = text.find_first_of("eE");     std::string_view base = text.substr(0, e_pos);     std::string_view exponent;     if (e_pos != std::string_view::npos) {         exponent = text.substr(e_pos + 1);         if (exponent.empty()) {             throw marshal_exception(format("big_decimal - incorrect empty exponent: {}", text));         }     }     size_t dot_pos = base.find_first_of(".");     std::string integer_str(base.substr(0, dot_pos));     std::string_view fraction;     if (dot_pos != std::string_view::npos) {         fraction = base.substr(dot_pos + 1);         integer_str.append(fraction);     }     std::string_view integer(integer_str);     const bool negative = !integer.empty() && integer.front() == '-';     integer.remove_prefix(negative || (!integer.empty() && integer.front() == '+'));     if (integer.empty()) {         throw marshal_exception(format("big_decimal - both integer and fraction are empty"));     } else if (!::isdigit(integer.front())) {         throw marshal_exception(format("big_decimal - incorrect integer: {}", text));     }     integer.remove_prefix(std::min(integer.find_first_not_of("0"), integer.size() - 1));     try {         _unscaled_value = boost::multiprecision::cpp_int(string_view_workaround(integer));     } catch (...) {         throw marshal_exception(format("big_decimal - failed to parse integer value: {}", integer));     }     if (negative) {         _unscaled_value *= -1;     }     try {         _scale = exponent.empty() ? 0 : -boost::lexical_cast<int32_t>(exponent);     } catch (...) {         throw marshal_exception(format("big_decimal - failed to parse exponent: {}", exponent));     }     _scale += fraction.size(); }
 boost::multiprecision::cpp_rational big_decimal::as_rational() const {     boost::multiprecision::cpp_int ten(10);     auto unscaled_value = static_cast<const boost::multiprecision::cpp_int&>(_unscaled_value);     boost::multiprecision::cpp_rational r = unscaled_value;     int32_t abs_scale = std::abs(_scale);     auto pow = boost::multiprecision::pow(ten, abs_scale);     if (_scale < 0) {         r *= pow;     } else {         r /= pow;     }     return r; }
 sstring big_decimal::to_string() const {     if (!_unscaled_value) {         return "0";     }     boost::multiprecision::cpp_int num = boost::multiprecision::abs(_unscaled_value);     auto str = num.str();     if (_scale < 0) {         for (int i = 0; i > _scale; i--) {             str.push_back('0');         }     } else if (_scale > 0) {         if (str.size() > unsigned(_scale)) {             str.insert(str.size() - _scale, 1, '.');         } else {             std::string nstr = "0.";             nstr.append(_scale - str.size(), '0');             nstr.append(str);             str = std::move(nstr);         }         while (str.back() == '0') {             str.pop_back();         }         if (str.back() == '.') {             str.pop_back();         }     }     if (_unscaled_value < 0) {         str.insert(0, 1, '-');     }     return str; }
 std::strong_ordering big_decimal::operator<=>(const big_decimal& other) const {     auto max_scale = std::max(_scale, other._scale);     boost::multiprecision::cpp_int rescale(10);     boost::multiprecision::cpp_int x = _unscaled_value * boost::multiprecision::pow(rescale, max_scale - _scale);     boost::multiprecision::cpp_int y = other._unscaled_value * boost::multiprecision::pow(rescale, max_scale - other._scale);     return x.compare(y) <=> 0; }
 big_decimal& big_decimal::operator+=(const big_decimal& other) {     if (_scale == other._scale) {         _unscaled_value += other._unscaled_value;     } else {         boost::multiprecision::cpp_int rescale(10);         auto max_scale = std::max(_scale, other._scale);         boost::multiprecision::cpp_int u = _unscaled_value * boost::multiprecision::pow(rescale,  max_scale - _scale);         boost::multiprecision::cpp_int v = other._unscaled_value * boost::multiprecision::pow(rescale, max_scale - other._scale);         _unscaled_value = u + v;         _scale = max_scale;     }     return *this; }
 big_decimal& big_decimal::operator-=(const big_decimal& other) {     if (_scale == other._scale) {         _unscaled_value -= other._unscaled_value;     } else {         boost::multiprecision::cpp_int rescale(10);         auto max_scale = std::max(_scale, other._scale);         boost::multiprecision::cpp_int u = _unscaled_value * boost::multiprecision::pow(rescale,  max_scale - _scale);         boost::multiprecision::cpp_int v = other._unscaled_value * boost::multiprecision::pow(rescale, max_scale - other._scale);         _unscaled_value = u - v;         _scale = max_scale;     }     return *this; }
 big_decimal big_decimal::operator+(const big_decimal& other) const {     big_decimal ret(*this);     ret += other;     return ret; }
 big_decimal big_decimal::operator-(const big_decimal& other) const {     big_decimal ret(*this);     ret -= other;     return ret; }
 big_decimal big_decimal::div(const ::uint64_t y, const rounding_mode mode) const {     if (mode != rounding_mode::HALF_EVEN) {         assert(0);     }     // Implementation of Division with Half to Even (aka Bankers) Rounding
    const boost::multiprecision::cpp_int sign = _unscaled_value >= 0 ? +1 : -1;     const boost::multiprecision::cpp_int a = sign * _unscaled_value;     // cpp_int uses lazy evaluation and for older versions of boost and some
    //   versions of gcc, expression templates have problem to implicitly
    //   convert to cpp_int, so we force the conversion explicitly before cpp_int
    //   is converted to uint64_t.
    const uint64_t r = boost::multiprecision::cpp_int{a % y}.convert_to<uint64_t>();     boost::multiprecision::cpp_int q = a / y;     if (2*r < y) {     } else if (2*r > y) {         q += 1;     } else if (q % 2 == 1) {         q += 1;     }     return big_decimal(_scale, sign * q); }
 static logging::logger tlogger("types");
 bytes_view_opt read_collection_value(bytes_view& in);
 void on_types_internal_error(std::exception_ptr ex) {     on_internal_error(tlogger, std::move(ex)); }
 template<typename T> requires requires {         typename T::duration;         requires std::same_as<typename T::duration, std::chrono::milliseconds>;     }
 sstring time_point_to_string(const T& tp) {     auto count = tp.time_since_epoch().count();     auto d = std::div(int64_t(count), int64_t(1000));     std::time_t seconds = d.quot;     std::tm tm;     if (!gmtime_r(&seconds, &tm)) {         return fmt::format("{} milliseconds (out of range)", count);     }     auto to_string = [] (const std::tm& tm) {         auto year_digits = tm.tm_year >= -1900 ? 4 : 5;         return fmt::format("{:-0{}d}-{:02d}-{:02d}T{:02d}:{:02d}:{:02d}",                 tm.tm_year + 1900, year_digits, tm.tm_mon + 1, tm.tm_mday,                 tm.tm_hour, tm.tm_min, tm.tm_sec);     };     auto millis = d.rem;     if (!millis) {         return fmt::format("{}", to_string(tm));     }     // adjust seconds for time points earlier than posix epoch
    // to keep the fractional millis positive
    if (millis < 0) {         millis += 1000;         seconds--;         gmtime_r(&seconds, &tm);     }     auto micros = millis * 1000;     return fmt::format("{}.{:06d}", to_string(tm), micros); }
    sstring inet_addr_type_impl::to_sstring(const seastar::net::inet_address& addr) {     std::ostringstream out;     out << addr;     return out.str(); }
 static const char* byte_type_name      = "org.apache.cassandra.db.marshal.ByteType";
 static const char* short_type_name     = "org.apache.cassandra.db.marshal.ShortType";
 static const char* int32_type_name     = "org.apache.cassandra.db.marshal.Int32Type";
 static const char* long_type_name      = "org.apache.cassandra.db.marshal.LongType";
 static const char* ascii_type_name     = "org.apache.cassandra.db.marshal.AsciiType";
 static const char* utf8_type_name      = "org.apache.cassandra.db.marshal.UTF8Type";
 static const char* bytes_type_name     = "org.apache.cassandra.db.marshal.BytesType";
 static const char* boolean_type_name   = "org.apache.cassandra.db.marshal.BooleanType";
 static const char* timeuuid_type_name  = "org.apache.cassandra.db.marshal.TimeUUIDType";
 static const char* timestamp_type_name = "org.apache.cassandra.db.marshal.TimestampType";
 static const char* date_type_name      = "org.apache.cassandra.db.marshal.DateType";
 static const char* simple_date_type_name = "org.apache.cassandra.db.marshal.SimpleDateType";
 static const char* time_type_name      = "org.apache.cassandra.db.marshal.TimeType";
 static const char* uuid_type_name      = "org.apache.cassandra.db.marshal.UUIDType";
 static const char* inet_addr_type_name = "org.apache.cassandra.db.marshal.InetAddressType";
 static const char* double_type_name    = "org.apache.cassandra.db.marshal.DoubleType";
 static const char* float_type_name     = "org.apache.cassandra.db.marshal.FloatType";
 static const char* varint_type_name    = "org.apache.cassandra.db.marshal.IntegerType";
 static const char* decimal_type_name    = "org.apache.cassandra.db.marshal.DecimalType";
 static const char* counter_type_name   = "org.apache.cassandra.db.marshal.CounterColumnType";
 static const char* duration_type_name = "org.apache.cassandra.db.marshal.DurationType";
 static const char* empty_type_name     = "org.apache.cassandra.db.marshal.EmptyType";
 template<typename T> struct simple_type_traits {     static constexpr size_t serialized_size = sizeof(T);     static T read_nonempty(managed_bytes_view v) {         return read_simple_exactly<T>(v);     } };
 template<> struct simple_type_traits<bool> {     static constexpr size_t serialized_size = 1;     static bool read_nonempty(managed_bytes_view v) {         return read_simple_exactly<int8_t>(v) != 0;     } };
 template<> struct simple_type_traits<db_clock::time_point> {     static constexpr size_t serialized_size = sizeof(uint64_t);     static db_clock::time_point read_nonempty(managed_bytes_view v) {         return db_clock::time_point(db_clock::duration(read_simple_exactly<int64_t>(v)));     } };
 template <typename T> simple_type_impl<T>::simple_type_impl(abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed)     : concrete_type<T>(k, std::move(name), std::move(value_length_if_fixed)) {}
 template <typename T> integer_type_impl<T>::integer_type_impl(         abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed)     : simple_type_impl<T>(k, name, std::move(value_length_if_fixed)) {}
 template <typename T> static bytes decompose_value(T v) {     bytes b(bytes::initialized_later(), sizeof(v));     write_unaligned<T>(b.begin(), net::hton(v));     return b; }
 template <typename T> static T parse_int(const integer_type_impl<T>& t, sstring_view s) {     try {         auto value64 = boost::lexical_cast<int64_t>(s.begin(), s.size());         auto value = static_cast<T>(value64);         if (value != value64) {             throw marshal_exception(format("Value out of range for type {}: '{}'", t.name(), s));         }         return static_cast<T>(value);     } catch (const boost::bad_lexical_cast& e) {         throw marshal_exception(format("Invalid number format '{}'", s));     } }
 // Note that although byte_type is of a fixed size,
// Cassandra (erroneously) treats it as a variable-size
// so we have to pass disengaged optional for the value size
byte_type_impl::byte_type_impl() : integer_type_impl{kind::byte, byte_type_name, {}}
 {}
 short_type_impl::short_type_impl() : integer_type_impl{kind::short_kind, short_type_name, {}}
 {}
 int32_type_impl::int32_type_impl() : integer_type_impl{kind::int32, int32_type_name, 4}
 {}
 long_type_impl::long_type_impl() : integer_type_impl{kind::long_kind, long_type_name, 8}
 {}
 string_type_impl::string_type_impl(kind k, sstring name)     : concrete_type(k, name, {}
) {}
 ascii_type_impl::ascii_type_impl() : string_type_impl(kind::ascii, ascii_type_name) {}
 utf8_type_impl::utf8_type_impl() : string_type_impl(kind::utf8, utf8_type_name) {}
 bytes_type_impl::bytes_type_impl()     : concrete_type(kind::bytes, bytes_type_name, {}
) {}
 boolean_type_impl::boolean_type_impl() : simple_type_impl<bool>(kind::boolean, boolean_type_name, 1) {}
 date_type_impl::date_type_impl() : concrete_type(kind::date, date_type_name, 8) {}
 timeuuid_type_impl::timeuuid_type_impl()     : concrete_type<utils::UUID>(               kind::timeuuid, timeuuid_type_name, 16) {}
 timestamp_type_impl::timestamp_type_impl() : simple_type_impl(kind::timestamp, timestamp_type_name, 8) {}
 static boost::posix_time::ptime get_time(const boost::smatch& sm) {     // Unfortunately boost::date_time  parsers are more strict with regards
    // to the expected date format than we need to be.
    auto year = boost::lexical_cast<int>(sm[1]);     auto month = boost::lexical_cast<int>(sm[2]);     auto day = boost::lexical_cast<int>(sm[3]);     boost::gregorian::date date(year, month, day);     auto hour = sm[5].length() ? boost::lexical_cast<int>(sm[5]) : 0;     auto minute = sm[6].length() ? boost::lexical_cast<int>(sm[6]) : 0;     auto second = sm[8].length() ? boost::lexical_cast<int>(sm[8]) : 0;     boost::posix_time::time_duration time(hour, minute, second);     if (sm[10].length()) {         static constexpr auto milliseconds_string_length = 3;         auto length = sm[10].length();         if (length > milliseconds_string_length) {             throw marshal_exception(format("Milliseconds length exceeds expected ({:d})", length));         }         auto value = boost::lexical_cast<int>(sm[10]);         while (length < milliseconds_string_length) {             value *= 10;             length++;         }         time += boost::posix_time::milliseconds(value);     }     return boost::posix_time::ptime(date, time); }
 static boost::posix_time::time_duration get_utc_offset(const std::string& s) {     static constexpr const char* formats[] = {             "%H:%M",             "%H%M",     };     for (auto&& f : formats) {         auto tif = new boost::posix_time::time_input_facet(f);         std::istringstream ss(s);         ss.imbue(std::locale(ss.getloc(), tif));         auto sign = ss.get();         boost::posix_time::ptime p;         ss >> p;         if (ss.good() && ss.peek() == std::istringstream::traits_type::eof()) {             return p.time_of_day() * (sign == '-' ? -1 : 1);         }     }     throw marshal_exception("Cannot get UTC offset for a timestamp"); }
 int64_t timestamp_from_string(sstring_view s) {     try {         std::string str;         str.resize(s.size());         std::transform(s.begin(), s.end(), str.begin(), ::tolower);         if (str == "now") {             return db_clock::now().time_since_epoch().count();         }         char* end;         auto v = std::strtoll(s.begin(), &end, 10);         if (end == s.begin() + s.size()) {             return v;         }         static const boost::regex date_re("^(\\d{4})-(\\d+)-(\\d+)([ tT](\\d+):(\\d+)(:(\\d+)(\\.(\\d+))?)?)?");         boost::smatch dsm;         if (!boost::regex_search(str, dsm, date_re)) {             throw marshal_exception(format("Unable to parse timestamp from '{}'", str));         }         auto t = get_time(dsm);         auto tz = dsm.suffix().str();         static const boost::regex tz_re("([\\+-]\\d{2}:?(\\d{2})?)");         boost::smatch tsm;         if (boost::regex_match(tz, tsm, tz_re)) {             t -= get_utc_offset(tsm.str());         } else if (tz.empty()) {             typedef boost::date_time::c_local_adjustor<boost::posix_time::ptime> local_tz;             // local_tz::local_to_utc(), where are you?
            auto t1 = local_tz::utc_to_local(t);             auto tz_offset = t1 - t;             auto t2 = local_tz::utc_to_local(t - tz_offset);             auto dst_offset = t2 - t;             t -= tz_offset + dst_offset;         } else if (tz != "z") {             throw marshal_exception(format("Unable to parse timezone '{}'", tz));         }         return (t - boost::posix_time::from_time_t(0)).total_milliseconds();     } catch (const marshal_exception& me) {         throw marshal_exception(format("unable to parse date '{}': {}", s, me.what()));     } catch (...) {         throw marshal_exception(format("unable to parse date '{}': {}", s, std::current_exception()));     } }
 db_clock::time_point timestamp_type_impl::from_sstring(sstring_view s) {     return db_clock::time_point(db_clock::duration(timestamp_from_string(s))); }
 simple_date_type_impl::simple_date_type_impl() : simple_type_impl{kind::simple_date, simple_date_type_name, {}}
 {}
 static date::year_month_day get_simple_date_time(const boost::smatch& sm) {     auto year = boost::lexical_cast<long>(sm[1]);     auto month = boost::lexical_cast<unsigned>(sm[2]);     auto day = boost::lexical_cast<unsigned>(sm[3]);     return date::year_month_day{date::year{year}, date::month{month}, date::day{day}}; }
 static uint32_t serialize(const std::string& input, int64_t days) {     if (days < std::numeric_limits<int32_t>::min()) {         throw marshal_exception(format("Input date {} is less than min supported date -5877641-06-23", input));     }     if (days > std::numeric_limits<int32_t>::max()) {         throw marshal_exception(format("Input date {} is greater than max supported date 5881580-07-11", input));     }     days += 1UL << 31;     return static_cast<uint32_t>(days); }
 uint32_t simple_date_type_impl::from_sstring(sstring_view s) {     char* end;     auto v = std::strtoll(s.begin(), &end, 10);     if (end == s.begin() + s.size()) {         return v;     }     auto str = std::string(s); // FIXME: this copy probably can be avoided
    static const boost::regex date_re("^(-?\\d+)-(\\d+)-(\\d+)");     boost::smatch dsm;     if (!boost::regex_match(str, dsm, date_re)) {         throw marshal_exception(format("Unable to coerce '{}' to a formatted date (long)", str));     }     auto t = get_simple_date_time(dsm);     return serialize(str, date::local_days(t).time_since_epoch().count()); }
 time_type_impl::time_type_impl() : simple_type_impl{kind::time, time_type_name, {}}
 {}
 int64_t time_type_impl::from_sstring(sstring_view s) {     static auto format_error = "Timestamp format must be hh:mm:ss[.fffffffff]";     auto hours_end = s.find(':');     if (hours_end == std::string::npos) {         throw marshal_exception(format_error);     }     int64_t hours = std::stol(sstring(s.substr(0, hours_end)));     if (hours < 0 || hours >= 24) {         throw marshal_exception(format("Hour out of bounds ({:d}).", hours));     }     auto minutes_end = s.find(':', hours_end + 1);     if (minutes_end == std::string::npos) {         throw marshal_exception(format_error);     }     int64_t minutes = std::stol(sstring(s.substr(hours_end + 1, hours_end - minutes_end)));     if (minutes < 0 || minutes >= 60) {         throw marshal_exception(format("Minute out of bounds ({:d}).", minutes));     }     auto seconds_end = s.find('.', minutes_end + 1);     if (seconds_end == std::string::npos) {         seconds_end = s.length();     }     int64_t seconds = std::stol(sstring(s.substr(minutes_end + 1, minutes_end - seconds_end)));     if (seconds < 0 || seconds >= 60) {         throw marshal_exception(format("Second out of bounds ({:d}).", seconds));     }     int64_t nanoseconds = 0;     if (seconds_end < s.length()) {         nanoseconds = std::stol(sstring(s.substr(seconds_end + 1)));         auto nano_digits = s.length() - (seconds_end + 1);         if (nano_digits > 9) {             throw marshal_exception(format("more than 9 nanosecond digits: {}", s));         }         nanoseconds *= std::pow(10, 9 - nano_digits);         if (nanoseconds < 0 || nanoseconds >= 1000 * 1000 * 1000) {             throw marshal_exception(format("Nanosecond out of bounds ({:d}).", nanoseconds));         }     }     std::chrono::nanoseconds result{};     result += std::chrono::hours(hours);     result += std::chrono::minutes(minutes);     result += std::chrono::seconds(seconds);     result += std::chrono::nanoseconds(nanoseconds);     return result.count(); }
 uuid_type_impl::uuid_type_impl()     : concrete_type(kind::uuid, uuid_type_name, 16) {}
 using inet_address = seastar::net::inet_address;
 inet_addr_type_impl::inet_addr_type_impl()     : concrete_type<inet_address>(kind::inet, inet_addr_type_name, {}
) {}
 // Integer of same length of a given type. This is useful because our
// ntoh functions only know how to operate on integers.
template <typename T> struct int_of_size;
 template <typename D, typename I> struct int_of_size_impl {     using dtype = D;     using itype = I;     static_assert(sizeof(dtype) == sizeof(itype), "size mismatch");     static_assert(alignof(dtype) == alignof(itype), "align mismatch"); };
 template <> struct int_of_size<double> :     public int_of_size_impl<double, uint64_t> {};
 template <> struct int_of_size<float> :     public int_of_size_impl<float, uint32_t> {};
 template <typename T> struct float_type_traits {     static constexpr size_t serialized_size = sizeof(typename int_of_size<T>::itype);     static double read_nonempty(managed_bytes_view v) {         return std::bit_cast<T>(read_simple_exactly<typename int_of_size<T>::itype>(v));     } };
 template<> struct simple_type_traits<float> : public float_type_traits<float> {};
 template<> struct simple_type_traits<double> : public float_type_traits<double> {};
 template <typename T> floating_type_impl<T>::floating_type_impl(         abstract_type::kind k, sstring name, std::optional<uint32_t> value_length_if_fixed)     : simple_type_impl<T>(k, std::move(name), std::move(value_length_if_fixed)) {}
 double_type_impl::double_type_impl() : floating_type_impl{kind::double_kind, double_type_name, 8}
 {}
 float_type_impl::float_type_impl() : floating_type_impl{kind::float_kind, float_type_name, 4}
 {}
 varint_type_impl::varint_type_impl() : concrete_type{kind::varint, varint_type_name, { }}
 { }
 decimal_type_impl::decimal_type_impl() : concrete_type{kind::decimal, decimal_type_name, { }}
 { }
 counter_type_impl::counter_type_impl()     : abstract_type{kind::counter, counter_type_name, {}}
 {}
 // TODO(jhaberku): Move this to Seastar.
template <size_t... Ts, class Function> auto generate_tuple_from_index(std::index_sequence<Ts...>, Function&& f) {     // To ensure that tuple is constructed in the correct order (because the evaluation order of the arguments to
    // `std::make_tuple` is unspecified), use braced initialization  (which does define the order). However, we still
    // need to figure out the type.
    using result_type = decltype(std::make_tuple(f(Ts)...));     return result_type{f(Ts)...}; }
 duration_type_impl::duration_type_impl()     : concrete_type(kind::duration, duration_type_name, {}
) {}
 using common_counter_type = cql_duration::common_counter_type;
 static std::tuple<common_counter_type, common_counter_type, common_counter_type> deserialize_counters(bytes_view v) {     auto deserialize_and_advance = [&v] (auto&& i) {         auto len = signed_vint::serialized_size_from_first_byte(v.front());         const auto d = signed_vint::deserialize(v);         v.remove_prefix(len);         if (v.empty() && (i != 2)) {             throw marshal_exception("Cannot deserialize duration");         }         return static_cast<common_counter_type>(d);     };     return generate_tuple_from_index(std::make_index_sequence<3>(), std::move(deserialize_and_advance)); }
 empty_type_impl::empty_type_impl()     : abstract_type(kind::empty, empty_type_name, 0) {}
 logging::logger collection_type_impl::_logger("collection_type_impl");
 const size_t collection_type_impl::max_elements;
 lw_shared_ptr<cql3::column_specification> collection_type_impl::make_collection_receiver(         const cql3::column_specification& collection, bool is_key) const {     struct visitor {         const cql3::column_specification& collection;         bool is_key;         lw_shared_ptr<cql3::column_specification> operator()(const abstract_type&) { abort(); }         lw_shared_ptr<cql3::column_specification> operator()(const list_type_impl&) {             return cql3::lists::value_spec_of(collection);         }         lw_shared_ptr<cql3::column_specification> operator()(const map_type_impl&) {             return is_key ? cql3::maps::key_spec_of(collection) : cql3::maps::value_spec_of(collection);         }         lw_shared_ptr<cql3::column_specification> operator()(const set_type_impl&) {             return cql3::sets::value_spec_of(collection);         }     };     return ::visit(*this, visitor{collection, is_key}); }
 listlike_collection_type_impl::listlike_collection_type_impl(         kind k, sstring name, data_type elements, bool is_multi_cell)     : collection_type_impl(k, name, is_multi_cell), _elements(elements) {}
 std::strong_ordering listlike_collection_type_impl::compare_with_map(const map_type_impl& map_type, bytes_view list, bytes_view map) const {     assert((is_set() && map_type.get_keys_type() == _elements) || (!is_set() && map_type.get_values_type() == _elements));     if (list.empty()) {         return map.empty() ? std::strong_ordering::equal : std::strong_ordering::less;     } else if (map.empty()) {         return std::strong_ordering::greater;     }     const abstract_type& element_type = *_elements;     size_t list_size = read_collection_size(list);     size_t map_size = read_collection_size(map);     bytes_view_opt list_value;     bytes_view_opt map_value[2];     // Lists are represented as vector<pair<timeuuid, value>>, sets are vector<pair<value, empty>>
    size_t map_value_index = is_list();     // Both set elements and map keys are sorted, so can be compared in linear order;
    // List elements are stored in both vectors in list index order.
    for (size_t i = 0; i < std::min(list_size, map_size); ++i) {         list_value = read_collection_value_nonnull(list);         map_value[0] = read_collection_value_nonnull(map);         // sets-as-maps happen to be serialized with NULL
        map_value[1] = read_collection_value(map);         if (!list_value) {             return std::strong_ordering::less;         }         // map_value[0] is known non-null, and sets will compare map_value[0].
        auto cmp = element_type.compare(*list_value, *map_value[map_value_index]);         if (cmp != 0) {             return cmp;         }     }     return list_size <=> map_size; }
 bytes listlike_collection_type_impl::serialize_map(const map_type_impl& map_type, const data_value& value) const {     assert((is_set() && map_type.get_keys_type() == _elements) || (!is_set() && map_type.get_values_type() == _elements));     const std::vector<std::pair<data_value, data_value>>& map = map_type.from_value(value);     // Lists are represented as vector<pair<timeuuid, value>>, sets are vector<pair<value, empty>>
    bool first = is_set();     size_t len = collection_size_len();     size_t psz = collection_value_len();     for (const std::pair<data_value, data_value>& entry : map) {         len += psz + (first ? entry.first : entry.second).serialized_size();     }     bytes b(bytes::initialized_later(), len);     bytes::iterator out = b.begin();     write_collection_size(out, map.size());     for (const std::pair<data_value, data_value>& entry : map) {         write_collection_value(out, _elements, first ? entry.first : entry.second);     }     return b; }
 void listlike_collection_type_impl::validate_for_storage(const FragmentedView auto& value) const {     for (auto val_opt : partially_deserialize_listlike(value)) {         if (!val_opt) {             throw exceptions::invalid_request_exception("Cannot store NULL in list or set");         }     } }
 template void listlike_collection_type_impl::validate_for_storage(const managed_bytes_view& value) const;
 template void listlike_collection_type_impl::validate_for_storage(const fragmented_temporary_buffer::view& value) const;
 static bool is_compatible_with_aux(const collection_type_impl& t, const abstract_type& previous) {     if (t.get_kind() != previous.get_kind()) {         return false;     }     auto& cprev = static_cast<const collection_type_impl&>(previous);     if (t.is_multi_cell() != cprev.is_multi_cell()) {         return false;     }     if (!t.is_multi_cell()) {         return t.is_compatible_with_frozen(cprev);     }     if (!t.name_comparator()->is_compatible_with(*cprev.name_comparator())) {         return false;     }     // the value comparator is only used for Cell values, so sorting doesn't matter
    return t.value_comparator()->is_value_compatible_with(*cprev.value_comparator()); }
  size_t collection_size_len() {     return sizeof(int32_t); }
 size_t collection_value_len() {     return sizeof(int32_t); }
  void write_collection_size(bytes::iterator& out, int size) {     serialize_int32(out, size); }
    void write_collection_value(bytes::iterator& out, std::optional<bytes_view> val_bytes_opt) {     if (!val_bytes_opt) {         serialize_int32(out, int32_t(-1));         return;     }     auto& val_bytes = *val_bytes_opt;     serialize_int32(out, int32_t(val_bytes.size()));     out = std::copy_n(val_bytes.begin(), val_bytes.size(), out); }
 // Passing the wrong integer type to a generic serialization function is a particularly
// easy mistake to do, so we want to disable template parameter deduction here.
// Hence std::type_identity.
template<typename T> void write_simple(bytes_ostream& out, std::type_identity_t<T> val) {     auto val_be = net::hton(val);     auto val_ptr = reinterpret_cast<const bytes::value_type*>(&val_be);     out.write(bytes_view(val_ptr, sizeof(T))); }
 void write_collection_value(bytes_ostream& out, atomic_cell_value_view val) {     write_simple<int32_t>(out, int32_t(val.size_bytes()));     for (auto&& frag : fragment_range(val)) {         out.write(frag);     } }
 void write_fragmented(managed_bytes_mutable_view& out, std::string_view val) {     while (val.size() > 0) {         size_t current_n = std::min(val.size(), out.current_fragment().size());         memcpy(out.current_fragment().data(), val.data(), current_n);         val.remove_prefix(current_n);         out.remove_prefix(current_n);     } }
 template<std::integral T> void write_simple(managed_bytes_mutable_view& out, std::type_identity_t<T> val) {     val = net::hton(val);     if (out.current_fragment().size() >= sizeof(T)) [[likely]] {         auto p = out.current_fragment().data();         out.remove_prefix(sizeof(T));         // FIXME use write_unaligned after it's merged.
        write_unaligned<T>(p, val);     } else if (out.size_bytes() >= sizeof(T)) {         write_fragmented(out, std::string_view(reinterpret_cast<const char*>(&val), sizeof(T)));     } else {         on_internal_error(tlogger, format("write_simple: attempted write of size {} to buffer of size {}", sizeof(T), out.size_bytes()));     } }
 void write_collection_size(managed_bytes_mutable_view& out, int size) {     write_simple<uint32_t>(out, uint32_t(size)); }
 void write_collection_value(managed_bytes_mutable_view& out, bytes_view val) {     write_simple<int32_t>(out, int32_t(val.size()));     write_fragmented(out, single_fragmented_view(val)); }
 void write_collection_value(managed_bytes_mutable_view& out, const managed_bytes_view_opt& val_opt) {     if (!val_opt) {         write_simple<int32_t>(out, int32_t(-1));         return;     }     auto& val = *val_opt;     write_simple<int32_t>(out, int32_t(val.size_bytes()));     write_fragmented(out, val); }
 void write_int32(bytes::iterator& out, int32_t value) {     return serialize_int32(out, value); }
 shared_ptr<const abstract_type> abstract_type::underlying_type() const {     struct visitor {         shared_ptr<const abstract_type> operator()(const abstract_type& t) { return t.shared_from_this(); }         shared_ptr<const abstract_type> operator()(const reversed_type_impl& r) { return r.underlying_type(); }     };     return visit(*this, visitor{}); }
 bool abstract_type::is_counter() const {     struct visitor {         bool operator()(const reversed_type_impl& r) { return r.underlying_type()->is_counter(); }         bool operator()(const abstract_type&) { return false; }         bool operator()(const counter_type_impl&) { return true; }     };     return visit(*this, visitor{}); }
 bool abstract_type::is_collection() const {     struct visitor {         bool operator()(const reversed_type_impl& r) { return r.underlying_type()->is_collection(); }         bool operator()(const abstract_type&) { return false; }         bool operator()(const collection_type_impl&) { return true; }     };     return visit(*this, visitor{}); }
 bool abstract_type::is_tuple() const {     struct visitor {         bool operator()(const abstract_type&) { return false; }         bool operator()(const reversed_type_impl& t) { return t.underlying_type()->is_tuple(); }         bool operator()(const tuple_type_impl&) { return true; }     };     return visit(*this, visitor{}); }
 bool abstract_type::is_multi_cell() const {     struct visitor {         bool operator()(const abstract_type&) { return false; }         bool operator()(const reversed_type_impl& t) { return t.underlying_type()->is_multi_cell(); }         bool operator()(const collection_type_impl& c) { return c.is_multi_cell(); }         bool operator()(const user_type_impl& u) { return u.is_multi_cell(); }     };     return visit(*this, visitor{}); }
 bool abstract_type::is_native() const { return !is_collection() && !is_tuple(); }
 bool abstract_type::is_string() const {     struct visitor {         bool operator()(const abstract_type&) { return false; }         bool operator()(const reversed_type_impl& t) { return t.underlying_type()->is_string(); }         bool operator()(const string_type_impl&) { return true; }     };     return visit(*this, visitor{}); }
 template<typename Predicate> requires CanHandleAllTypes<Predicate> static bool find(const abstract_type& t, const Predicate& f) {     struct visitor {         const Predicate& f;         bool operator()(const abstract_type&) { return false; }         bool operator()(const reversed_type_impl& r) { return find(*r.underlying_type(), f); }         bool operator()(const tuple_type_impl& t) {             return boost::algorithm::any_of(t.all_types(), [&] (const data_type& dt) { return find(*dt, f); });         }         bool operator()(const map_type_impl& m) { return find(*m.get_keys_type(), f) || find(*m.get_values_type(), f); }         bool operator()(const listlike_collection_type_impl& l) { return find(*l.get_elements_type(), f); }     };     return visit(t, [&](const auto& t) {         if (f(t)) {             return true;         }         return visitor{f}(t);     }); }
 bool abstract_type::references_duration() const {     struct visitor {         bool operator()(const abstract_type&) const { return false; }         bool operator()(const duration_type_impl&) const { return true; }     };     return find(*this, visitor{}); }
 bool abstract_type::references_user_type(const sstring& keyspace, const bytes& name) const {     struct visitor {         const sstring& keyspace;         const bytes& name;         bool operator()(const abstract_type&) const { return false; }         bool operator()(const user_type_impl& u) const { return u._keyspace == keyspace && u._name == name; }     };     return find(*this, visitor{keyspace, name}); }
 namespace { struct is_byte_order_equal_visitor {     template <typename T> bool operator()(const simple_type_impl<T>&) { return true; }     bool operator()(const concrete_type<utils::UUID>&) { return true; }     bool operator()(const abstract_type&) { return false; }     bool operator()(const reversed_type_impl& t) { return t.underlying_type()->is_byte_order_equal(); }     bool operator()(const string_type_impl&) { return true; }     bool operator()(const bytes_type_impl&) { return true; }     bool operator()(const timestamp_date_base_class&) { return true; }     bool operator()(const inet_addr_type_impl&) { return true; }     bool operator()(const duration_type_impl&) { return true; }     // FIXME: origin returns false for list.  Why?
    bool operator()(const set_type_impl& s) { return s.get_elements_type()->is_byte_order_equal(); } }; }
 bool abstract_type::is_byte_order_equal() const { return visit(*this, is_byte_order_equal_visitor{}); }
 static bool check_compatibility(const tuple_type_impl &t, const abstract_type& previous, bool (abstract_type::*predicate)(const abstract_type&) const);
 static bool is_fixed_size_int_type(const abstract_type& t) {     using k = abstract_type::kind;     switch (t.get_kind()) {     case k::byte:     case k::short_kind:     case k::int32:     case k::long_kind:         return true;     case k::ascii:     case k::boolean:     case k::bytes:     case k::counter:     case k::date:     case k::decimal:     case k::double_kind:     case k::duration:     case k::empty:     case k::float_kind:     case k::inet:     case k::list:     case k::map:     case k::reversed:     case k::set:     case k::simple_date:     case k::time:     case k::timestamp:     case k::timeuuid:     case k::tuple:     case k::user:     case k::utf8:     case k::uuid:     case k::varint:         return false;     }     __builtin_unreachable(); }
 bool abstract_type::is_compatible_with(const abstract_type& previous) const {     if (this == &previous) {         return true;     }     struct visitor {         const abstract_type& previous;         bool operator()(const reversed_type_impl& t) {             if (previous.is_reversed()) {                 return t.underlying_type()->is_compatible_with(*previous.underlying_type());             }             return false;         }         bool operator()(const utf8_type_impl&) {             // Anything that is ascii is also utf8, and they both use bytes comparison
            return previous.is_string();         }         bool operator()(const bytes_type_impl&) {             // Both ascii_type_impl and utf8_type_impl really use bytes comparison and
            // bytesType validate everything, so it is compatible with the former.
            return previous.is_string();         }         bool operator()(const date_type_impl& t) {             if (previous.get_kind() == kind::timestamp) {                 static logging::logger date_logger(date_type_name);                 date_logger.warn("Changing from TimestampType to DateType is allowed, but be wary "                                  "that they sort differently for pre-unix-epoch timestamps "                                  "(negative timestamp values) and thus this change will corrupt "                                  "your data if you have such negative timestamp. There is no "                                  "reason to switch from DateType to TimestampType except if you "                                  "were using DateType in the first place and switched to "                                  "TimestampType by mistake.");                 return true;             }             return false;         }         bool operator()(const timestamp_type_impl& t) {             if (previous.get_kind() == kind::date) {                 static logging::logger timestamp_logger(timestamp_type_name);                 timestamp_logger.warn("Changing from DateType to TimestampType is allowed, but be wary "                                       "that they sort differently for pre-unix-epoch timestamps "                                       "(negative timestamp values) and thus this change will corrupt "                                       "your data if you have such negative timestamp. So unless you "                                       "know that you don't have *any* pre-unix-epoch timestamp you "                                       "should change back to DateType");                 return true;             }             return false;         }         bool operator()(const tuple_type_impl& t) {             return check_compatibility(t, previous, &abstract_type::is_compatible_with);         }         bool operator()(const collection_type_impl& t) { return is_compatible_with_aux(t, previous); }         bool operator()(const varint_type_impl& t) {             return is_fixed_size_int_type(previous);         }         bool operator()(const abstract_type& t) { return false; }     };     return visit(*this, visitor{previous}); }
 cql3::cql3_type abstract_type::as_cql3_type() const {     return cql3::cql3_type(shared_from_this()); }
 static sstring cql3_type_name_impl(const abstract_type& t) {     struct visitor {         sstring operator()(const ascii_type_impl&) { return "ascii"; }         sstring operator()(const boolean_type_impl&) { return "boolean"; }         sstring operator()(const byte_type_impl&) { return "tinyint"; }         sstring operator()(const bytes_type_impl&) { return "blob"; }         sstring operator()(const counter_type_impl&) { return "counter"; }         sstring operator()(const timestamp_date_base_class&) { return "timestamp"; }         sstring operator()(const decimal_type_impl&) { return "decimal"; }         sstring operator()(const double_type_impl&) { return "double"; }         sstring operator()(const duration_type_impl&) { return "duration"; }         sstring operator()(const empty_type_impl&) { return "empty"; }         sstring operator()(const float_type_impl&) { return "float"; }         sstring operator()(const inet_addr_type_impl&) { return "inet"; }         sstring operator()(const int32_type_impl&) { return "int"; }         sstring operator()(const list_type_impl& l) {             return format("list<{}>", l.get_elements_type()->as_cql3_type());         }         sstring operator()(const long_type_impl&) { return "bigint"; }         sstring operator()(const map_type_impl& m) {             return format("map<{}, {}>", m.get_keys_type()->as_cql3_type(), m.get_values_type()->as_cql3_type());         }         sstring operator()(const reversed_type_impl& r) { return cql3_type_name_impl(*r.underlying_type()); }         sstring operator()(const set_type_impl& s) { return format("set<{}>", s.get_elements_type()->as_cql3_type()); }         sstring operator()(const short_type_impl&) { return "smallint"; }         sstring operator()(const simple_date_type_impl&) { return "date"; }         sstring operator()(const time_type_impl&) { return "time"; }         sstring operator()(const timeuuid_type_impl&) { return "timeuuid"; }         sstring operator()(const tuple_type_impl& t) {             return format("tuple<{}>", fmt::join(t.all_types() | boost::adaptors::transformed(std::mem_fn(                                                                             &abstract_type::as_cql3_type)), ", "));         }                  sstring operator()(const utf8_type_impl&) { return "text"; }         sstring operator()(const uuid_type_impl&) { return "uuid"; }         sstring operator()(const varint_type_impl&) { return "varint"; }     };     return visit(t, visitor{}); }
 const sstring& abstract_type::cql3_type_name() const {     if (_cql3_type_name.empty()) {         auto name = cql3_type_name_impl(*this);         if (!is_native() && !is_multi_cell()) {             name = "frozen<" + name + ">";         }         _cql3_type_name = name;     }     return _cql3_type_name; }
 void write_collection_value(bytes::iterator& out, data_type type, const data_value& value) {     if (value.is_null()) {         auto val_len = -1;         serialize_int32(out, val_len);         return;     }     size_t val_len = value.serialized_size();     serialize_int32(out, val_len);     value.serialize(out); }
 map_type map_type_impl::get_instance(data_type keys, data_type values, bool is_multi_cell) {     return intern::get_instance(std::move(keys), std::move(values), is_multi_cell); }
 sstring make_map_type_name(data_type keys, data_type values, bool is_multi_cell) {     sstring ret = "";     if (!is_multi_cell) {         ret = "org.apache.cassandra.db.marshal.FrozenType(";     }     ret += "org.apache.cassandra.db.marshal.MapType(" + keys->name() + "," + values->name() + ")";     if (!is_multi_cell) {         ret += ")";     }     return ret; }
 map_type_impl::map_type_impl(data_type keys, data_type values, bool is_multi_cell)         : concrete_type(kind::map, make_map_type_name(keys, values, is_multi_cell), is_multi_cell)         , _keys(std::move(keys))         , _values(std::move(values)) {     _contains_set_or_map = true; }
 data_type map_type_impl::freeze() const {     if (_is_multi_cell) {         return get_instance(_keys, _values, false);     } else {         return shared_from_this();     } }
 bool map_type_impl::is_compatible_with_frozen(const collection_type_impl& previous) const {     assert(!_is_multi_cell);     auto* p = dynamic_cast<const map_type_impl*>(&previous);     if (!p) {         return false;     }     return _keys->is_compatible_with(*p->_keys)             && _values->is_compatible_with(*p->_values); }
 bool map_type_impl::is_value_compatible_with_frozen(const collection_type_impl& previous) const {     assert(!_is_multi_cell);     auto* p = dynamic_cast<const map_type_impl*>(&previous);     if (!p) {         return false;     }     return _keys->is_compatible_with(*p->_keys)             && _values->is_value_compatible_with(*p->_values); }
 std::strong_ordering map_type_impl::compare_maps(data_type keys, data_type values, managed_bytes_view o1, managed_bytes_view o2) {     if (o1.empty()) {         return o2.empty() ? std::strong_ordering::equal : std::strong_ordering::less;     } else if (o2.empty()) {         return std::strong_ordering::greater;     }     int size1 = read_collection_size(o1);     int size2 = read_collection_size(o2);     // FIXME: use std::lexicographical_compare()
    for (int i = 0; i < std::min(size1, size2); ++i) {         auto k1 = read_collection_key(o1);         auto k2 = read_collection_key(o2);         auto cmp = keys->compare(k1, k2);         if (cmp != 0) {             return cmp;         }         auto v1 = read_collection_value_nonnull(o1);         auto v2 = read_collection_value_nonnull(o2);         cmp = values->compare(v1, v2);         if (cmp != 0) {             return cmp;         }     }     return size1 <=> size2; }
 static size_t map_serialized_size(const map_type_impl::native_type* m) {     size_t len = collection_size_len();     size_t psz = collection_value_len();     for (auto&& kv : *m) {         len += psz + kv.first.serialized_size();         len += psz + kv.second.serialized_size();     }     return len; }
 static void serialize_map(const map_type_impl& t, const void* value, bytes::iterator& out) {     auto& m = t.from_value(value);     write_collection_size(out, m.size());     for (auto&& kv : m) {         write_collection_value(out, t.get_keys_type(), kv.first);         write_collection_value(out, t.get_values_type(), kv.second);     } }
 template <FragmentedView View> data_value map_type_impl::deserialize(View in) const {     native_type m;     auto size = read_collection_size(in);     for (int i = 0; i < size; ++i) {         auto k = _keys->deserialize(read_collection_key(in));         auto v = _values->deserialize(read_collection_value_nonnull(in));         m.insert(m.end(), std::make_pair(std::move(k), std::move(v)));     }     return make_value(std::move(m)); }
 template data_value map_type_impl::deserialize<>(ser::buffer_view<bytes_ostream::fragment_iterator>) const;
  static sstring map_to_string(const std::vector<std::pair<data_value, data_value>>& v, bool include_frozen_type) {     std::ostringstream out;     if (include_frozen_type) {         out << "(";     }     fmt::print(out, "{}", fmt::join(v | boost::adaptors::transformed([] (const std::pair<data_value, data_value>& p) {         std::ostringstream out;         const auto& k = p.first;         const auto& v = p.second;         out << "{" << k.type()->to_string_impl(k) << " : ";         out << v.type()->to_string_impl(v) << "}";         return out.str();     }), ", "));     if (include_frozen_type) {         out << ")";     }     return out.str(); }
 bytes map_type_impl::serialize_partially_deserialized_form(         const std::vector<std::pair<bytes_view, bytes_view>>& v) {     size_t len = collection_value_len() * v.size() * 2 + collection_size_len();     for (auto&& e : v) {         len += e.first.size() + e.second.size();     }     bytes b(bytes::initialized_later(), len);     bytes::iterator out = b.begin();     write_collection_size(out, v.size());     for (auto&& e : v) {         write_collection_value(out, e.first);         write_collection_value(out, e.second);     }     return b; }
 managed_bytes map_type_impl::serialize_partially_deserialized_form_fragmented(         const std::vector<std::pair<managed_bytes_view, managed_bytes_view>>& v) {     size_t len = collection_value_len() * v.size() * 2 + collection_size_len();     for (auto&& e : v) {         len += e.first.size() + e.second.size();     }     managed_bytes b(managed_bytes::initialized_later(), len);     managed_bytes_mutable_view out = b;     write_collection_size(out, v.size());     for (auto&& e : v) {         write_collection_value(out, e.first);         write_collection_value(out, e.second);     }     return b; }
 static std::optional<data_type> update_user_type_aux(         const map_type_impl& m, const shared_ptr<const user_type_impl> updated) {     auto old_keys = m.get_keys_type();     auto old_values = m.get_values_type();     auto k = old_keys->update_user_type(updated);     auto v = old_values->update_user_type(updated);     if (!k && !v) {         return std::nullopt;     }     return std::make_optional(static_pointer_cast<const abstract_type>(         map_type_impl::get_instance(k ? *k : old_keys, v ? *v : old_values, m.is_multi_cell()))); }
 static void serialize(const abstract_type& t, const void* value, bytes::iterator& out);
 set_type set_type_impl::get_instance(data_type elements, bool is_multi_cell) {     return intern::get_instance(elements, is_multi_cell); }
 sstring make_set_type_name(data_type elements, bool is_multi_cell) {     sstring ret = "";     if (!is_multi_cell) {         ret = "org.apache.cassandra.db.marshal.FrozenType(";     }     ret += "org.apache.cassandra.db.marshal.SetType(" + elements->name() + ")";     if (!is_multi_cell) {         ret += ")";     }     return ret; }
 set_type_impl::set_type_impl(data_type elements, bool is_multi_cell)     : concrete_type(kind::set, make_set_type_name(elements, is_multi_cell), elements, is_multi_cell) {         _contains_set_or_map = true;     }
 data_type set_type_impl::value_comparator() const {     return empty_type; }
 data_type set_type_impl::freeze() const {     if (_is_multi_cell) {         return get_instance(_elements, false);     } else {         return shared_from_this();     } }
 bool set_type_impl::is_compatible_with_frozen(const collection_type_impl& previous) const {     assert(!_is_multi_cell);     auto* p = dynamic_cast<const set_type_impl*>(&previous);     if (!p) {         return false;     }     return _elements->is_compatible_with(*p->_elements); }
 bool set_type_impl::is_value_compatible_with_frozen(const collection_type_impl& previous) const {     return is_compatible_with(previous); }
  static size_t listlike_serialized_size(const std::vector<data_value>* s) {     size_t len = collection_size_len();     size_t psz = collection_value_len();     for (auto&& e : *s) {         len += psz + e.serialized_size();     }     return len; }
 static void serialize_set(const set_type_impl& t, const void* value, bytes::iterator& out) {     auto& s = t.from_value(value);     write_collection_size(out, s.size());     for (auto&& e : s) {         write_collection_value(out, t.get_elements_type(), e);     } }
 template <FragmentedView View> data_value set_type_impl::deserialize(View in) const {     auto nr = read_collection_size(in);     native_type s;     s.reserve(nr);     for (int i = 0; i != nr; ++i) {         auto e = _elements->deserialize(read_collection_value_nonnull(in));         if (e.is_null()) {             throw marshal_exception("Cannot deserialize a set");         }         s.push_back(std::move(e));     }     return make_value(std::move(s)); }
 template data_value set_type_impl::deserialize<>(ser::buffer_view<bytes_ostream::fragment_iterator>) const;
 bytes set_type_impl::serialize_partially_deserialized_form(         const std::vector<bytes_view>& v) {     return pack(v.begin(), v.end(), v.size()); }
 managed_bytes set_type_impl::serialize_partially_deserialized_form_fragmented(         const std::vector<managed_bytes_view_opt>& v) {     return pack_fragmented(v.begin(), v.end(), v.size()); }
 template <FragmentedView View> utils::chunked_vector<managed_bytes_opt> partially_deserialize_listlike(View in) {     auto nr = read_collection_size(in);     utils::chunked_vector<managed_bytes_opt> elements;     elements.reserve(nr);     for (int i = 0; i != nr; ++i) {         elements.emplace_back(read_collection_value(in));     }     return elements; }
 template utils::chunked_vector<managed_bytes_opt> partially_deserialize_listlike(managed_bytes_view in);
 template utils::chunked_vector<managed_bytes_opt> partially_deserialize_listlike(fragmented_temporary_buffer::view in);
 template <FragmentedView View> std::vector<std::pair<managed_bytes, managed_bytes>> partially_deserialize_map(View in) {     auto nr = read_collection_size(in);     std::vector<std::pair<managed_bytes, managed_bytes>> elements;     elements.reserve(nr);     for (int i = 0; i != nr; ++i) {         auto key = managed_bytes(read_collection_key(in));         auto value = managed_bytes_opt(read_collection_value_nonnull(in));         if (!value) {             on_internal_error(tlogger, "NULL value in map");         }         elements.emplace_back(std::move(key), std::move(*value));     }     return elements; }
 template std::vector<std::pair<managed_bytes, managed_bytes>> partially_deserialize_map(managed_bytes_view in);
 template std::vector<std::pair<managed_bytes, managed_bytes>> partially_deserialize_map(fragmented_temporary_buffer::view in);
 list_type list_type_impl::get_instance(data_type elements, bool is_multi_cell) {     return intern::get_instance(elements, is_multi_cell); }
 sstring make_list_type_name(data_type elements, bool is_multi_cell) {     sstring ret = "";     if (!is_multi_cell) {         ret = "org.apache.cassandra.db.marshal.FrozenType(";     }     ret += "org.apache.cassandra.db.marshal.ListType(" + elements->name() + ")";     if (!is_multi_cell) {         ret += ")";     }     return ret; }
 list_type_impl::list_type_impl(data_type elements, bool is_multi_cell)     : concrete_type(kind::list, make_list_type_name(elements, is_multi_cell), elements, is_multi_cell) {         _contains_set_or_map = _elements->contains_set_or_map();     }
 data_type list_type_impl::name_comparator() const {     return timeuuid_type; }
 data_type list_type_impl::value_comparator() const {     return _elements; }
 data_type list_type_impl::freeze() const {     if (_is_multi_cell) {         return get_instance(_elements, false);     } else {         return shared_from_this();     } }
 bool list_type_impl::is_compatible_with_frozen(const collection_type_impl& previous) const {     assert(!_is_multi_cell);     auto* p = dynamic_cast<const list_type_impl*>(&previous);     if (!p) {         return false;     }     return _elements->is_compatible_with(*p->_elements); }
 static bool is_value_compatible_with_internal(const abstract_type& t, const abstract_type& other);
 bool list_type_impl::is_value_compatible_with_frozen(const collection_type_impl& previous) const {     auto& lp = dynamic_cast<const list_type_impl&>(previous);     return is_value_compatible_with_internal(*_elements, *lp._elements); }
  static void serialize_list(const list_type_impl& t, const void* value, bytes::iterator& out) {     auto& s = t.from_value(value);     write_collection_size(out, s.size());     for (auto&& e : s) {         write_collection_value(out, t.get_elements_type(), e);     } }
 template <FragmentedView View> data_value list_type_impl::deserialize(View in) const {     auto nr = read_collection_size(in);     native_type s;     s.reserve(nr);     for (int i = 0; i != nr; ++i) {         auto serialized_value_opt = read_collection_value(in);         if (serialized_value_opt) {             auto e = _elements->deserialize(*serialized_value_opt);             s.push_back(std::move(e));         } else {             s.push_back(data_value::make_null(data_type(shared_from_this())));         }     }     return make_value(std::move(s)); }
 template data_value list_type_impl::deserialize<>(ser::buffer_view<bytes_ostream::fragment_iterator>) const;
 static sstring vector_to_string(const std::vector<data_value>& v, std::string_view sep) {     return fmt::to_string(fmt::join(             v | boost::adaptors::transformed([] (const data_value& e) { return e.type()->to_string_impl(e); }),             sep)); }
 template <typename F> static std::optional<data_type> update_listlike(         const listlike_collection_type_impl& c, F&& f, shared_ptr<const user_type_impl> updated) {     if (auto e = c.get_elements_type()->update_user_type(updated)) {         return std::make_optional<data_type>(f(std::move(*e), c.is_multi_cell()));     }     return std::nullopt; }
 tuple_type_impl::tuple_type_impl(kind k, sstring name, std::vector<data_type> types, bool freeze_inner)         : concrete_type(k, std::move(name), { }
), _types(std::move(types)) {     if (freeze_inner) {         for (auto& t : _types) {             t = t->freeze();         }     }     set_contains_collections(); }
 tuple_type_impl::tuple_type_impl(std::vector<data_type> types, bool freeze_inner)         : tuple_type_impl{kind::tuple, make_name(types), std::move(types), freeze_inner}
 {     set_contains_collections(); }
 tuple_type_impl::tuple_type_impl(std::vector<data_type> types)         : tuple_type_impl(std::move(types), true) {     set_contains_collections(); }
 void tuple_type_impl::set_contains_collections() {     for (const data_type& t : _types) {         if (t->contains_set_or_map()) {             _contains_set_or_map = true;             break;         }     }     if (_contains_set_or_map) {         _contains_collection = true;         return;     }     for (const data_type& t : _types) {         if (t->contains_collection()) {             _contains_collection = true;             break;         }     } }
 shared_ptr<const tuple_type_impl> tuple_type_impl::get_instance(std::vector<data_type> types) {     return intern::get_instance(std::move(types)); }
 template <FragmentedView View> static void validate_aux(const tuple_type_impl& t, View v) {     auto ti = t.all_types().begin();     while (ti != t.all_types().end() && v.size_bytes()) {         std::optional<View> e = read_tuple_element(v);         if (e) {             (*ti)->validate(*e);         }         ++ti;     }     size_t extra_elements = 0;     while (!v.empty()) {         read_tuple_element(v);         extra_elements += 1;     }     if (extra_elements > 0) {         // This function is called for both tuple and user_type, print the name too
        throw marshal_exception(format("Value of type {} contained too many fields (expected {}, got {})",                                 t.name(), t.size(), t.size() + extra_elements));     } }
 namespace { template <FragmentedView View> struct validate_visitor {     const View& v;     ;          void operator()(const abstract_type&) {}     template <typename T> void operator()(const integer_type_impl<T>& t) {         if (v.empty()) {             return;         }         if (v.size_bytes() != sizeof(T)) {             throw marshal_exception(format("Validation failed for type {}: got {:d} bytes", t.name(), v.size_bytes()));         }     }                                                  template <typename T> void operator()(const floating_type_impl<T>& t) {         if (v.empty()) {             return;         }         if (v.size_bytes() != sizeof(T)) {             throw marshal_exception(format("Expected {:d} bytes for a floating type, got {:d}", sizeof(T), v.size_bytes()));         }     }     void operator()(const simple_date_type_impl& t) {         if (v.empty()) {             return;         }         if (v.size_bytes() != 4) {             throw marshal_exception(format("Expected 4 byte long for date ({:d})", v.size_bytes()));         }     }                                    }; }
 template <FragmentedView View> void abstract_type::validate(const View& view) const {     visit(*this, validate_visitor<View>{view}); }
 // Explicit instantiation.
template void abstract_type::validate<>(const single_fragmented_view&) const;
 template void abstract_type::validate<>(const fragmented_temporary_buffer::view&) const;
 template void abstract_type::validate<>(const managed_bytes_view&) const;
 void abstract_type::validate(bytes_view v) const {     visit(*this, validate_visitor<single_fragmented_view>{single_fragmented_view(v)}); }
 static void serialize_aux(const tuple_type_impl& type, const tuple_type_impl::native_type* val, bytes::iterator& out) {     assert(val);     auto& elems = *val;     assert(elems.size() <= type.size());     for (size_t i = 0; i < elems.size(); ++i) {         const abstract_type& t = type.type(i)->without_reversed();         const data_value& v = elems[i];         if (!v.is_null() && t != *v.type()) {             throw std::runtime_error(format("tuple element type mismatch: expected {}, got {}", t.name(), v.type()->name()));         }         if (v.is_null()) {             write(out, int32_t(-1));         } else {             write(out, int32_t(v.serialized_size()));             v.serialize(out);         }     } }
  static void serialize_varint_aux(bytes::iterator& out, const boost::multiprecision::cpp_int& num, uint8_t mask) {     struct inserter_with_prefix {         bytes::iterator& out;         uint8_t mask;         bool first = true;         inserter_with_prefix& operator*() {             return *this;         }         inserter_with_prefix& operator=(uint8_t value) {             if (first) {                 if (value & 0x80) {                     *out++ = 0 ^ mask;                 }                 first = false;             }             *out = value ^ mask;             return *this;         }         inserter_with_prefix& operator++() {             ++out;             return *this;         }     };     export_bits(num, inserter_with_prefix{out, mask}, 8); }
 static void serialize_varint(bytes::iterator& out, const boost::multiprecision::cpp_int& num) {     if (num < 0) {         serialize_varint_aux(out, -num - 1, 0xff);     } else {         serialize_varint_aux(out, num, 0);     } }
   namespace { struct serialize_visitor {     bytes::iterator& out;     ;     void operator()(const reversed_type_impl& t, const void* v) { return serialize(*t.underlying_type(), v, out); }     template <typename T>     void operator()(const integer_type_impl<T>& t, const typename integer_type_impl<T>::native_type* v1) {         if (v1->empty()) {             return;         }         auto v = v1->get();         auto u = net::hton(v);         out = std::copy_n(reinterpret_cast<const char*>(&u), sizeof(u), out);     }     void operator()(const string_type_impl& t, const string_type_impl::native_type* v) {         out = std::copy(v->begin(), v->end(), out);     }     void operator()(const bytes_type_impl& t, const bytes* v) {         out = std::copy(v->begin(), v->end(), out);     }     void operator()(const boolean_type_impl& t, const boolean_type_impl::native_type* v) {         if (!v->empty()) {             *out++ = char(*v);         }     }     void operator()(const timestamp_date_base_class& t, const timestamp_date_base_class::native_type* v1) {         if (v1->empty()) {             return;         }         uint64_t v = v1->get().time_since_epoch().count();         v = net::hton(v);         out = std::copy_n(reinterpret_cast<const char*>(&v), sizeof(v), out);     }     void operator()(const timeuuid_type_impl& t, const timeuuid_type_impl::native_type* uuid1) {         if (uuid1->empty()) {             return;         }         auto uuid = uuid1->get();         uuid.serialize(out);     }     void operator()(const simple_date_type_impl& t, const simple_date_type_impl::native_type* v1) {         if (v1->empty()) {             return;         }         uint32_t v = v1->get();         v = net::hton(v);         out = std::copy_n(reinterpret_cast<const char*>(&v), sizeof(v), out);     }     void operator()(const time_type_impl& t, const time_type_impl::native_type* v1) {         if (v1->empty()) {             return;         }         uint64_t v = v1->get();         v = net::hton(v);         out = std::copy_n(reinterpret_cast<const char*>(&v), sizeof(v), out);     }     void operator()(const empty_type_impl& t, const void*) {}     void operator()(const uuid_type_impl& t, const uuid_type_impl::native_type* value) {         if (value->empty()) {             return;         }         value->get().serialize(out);     }     void operator()(const inet_addr_type_impl& t, const inet_addr_type_impl::native_type* ipv) {         if (ipv->empty()) {             return;         }         auto& ip = ipv->get();         switch (ip.in_family()) {         case inet_address::family::INET: {             const ::in_addr& in = ip;             out = std::copy_n(reinterpret_cast<const char*>(&in.s_addr), sizeof(in.s_addr), out);             break;         }         case inet_address::family::INET6: {             const ::in6_addr& i6 = ip;             out = std::copy_n(i6.s6_addr, ip.size(), out);             break;         }         }     }     template <typename T>     void operator()(const floating_type_impl<T>& t, const typename floating_type_impl<T>::native_type* value) {         if (value->empty()) {             return;         }         T d = *value;         if (std::isnan(d)) {             // Java's Double.doubleToLongBits() documentation specifies that
            // any nan must be serialized to the same specific value
            d = std::numeric_limits<T>::quiet_NaN();         }         typename int_of_size<T>::itype i;         memcpy(&i, &d, sizeof(T));         auto u = net::hton(i);         out = std::copy_n(reinterpret_cast<const char*>(&u), sizeof(u), out);     }     void operator()(const varint_type_impl& t, const varint_type_impl::native_type* num1) {         if (num1->empty()) {             return;         }         serialize_varint(out, num1->get());     }     void operator()(const decimal_type_impl& t, const decimal_type_impl::native_type* bd1) {         if (bd1->empty()) {             return;         }         auto&& bd = std::move(*bd1).get();         auto u = net::hton(bd.scale());         out = std::copy_n(reinterpret_cast<const char*>(&u), sizeof(int32_t), out);         serialize_varint(out, bd.unscaled_value());     }     void operator()(const counter_type_impl& t, const void*) { fail(unimplemented::cause::COUNTERS); }     void operator()(const duration_type_impl& t, const duration_type_impl::native_type* m) {         if (m->empty()) {             return;         }         const auto& d = m->get();         out += signed_vint::serialize(d.months, out);         out += signed_vint::serialize(d.days, out);         out += signed_vint::serialize(d.nanoseconds, out);     }     void operator()(const list_type_impl& t, const void* value) {         serialize_list(t, value, out);     }     void operator()(const map_type_impl& t, const void* value) {         serialize_map(t, value, out);     }     void operator()(const set_type_impl& t, const void* value) {         serialize_set(t, value, out);     }     void operator()(const tuple_type_impl& t, const tuple_type_impl::native_type* value) {         return serialize_aux(t, value, out);     } }; }
 static void serialize(const abstract_type& t, const void* value, bytes::iterator& out) {     visit(t, value, serialize_visitor{out}); }
 template <FragmentedView View> data_value collection_type_impl::deserialize_impl(View v) const {     struct visitor {         View v;         ;         data_value operator()(const abstract_type&) {             on_internal_error(tlogger, "collection_type_impl::deserialize called on a non-collection type. This should be impossible.");         }         data_value operator()(const list_type_impl& t) {             return t.deserialize(v);         }         data_value operator()(const map_type_impl& t) {             return t.deserialize(v);         }         data_value operator()(const set_type_impl& t) {             return t.deserialize(v);         }     };     return ::visit(*this, visitor{v}); }
 // Explicit instantiation.
// This should be repeated for every View type passed to collection_type_impl::deserialize.
template data_value collection_type_impl::deserialize_impl<>(ser::buffer_view<bytes_ostream::fragment_iterator>) const;
 template data_value collection_type_impl::deserialize_impl<>(fragmented_temporary_buffer::view) const;
 template data_value collection_type_impl::deserialize_impl<>(single_fragmented_view) const;
 template data_value collection_type_impl::deserialize_impl<>(managed_bytes_view) const;
 template int read_collection_size(ser::buffer_view<bytes_ostream::fragment_iterator>& in);
 template ser::buffer_view<bytes_ostream::fragment_iterator> read_collection_value_nonnull(ser::buffer_view<bytes_ostream::fragment_iterator>& in);
 template <FragmentedView View> data_value deserialize_aux(const tuple_type_impl& t, View v) {     tuple_type_impl::native_type ret;     ret.reserve(t.all_types().size());     auto ti = t.all_types().begin();     while (ti != t.all_types().end() && v.size_bytes()) {         data_value obj = data_value::make_null(*ti);         std::optional<View> e = read_tuple_element(v);         if (e) {             obj = (*ti)->deserialize(*e);         }         ret.push_back(std::move(obj));         ++ti;     }     while (ti != t.all_types().end()) {         ret.push_back(data_value::make_null(*ti++));     }     return data_value::make(t.shared_from_this(), std::make_unique<tuple_type_impl::native_type>(std::move(ret))); }
 template<FragmentedView View> utils::multiprecision_int deserialize_value(const varint_type_impl&, View v) {     if (v.empty()) {         throw marshal_exception("cannot deserialize multiprecision int - empty buffer");     }     skip_empty_fragments(v);     bool negative = v.current_fragment().front() < 0;     utils::multiprecision_int num;   while (v.size_bytes()) {     for (uint8_t b : v.current_fragment()) {         if (negative) {             b = ~b;         }         num <<= 8;         num += b;     }     v.remove_current();   }     if (negative) {         num += 1;     }     return negative ? -num : num; }
 template<typename T, FragmentedView View> T deserialize_value(const floating_type_impl<T>&, View v) {     typename int_of_size<T>::itype i = read_simple<typename int_of_size<T>::itype>(v);     if (v.size_bytes()) {         throw marshal_exception(format("cannot deserialize floating - {:d} bytes left", v.size_bytes()));     }     T d;     memcpy(&d, &i, sizeof(T));     return d; }
 template<FragmentedView View> big_decimal deserialize_value(const decimal_type_impl&, View v) {     auto scale = read_simple<int32_t>(v);     auto unscaled = deserialize_value(static_cast<const varint_type_impl&>(*varint_type), v);     return big_decimal(scale, unscaled); }
 template<FragmentedView View> cql_duration deserialize_value(const duration_type_impl& t, View v) {     common_counter_type months, days, nanoseconds;     std::tie(months, days, nanoseconds) = with_linearized(v, [] (bytes_view bv) {         return deserialize_counters(bv);     });     return cql_duration(months_counter(months), days_counter(days), nanoseconds_counter(nanoseconds)); }
 template<FragmentedView View> inet_address deserialize_value(const inet_addr_type_impl&, View v) {     switch (v.size_bytes()) {     case 4:         // gah. read_simple_be, please...
        return inet_address(::in_addr{net::hton(read_simple<uint32_t>(v))});     case 16:;         ::in6_addr buf;         read_fragmented(v, sizeof(buf), reinterpret_cast<bytes::value_type*>(&buf));         return inet_address(buf);     default:         throw marshal_exception(format("cannot deserialize inet_address, unsupported size {:d} bytes", v.size_bytes()));     } }
 template<FragmentedView View> utils::UUID deserialize_value(const uuid_type_impl&, View v) {     auto msb = read_simple<uint64_t>(v);     auto lsb = read_simple<uint64_t>(v);     if (v.size_bytes()) {         throw marshal_exception(format("cannot deserialize uuid, {:d} bytes left", v.size_bytes()));     }     return utils::UUID(msb, lsb); }
 template<FragmentedView View> utils::UUID deserialize_value(const timeuuid_type_impl&, View v) {     return deserialize_value(static_cast<const uuid_type_impl&>(*uuid_type), v); }
 template<FragmentedView View> db_clock::time_point deserialize_value(const timestamp_date_base_class&, View v) {     auto v2 = read_simple_exactly<uint64_t>(v);     return db_clock::time_point(db_clock::duration(v2)); }
 template<FragmentedView View> uint32_t deserialize_value(const simple_date_type_impl&, View v) {     return read_simple_exactly<uint32_t>(v); }
 template<FragmentedView View> int64_t deserialize_value(const time_type_impl&, View v) {     return read_simple_exactly<int64_t>(v); }
 template<FragmentedView View> bool deserialize_value(const boolean_type_impl&, View v) {     if (v.size_bytes() != 1) {         throw marshal_exception(format("cannot deserialize boolean, size mismatch ({:d})", v.size_bytes()));     }     skip_empty_fragments(v);     return v.current_fragment().front() != 0; }
 template<typename T, FragmentedView View> T deserialize_value(const integer_type_impl<T>& t, View v) {     return read_simple_exactly<T>(v); }
 template<FragmentedView View> sstring deserialize_value(const string_type_impl&, View v) {     // FIXME: validation?
    sstring buf(sstring::initialized_later(), v.size_bytes());     auto out = buf.begin();     while (v.size_bytes()) {         out = std::copy(v.current_fragment().begin(), v.current_fragment().end(), out);         v.remove_current();     }     return buf; }
 template<typename T> decltype(auto) deserialize_value(const T& t, bytes_view v) {     return deserialize_value(t, single_fragmented_view(v)); }
 namespace { template <FragmentedView View> struct deserialize_visitor {     View v;     data_value operator()(const reversed_type_impl& t) { return t.underlying_type()->deserialize(v); }     template <typename T> data_value operator()(const T& t) {         if (!v.size_bytes()) {             return t.make_empty();         }         return t.make_value(deserialize_value(t, v));     }     data_value operator()(const ascii_type_impl& t) {          return t.make_value(deserialize_value(t, v));     }     data_value operator()(const utf8_type_impl& t) {          return t.make_value(deserialize_value(t, v));     }     data_value operator()(const bytes_type_impl& t) {         return t.make_value(std::make_unique<bytes_type_impl::native_type>(linearized(v)));     }     data_value operator()(const counter_type_impl& t) {         return static_cast<const long_type_impl&>(*long_type).make_value(read_simple_exactly<int64_t>(v));     }     data_value operator()(const list_type_impl& t) {         return t.deserialize(v);     }     data_value operator()(const map_type_impl& t) {         return t.deserialize(v);     }     data_value operator()(const set_type_impl& t) {         return t.deserialize(v);     }     data_value operator()(const tuple_type_impl& t) { return deserialize_aux(t, v); }     data_value operator()(const user_type_impl& t) { return deserialize_aux(t, v); }     data_value operator()(const empty_type_impl& t) { return data_value(empty_type_representation()); } }; }
 template <FragmentedView View> data_value abstract_type::deserialize_impl(View v) const {     return visit(*this, deserialize_visitor<View>{v}); }
 // Explicit instantiation.
// This should be repeated for every type passed to deserialize().
template data_value abstract_type::deserialize_impl<>(fragmented_temporary_buffer::view) const;
 template data_value abstract_type::deserialize_impl<>(single_fragmented_view) const;
 template data_value abstract_type::deserialize_impl<>(ser::buffer_view<bytes_ostream::fragment_iterator>) const;
 template data_value abstract_type::deserialize_impl<>(managed_bytes_view) const;
 std::strong_ordering compare_aux(const tuple_type_impl& t, const managed_bytes_view& v1, const managed_bytes_view& v2) {     // This is a slight modification of lexicographical_tri_compare:
    // when the only difference between the tuples is that one of them has additional trailing nulls,
    // we consider them equal. For example, in the following CQL scenario:
    // 1. create type ut (a int);
    // 2. create table cf (a int primary key, b frozen<ut>);
    // 3. insert into cf (a, b) values (0, (0));
    // 4. alter type ut add b int;
    // 5. select * from cf where b = {a:0,b:null};
    // the row with a = 0 should be returned, even though the value stored in the database is shorter
    // (by one null) than the value given by the user.
    auto types_first = t.all_types().begin();     auto types_last = t.all_types().end();     auto first1 = tuple_deserializing_iterator::start(v1);     auto last1 = tuple_deserializing_iterator::finish(v1);     auto first2 = tuple_deserializing_iterator::start(v2);     auto last2 = tuple_deserializing_iterator::finish(v2);     while (types_first != types_last && first1 != last1 && first2 != last2) {         if (auto c = tri_compare_opt(*types_first, *first1, *first2); c != 0) {             return c;         }         ++first1;         ++first2;         ++types_first;     }     while (types_first != types_last && first1 != last1) {         if (*first1) {             return std::strong_ordering::greater;         }         ++first1;         ++types_first;     }     while (types_first != types_last && first2 != last2) {         if (*first2) {             return std::strong_ordering::less;         }         ++first2;         ++types_first;     }     return std::strong_ordering::equal; }
 namespace { struct compare_visitor {     managed_bytes_view v1;     managed_bytes_view v2;     template <std::invocable<> Func>     requires std::same_as<std::strong_ordering, std::invoke_result_t<Func>>     std::strong_ordering with_empty_checks(Func func) {         if (v1.empty()) {             return v2.empty() ? std::strong_ordering::equal : std::strong_ordering::less;         }         if (v2.empty()) {             return std::strong_ordering::greater;         }         return func();     }     template <typename T> std::strong_ordering operator()(const simple_type_impl<T>&) {       return with_empty_checks([&] {         T a = simple_type_traits<T>::read_nonempty(v1);         T b = simple_type_traits<T>::read_nonempty(v2);         return a <=> b;       });     }     std::strong_ordering operator()(const string_type_impl&) { return compare_unsigned(v1, v2); }     std::strong_ordering operator()(const bytes_type_impl&) { return compare_unsigned(v1, v2); }     std::strong_ordering operator()(const duration_type_impl&) { return compare_unsigned(v1, v2); }     std::strong_ordering operator()(const inet_addr_type_impl&) { return compare_unsigned(v1, v2); }     std::strong_ordering operator()(const date_type_impl&) {         // This is not the same behaviour as timestamp_type_impl
        return compare_unsigned(v1, v2);     }     std::strong_ordering operator()(const timeuuid_type_impl&) {       return with_empty_checks([&] {         return with_linearized(v1, [&] (bytes_view v1) {             return with_linearized(v2, [&] (bytes_view v2) {                 return utils::timeuuid_tri_compare(v1, v2);             });         });       });     }     std::strong_ordering operator()(const listlike_collection_type_impl& l) {         using llpdi = listlike_partial_deserializing_iterator;         return with_empty_checks([&] {             return std::lexicographical_compare_three_way(llpdi::begin(v1), llpdi::end(v1), llpdi::begin(v2),                     llpdi::end(v2),                     [&] (const managed_bytes_view_opt& o1, const managed_bytes_view_opt& o2) {                         if (!o1.has_value() || !o2.has_value()) {                             return o1.has_value() <=> o2.has_value();                         } else {                             return l.get_elements_type()->compare(*o1, *o2);                         }             });         });     }     std::strong_ordering operator()(const map_type_impl& m) {         return map_type_impl::compare_maps(m.get_keys_type(), m.get_values_type(), v1, v2);     }     std::strong_ordering operator()(const uuid_type_impl&) {         if (v1.size() < 16) {             return v2.size() < 16 ? std::strong_ordering::equal : std::strong_ordering::less;         }         if (v2.size() < 16) {             return std::strong_ordering::greater;         }         auto c1 = (v1[6] >> 4) & 0x0f;         auto c2 = (v2[6] >> 4) & 0x0f;         if (c1 != c2) {             return c1 <=> c2;         }         if (c1 == 1) {             return with_linearized(v1, [&] (bytes_view v1) {                 return with_linearized(v2, [&] (bytes_view v2) {                     return utils::uuid_tri_compare_timeuuid(v1, v2);                 });             });         }         return compare_unsigned(v1, v2);     }     std::strong_ordering operator()(const empty_type_impl&) { return std::strong_ordering::equal; }     std::strong_ordering operator()(const tuple_type_impl& t) { return compare_aux(t, v1, v2); }     std::strong_ordering operator()(const counter_type_impl&) {         // untouched (empty) counter evaluates as 0
        const auto a = v1.empty() ? 0 : simple_type_traits<int64_t>::read_nonempty(v1);         const auto b = v2.empty() ? 0 : simple_type_traits<int64_t>::read_nonempty(v2);         return a <=> b;     }     std::strong_ordering operator()(const decimal_type_impl& d) {       return with_empty_checks([&] {         auto a = deserialize_value(d, v1);         auto b = deserialize_value(d, v2);         return a <=> b;       });     }     std::strong_ordering operator()(const varint_type_impl& v) {       return with_empty_checks([&] {         auto a = deserialize_value(v, v1);         auto b = deserialize_value(v, v2);         return a == b ? std::strong_ordering::equal : a < b ? std::strong_ordering::less : std::strong_ordering::greater;       });     }     template <typename T> std::strong_ordering operator()(const floating_type_impl<T>&) {       return with_empty_checks([&] {         T a = simple_type_traits<T>::read_nonempty(v1);         T b = simple_type_traits<T>::read_nonempty(v2);         // in java world NaN == NaN and NaN is greater than anything else
        if (std::isnan(a) && std::isnan(b)) {             return std::strong_ordering::equal;         } else if (std::isnan(a)) {             return std::strong_ordering::greater;         } else if (std::isnan(b)) {             return std::strong_ordering::less;         }         // also -0 < 0
        if (std::signbit(a) && !std::signbit(b)) {             return std::strong_ordering::less;         } else if (!std::signbit(a) && std::signbit(b)) {             return std::strong_ordering::greater;         }         // note: float <=> returns std::partial_ordering
        return a == b ? std::strong_ordering::equal : a < b ? std::strong_ordering::less : std::strong_ordering::greater;       });     }     std::strong_ordering operator()(const reversed_type_impl& r) { return r.underlying_type()->compare(v2, v1); } }; }
 std::strong_ordering abstract_type::compare(bytes_view v1, bytes_view v2) const {     return compare(managed_bytes_view(v1), managed_bytes_view(v2)); }
 std::strong_ordering abstract_type::compare(managed_bytes_view v1, managed_bytes_view v2) const {     try {         return visit(*this, compare_visitor{v1, v2});     } catch (const marshal_exception&) {         on_types_internal_error(std::current_exception());     } }
 std::strong_ordering abstract_type::compare(managed_bytes_view v1, bytes_view v2) const {     return compare(v1, managed_bytes_view(v2)); }
 std::strong_ordering abstract_type::compare(bytes_view v1, managed_bytes_view v2) const {     return compare(managed_bytes_view(v1), v2); }
 bool abstract_type::equal(bytes_view v1, bytes_view v2) const {     return ::visit(*this, [&](const auto& t) {         if (is_byte_order_equal_visitor{}(t)) {             return compare_unsigned(v1, v2) == 0;         }         return compare_visitor{v1, v2}(t) == 0;     }); }
 bool abstract_type::equal(managed_bytes_view v1, managed_bytes_view v2) const {     return ::visit(*this, [&](const auto& t) {         if (is_byte_order_equal_visitor{}(t)) {             return compare_unsigned(v1, v2) == 0;         }         return compare_visitor{v1, v2}(t) == 0;     }); }
 bool abstract_type::equal(managed_bytes_view v1, bytes_view v2) const {     return equal(v1, managed_bytes_view(v2)); }
 bool abstract_type::equal(bytes_view v1, managed_bytes_view v2) const {     return equal(managed_bytes_view(v1), v2); }
 // Count number of ':' which are not preceded by '\'.
static std::size_t count_segments(sstring_view v) {     std::size_t segment_count = 1;     char prev_ch = '.';     for (char ch : v) {         if (ch == ':' && prev_ch != '\\') {             ++segment_count;         }         prev_ch = ch;     }     return segment_count; }
 // Split on ':', unless it's preceded by '\'.
static std::vector<sstring_view> split_field_strings(sstring_view v) {     if (v.empty()) {         return std::vector<sstring_view>();     }     std::vector<sstring_view> result;     result.reserve(count_segments(v));     std::size_t prev = 0;     char prev_ch = '.';     for (std::size_t i = 0; i < v.size(); ++i) {         if (v[i] == ':' && prev_ch != '\\') {             result.push_back(v.substr(prev, i - prev));             prev = i + 1;         }         prev_ch = v[i];     }     result.push_back(v.substr(prev, v.size() - prev));     return result; }
 // Replace "\:" with ":" and "\@" with "@".
static std::string unescape(sstring_view s) {     return boost::regex_replace(std::string(s), boost::regex("\\\\([@:])"), "$1"); }
 // Replace ":" with "\:" and "@" with "\@".
static std::string escape(sstring_view s) {     return boost::regex_replace(std::string(s), boost::regex("[@:]"), "\\\\$0"); }
 // Concat list of bytes into a single bytes.
static bytes concat_fields(const std::vector<bytes>& fields, const std::vector<int32_t> field_len) {     std::size_t result_size = 4 * fields.size();     for (int32_t len : field_len) {         result_size += len > 0 ? len : 0;     }     bytes result{bytes::initialized_later(), result_size};     bytes::iterator it = result.begin();     for (std::size_t i = 0; i < fields.size(); ++i) {         int32_t tmp = net::hton(field_len[i]);         it = std::copy_n(reinterpret_cast<const int8_t*>(&tmp), sizeof(tmp), it);         if (field_len[i] > 0) {             it = std::copy(std::begin(fields[i]), std::end(fields[i]), it);         }     }     return result; }
 size_t abstract_type::hash(bytes_view v) const {     return hash(managed_bytes_view(v)); }
 size_t abstract_type::hash(managed_bytes_view v) const {     struct visitor {         managed_bytes_view v;         size_t operator()(const reversed_type_impl& t) { return t.underlying_type()->hash(v); }         size_t operator()(const abstract_type& t) { return std::hash<managed_bytes_view>()(v); }         size_t operator()(const tuple_type_impl& t) {             auto apply_hash = [] (auto&& type_value) {                 auto&& type = boost::get<0>(type_value);                 auto&& value = boost::get<1>(type_value);                 return value ? type->hash(*value) : 0;             };             // FIXME: better accumulation function
            return boost::accumulate(combine(t.all_types(), t.make_range(v)) | boost::adaptors::transformed(apply_hash),                     0, std::bit_xor<>());         }         size_t operator()(const varint_type_impl& t) {             return std::hash<sstring>()(with_linearized(v, [&] (bytes_view bv) { return t.to_string(bv); }));         }         size_t operator()(const decimal_type_impl& t) {             return std::hash<sstring>()(with_linearized(v, [&] (bytes_view bv) { return t.to_string(bv); }));         }         size_t operator()(const counter_type_impl&) { fail(unimplemented::cause::COUNTERS); }         size_t operator()(const empty_type_impl&) { return 0; }     };     return visit(*this, visitor{v}); }
 static size_t concrete_serialized_size(const byte_type_impl::native_type&) { return sizeof(int8_t); }
 static size_t concrete_serialized_size(const short_type_impl::native_type&) { return sizeof(int16_t); }
 static size_t concrete_serialized_size(const int32_type_impl::native_type&) { return sizeof(int32_t); }
 static size_t concrete_serialized_size(const long_type_impl::native_type&) { return sizeof(int64_t); }
 static size_t concrete_serialized_size(const float_type_impl::native_type&) { return sizeof(float); }
 static size_t concrete_serialized_size(const double_type_impl::native_type&) { return sizeof(double); }
 static size_t concrete_serialized_size(const boolean_type_impl::native_type&) { return 1; }
 static size_t concrete_serialized_size(const timestamp_date_base_class::native_type&) { return 8; }
 static size_t concrete_serialized_size(const timeuuid_type_impl::native_type&) { return 16; }
 static size_t concrete_serialized_size(const simple_date_type_impl::native_type&) { return 4; }
 static size_t concrete_serialized_size(const string_type_impl::native_type& v) { return v.size(); }
 static size_t concrete_serialized_size(const bytes_type_impl::native_type& v) { return v.size(); }
 static size_t concrete_serialized_size(const inet_addr_type_impl::native_type& v) { return v.get().size(); }
 static size_t concrete_serialized_size_aux(const boost::multiprecision::cpp_int& num) {     if (num) {         return align_up(boost::multiprecision::msb(num) + 2, 8u) / 8;     } else {         return 1;     } }
 static size_t concrete_serialized_size(const boost::multiprecision::cpp_int& num) {     if (num < 0) {         return concrete_serialized_size_aux(-num - 1);     }     return concrete_serialized_size_aux(num); }
 static size_t concrete_serialized_size(const utils::multiprecision_int& num) {     return concrete_serialized_size(static_cast<const boost::multiprecision::cpp_int&>(num)); }
 static size_t concrete_serialized_size(const varint_type_impl::native_type& v) {     return concrete_serialized_size(v.get()); }
 static size_t concrete_serialized_size(const decimal_type_impl::native_type& v) {     const boost::multiprecision::cpp_int& uv = v.get().unscaled_value();     return sizeof(int32_t) + concrete_serialized_size(uv); }
 static size_t concrete_serialized_size(const duration_type_impl::native_type& v) {     const auto& d = v.get();     return signed_vint::serialized_size(d.months) + signed_vint::serialized_size(d.days) +            signed_vint::serialized_size(d.nanoseconds); }
 static size_t concrete_serialized_size(const tuple_type_impl::native_type& v) {     size_t len = 0;     for (auto&& e : v) {         len += 4 + e.serialized_size();     }     return len; }
 static size_t serialized_size(const abstract_type& t, const void* value);
 namespace { struct serialized_size_visitor {     size_t operator()(const reversed_type_impl& t, const void* v) { return serialized_size(*t.underlying_type(), v); }     size_t operator()(const empty_type_impl&, const void*) { return 0; }     template <typename T>     size_t operator()(const concrete_type<T>& t, const typename concrete_type<T>::native_type* v) {         if (v->empty()) {             return 0;         }         return concrete_serialized_size(*v);     }     size_t operator()(const counter_type_impl&, const void*) { fail(unimplemented::cause::COUNTERS); }     size_t operator()(const map_type_impl& t, const map_type_impl::native_type* v) { return map_serialized_size(v); }     size_t operator()(const concrete_type<std::vector<data_value>, listlike_collection_type_impl>& t,             const std::vector<data_value>* v) {         return listlike_serialized_size(v);     } }; }
 static size_t serialized_size(const abstract_type& t, const void* value) {     return visit(t, value, serialized_size_visitor{}); }
 template<typename T> static bytes serialize_value(const T& t, const typename T::native_type& v) {     bytes b(bytes::initialized_later(), serialized_size_visitor{}(t, &v));     auto i = b.begin();     serialize_visitor{i}(t, &v);     return b; }
 seastar::net::inet_address inet_addr_type_impl::from_sstring(sstring_view s) {     try {         return inet_address(std::string(s.data(), s.size()));     } catch (...) {         throw marshal_exception(format("Failed to parse inet_addr from '{}'", s));     } }
 utils::UUID uuid_type_impl::from_sstring(sstring_view s) {     static const boost::regex re("^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$");     if (!boost::regex_match(s.begin(), s.end(), re)) {         throw marshal_exception(format("Cannot parse uuid from '{}'", s));     }     return utils::UUID(s); }
 utils::UUID timeuuid_type_impl::from_sstring(sstring_view s) {     static const boost::regex re("^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$");     if (!boost::regex_match(s.begin(), s.end(), re)) {         throw marshal_exception(format("Invalid UUID format ({})", s));     }     utils::UUID v(s);     if (v.version() != 1) {         throw marshal_exception(format("Unsupported UUID version ({:d})", v.version()));     }     return v; }
 namespace { struct from_string_visitor {     sstring_view s;     bytes operator()(const reversed_type_impl& r) { return r.underlying_type()->from_string(s); }     template <typename T> bytes operator()(const integer_type_impl<T>& t) { return decompose_value(parse_int(t, s)); }          bytes operator()(const string_type_impl&) {         return to_bytes(bytes_view(reinterpret_cast<const int8_t*>(s.begin()), s.size()));     }     bytes operator()(const bytes_type_impl&) { return from_hex(s); }     bytes operator()(const boolean_type_impl& t) {         sstring s_lower(s.begin(), s.end());         std::transform(s_lower.begin(), s_lower.end(), s_lower.begin(), ::tolower);         bool v;         if (s.empty() || s_lower == "false") {             v = false;         } else if (s_lower == "true") {             v = true;         } else {             throw marshal_exception(format("unable to make boolean from '{}'", s));         }         return serialize_value(t, v);     }     bytes operator()(const timeuuid_type_impl&) {         if (s.empty()) {             return bytes();         }         return timeuuid_type_impl::from_sstring(s).serialize();     }     bytes operator()(const timestamp_date_base_class& t) {         if (s.empty()) {             return bytes();         }         return serialize_value(t, timestamp_type_impl::from_sstring(s));     }     bytes operator()(const simple_date_type_impl& t) {         if (s.empty()) {             return bytes();         }         return serialize_value(t, simple_date_type_impl::from_sstring(s));     }     bytes operator()(const time_type_impl& t) {         if (s.empty()) {             return bytes();         }         return serialize_value(t, time_type_impl::from_sstring(s));     }     bytes operator()(const uuid_type_impl&) {         if (s.empty()) {             return bytes();         }         return uuid_type_impl::from_sstring(s).serialize();     }     template <typename T> bytes operator()(const floating_type_impl<T>& t) {         if (s.empty()) {             return bytes();         }         try {             auto d = boost::lexical_cast<T>(s.begin(), s.size());             return serialize_value(t, d);         } catch (const boost::bad_lexical_cast& e) {             throw marshal_exception(format("Invalid number format '{}'", s));         }     }     bytes operator()(const varint_type_impl& t) {         if (s.empty()) {             return bytes();         }         try {             std::string str(s.begin(), s.end());             varint_type_impl::native_type num(str);             return serialize_value(t, num);         } catch (...) {             throw marshal_exception(format("unable to make int from '{}'", s));         }     }     bytes operator()(const decimal_type_impl& t) {         if (s.empty()) {             return bytes();         }         try {             decimal_type_impl::native_type bd(s);             return serialize_value(t, bd);         } catch (...) {             throw marshal_exception(format("unable to make BigDecimal from '{}'", s));         }     }     bytes operator()(const counter_type_impl&) {         fail(unimplemented::cause::COUNTERS);         return bytes();     }     bytes operator()(const duration_type_impl& t) {         if (s.empty()) {             return bytes();         }         try {             return serialize_value(t, cql_duration(s));         } catch (cql_duration_error const& e) {             throw marshal_exception(e.what());         }     }     bytes operator()(const empty_type_impl&) { return bytes(); }     bytes operator()(const inet_addr_type_impl& t) {         // FIXME: support host names
        if (s.empty()) {             return bytes();         }         return serialize_value(t, t.from_sstring(s));     }     bytes operator()(const tuple_type_impl& t) {         std::vector<sstring_view> field_strings = split_field_strings(s);         if (field_strings.size() > t.size()) {             throw marshal_exception(                     format("Invalid tuple literal: too many elements. Type {} expects {:d} but got {:d}",                             t.as_cql3_type(), t.size(), field_strings.size()));         }         std::vector<bytes> fields(field_strings.size());         std::vector<int32_t> field_len(field_strings.size(), -1);         for (std::size_t i = 0; i < field_strings.size(); ++i) {             if (field_strings[i] != "@") {                 std::string field_string = unescape(field_strings[i]);                 fields[i] = t.type(i)->from_string(field_string);                 field_len[i] = fields[i].size();             }         }         return concat_fields(fields, field_len);     }     bytes operator()(const collection_type_impl&) {         // FIXME:
        abort();         return bytes();     } }; }
 bytes abstract_type::from_string(sstring_view s) const { return visit(*this, from_string_visitor{s}); }
  template <typename N, typename A, typename F> static sstring format_if_not_empty(         const concrete_type<N, A>& type, const typename concrete_type<N, A>::native_type* b, F&& f) {     if (b->empty()) {         return {};     }     return f(static_cast<const N&>(*b)); }
 static sstring to_string_impl(const abstract_type& t, const void* v);
 namespace { struct to_string_impl_visitor {     template <typename T>     sstring operator()(const concrete_type<T>& t, const typename concrete_type<T>::native_type* v) {         return format_if_not_empty(t, v, [] (const T& v) { return to_sstring(v); });     }               sstring operator()(const counter_type_impl& c, const void*) { fail(unimplemented::cause::COUNTERS); }                    sstring operator()(const empty_type_impl&, const void*) { return sstring(); }          sstring operator()(const list_type_impl& l, const list_type_impl::native_type* v) {         return format_if_not_empty(                 l, v, [] (const list_type_impl::native_type& v) { return vector_to_string(v, ", "); });     }     sstring operator()(const set_type_impl& s, const set_type_impl::native_type* v) {         return format_if_not_empty(s, v, [] (const set_type_impl::native_type& v) { return vector_to_string(v, "; "); });     }     sstring operator()(const map_type_impl& m, const map_type_impl::native_type* v) {         return format_if_not_empty(                 m, v, [&m] (const map_type_impl::native_type& v) { return map_to_string(v, !m.is_multi_cell()); });     }     sstring operator()(const reversed_type_impl& r, const void* v) { return to_string_impl(*r.underlying_type(), v); }                                    }; }
 static sstring to_string_impl(const abstract_type& t, const void* v) {     return visit(t, v, to_string_impl_visitor{}); }
 sstring abstract_type::to_string_impl(const data_value& v) const {     return ::to_string_impl(*this, get_value_ptr(v)); }
 static bool check_compatibility(const tuple_type_impl &t, const abstract_type& previous, bool (abstract_type::*predicate)(const abstract_type&) const) {     auto* x = dynamic_cast<const tuple_type_impl*>(&previous);     if (!x) {         return false;     }     auto c = std::mismatch(                 t.all_types().begin(), t.all_types().end(),                 x->all_types().begin(), x->all_types().end(),                 [predicate] (data_type a, data_type b) { return ((*a).*predicate)(*b); });     return c.second == x->all_types().end();  // this allowed to be longer
}
   // Needed to handle ReversedType in value-compatibility checks.
static bool is_value_compatible_with_internal(const abstract_type& t, const abstract_type& other) {     struct visitor {         const abstract_type& other;         bool operator()(const abstract_type& t) { return t.is_compatible_with(other); }                                                                                      };     return visit(t, visitor{other}); }
 bool abstract_type::is_value_compatible_with(const abstract_type& other) const {     return is_value_compatible_with_internal(*this, *other.underlying_type()); }
 std::optional<size_t> user_type_impl::idx_of_field(const bytes& name) const {     for (size_t i = 0; i < _field_names.size(); ++i) {         if (name == _field_names[i]) {             return {i};         }     }     return {}; }
 shared_ptr<const user_type_impl> user_type_impl::get_instance(sstring keyspace, bytes name,         std::vector<bytes> field_names, std::vector<data_type> field_types, bool multi_cell) {     return intern::get_instance(std::move(keyspace), std::move(name), std::move(field_names), std::move(field_types), multi_cell); }
 sstring tuple_type_impl::make_name(const std::vector<data_type>& types) {     // To keep format compatibility with Origin we never wrap
    // tuple name into
    // "org.apache.cassandra.db.marshal.FrozenType(...)".
    // Even when the tuple is frozen.
    // For more details see #4087
    return format("org.apache.cassandra.db.marshal.TupleType({})", fmt::join(types | boost::adaptors::transformed(std::mem_fn(&abstract_type::name)), ", ")); }
 static std::optional<std::vector<data_type>> update_types(const std::vector<data_type> types, const user_type updated) {     std::optional<std::vector<data_type>> new_types = std::nullopt;     for (uint32_t i = 0; i < types.size(); ++i) {         auto&& ut = types[i]->update_user_type(updated);         if (ut) {             if (!new_types) {                 new_types = types;             }             (*new_types)[i] = std::move(*ut);         }     }     return new_types; }
 static std::optional<data_type> update_user_type_aux(         const tuple_type_impl& t, const shared_ptr<const user_type_impl> updated) {     if (auto new_types = update_types(t.all_types(), updated)) {         return std::make_optional(tuple_type_impl::get_instance(std::move(*new_types)));     }     return std::nullopt; }
 namespace { struct native_value_clone_visitor {     const void* from;     void* operator()(const reversed_type_impl& t) {         return visit(*t.underlying_type(), native_value_clone_visitor{from});     }     template <typename N, typename A> void* operator()(const concrete_type<N, A>&) {         using nt = typename concrete_type<N, A>::native_type;         return new nt(*reinterpret_cast<const nt*>(from));     }     void* operator()(const counter_type_impl&) { fail(unimplemented::cause::COUNTERS); }     void* operator()(const empty_type_impl&) {         return new empty_type_representation();     } }; }
 void* abstract_type::native_value_clone(const void* from) const {     return visit(*this, native_value_clone_visitor{from}); }
 namespace { struct native_value_delete_visitor {     void* object;     template <typename N, typename A> void operator()(const concrete_type<N, A>&) {         delete reinterpret_cast<typename concrete_type<N, A>::native_type*>(object);     }     void operator()(const reversed_type_impl& t) {         return visit(*t.underlying_type(), native_value_delete_visitor{object});     }     void operator()(const counter_type_impl&) { fail(unimplemented::cause::COUNTERS); }     void operator()(const empty_type_impl&) {         delete reinterpret_cast<empty_type_representation*>(object);     } }; }
 static void native_value_delete(const abstract_type& t, void* object) {     visit(t, native_value_delete_visitor{object}); }
 namespace { struct native_typeid_visitor {     template <typename N, typename A> const std::type_info& operator()(const concrete_type<N, A>&) {         return typeid(typename concrete_type<N, A>::native_type);     }     const std::type_info& operator()(const reversed_type_impl& t) {         return visit(*t.underlying_type(), native_typeid_visitor{});     }     const std::type_info& operator()(const counter_type_impl&) { fail(unimplemented::cause::COUNTERS); }     const std::type_info& operator()(const empty_type_impl&) {         // Can't happen
        abort();     } }; }
 const std::type_info& abstract_type::native_typeid() const {     return visit(*this, native_typeid_visitor{}); }
 bytes abstract_type::decompose(const data_value& value) const {     if (!value._value) {         return {};     }     bytes b(bytes::initialized_later(), serialized_size(*this, value._value));     auto i = b.begin();     value.serialize(i);     return b; }
  size_t data_value::serialized_size() const {     if (!_value) {         return 0;     }     return ::serialized_size(*_type, _value); }
 void data_value::serialize(bytes::iterator& out) const {     if (_value) {         ::serialize(*_type, _value, out);     } }
 bytes_opt data_value::serialize() const {     if (!_value) {         return std::nullopt;     }     bytes b(bytes::initialized_later(), serialized_size());     auto i = b.begin();     serialize(i);     return b; }
 bytes data_value::serialize_nonnull() const {     if (!_value) {         on_internal_error(tlogger, "serialize_nonnull called on null");     }     return std::move(*serialize()); }
 sstring abstract_type::get_string(const bytes& b) const {     struct visitor {         const bytes& b;         sstring operator()(const abstract_type& t) {             t.validate(b);             return t.to_string(b);         }         sstring operator()(const reversed_type_impl& r) { return r.underlying_type()->get_string(b); }     };     return visit(*this, visitor{b}); }
 sstring user_type_impl::get_name_as_string() const {     auto real_utf8_type = static_cast<const utf8_type_impl*>(utf8_type.get());     return ::deserialize_value(*real_utf8_type, _name); }
 sstring user_type_impl::get_name_as_cql_string() const {     return cql3::util::maybe_quote(get_name_as_string()); }
 std::ostream& user_type_impl::describe(std::ostream& os) const {     os << "CREATE TYPE " << cql3::util::maybe_quote(_keyspace) << "." << get_name_as_cql_string() << " (\n";     for (size_t i = 0; i < _string_field_names.size(); i++) {         os << "    " << cql3::util::maybe_quote(_string_field_names[i]) << " " << _types[i]->cql3_type_name();         if (i < _string_field_names.size() - 1) {             os << ",";         }         os << "\n";     }     os << ");";     return os; }
 data_type user_type_impl::freeze() const {     if (_is_multi_cell) {         return get_instance(_keyspace, _name, _field_names, _types, false);     } else {         return shared_from_this();     } }
 sstring user_type_impl::make_name(sstring keyspace,                           bytes name,                           std::vector<bytes> field_names,                           std::vector<data_type> field_types,                           bool is_multi_cell) {     std::ostringstream os;     if (!is_multi_cell) {         os << "org.apache.cassandra.db.marshal.FrozenType(";     }     for (size_t i = 0; i < field_names.size(); ++i) {         os << ",";         os << field_types[i]->name(); // FIXME: ignore frozen<>
    }     os << ")";     if (!is_multi_cell) {         os << ")";     }     return os.str(); }
  std::optional<data_type> abstract_type::update_user_type(const shared_ptr<const user_type_impl> updated) const {     struct visitor {         const shared_ptr<const user_type_impl> updated;         std::optional<data_type> operator()(const abstract_type&) { return std::nullopt; }         std::optional<data_type> operator()(const empty_type_impl&) {             return std::nullopt;         }         std::optional<data_type> operator()(const reversed_type_impl& r) {             return r.underlying_type()->update_user_type(updated);         }         std::optional<data_type> operator()(const user_type_impl& u) { return update_user_type_aux(u, updated); }         std::optional<data_type> operator()(const tuple_type_impl& t) { return update_user_type_aux(t, updated); }         std::optional<data_type> operator()(const map_type_impl& m) { return update_user_type_aux(m, updated); }         std::optional<data_type> operator()(const set_type_impl& s) {             return update_listlike(s, set_type_impl::get_instance, updated);         }         std::optional<data_type> operator()(const list_type_impl& l) {             return update_listlike(l, list_type_impl::get_instance, updated);         }     };     return visit(*this, visitor{updated}); }
 static bytes_ostream serialize_for_cql_aux(const map_type_impl&, collection_mutation_view_description mut) {     bytes_ostream out;     auto len_slot = out.write_place_holder(collection_size_len());     int elements = 0;     for (auto&& e : mut.cells) {         if (e.second.is_live(mut.tomb, false)) {             write_collection_value(out, atomic_cell_value_view(e.first));             write_collection_value(out, e.second.value());             elements += 1;         }     }     write_collection_size(len_slot, elements);     return out; }
 static bytes_ostream serialize_for_cql_aux(const set_type_impl&, collection_mutation_view_description mut) {     bytes_ostream out;     auto len_slot = out.write_place_holder(collection_size_len());     int elements = 0;     for (auto&& e : mut.cells) {         if (e.second.is_live(mut.tomb, false)) {             write_collection_value(out, atomic_cell_value_view(e.first));             elements += 1;         }     }     write_collection_size(len_slot, elements);     return out; }
 static bytes_ostream serialize_for_cql_aux(const list_type_impl&, collection_mutation_view_description mut) {     bytes_ostream out;     auto len_slot = out.write_place_holder(collection_size_len());     int elements = 0;     for (auto&& e : mut.cells) {         if (e.second.is_live(mut.tomb, false)) {             write_collection_value(out, e.second.value());             elements += 1;         }     }     write_collection_size(len_slot, elements);     return out; }
 static bytes_ostream serialize_for_cql_aux(const user_type_impl& type, collection_mutation_view_description mut) {     assert(type.is_multi_cell());     assert(mut.cells.size() <= type.size());     bytes_ostream out;     size_t curr_field_pos = 0;     for (auto&& e : mut.cells) {         auto field_pos = deserialize_field_index(e.first);         assert(field_pos < type.size());         // Some fields don't have corresponding cells -- these fields are null.
        while (curr_field_pos < field_pos) {             write_simple<int32_t>(out, int32_t(-1));             ++curr_field_pos;         }         if (e.second.is_live(mut.tomb, false)) {             auto value = e.second.value();             write_simple<int32_t>(out, int32_t(value.size_bytes()));             for (auto&& frag : fragment_range(value)) {                 out.write(frag);             }         } else {             write_simple<int32_t>(out, int32_t(-1));         }         ++curr_field_pos;     }     // Trailing null fields
    while (curr_field_pos < type.size()) {         write_simple<int32_t>(out, int32_t(-1));         ++curr_field_pos;     }     return out; }
  bytes serialize_field_index(size_t idx) {     if (idx >= size_t(std::numeric_limits<int16_t>::max())) {         // should've been rejected earlier, but just to be sure...
        throw std::runtime_error(format("index for user type field too large: {}", idx));     }     bytes b(bytes::initialized_later(), sizeof(int16_t));     write_be(reinterpret_cast<char*>(b.data()), static_cast<int16_t>(idx));     return b; }
 size_t deserialize_field_index(const bytes_view& b) {     assert(b.size() == sizeof(int16_t));     return read_be<int16_t>(reinterpret_cast<const char*>(b.data())); }
 size_t deserialize_field_index(managed_bytes_view b) {     assert(b.size_bytes() == sizeof(int16_t));     return be_to_cpu(read_simple_native<int16_t>(b)); }
 thread_local const shared_ptr<const abstract_type> byte_type(make_shared<byte_type_impl>());
 thread_local const shared_ptr<const abstract_type> short_type(make_shared<short_type_impl>());
 thread_local const shared_ptr<const abstract_type> int32_type(make_shared<int32_type_impl>());
 thread_local const shared_ptr<const abstract_type> long_type(make_shared<long_type_impl>());
 thread_local const shared_ptr<const abstract_type> ascii_type(make_shared<ascii_type_impl>());
 thread_local const shared_ptr<const abstract_type> bytes_type(make_shared<bytes_type_impl>());
 thread_local const shared_ptr<const abstract_type> utf8_type(make_shared<utf8_type_impl>());
 thread_local const shared_ptr<const abstract_type> boolean_type(make_shared<boolean_type_impl>());
 thread_local const shared_ptr<const abstract_type> date_type(make_shared<date_type_impl>());
 thread_local const shared_ptr<const abstract_type> timeuuid_type(make_shared<timeuuid_type_impl>());
 thread_local const shared_ptr<const abstract_type> timestamp_type(make_shared<timestamp_type_impl>());
 thread_local const shared_ptr<const abstract_type> simple_date_type(make_shared<simple_date_type_impl>());
 thread_local const shared_ptr<const abstract_type> time_type(make_shared<time_type_impl>());
 thread_local const shared_ptr<const abstract_type> uuid_type(make_shared<uuid_type_impl>());
 thread_local const shared_ptr<const abstract_type> inet_addr_type(make_shared<inet_addr_type_impl>());
 thread_local const shared_ptr<const abstract_type> float_type(make_shared<float_type_impl>());
 thread_local const shared_ptr<const abstract_type> double_type(make_shared<double_type_impl>());
 thread_local const shared_ptr<const abstract_type> varint_type(make_shared<varint_type_impl>());
 thread_local const shared_ptr<const abstract_type> decimal_type(make_shared<decimal_type_impl>());
 thread_local const shared_ptr<const abstract_type> counter_type(make_shared<counter_type_impl>());
 thread_local const shared_ptr<const abstract_type> duration_type(make_shared<duration_type_impl>());
 thread_local const data_type empty_type(make_shared<empty_type_impl>());
 data_type abstract_type::parse_type(const sstring& name) {     static thread_local const std::unordered_map<sstring, data_type> types = {         { byte_type_name,      byte_type      },         { short_type_name,     short_type     },         { int32_type_name,     int32_type     },         { long_type_name,      long_type      },         { ascii_type_name,     ascii_type     },         { bytes_type_name,     bytes_type     },         { utf8_type_name,      utf8_type      },         { boolean_type_name,   boolean_type   },         { date_type_name,      date_type      },         { timeuuid_type_name,  timeuuid_type  },         { timestamp_type_name, timestamp_type },         { simple_date_type_name, simple_date_type },         { time_type_name,      time_type },         { uuid_type_name,      uuid_type      },         { inet_addr_type_name, inet_addr_type },         { float_type_name,     float_type     },         { double_type_name,    double_type    },         { varint_type_name,    varint_type    },         { decimal_type_name,   decimal_type   },         { counter_type_name,   counter_type   },         { duration_type_name,  duration_type  },         { empty_type_name,     empty_type     },     };     auto it = types.find(name);     if (it == types.end()) {         throw std::invalid_argument(format("unknown type: {}\n", name));     }     return it->second; }
 data_value::~data_value() {     if (_value) {         native_value_delete(*_type, _value);     } }
 data_value::data_value(const data_value& v) : _value(nullptr), _type(v._type) {     if (v._value) {         _value = _type->native_value_clone(v._value);     } }
 data_value& data_value::operator=(data_value&& x) {     auto tmp = std::move(x);     std::swap(tmp._value, this->_value);     std::swap(tmp._type, this->_type);     return *this; }
 data_value::data_value(bytes v) : data_value(make_new(bytes_type, v)) { }
 data_value::data_value(sstring&& v) : data_value(make_new(utf8_type, std::move(v))) { }
 data_value::data_value(const char* v) : data_value(std::string_view(v)) { }
 data_value::data_value(std::string_view v) : data_value(sstring(v)) { }
 data_value::data_value(const std::string& v) : data_value(std::string_view(v)) { }
 data_value::data_value(const sstring& v) : data_value(std::string_view(v)) { }
 data_value::data_value(ascii_native_type v) : data_value(make_new(ascii_type, v.string)) { }
 data_value::data_value(bool v) : data_value(make_new(boolean_type, v)) { }
 data_value::data_value(int8_t v) : data_value(make_new(byte_type, v)) { }
 data_value::data_value(int16_t v) : data_value(make_new(short_type, v)) { }
 data_value::data_value(int32_t v) : data_value(make_new(int32_type, v)) { }
 data_value::data_value(int64_t v) : data_value(make_new(long_type, v)) { }
 data_value::data_value(utils::UUID v) : data_value(make_new(uuid_type, v)) { }
 data_value::data_value(float v) : data_value(make_new(float_type, v)) { }
 data_value::data_value(double v) : data_value(make_new(double_type, v)) { }
 data_value::data_value(seastar::net::inet_address v) : data_value(make_new(inet_addr_type, v)) { }
 data_value::data_value(seastar::net::ipv4_address v) : data_value(seastar::net::inet_address(v)) { }
 data_value::data_value(seastar::net::ipv6_address v) : data_value(seastar::net::inet_address(v)) { }
 data_value::data_value(simple_date_native_type v) : data_value(make_new(simple_date_type, v.days)) { }
 data_value::data_value(db_clock::time_point v) : data_value(make_new(timestamp_type, v)) { }
 data_value::data_value(time_native_type v) : data_value(make_new(time_type, v.nanoseconds)) { }
 data_value::data_value(timeuuid_native_type v) : data_value(make_new(timeuuid_type, v.uuid)) { }
 data_value::data_value(date_type_native_type v) : data_value(make_new(date_type, v.tp)) { }
 data_value::data_value(utils::multiprecision_int v) : data_value(make_new(varint_type, v)) { }
 data_value::data_value(big_decimal v) : data_value(make_new(decimal_type, v)) { }
 data_value::data_value(cql_duration d) : data_value(make_new(duration_type, d)) { }
 data_value::data_value(empty_type_representation e) : data_value(make_new(empty_type, e)) { }
 sstring data_value::to_parsable_string() const {     // For some reason trying to do it using fmt::format refuses to compile
    // auto to_parsable_str_transform = boost::adaptors::transformed([](const data_value& dv) -> sstring {
    //     return dv.to_parsable_string();
    // });
    if (_type->without_reversed().is_list()) {         const list_type_impl::native_type* the_list = (const list_type_impl::native_type*)_value;         std::ostringstream result;         result << "[";         for (size_t i = 0; i < the_list->size(); i++) {             if (i != 0) {                 result << ", ";             }             result << (*the_list)[i].to_parsable_string();         }         result << "]";         return result.str();         //return fmt::format("[{}]", fmt::join(*the_list | to_parsable_str_transform, ", "));
    }     if (_type->without_reversed().is_set()) {         const set_type_impl::native_type* the_set = (const set_type_impl::native_type*)_value;         std::ostringstream result;         result << "{";         for (size_t i = 0; i < the_set->size(); i++) {             if (i != 0) {                 result << ", ";             }             result << (*the_set)[i].to_parsable_string();         }         result << "}";         return result.str();         //return fmt::format("{{{}}}", fmt::join(*the_set | to_parsable_str_transform, ", "));
    }     if (_type->without_reversed().is_map()) {         const map_type_impl::native_type* the_map = (const map_type_impl::native_type*)_value;         std::ostringstream result;         result << "{";         for (size_t i = 0; i < the_map->size(); i++) {             if (i != 0) {                 result << ", ";             }             result << (*the_map)[i].first.to_parsable_string() << ":" << (*the_map)[i].second.to_parsable_string();         }         result << "}";         return result.str();         //auto to_map_elem_transform = boost::adaptors::transformed(
        //    [](const std::pair<data_value, data_value>& map_elem) -> sstring {
        //        return fmt::format("{{{}:{}}}", map_elem.first.to_parsable_string(), map_elem.second.to_parsable_string());
        //    }
        //);
        //
        //return fmt::format("{{{}}}", fmt::join(*the_map | to_map_elem_transform, ", "));
    }     if (_type->without_reversed().is_user_type()) {         const user_type_impl* user_typ = dynamic_cast<const user_type_impl*>(&_type->without_reversed());         const user_type_impl::native_type* field_values = (const user_type_impl::native_type*)_value;         std::ostringstream result;         result << "{";         for (std::size_t i = 0; i < field_values->size(); i++) {             if (i != 0) {                 result << ", ";             }             result << user_typ->string_field_names().at(i) << ":" << (*field_values)[i].to_parsable_string();         }         result << "}";         return result.str();     }     if (_type->without_reversed().is_tuple()) {         const tuple_type_impl::native_type* tuple_elements = (const tuple_type_impl::native_type*)_value;         std::ostringstream result;         result << "(";         for (std::size_t i = 0; i < tuple_elements->size(); i++) {             if (i != 0) {                 result << ", ";             }             result << (*tuple_elements)[i].to_parsable_string();         }         result << ")";         return result.str();     }     abstract_type::kind type_kind = _type->without_reversed().get_kind();     if (type_kind == abstract_type::kind::date || type_kind == abstract_type::kind::timestamp) {         // Put timezone information after a date or timestamp to specify that it's in UTC
        // Otherwise it will be parsed as a date in the local timezone.
        return fmt::format("'{}+0000'", *this);     }     if (type_kind == abstract_type::kind::utf8         || type_kind == abstract_type::kind::ascii         || type_kind == abstract_type::kind::inet         || type_kind == abstract_type::kind::time     ) {         // Put quotes on types that require it
        return fmt::format("'{}'", *this);     }     // For simple types the default operator<< should work ok
    return fmt::format("{}", *this); }
 data_value make_list_value(data_type type, list_type_impl::native_type value) {     return data_value::make_new(std::move(type), std::move(value)); }
 data_value make_set_value(data_type type, set_type_impl::native_type value) {     return data_value::make_new(std::move(type), std::move(value)); }
 data_value make_map_value(data_type type, map_type_impl::native_type value) {     return data_value::make_new(std::move(type), std::move(value)); }
 data_value make_tuple_value(data_type type, tuple_type_impl::native_type value) {     return data_value::make_new(std::move(type), std::move(value)); }
 data_value make_user_value(data_type type, user_type_impl::native_type value) {     return data_value::make_new(std::move(type), std::move(value)); }
 std::ostream& operator<<(std::ostream& out, const data_value& v) {     if (v.is_null()) {         return out << "null";     }     return out << v.type()->to_string_impl(v); }
 shared_ptr<const reversed_type_impl> reversed_type_impl::get_instance(data_type type) {     return intern::get_instance(std::move(type)); }
  bool abstract_type::contains_set_or_map() const {     return _contains_set_or_map; }
 bool abstract_type::contains_collection() const {     return _contains_collection; }
 bool abstract_type::bound_value_needs_to_be_reserialized() const {     // If a value contains set or map, then this collection can be sent in the wrong order
    // or there could be duplicates inside. We need to reserialize it into proper set or map.
    if (contains_set_or_map()) {         return true;     }     return false; }
 // compile once the template instance that was externed in marshal_exception.hh
namespace seastar { template void throw_with_backtrace<marshal_exception, sstring>(sstring&&); }
 namespace cql3 { column_identifier::column_identifier(sstring raw_text, bool keep_case) {     _text = std::move(raw_text);     if (!keep_case) {         std::transform(_text.begin(), _text.end(), _text.begin(), ::tolower);     }     bytes_ = to_bytes(_text); } column_identifier::column_identifier(bytes bytes_, data_type type)     : bytes_(std::move(bytes_))     , _text(type->get_string(this->bytes_)) { } column_identifier::column_identifier(bytes bytes_, sstring text)     : bytes_(std::move(bytes_))     , _text(std::move(text)) { } bool column_identifier::operator==(const column_identifier& other) const {     return bytes_ == other.bytes_; } const bytes& column_identifier::name() const {     return bytes_; } sstring column_identifier::to_string() const {     return _text; } sstring column_identifier::to_cql_string() const {     return util::maybe_quote(_text); } sstring column_identifier_raw::to_cql_string() const {     return util::maybe_quote(_text); } column_identifier_raw::column_identifier_raw(sstring raw_text, bool keep_case)     : _raw_text{raw_text}     , _text{raw_text} {     if (!keep_case) {         std::transform(_text.begin(), _text.end(), _text.begin(), ::tolower);     } } ::shared_ptr<column_identifier> column_identifier_raw::prepare(const schema& s) const {     return prepare_column_identifier(s); } ::shared_ptr<column_identifier> column_identifier_raw::prepare_column_identifier(const schema& schema) const {     if (schema.regular_column_name_type() == utf8_type) {         return ::make_shared<column_identifier>(_text, true);     }     // We have a Thrift-created table with a non-text comparator.  We need to parse column names with the comparator
    // to get the correct ByteBuffer representation.  However, this doesn't apply to key aliases, so we need to
    // make a special check for those and treat them normally.  See CASSANDRA-8178.
    auto text_bytes = to_bytes(_text);     auto def = schema.get_column_definition(text_bytes);     if (def) {         return ::make_shared<column_identifier>(std::move(text_bytes), _text);     }     return ::make_shared<column_identifier>(schema.regular_column_name_type()->from_string(_raw_text), _text); } bool column_identifier_raw::processes_selection() const {     return false; } bool column_identifier_raw::operator==(const column_identifier_raw& other) const {     return _text == other._text; } sstring column_identifier_raw::to_string() const {     return _text; }  }
 bool cql3::column_identifier::text_comparator::operator()(const cql3::column_identifier& c1, const cql3::column_identifier& c2) const {     return c1.text() < c2.text(); }
 namespace cql3 { column_specification::column_specification(std::string_view ks_name_, std::string_view cf_name_, ::shared_ptr<column_identifier> name_, data_type type_)         : ks_name(ks_name_)         , cf_name(cf_name_)         , name(name_)         , type(type_)     { } bool column_specification::all_in_same_table(const std::vector<lw_shared_ptr<column_specification>>& names) {     assert(!names.empty());     auto first = names.front();     return std::all_of(std::next(names.begin()), names.end(), [first] (auto&& spec) {         return spec->ks_name == first->ks_name && spec->cf_name == first->cf_name;     }); } }
 namespace utils { static int64_t make_thread_local_node(int64_t node) {     // An atomic counter to issue thread identifiers.
    // We should take current core number into consideration
    // because create_time_safe() doesn't synchronize across cores and
    // it's easy to get duplicates. Use an own counter since
    // seastar::this_shard_id() may not yet be available.
    static std::atomic<int64_t> thread_id_counter;     static thread_local int64_t thread_id = thread_id_counter.fetch_add(1);     // Mix in the core number into Organisational Unique
    // Identifier, to leave NIC intact, assuming tampering
    // with NIC is more likely to lead to collision within
    // a single network than tampering with OUI.
    //
    // Make sure the result fits into 6 bytes reserved for MAC
    // (adding the core number may overflow the original
    // value).
    return (node + (thread_id << 32)) & 0xFFFF'FFFF'FFFFL; } static int64_t make_random_node() {     static int64_t random_node = [] {         int64_t node = 0;         std::random_device rndgen;         do {             auto i = rndgen();             node = i;             if (sizeof(i) < sizeof(node)) {                 node = (node << 32) + rndgen();             }         } while (node == 0); // 0 may mean "node is uninitialized", so avoid it.
        return node;     }();     return random_node; } static int64_t make_node() {     static int64_t global_node = 3;     return make_thread_local_node(global_node); } static int64_t make_clock_seq_and_node() {     // The original Java code did this, shuffling the number of millis
    // since the epoch, and taking 14 bits of it. We don't do exactly
    // the same, but the idea is the same.
    //long clock = new Random(System.currentTimeMillis()).nextLong();
    unsigned int seed = std::chrono::system_clock::now().time_since_epoch().count();     int clock = rand_r(&seed);     long lsb = 0;     lsb |= 0x8000000000000000L;                 // variant (2 bits)
    lsb |= (clock & 0x0000000000003FFFL) << 48; // clock sequence (14 bits)
    lsb |= make_node();                          // 6 bytes
    return lsb; } UUID UUID_gen::get_name_UUID(bytes_view b) {     return get_name_UUID(reinterpret_cast<const unsigned char*>(b.begin()), b.size()); } UUID UUID_gen::get_name_UUID(sstring_view s) {     static_assert(sizeof(char) == sizeof(sstring_view::value_type), "Assumed that str.size() counts in chars");     return get_name_UUID(reinterpret_cast<const unsigned char*>(s.begin()), s.size()); } UUID UUID_gen::get_name_UUID(const unsigned char *s, size_t len) {     bytes digest = md5_hasher::calculate(std::string_view(reinterpret_cast<const char*>(s), len));     // set version to 3
    digest[6] &= 0x0f;     digest[6] |= 0x30;     // set variant to IETF variant
    digest[8] &= 0x3f;     digest[8] |= 0x80;     return get_UUID(digest); } UUID UUID_gen::negate(UUID o) {     auto lsb = o.get_least_significant_bits();     const long clock_mask = 0x0000000000003FFFL;     // We flip the node-and-clock-seq octet of the UUID for time-UUIDs. This
    // creates a virtual node with a time which cannot be generated anymore, so
    // is safe against collisions.
    // For name UUIDs we flip the same octet. Name UUIDs being an md5 hash over
    // a buffer, flipping any bit should be safe against collisions.
    long clock = (lsb >> 48) & clock_mask;     clock = ~clock & clock_mask;     lsb &= ~(clock_mask << 48); // zero current clock
    lsb |= (clock << 48); // write new clock
    return UUID(o.get_most_significant_bits(), lsb); } const thread_local int64_t UUID_gen::spoof_node = make_thread_local_node(make_random_node()); const thread_local int64_t UUID_gen::clock_seq_and_node = make_clock_seq_and_node(); thread_local UUID_gen UUID_gen::_instance; }
 // namespace utils
namespace utils { static logging::logger filterlog("bloom_filter"); filter_ptr i_filter::get_filter(int64_t num_elements, double max_false_pos_probability, filter_format fformat) {     assert(seastar::thread::running_in_thread());     if (max_false_pos_probability > 1.0) {         throw std::invalid_argument(format("Invalid probability {:f}: must be lower than 1.0", max_false_pos_probability));     }     if (max_false_pos_probability == 1.0) {         return std::make_unique<filter::always_present_filter>();     }     int buckets_per_element = bloom_calculations::max_buckets_per_element(num_elements);     auto spec = bloom_calculations::compute_bloom_spec(buckets_per_element, max_false_pos_probability);     return filter::create_filter(spec.K, num_elements, spec.buckets_per_element, fformat); }  }
 namespace utils { namespace filter { thread_local bloom_filter::stats bloom_filter::_shard_stats; template<typename Func> void for_each_index(hashed_key hk, int count, int64_t max, filter_format format, Func&& func) {     auto h = hk.hash();     int64_t base = (format == filter_format::k_l_format) ? h[0] : h[1];     int64_t inc = (format == filter_format::k_l_format) ? h[1] : h[0];     for (int i = 0; i < count; i++) {         if (func(std::abs(base % max)) == stop_iteration::yes) {             break;         }         base = static_cast<int64_t>(static_cast<uint64_t>(base) + static_cast<uint64_t>(inc));     } } bloom_filter::bloom_filter(int hashes, bitmap&& bs, filter_format format) noexcept     : _bitset(std::move(bs))     , _hash_count(hashes)     , _format(format) {     _stats.memory_size += memory_size(); } bloom_filter::~bloom_filter() noexcept {     _stats.memory_size -= memory_size(); } bool bloom_filter::is_present(hashed_key key) {     bool result = true;     for_each_index(key, _hash_count, _bitset.size(), _format, [this, &result] (auto i) {         if (!_bitset.test(i)) {             result = false;             return stop_iteration::yes;         }         return stop_iteration::no;     });     return result; } void bloom_filter::add(const bytes_view& key) {     for_each_index(make_hashed_key(key), _hash_count, _bitset.size(), _format, [this] (auto i) {         _bitset.set(i);         return stop_iteration::no;     }); } bool bloom_filter::is_present(const bytes_view& key) {     return is_present(make_hashed_key(key)); }   } }
 namespace utils { namespace bloom_calculations { const std::vector<std::vector<double>> probs = {         {1.0}, // dummy row representing 0 buckets per element
        {1.0, 1.0}, // dummy row representing 1 buckets per element
        {1.0, 0.393,  0.400},         {1.0, 0.283,  0.237,   0.253},         {1.0, 0.221,  0.155,   0.147,   0.160},         {1.0, 0.181,  0.109,   0.092,   0.092,   0.101}, // 5
        {1.0, 0.154,  0.0804,  0.0609,  0.0561,  0.0578,   0.0638},         {1.0, 0.133,  0.0618,  0.0423,  0.0359,  0.0347,   0.0364},         {1.0, 0.118,  0.0489,  0.0306,  0.024,   0.0217,   0.0216,   0.0229},         {1.0, 0.105,  0.0397,  0.0228,  0.0166,  0.0141,   0.0133,   0.0135,   0.0145},         {1.0, 0.0952, 0.0329,  0.0174,  0.0118,  0.00943,  0.00844,  0.00819,  0.00846}, // 10
        {1.0, 0.0869, 0.0276,  0.0136,  0.00864, 0.0065,   0.00552,  0.00513,  0.00509},         {1.0, 0.08,   0.0236,  0.0108,  0.00646, 0.00459,  0.00371,  0.00329,  0.00314},         {1.0, 0.074,  0.0203,  0.00875, 0.00492, 0.00332,  0.00255,  0.00217,  0.00199,  0.00194},         {1.0, 0.0689, 0.0177,  0.00718, 0.00381, 0.00244,  0.00179,  0.00146,  0.00129,  0.00121,  0.0012},         {1.0, 0.0645, 0.0156,  0.00596, 0.003,   0.00183,  0.00128,  0.001,    0.000852, 0.000775, 0.000744}, // 15
        {1.0, 0.0606, 0.0138,  0.005,   0.00239, 0.00139,  0.000935, 0.000702, 0.000574, 0.000505, 0.00047,  0.000459},         {1.0, 0.0571, 0.0123,  0.00423, 0.00193, 0.00107,  0.000692, 0.000499, 0.000394, 0.000335, 0.000302, 0.000287, 0.000284},         {1.0, 0.054,  0.0111,  0.00362, 0.00158, 0.000839, 0.000519, 0.00036,  0.000275, 0.000226, 0.000198, 0.000183, 0.000176},         {1.0, 0.0513, 0.00998, 0.00312, 0.0013,  0.000663, 0.000394, 0.000264, 0.000194, 0.000155, 0.000132, 0.000118, 0.000111, 0.000109},         {1.0, 0.0488, 0.00906, 0.0027,  0.00108, 0.00053,  0.000303, 0.000196, 0.00014,  0.000108, 8.89e-05, 7.77e-05, 7.12e-05, 6.79e-05, 6.71e-05} // 20
};  // the first column is a dummy column representing K=0.
static std::vector<int> initialize_opt_k() {     std::vector<int> arr;     arr.resize(probs.size());     for (auto i = 0; i < int(probs.size()); i++) {         double min = std::numeric_limits<double>::max();         auto& prob = probs[i];         for (auto j = 0; j < int(prob.size()); j++) {             if (prob[j] < min) {                 min = prob[j];                 arr[i] = std::max(min_k, j);             }         }     }     return arr; } const std::vector<int> opt_k_per_buckets = initialize_opt_k(); } }
 utils::rate_limiter::rate_limiter(size_t rate)         : _units_per_s(rate) {     if (_units_per_s != 0) {         _timer.set_callback(std::bind(&rate_limiter::on_timer, this));         _timer.arm(lowres_clock::now() + std::chrono::seconds(1),                 std::optional<lowres_clock::duration> {                         std::chrono::seconds(1) });     } }
 void utils::rate_limiter::on_timer() {     _sem.signal(_units_per_s - _sem.current()); }
 future<> utils::rate_limiter::reserve(size_t u) {     if (_units_per_s == 0) {         return make_ready_future<>();     }     if (u <= _units_per_s) {         return _sem.wait(u);     }     auto n = std::min(u, _units_per_s);     auto r = u - n;     return _sem.wait(n).then([this, r] {         return reserve(r);     }); }
 class utils::file_lock::impl { public:     impl(fs::path path)             : _path(std::move(path)), _fd(                     file_desc::open(_path.native(), O_RDWR | O_CREAT | O_CLOEXEC,                     S_IRWXU)) {         if (::lockf(_fd.get(), F_TLOCK, 0) != 0) {             throw std::system_error(errno, std::system_category(),                         "Could not acquire lock: " + _path.native());         }     }     impl(impl&&) = default;     ~impl() {         if (!_path.empty()) {             ::unlink(_path.c_str());         }         assert(_fd.get() != -1);         auto r = ::lockf(_fd.get(), F_ULOCK, 0);         assert(r == 0);     }     fs::path _path;     file_desc _fd; };
 utils::file_lock::file_lock(fs::path path)     : _impl(std::make_unique<impl>(std::move(path))) {}
 utils::file_lock::file_lock(file_lock&& f) noexcept     : _impl(std::move(f._impl)) {}
 utils::file_lock::~file_lock() {}
 fs::path utils::file_lock::path() const {     return _impl ? _impl->_path : ""; }
 future<utils::file_lock> utils::file_lock::acquire(fs::path path) {     // meh. not really any future stuff here. but pretend, for the
    // day when a future version of lock etc is added.
    try {         return make_ready_future<file_lock>(file_lock(path));     } catch (...) {         return make_exception_future<utils::file_lock>(std::current_exception());     } }
  namespace utils { void dynamic_bitset::set(size_t n) noexcept {     for (auto& level : _bits) {         auto idx = n / bits_per_int;         auto old = level[idx];         level[idx] |= int_type(1u) << (n % bits_per_int);         if (old) {             break;         }         n = idx; // prepare for next level
    } } void dynamic_bitset::clear(size_t n) noexcept {     for (auto& level : _bits) {         auto idx = n / bits_per_int;         auto old = level[idx];         level[idx] &= ~(int_type(1u) << (n % bits_per_int));         if (!old || level[idx]) {             break;         }         n = idx; // prepare for next level
    } } size_t dynamic_bitset::find_first_set() const noexcept {     size_t pos = 0;     for (auto& vv : _bits | boost::adaptors::reversed) {         auto v = vv[pos];         pos *= bits_per_int;         if (v) {             pos += count_trailing_zeros(v);         } else {             return npos;         }     }     return pos; } size_t dynamic_bitset::find_next_set(size_t n) const noexcept {     ++n;     unsigned level = 0;     unsigned nlevels = _bits.size();     // Climb levels until we find a set bit in the right place
    while (level != nlevels) {         if (n >= _bits_count) {             return npos;         }         auto v = _bits[level][level_idx(level, n)];         v &= ~mask_lower_bits(level_remainder(level, n));         if (v) {             break;         }         ++level;         n = align_up(n, size_t(1) << (level_shift * level));     }     if (level == nlevels) {         return npos;     }     // Descend levels until we reach level 0
    do {         auto v = _bits[level][level_idx(level, n)];         v &= ~mask_lower_bits(level_remainder(level, n));         n = align_down(n, size_t(1) << (level_shift * (level + 1)));         n += count_trailing_zeros(v) << (level_shift * level);     } while (level-- != 0);     return n; } size_t dynamic_bitset::find_last_set() const noexcept {     size_t pos = 0;     for (auto& vv : _bits | boost::adaptors::reversed) {         auto v = vv[pos];         pos *= bits_per_int;         if (v) {             pos += bits_per_int - 1 - count_leading_zeros(v);         } else {             return npos;         }     }     return pos; } dynamic_bitset::dynamic_bitset(size_t nr_bits)     : _bits_count(nr_bits) {     auto div_ceil = [] (size_t num, size_t den) {         return (num + den - 1) / den;     };     // 1-64: 1 level
    // 65-4096: 2 levels
    // 4097-262144: 3 levels
    // etc.
    unsigned nr_levels = div_ceil(log2ceil(align_up(nr_bits, size_t(bits_per_int))), level_shift);     _bits.resize(nr_levels);     size_t level_bits = nr_bits;     for (unsigned level = 0; level != nr_levels; ++level) {         auto level_words = align_up(level_bits, bits_per_int) / bits_per_int;         _bits[level].resize(level_words);         level_bits = level_words; // for next iteration
    } } }
   std::unique_ptr<bytes_view::value_type[]> managed_bytes::do_linearize_pure() const {     auto b = _u.ptr;     auto data = std::unique_ptr<bytes_view::value_type[]>(new bytes_view::value_type[b->size]);     auto e = data.get();     while (b) {         e = std::copy_n(b->data, b->frag_size, e);         b = b->next;     }     return data; }
      bool should_stop_on_system_error(const std::system_error& e) {     if (e.code().category() == std::system_category()) { 	// Whitelist of errors that don't require us to stop the server:
	switch (e.code().value()) {         case EEXIST:         case ENOENT:             return false;         default:             break;         }     }     return true; }
  void* utils::internal::try_catch_dynamic(std::exception_ptr& eptr, const std::type_info* catch_type) noexcept {     // In both libstdc++ and libc++, exception_ptr has just one field
    // which is a pointer to the exception data
    void* raw_ptr = reinterpret_cast<void*&>(eptr);     const std::type_info* ex_type = utils::abi::get_cxa_exception(raw_ptr)->exceptionType;     // __do_catch can return true and set raw_ptr to nullptr, but only in the case
    // when catch_type is a pointer and a nullptr is thrown. try_catch_dynamic
    // doesn't work with catching pointers.
    if (catch_type->__do_catch(ex_type, &raw_ptr, 1)) {         return raw_ptr;     }     return nullptr; }
 namespace bpo = boost::program_options;
 template<> std::istream& std::operator>>(std::istream& is, std::unordered_map<seastar::sstring, seastar::sstring>& map) {    std::istreambuf_iterator<char> i(is), e;    int level = 0;    bool sq = false, dq = false, qq = false;    sstring key, val;    sstring* ps = &key;    auto add = [&] {       if (!key.empty()) {          map[key] = std::move(val);       }       key = {};       val = {};       ps = &key;    };    while (i != e && level >= 0) {       auto c = *i++;       switch (c) {       case '\\':          qq = !qq;          if (qq) {             continue;          }          break;       case '\'':          if (!qq) {             sq = !sq;          }          break;       case '"':          if (!qq) {             dq = !dq;          }          break;       case '=':          if (level <= 1 && !sq && !dq && !qq) {             ps = &val;             continue;          }          break;       case '{': case '[':          if (!sq && !dq && !qq) {             ++level;             continue;          }          break;       case ']': case '}':          if (!sq && !dq && !qq && level > 0) {             --level;             continue;          }          break;       case ',':          if (level == 1 && !sq && !dq && !qq) {             add();             continue;          }          break;       case ' ': case '\t': case '\n':          if (!sq && !dq && !qq) {             continue;          }          break;       default:          break;       }       if (level == 0) {          ++level;       }       qq = false;       ps->append(&c, 1);    }    add();    return is; }
 template<> std::istream& std::operator>>(std::istream& is, std::vector<seastar::sstring>& res) {    std::istreambuf_iterator<char> i(is), e;    int level = 0;    bool sq = false, dq = false, qq = false;    sstring val;    auto add = [&] {       if (!val.empty()) {          res.emplace_back(std::exchange(val, {}));       }       val = {};    };    while (i != e && level >= 0) {       auto c = *i++;       switch (c) {       case '\\':          qq = !qq;          if (qq) {             continue;          }          break;       case '\'':          if (!qq) {             sq = !sq;          }          break;       case '"':          if (!qq) {             dq = !dq;          }          break;       case '{': case '[':          if (!sq && !dq && !qq) {             ++level;             continue;          }          break;       case '}': case ']':          if (!sq && !dq && !qq && level > 0) {             --level;             continue;          }          break;       case ',':          if (level == 1 && !sq && !dq && !qq) {             add();             continue;          }          break;       case ' ': case '\t': case '\n':          if (!sq && !dq && !qq) {             continue;          }          break;       default:          break;       }       if (level == 0) {          ++level;       }       qq = false;       val.append(&c, 1);    }    add();    return is; }
 thread_local unsigned utils::config_file::s_shard_id = 0;
 json::json_return_type utils::config_type::to_json(const void* value) const {     return _to_json(value); }
 bool utils::config_file::config_src::matches(std::string_view name) const {     // The below line provides support for option names in the "long_name,short_name" format,
    // such as "workdir,W". We only want the long name ("workdir") to be used in the YAML.
    // But since at some point (due to a bug) the YAML config parser expected the silly
    // double form ("workdir,W") instead, we support both for backward compatibility.
    std::string_view long_name = _name.substr(0, _name.find_first_of(','));     if (_name == name || long_name == name) {         return true;     }     if (!_alias.empty() && _alias == name) {         return true;     }     return false; }
 json::json_return_type utils::config_file::config_src::value_as_json() const {     return _type->to_json(current_value()); }
  utils::config_file::config_file(std::initializer_list<cfg_ref> cfgs)     : _cfgs(cfgs) {}
 void utils::config_file::add(cfg_ref cfg, std::unique_ptr<any_value> value) {     if (_per_shard_values.size() != 1) {         throw std::runtime_error("Can only add config_src to config_file during initialization");     }     _cfgs.emplace_back(cfg);     auto undo = defer([&] { _cfgs.pop_back(); });     cfg.get()._per_shard_values_offset = _per_shard_values[0].size();     _per_shard_values[0].emplace_back(std::move(value));     undo.cancel(); }
 void utils::config_file::add(std::initializer_list<cfg_ref> cfgs) {     _cfgs.insert(_cfgs.end(), cfgs.begin(), cfgs.end()); }
 void utils::config_file::add(const std::vector<cfg_ref> & cfgs) {     _cfgs.insert(_cfgs.end(), cfgs.begin(), cfgs.end()); }
 bpo::options_description utils::config_file::get_options_description() {     bpo::options_description opts("");     return get_options_description(opts); }
 bpo::options_description utils::config_file::get_options_description(boost::program_options::options_description opts) {     auto init = opts.add_options();     add_options(init);     return opts; }
 bpo::options_description_easy_init& utils::config_file::add_options(bpo::options_description_easy_init& init) {     for (config_src& src : _cfgs) {         if (src.status() == value_status::Used) {             src.add_command_line_option(init);         }     }     return init; }
 void utils::config_file::read_from_yaml(const sstring& yaml, error_handler h) {     read_from_yaml(yaml.c_str(), std::move(h)); }
 void utils::config_file::read_from_yaml(const char* yaml, error_handler h) {     std::unordered_map<sstring, cfg_ref> values;     if (!h) {         h = [](auto & opt, auto & msg, auto) {             throw std::invalid_argument(msg + " : " + opt);         };     }     auto doc = YAML::Load(yaml);     for (auto node : doc) {         auto label = node.first.as<sstring>();         auto i = std::find_if(_cfgs.begin(), _cfgs.end(), [&label](const config_src& cfg) { return cfg.matches(label); });         if (i == _cfgs.end()) {             h(label, "Unknown option", std::nullopt);             continue;         }         config_src& cfg = *i;         if (cfg.source() > config_source::SettingsFile) {             // already set
            continue;         }         switch (cfg.status()) {         case value_status::Invalid:             h(label, "Option is not applicable", cfg.status());             continue;         case value_status::Unused:         default:             break;         }         if (node.second.IsNull()) {             continue;         }         // Still, a syntax error is an error warning, not a fail
        try {             cfg.set_value(node.second);         } catch (std::exception& e) {             h(label, e.what(), cfg.status());         } catch (...) {             h(label, "Could not convert value", cfg.status());         }     } }
 utils::config_file::configs utils::config_file::set_values() const {     return boost::copy_range<configs>(_cfgs | boost::adaptors::filtered([] (const config_src& cfg) {         return cfg.status() > value_status::Used || cfg.source() > config_source::None;     })); }
 utils::config_file::configs utils::config_file::unset_values() const {     configs res;     for (config_src& cfg : _cfgs) {         if (cfg.status() > value_status::Used) {             continue;         }         if (cfg.source() > config_source::None) {             continue;         }         res.emplace_back(cfg);     }     return res; }
 future<> utils::config_file::read_from_file(file f, error_handler h) {     return make_ready_future<>(); }
 future<> utils::config_file::read_from_file(const sstring& filename, error_handler h) {     return open_file_dma(filename, open_flags::ro).then([this, h](file f) {        return read_from_file(std::move(f), h);     }); }
 future<> utils::config_file::broadcast_to_all_shards() {     return async([this] {         if (_per_shard_values.size() != smp::count) {             _per_shard_values.resize(smp::count);             smp::invoke_on_all([this] {                 auto cpu = this_shard_id();                 if (cpu != 0) {                     s_shard_id = cpu;                     auto& shard_0_values = _per_shard_values[0];                     auto nr_values = shard_0_values.size();                     auto& this_shard_values = _per_shard_values[cpu];                     this_shard_values.resize(nr_values);                     for (size_t i = 0; i != nr_values; ++i) {                         this_shard_values[i] = shard_0_values[i]->clone();                     }                 }             }).get();         } else {             smp::invoke_on_all([this] {                 if (s_shard_id != 0) {                     auto& shard_0_values = _per_shard_values[0];                     auto nr_values = shard_0_values.size();                     auto& this_shard_values = _per_shard_values[s_shard_id];                     for (size_t i = 0; i != nr_values; ++i) {                         this_shard_values[i]->update_from(shard_0_values[i].get());                     }                 }             }).get();         }         // #4713
        // We can have values retained that are not pointing to
        // our storage (extension config). Need to broadcast
        // these configs as well.
        std::set<config_file *> files;         for (config_src& v : _cfgs) {             auto f = v.get_config_file();             if (f != this) {                 files.insert(f);             }         }         for (auto* f : files) {             f->broadcast_to_all_shards().get();         }     }); }
 sstring utils::config_file::config_src::source_name() const noexcept {     auto src = source();     switch (src) {     case utils::config_file::config_source::None:         return "default";     case utils::config_file::config_source::SettingsFile:         return "config";     case utils::config_file::config_source::CommandLine:         return "cli";     case utils::config_file::config_source::Internal:         return "internal";     case utils::config_file::config_source::CQL:         return "cql";     case utils::config_file::config_source::API:         return "api";     }     __builtin_unreachable(); }
 namespace utils { std::string multiprecision_int::str() const {     return _v.str(); } std::ostream& operator<<(std::ostream& os, const multiprecision_int& x) {     return os << x._v; } }
 using u32 = uint32_t;
 using u64 = uint64_t;
 static u64 pmul(u32 p1, u32 p2) {     // Because the representation is bit-reversed, we need to shift left
    // one bit after multiplying so that the highest bit of the 64-bit result
    // corresponds to the coefficient of x^63 and not x^62.
    return clmul(p1, p2) << 1; }
 static u32 pmul_mod(u32 p1, u32 p2) {     return crc32_fold_barrett_u64(pmul(p1, p2)); }
 static u32 mul_by_x_pow_mul8(u32 p, u64 e) {     u32 x0 = crc32_x_pow_radix_8_table_base_0[e & 0xff];     if (__builtin_expect(e < 0x100, false)) {         return pmul_mod(p, x0);     }     u32 x1 = crc32_x_pow_radix_8_table_base_8[(e >> 8) & 0xff];     u32 x2 = crc32_x_pow_radix_8_table_base_16[(e >> 16) & 0xff];     u64 y0 = pmul(p, x0);     u64 y1 = pmul(x1, x2);     u32 z0 = crc32_fold_barrett_u64(y0);     u32 z1 = crc32_fold_barrett_u64(y1);     if (__builtin_expect(e < 0x1000000, true)) {         return pmul_mod(z0, z1);     }     u32 x3 = crc32_x_pow_radix_8_table_base_24[(e >> 24) & 0xff];     z1 = pmul_mod(z1, x3);     if (__builtin_expect(e < 0x100000000, true)) {         return pmul_mod(z0, z1);     }     u32 x4 = crc32_x_pow_radix_8_table_base_0[(e >> 32) & 0xff];     u32 x5 = crc32_x_pow_radix_8_table_base_8[(e >> 40) & 0xff];     u32 x6 = crc32_x_pow_radix_8_table_base_16[(e >> 48) & 0xff];     u32 x7 = crc32_x_pow_radix_8_table_base_24[(e >> 56) & 0xff];     u64 y2 = pmul(x4, x5);     u64 y3 = pmul(x6, x7);     u64 u0 = pmul(z0, z1);     u32 z2 = crc32_fold_barrett_u64(y2);     u32 z3 = crc32_fold_barrett_u64(y3);     u64 u1 = pmul(z2, z3);     u32 v0 = crc32_fold_barrett_u64(u0);     u32 v1 = crc32_fold_barrett_u64(u1);     return pmul_mod(v0, v1); }
  template <int bits> static constexpr std::array<uint32_t, bits> make_crc32_power_table() {     std::array<uint32_t, bits> pows;     pows[0] = 0x00800000; // x^8
    for (int i = 1; i < bits; ++i) {         //   x^(2*N)          mod G(x)
        // = (x^N)*(x^N)      mod G(x)
        // = (x^N mod G(x))^2 mod G(x)
        pows[i] = crc32_fold_barrett_u64(clmul(pows[i - 1], pows[i - 1]) << 1);     }     return pows; }
 static constexpr std::array<uint32_t, 256> make_crc32_table(int base, int radix_bits, uint32_t one, std::array<uint32_t, 32> pows) {     std::array<uint32_t, 256> table;     for (int i = 0; i < (1 << radix_bits); ++i) {         uint32_t product = one;         for (int j = 0; j < radix_bits; ++j) {             if (i & (1 << j)) {                 product = crc32_fold_barrett_u64(clmul(product, pows[base + j]) << 1);             }         }         table[i] = product;     }     return table; }
 static constexpr int bits = 32;
 static constexpr int radix_bits = 8;
 static constexpr uint32_t one = 0x80000000;
 // x^0
static constexpr auto pows = make_crc32_power_table<bits>();
 // pows[i] = x^(2^i*8) mod G(x)
constinit std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_0 = make_crc32_table(0, radix_bits, one, pows);
 constinit std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_8 = make_crc32_table(8, radix_bits, one, pows);
 constinit std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_16 = make_crc32_table(16, radix_bits, one, pows);
 constinit std::array<uint32_t, 256> crc32_x_pow_radix_8_table_base_24 = make_crc32_table(24, radix_bits, one, pows);
 namespace utils { inline size_t iovec_len(const std::vector<iovec>& iov) {     size_t ret = 0;     for (auto&& e : iov) {         ret += e.iov_len;     }     return ret; } }
 namespace s3 { static logging::logger s3l("s3"); future<> ignore_reply(const http::reply& rep, input_stream<char>&& in_) {     auto in = std::move(in_);     co_await util::skip_entire_stream(in); } class dns_connection_factory : public http::experimental::connection_factory { protected:     std::string _host;     int _port;     struct state {         bool initialized = false;         socket_address addr;         ::shared_ptr<tls::certificate_credentials> creds;     };     lw_shared_ptr<state> _state;     shared_future<> _done;     future<> initialize(bool use_https) {         auto state = _state;         co_await coroutine::all(             [state, host = _host, port = _port] () -> future<> {                 auto hent = co_await net::dns::get_host_by_name(host, net::inet_address::family::INET);                 state->addr = socket_address(hent.addr_list.front(), port);             },             [state, use_https] () -> future<> {                 if (use_https) {                     tls::credentials_builder cbuild;                     co_await cbuild.set_system_trust();                     state->creds = cbuild.build_certificate_credentials();                 }             }         );         state->initialized = true;         s3l.debug("Initialized factory, address={} tls={}", state->addr, state->creds == nullptr ? "no" : "yes");     } public:     dns_connection_factory(std::string host, int port, bool use_https)         : _host(std::move(host))         , _port(port)         , _state(make_lw_shared<state>())         , _done(initialize(use_https))     {     }     virtual future<connected_socket> make() override {         if (!_state->initialized) {             s3l.debug("Waiting for factory to initialize");             co_await _done.get_future();         }         if (_state->creds) {             s3l.debug("Making new HTTPS connection addr={} host={}", _state->addr, _host);             co_return co_await tls::connect(_state->creds, _state->addr, _host);         } else {             s3l.debug("Making new HTTP connection");             co_return co_await seastar::connect(_state->addr, {}, transport::TCP);         }     } }; client::client(std::string host, endpoint_config_ptr cfg, global_factory gf, private_tag)         : _host(std::move(host))         , _cfg(std::move(cfg))         , _http(std::make_unique<dns_connection_factory>(_host, _cfg->port, _cfg->use_https))         , _gf(std::move(gf)) { } void client::update_config(endpoint_config_ptr cfg) {     if (_cfg->port != cfg->port || _cfg->use_https != cfg->use_https) {         throw std::runtime_error("Updating port and/or https usage is not possible");     }     _cfg = std::move(cfg); } shared_ptr<client> client::make(std::string endpoint, endpoint_config_ptr cfg, global_factory gf) {     return seastar::make_shared<client>(std::move(endpoint), std::move(cfg), std::move(gf), private_tag{}); } void client::authorize(http::request& req) {     if (!_cfg->aws) {         return;     }     auto time_point_str = utils::aws::format_time_point(db_clock::now());     auto time_point_st = time_point_str.substr(0, 8);     req._headers["x-amz-date"] = time_point_str;     req._headers["x-amz-content-sha256"] = "UNSIGNED-PAYLOAD";     std::map<std::string_view, std::string_view> signed_headers;     sstring signed_headers_list = "";     // AWS requires all x-... and Host: headers to be signed
    signed_headers["host"] = req._headers["Host"];     for (const auto& h : req._headers) {         if (h.first[0] == 'x' && h.first[1] == '-') {             signed_headers[h.first] = h.second;         }     }     unsigned header_nr = signed_headers.size();     for (const auto& h : signed_headers) {         signed_headers_list += format("{}{}", h.first, header_nr == 1 ? "" : ";");         header_nr--;     }     sstring query_string = "";     std::map<std::string_view, std::string_view> query_parameters;     for (const auto& q : req.query_parameters) {         query_parameters[q.first] = q.second;     }     unsigned query_nr = query_parameters.size();     for (const auto& q : query_parameters) {         query_string += format("{}={}{}", q.first, q.second, query_nr == 1 ? "" : "&");         query_nr--;     }     auto sig = utils::aws::get_signature(_cfg->aws->key, _cfg->aws->secret, _host, req._url, req._method,         utils::aws::omit_datestamp_expiration_check,         signed_headers_list, signed_headers,         utils::aws::unsigned_content,         _cfg->aws->region, "s3", query_string);     req._headers["Authorization"] = format("AWS4-HMAC-SHA256 Credential={}/{}/{}/s3/aws4_request,SignedHeaders={},Signature={}", _cfg->aws->key, time_point_st, _cfg->aws->region, signed_headers_list, sig); } future<> client::get_object_header(sstring object_name, http::experimental::client::reply_handler handler) {     s3l.trace("HEAD {}", object_name);     auto req = http::request::make("HEAD", _host, object_name);     authorize(req);     return _http.make_request(std::move(req), std::move(handler)); } future<uint64_t> client::get_object_size(sstring object_name) {     uint64_t len = 0;     co_await get_object_header(std::move(object_name), [&len] (const http::reply& rep, input_stream<char>&& in_) mutable -> future<> {         len = rep.content_length;         return make_ready_future<>(); // it's HEAD with no body
    });     co_return len; } // TODO: possibly move this to seastar's http subsystem.
static std::time_t parse_http_last_modified_time(const sstring& object_name, sstring last_modified) {     std::tm tm = {0};     // format conforms to HTTP-date, defined in the specification (RFC 7231).
    if (strptime(last_modified.c_str(), "%a, %d %b %Y %H:%M:%S %Z", &tm) == nullptr) {         s3l.warn("Unable to parse {} as Last-Modified for {}", last_modified, object_name);     } else {         s3l.trace("Successfully parsed {} as Last-modified for {}", last_modified, object_name);     }     return std::mktime(&tm); } future<client::stats> client::get_object_stats(sstring object_name) {     struct stats st{};     co_await get_object_header(object_name, [&] (const http::reply& rep, input_stream<char>&& in_) mutable -> future<> {         st.size = rep.content_length;         st.last_modified = parse_http_last_modified_time(object_name, rep.get_header("Last-Modified"));         return make_ready_future<>();     });     co_return st; } future<temporary_buffer<char>> client::get_object_contiguous(sstring object_name, std::optional<range> range) {     auto req = http::request::make("GET", _host, object_name);     http::reply::status_type expected = http::reply::status_type::ok;     if (range) {         auto range_header = format("bytes={}-{}", range->off, range->off + range->len - 1);         s3l.trace("GET {} contiguous range='{}'", object_name, range_header);         req._headers["Range"] = std::move(range_header);         expected = http::reply::status_type::partial_content;     } else {         s3l.trace("GET {} contiguous", object_name);     }     size_t off = 0;     std::optional<temporary_buffer<char>> ret;     authorize(req);     co_await _http.make_request(std::move(req), [&off, &ret, &object_name] (const http::reply& rep, input_stream<char>&& in_) mutable -> future<> {         auto in = std::move(in_);         ret = temporary_buffer<char>(rep.content_length);         s3l.trace("Consume {} bytes for {}", ret->size(), object_name);         co_await in.consume([&off, &ret] (temporary_buffer<char> buf) mutable {             if (buf.empty()) {                 return make_ready_future<consumption_result<char>>(stop_consuming(std::move(buf)));             }             size_t to_copy = std::min(ret->size() - off, buf.size());             if (to_copy > 0) {                 std::copy_n(buf.get(), to_copy, ret->get_write() + off);                 off += to_copy;             }             return make_ready_future<consumption_result<char>>(continue_consuming());         });     }, expected);     ret->trim(off);     s3l.trace("Consumed {} bytes of {}", off, object_name);     co_return std::move(*ret); } future<> client::put_object(sstring object_name, temporary_buffer<char> buf) {     s3l.trace("PUT {}", object_name);     auto req = http::request::make("PUT", _host, object_name);     auto len = buf.size();     req.write_body("bin", len, [buf = std::move(buf)] (output_stream<char>&& out_) mutable -> future<> {         auto out = std::move(out_);         std::exception_ptr ex;         try {             co_await out.write(buf.get(), buf.size());             co_await out.flush();         } catch (...) {             ex = std::current_exception();         }         co_await out.close();         if (ex) {             co_await coroutine::return_exception_ptr(std::move(ex));         }     });     authorize(req);     co_await _http.make_request(std::move(req), ignore_reply); } future<> client::put_object(sstring object_name, ::memory_data_sink_buffers bufs) {     s3l.trace("PUT {} (buffers)", object_name);     auto req = http::request::make("PUT", _host, object_name);     auto len = bufs.size();     req.write_body("bin", len, [bufs = std::move(bufs)] (output_stream<char>&& out_) mutable -> future<> {         auto out = std::move(out_);         std::exception_ptr ex;         try {             for (const auto& buf : bufs.buffers()) {                 co_await out.write(buf.get(), buf.size());             }             co_await out.flush();         } catch (...) {             ex = std::current_exception();         }         co_await out.close();         if (ex) {             co_await coroutine::return_exception_ptr(std::move(ex));         }     });     authorize(req);     co_await _http.make_request(std::move(req), ignore_reply); } future<> client::delete_object(sstring object_name) {     s3l.trace("DELETE {}", object_name);     auto req = http::request::make("DELETE", _host, object_name);     authorize(req);     co_await _http.make_request(std::move(req), ignore_reply, http::reply::status_type::no_content); } class client::upload_sink : public data_sink_impl {     // "Each part must be at least 5 MB in size, except the last part."
    // https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPart.html
    static constexpr size_t minimum_part_size = 5 << 20;     static constexpr int flush_concurrency = 3;     shared_ptr<client> _client;     http::experimental::client& _http;     sstring _object_name;     memory_data_sink_buffers _bufs;     sstring _upload_id;     utils::chunked_vector<sstring> _part_etags;     semaphore _flush_sem{flush_concurrency};     future<> start_upload();     future<> finalize_upload();     future<> maybe_flush();     future<> do_flush();     future<> upload_part(unsigned part_number, memory_data_sink_buffers bufs);     future<> abort_upload();     bool upload_started() const noexcept {         return !_upload_id.empty();     } public:     upload_sink(shared_ptr<client> cln, sstring object_name)         : _client(std::move(cln))         , _http(_client->_http)         , _object_name(std::move(object_name))     {     }     virtual future<> put(net::packet) override {         throw_with_backtrace<std::runtime_error>("s3 put(net::packet) unsupported");     }     virtual future<> put(temporary_buffer<char> buf) override {         _bufs.put(std::move(buf));         return maybe_flush();     }     virtual future<> put(std::vector<temporary_buffer<char>> data) override {         for (auto&& buf : data) {             _bufs.put(std::move(buf));         }         return maybe_flush();     }     virtual future<> flush() override {         return finalize_upload();     }     virtual future<> close() override;     virtual size_t buffer_size() const noexcept override {         return 128 * 1024;     } }; future<> client::upload_sink::maybe_flush() {     if (_bufs.size() >= minimum_part_size) {         co_await do_flush();     } } future<> client::upload_sink::do_flush() {     if (!upload_started()) {         co_await start_upload();     }     auto pn = _part_etags.size();     _part_etags.emplace_back();     co_await upload_part(pn, std::move(_bufs)); } sstring parse_multipart_upload_id(sstring& body) {     auto doc = std::make_unique<rapidxml::xml_document<>>();     try {         doc->parse<0>(body.data());     } catch (const rapidxml::parse_error& e) {         s3l.warn("cannot parse initiate multipart upload response: {}", e.what());         // The caller is supposed to check the upload-id to be empty
        // and handle the error the way it prefers
        return "";     }     auto root_node = doc->first_node("InitiateMultipartUploadResult");     auto uploadid_node = root_node->first_node("UploadId");     return uploadid_node->value(); } static constexpr std::string_view multipart_upload_complete_header =         "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n"         "<CompleteMultipartUpload xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">"; static constexpr std::string_view multipart_upload_complete_entry =         "<Part><ETag>{}</ETag><PartNumber>{}</PartNumber></Part>"; static constexpr std::string_view multipart_upload_complete_trailer =         "</CompleteMultipartUpload>"; unsigned prepare_multipart_upload_parts(const utils::chunked_vector<sstring>& etags) {     unsigned ret = multipart_upload_complete_header.size();     unsigned nr = 1;     for (auto& etag : etags) {         if (etag.empty()) {             // 0 here means some part failed to upload, see comment in upload_part()
            // Caller checks it an aborts the multipart upload altogether
            return 0;         }         // length of the format string - four braces + length of the etag + length of the number
        ret += multipart_upload_complete_entry.size() - 4 + etag.size() + format("{}", nr).size();         nr++;     }     ret += multipart_upload_complete_trailer.size();     return ret; } future<> dump_multipart_upload_parts(output_stream<char> out, const utils::chunked_vector<sstring>& etags) {     std::exception_ptr ex;     try {         co_await out.write(multipart_upload_complete_header.data(), multipart_upload_complete_header.size());         unsigned nr = 1;         for (auto& etag : etags) {             assert(!etag.empty());             co_await out.write(format(multipart_upload_complete_entry.data(), etag, nr));             nr++;         }         co_await out.write(multipart_upload_complete_trailer.data(), multipart_upload_complete_trailer.size());         co_await out.flush();     } catch (...) {         ex = std::current_exception();     }     co_await out.close();     if (ex) {         co_await coroutine::return_exception_ptr(std::move(ex));     } } future<> client::upload_sink::start_upload() {     s3l.trace("POST uploads {}", _object_name);     auto rep = http::request::make("POST", _client->_host, _object_name);     rep.query_parameters["uploads"] = "";     _client->authorize(rep);     co_await _http.make_request(std::move(rep), [this] (const http::reply& rep, input_stream<char>&& in_) -> future<> {         auto in = std::move(in_);         auto body = co_await util::read_entire_stream_contiguous(in);         _upload_id = parse_multipart_upload_id(body);         if (_upload_id.empty()) {             co_await coroutine::return_exception(std::runtime_error("cannot initiate upload"));         }         s3l.trace("created uploads for {} -> id = {}", _object_name, _upload_id);     }); } future<> client::upload_sink::upload_part(unsigned part_number, memory_data_sink_buffers bufs) {     s3l.trace("PUT part {} {} bytes in {} buffers (upload id {})", part_number, bufs.size(), bufs.buffers().size(), _upload_id);     auto req = http::request::make("PUT", _client->_host, _object_name);     req._headers["Content-Length"] = format("{}", bufs.size());     req.query_parameters["partNumber"] = format("{}", part_number + 1);     req.query_parameters["uploadId"] = _upload_id;     req.write_body("bin", bufs.size(), [this, part_number, bufs = std::move(bufs)] (output_stream<char>&& out_) mutable -> future<> {         auto out = std::move(out_);         std::exception_ptr ex;         s3l.trace("upload {} part data (upload id {})", part_number, _upload_id);         try {             for (auto&& buf : bufs.buffers()) {                 co_await out.write(buf.get(), buf.size());             }             co_await out.flush();         } catch (...) {             ex = std::current_exception();         }         co_await out.close();         if (ex) {             co_await coroutine::return_exception_ptr(std::move(ex));         }     });     // Do upload in the background so that several parts could go in parallel.
    // The semaphore is used for two things -- control the concurrency and let
    // the finalize_upload() wait in any background activity before checking
    // the progress.
    //
    // In case part upload goes wrong and doesn't happen, the _part_etags[part]
    // is not set, so the finalize_upload() sees it and aborts the whole thing.
    _client->authorize(req);     auto units = co_await get_units(_flush_sem, 1);     (void)_http.make_request(std::move(req), [this, part_number] (const http::reply& rep, input_stream<char>&& in_) mutable -> future<> {         auto etag = rep.get_header("ETag");         s3l.trace("uploaded {} part data -> etag = {} (upload id {})", part_number, etag, _upload_id);         _part_etags[part_number] = std::move(etag);         return make_ready_future<>();     }).handle_exception([this, part_number] (auto ex) {         // ... the exact exception only remains in logs
        s3l.warn("couldn't upload part {}: {} (upload id {})", part_number, ex, _upload_id);     }).finally([units = std::move(units)] {}); } future<> client::upload_sink::abort_upload() {     s3l.trace("DELETE upload {}", _upload_id);     auto req = http::request::make("DELETE", _client->_host, _object_name);     req.query_parameters["uploadId"] = std::exchange(_upload_id, ""); // now upload_started() returns false
    _client->authorize(req);     co_await _http.make_request(std::move(req), ignore_reply, http::reply::status_type::no_content); } future<> client::upload_sink::finalize_upload() {     if (_bufs.size() == 0) {         co_return;     }     co_await do_flush();     s3l.trace("wait for {} parts to complete (upload id {})", _part_etags.size(), _upload_id);     co_await _flush_sem.wait(flush_concurrency);     unsigned parts_xml_len = prepare_multipart_upload_parts(_part_etags);     if (parts_xml_len == 0) {         co_await abort_upload();         co_await coroutine::return_exception(std::runtime_error("couldn't upload parts"));     }     s3l.trace("POST upload completion {} parts (upload id {})", _part_etags.size(), _upload_id);     auto req = http::request::make("POST", _client->_host, _object_name);     req.query_parameters["uploadId"] = std::exchange(_upload_id, ""); // now upload_started() returns false
    req.write_body("xml", parts_xml_len, [this] (output_stream<char>&& out) -> future<> {         return dump_multipart_upload_parts(std::move(out), _part_etags);     });     _client->authorize(req);     co_await _http.make_request(std::move(req), ignore_reply); } future<> client::upload_sink::close() {     if (upload_started()) {         s3l.warn("closing incomplete multipart upload -> aborting");         co_await abort_upload();     } else {         s3l.trace("closing multipart upload");     } } data_sink client::make_upload_sink(sstring object_name) {     return data_sink(std::make_unique<upload_sink>(shared_from_this(), std::move(object_name))); } class client::readable_file : public file_impl {     shared_ptr<client> _client;     http::experimental::client& _http;     sstring _object_name;     [[noreturn]] void unsupported() {         throw_with_backtrace<std::logic_error>("unsupported operation on s3 readable file");     } public:     readable_file(shared_ptr<client> cln, sstring object_name)         : _client(std::move(cln))         , _http(_client->_http)         , _object_name(std::move(object_name))     {     }     virtual future<size_t> write_dma(uint64_t pos, const void* buffer, size_t len, const io_priority_class& pc) override { unsupported(); }     virtual future<size_t> write_dma(uint64_t pos, std::vector<iovec> iov, const io_priority_class& pc) override { unsupported(); }     virtual future<> truncate(uint64_t length) override { unsupported(); }     virtual subscription<directory_entry> list_directory(std::function<future<> (directory_entry de)> next) override { unsupported(); }     virtual future<> flush(void) override { return make_ready_future<>(); }     virtual future<> allocate(uint64_t position, uint64_t length) override { return make_ready_future<>(); }     virtual future<> discard(uint64_t offset, uint64_t length) override { return make_ready_future<>(); }     class readable_file_handle_impl final : public file_handle_impl {         client::handle _h;         sstring _object_name;     public:         readable_file_handle_impl(client::handle h, sstring object_name)                 : _h(std::move(h))                 , _object_name(std::move(object_name))         {}         virtual std::unique_ptr<file_handle_impl> clone() const override {             return std::make_unique<readable_file_handle_impl>(_h, _object_name);         }         virtual shared_ptr<file_impl> to_file() && override {             return make_shared<readable_file>(std::move(_h).to_client(), std::move(_object_name));         }     };     virtual std::unique_ptr<file_handle_impl> dup() override {         return std::make_unique<readable_file_handle_impl>(client::handle(*_client), _object_name);     }     virtual future<uint64_t> size(void) override {         return _client->get_object_size(_object_name);     }     virtual future<struct stat> stat(void) override {         auto object_stats = co_await _client->get_object_stats(_object_name);         struct stat ret {};         ret.st_nlink = 1;         ret.st_mode = S_IFREG | S_IRUSR | S_IRGRP | S_IROTH;         ret.st_size = object_stats.size;         ret.st_blksize = 1 << 10; // huh?
        ret.st_blocks = object_stats.size >> 9;         // objects are immutable on S3, therefore we can use Last-Modified to set both st_mtime and st_ctime
        ret.st_mtime = object_stats.last_modified;         ret.st_ctime = object_stats.last_modified;         co_return ret;     }     virtual future<size_t> read_dma(uint64_t pos, void* buffer, size_t len, const io_priority_class& pc) override {         auto buf = co_await _client->get_object_contiguous(_object_name, range{ pos, len });         std::copy_n(buf.get(), buf.size(), reinterpret_cast<uint8_t*>(buffer));         co_return buf.size();     }     virtual future<size_t> read_dma(uint64_t pos, std::vector<iovec> iov, const io_priority_class& pc) override {         auto buf = co_await _client->get_object_contiguous(_object_name, range{ pos, utils::iovec_len(iov) });         uint64_t off = 0;         for (auto& v : iov) {             auto sz = std::min(v.iov_len, buf.size() - off);             if (sz == 0) {                 break;             }             std::copy_n(buf.get() + off, sz, reinterpret_cast<uint8_t*>(v.iov_base));             off += sz;         }         co_return off;     }     virtual future<temporary_buffer<uint8_t>> dma_read_bulk(uint64_t offset, size_t range_size, const io_priority_class& pc) override {         auto buf = co_await _client->get_object_contiguous(_object_name, range{ offset, range_size });         co_return temporary_buffer<uint8_t>(reinterpret_cast<uint8_t*>(buf.get_write()), buf.size(), buf.release());     }     virtual future<> close() override {         return make_ready_future<>();     } }; file client::make_readable_file(sstring object_name) {     return file(make_shared<readable_file>(shared_from_this(), std::move(object_name))); } future<> client::close() {     co_await _http.close(); } }
 // s3 namespace
namespace dht { static logging::logger logger("i_partitioner"); sharder::sharder(unsigned shard_count, unsigned sharding_ignore_msb_bits)     : _shard_count(shard_count)     // if one shard, ignore sharding_ignore_msb_bits as they will just cause needless
    // range breaks
    , _sharding_ignore_msb_bits(shard_count > 1 ? sharding_ignore_msb_bits : 0)     , _shard_start(init_zero_based_shard_start(_shard_count, _sharding_ignore_msb_bits)) {} unsigned sharder::shard_of(const token& t) const {     return dht::shard_of(_shard_count, _sharding_ignore_msb_bits, t); } token sharder::token_for_next_shard(const token& t, shard_id shard, unsigned spans) const {     return dht::token_for_next_shard(_shard_start, _shard_count, _sharding_ignore_msb_bits, t, shard, spans); }   std::unique_ptr<dht::i_partitioner> make_partitioner(sstring partitioner_name) {     try {         return create_object<i_partitioner>(partitioner_name);     } catch (std::exception& e) {         auto supported_partitioners = fmt::join(             class_registry<i_partitioner>::classes() |             boost::adaptors::map_keys,             ", ");         throw std::runtime_error(format("Partitioner {} is not supported, supported partitioners = {{ {} }} : {}",                 partitioner_name, supported_partitioners, e.what()));     } } bool decorated_key::equal(const schema& s, const decorated_key& other) const {     if (_token == other._token) {         return _key.legacy_equal(s, other._key);     }     return false; } std::strong_ordering decorated_key::tri_compare(const schema& s, const decorated_key& other) const {     auto r = _token <=> other._token;     if (r != 0) {         return r;     } else {         return _key.legacy_tri_compare(s, other._key);     } } std::strong_ordering decorated_key::tri_compare(const schema& s, const ring_position& other) const {     auto r = _token <=> other.token();     if (r != 0) {         return r;     } else if (other.has_key()) {         return _key.legacy_tri_compare(s, *other.key());     }     return 0 <=> other.relation_to_keys(); } bool decorated_key::less_compare(const schema& s, const ring_position& other) const {     return tri_compare(s, other) < 0; } bool decorated_key::less_compare(const schema& s, const decorated_key& other) const {     return tri_compare(s, other) < 0; } decorated_key::less_comparator::less_comparator(schema_ptr s)     : s(std::move(s)) { } bool decorated_key::less_comparator::operator()(const decorated_key& lhs, const decorated_key& rhs) const {     return lhs.less_compare(*s, rhs); } bool decorated_key::less_comparator::operator()(const ring_position& lhs, const decorated_key& rhs) const {     return rhs.tri_compare(*s, lhs) > 0; } bool decorated_key::less_comparator::operator()(const decorated_key& lhs, const ring_position& rhs) const {     return lhs.tri_compare(*s, rhs) < 0; }      std::optional<dht::token_range> selective_token_range_sharder::next() {     if (_done) {         return {};     }     while (_range.overlaps(dht::token_range(_start_boundary, {}), dht::token_comparator())             && !(_start_boundary && _start_boundary->value() == maximum_token())) {         auto end_token = _sharder.token_for_next_shard(_start_token, _next_shard);         auto candidate = dht::token_range(std::move(_start_boundary), range_bound<dht::token>(end_token, false));         auto intersection = _range.intersection(std::move(candidate), dht::token_comparator());         _start_token = _sharder.token_for_next_shard(end_token, _shard);         _start_boundary = range_bound<dht::token>(_start_token);         if (intersection) {             return *intersection;         }     }     _done = true;     return {}; } std::optional<ring_position_range_and_shard> ring_position_range_sharder::next(const schema& s) {     if (_done) {         return {};     }     auto shard = _range.start() ? _sharder.shard_of(_range.start()->value().token()) : token::shard_of_minimum_token();     auto next_shard = shard + 1 < _sharder.shard_count() ? shard + 1 : 0;     auto shard_boundary_token = _sharder.token_for_next_shard(_range.start() ? _range.start()->value().token() : minimum_token(), next_shard);     auto shard_boundary = ring_position::starting_at(shard_boundary_token);     if ((!_range.end() || shard_boundary.less_compare(s, _range.end()->value()))             && shard_boundary_token != maximum_token()) {         // split the range at end_of_shard
        auto start = _range.start();         auto end = range_bound<ring_position>(shard_boundary, false);         _range = dht::partition_range(                 range_bound<ring_position>(std::move(shard_boundary), true),                 std::move(_range.end()));         return ring_position_range_and_shard{dht::partition_range(std::move(start), std::move(end)), shard};     }     _done = true;     return ring_position_range_and_shard{std::move(_range), shard}; } ring_position_range_vector_sharder::ring_position_range_vector_sharder(const sharder& sharder, dht::partition_range_vector ranges)         : _ranges(std::move(ranges))         , _sharder(sharder)         , _current_range(_ranges.begin()) {     next_range(); } std::optional<ring_position_range_and_shard_and_element> ring_position_range_vector_sharder::next(const schema& s) {     if (!_current_sharder) {         return std::nullopt;     }     auto range_and_shard = _current_sharder->next(s);     while (!range_and_shard && _current_range != _ranges.end()) {         next_range();         range_and_shard = _current_sharder->next(s);     }     auto ret = std::optional<ring_position_range_and_shard_and_element>();     if (range_and_shard) {         ret.emplace(std::move(*range_and_shard), _current_range - _ranges.begin() - 1);     }     return ret; } future<utils::chunked_vector<partition_range>> split_range_to_single_shard(const schema& s, const partition_range& pr, shard_id shard) {     const sharder& sharder = s.get_sharder();     auto next_shard = shard + 1 == sharder.shard_count() ? 0 : shard + 1;     auto start_token = pr.start() ? pr.start()->value().token() : minimum_token();     auto start_shard = sharder.shard_of(start_token);     auto start_boundary = start_shard == shard ? pr.start() : range_bound<ring_position>(ring_position::starting_at(sharder.token_for_next_shard(start_token, shard)));     return repeat_until_value([&sharder,             &pr,             cmp = ring_position_comparator(s),             ret = utils::chunked_vector<partition_range>(),             start_token,             start_boundary,             shard,             next_shard] () mutable {         if (pr.overlaps(partition_range(start_boundary, {}), cmp)                 && !(start_boundary && start_boundary->value().token() == maximum_token())) {             auto end_token = sharder.token_for_next_shard(start_token, next_shard);             auto candidate = partition_range(std::move(start_boundary), range_bound<ring_position>(ring_position::starting_at(end_token), false));             auto intersection = pr.intersection(std::move(candidate), cmp);             if (intersection) {                 ret.push_back(std::move(*intersection));             }             start_token = sharder.token_for_next_shard(end_token, shard);             start_boundary = range_bound<ring_position>(ring_position::starting_at(start_token));             return make_ready_future<std::optional<utils::chunked_vector<partition_range>>>();         }         return make_ready_future<std::optional<utils::chunked_vector<partition_range>>>(std::move(ret));     }); } std::strong_ordering ring_position::tri_compare(const schema& s, const ring_position& o) const {     return ring_position_comparator(s)(*this, o); } std::strong_ordering token_comparator::operator()(const token& t1, const token& t2) const {     return t1 <=> t2; } bool ring_position::equal(const schema& s, const ring_position& other) const {     return tri_compare(s, other) == 0; } bool ring_position::less_compare(const schema& s, const ring_position& other) const {     return tri_compare(s, other) < 0; }  std::strong_ordering ring_position_comparator_for_sstables::operator()(ring_position_view lh, sstables::decorated_key_view rh) const {     auto token_cmp = *lh._token <=> rh.token();     if (token_cmp != 0) {         return token_cmp;     }     if (lh._key) {         auto rel = rh.key().tri_compare(s, *lh._key);         if (rel != std::strong_ordering::equal) {             return 0 <=> rel;         }     }     return lh._weight <=> 0; } std::strong_ordering ring_position_comparator_for_sstables::operator()(sstables::decorated_key_view a, ring_position_view b) const {     return 0 <=> (*this)(b, a); }    flat_mutation_reader_v2::filter incremental_owned_ranges_checker::make_partition_filter(const dht::token_range_vector& sorted_owned_ranges) {     return [checker = incremental_owned_ranges_checker(sorted_owned_ranges)] (const dht::decorated_key& dk) mutable {         return checker.belongs_to_current_node(dk.token());     }; }   }
 namespace dht { using uint128_t = unsigned __int128; inline int64_t long_token(const token& t) {     if (t.is_minimum() || t.is_maximum()) {         return std::numeric_limits<int64_t>::min();     }     return t._data; } static const token min_token{ token::kind::before_all_keys, 0 }; static const token max_token{ token::kind::after_all_keys, 0 };  const token& maximum_token() noexcept {     return max_token; }   sstring token::to_sstring() const {     return seastar::to_sstring<sstring>(long_token(*this)); } token token::midpoint(const token& t1, const token& t2) {     uint64_t l1 = long_token(t1);     uint64_t l2 = long_token(t2);     int64_t mid = l1 + (l2 - l1)/2;     return token{kind::key, mid}; } token token::get_random_token() {     static thread_local std::default_random_engine re{std::random_device{}()};     // std::numeric_limits<int64_t>::min() value is reserved and shouldn't
    // be used for regular tokens.
    static thread_local std::uniform_int_distribution<int64_t> dist(             std::numeric_limits<int64_t>::min() + 1);     return token(kind::key, dist(re)); } token token::from_sstring(const sstring& t) {     auto lp = boost::lexical_cast<long>(t);     if (lp == std::numeric_limits<long>::min()) {         return minimum_token();     } else {         return token(kind::key, uint64_t(lp));     } } token token::from_bytes(bytes_view bytes) {     if (bytes.size() != sizeof(int64_t)) {         throw runtime_exception(format("Invalid token. Should have size {:d}, has size {:d}\n", sizeof(int64_t), bytes.size()));     }     auto tok = net::ntoh(read_unaligned<int64_t>(bytes.begin()));     if (tok == std::numeric_limits<int64_t>::min()) {         return minimum_token();     } else {         return dht::token(dht::token::kind::key, tok);     } } static float ratio_helper(int64_t a, int64_t b) {     uint64_t val = (a > b)? static_cast<uint64_t>(a) - static_cast<uint64_t>(b) : (static_cast<uint64_t>(a) - static_cast<uint64_t>(b) - 1);     return val/(float)std::numeric_limits<uint64_t>::max(); } std::map<token, float> token::describe_ownership(const std::vector<token>& sorted_tokens) {     std::map<token, float> ownerships;     auto i = sorted_tokens.begin();     // 0-case
    if (i == sorted_tokens.end()) {         throw runtime_exception("No nodes present in the cluster. Has this node finished starting up?");     }     // 1-case
    if (sorted_tokens.size() == 1) {         ownerships[sorted_tokens[0]] = 1.0;     // n-case
    } else {         const token& start = sorted_tokens[0];         int64_t ti = long_token(start);  // The first token and its value
        int64_t start_long = ti;         int64_t tim1 = ti; // The last token and its value (after loop)
        for (i++; i != sorted_tokens.end(); i++) {             ti = long_token(*i); // The next token and its value
            ownerships[*i]= ratio_helper(ti, tim1);  // save (T(i) -> %age)
            tim1 = ti;         }         // The start token's range extends backward to the last token, which is why both were saved above.
        ownerships[start] = ratio_helper(start_long, ti);     }     return ownerships; } data_type token::get_token_validator() {     return long_type; } uint64_t unbias(const token& t) {     return uint64_t(long_token(t)) + uint64_t(std::numeric_limits<int64_t>::min()); } token bias(uint64_t n) {     return token(token::kind::key, n - uint64_t(std::numeric_limits<int64_t>::min())); } inline unsigned zero_based_shard_of(uint64_t token, unsigned shards, unsigned sharding_ignore_msb_bits) {     // This is the master function, the inverses have to match it wrt. rounding errors.
    token <<= sharding_ignore_msb_bits;     // Treat "token" as a fraction in the interval [0, 1); compute:
    //    shard = floor((0.token) * shards)
    return (uint128_t(token) * shards) >> 64; } std::vector<uint64_t> init_zero_based_shard_start(unsigned shards, unsigned sharding_ignore_msb_bits) {     // computes the inverse of zero_based_shard_of(). ret[s] will return the smallest token that belongs to s
    if (shards == 1) {         // Avoid the while loops below getting confused finding the "edge" between two nonexistent shards
        return std::vector<uint64_t>(1, uint64_t(0));     }     auto ret = std::vector<uint64_t>(shards);     for (auto s : boost::irange<unsigned>(0, shards)) {         uint64_t token = (uint128_t(s) << 64) / shards;         token >>= sharding_ignore_msb_bits;   // leftmost bits are ignored by zero_based_shard_of
        // token is the start of the next shard, and can be slightly before due to rounding errors; adjust
        while (zero_based_shard_of(token, shards, sharding_ignore_msb_bits) != s) {             ++token;         }         ret[s] = token;     }     return ret; } unsigned shard_of(unsigned shard_count, unsigned sharding_ignore_msb_bits, const token& t) {     switch (t._kind) {         case token::kind::before_all_keys:             return token::shard_of_minimum_token();         case token::kind::after_all_keys:             return shard_count - 1;         case token::kind::key:             uint64_t adjusted = unbias(t);             return zero_based_shard_of(adjusted, shard_count, sharding_ignore_msb_bits);     }     abort(); } token token_for_next_shard(const std::vector<uint64_t>& shard_start, unsigned shard_count, unsigned sharding_ignore_msb_bits, const token& t, shard_id shard, unsigned spans) {     uint64_t n = 0;     switch (t._kind) {         case token::kind::before_all_keys:             break;         case token::kind::after_all_keys:             return maximum_token();         case token::kind::key:             n = unbias(t);             break;     }     auto s = zero_based_shard_of(n, shard_count, sharding_ignore_msb_bits);     if (!sharding_ignore_msb_bits) {         // This ought to be the same as the else branch, but avoids shifts by 64
        n = shard_start[shard];         if (spans > 1 || shard <= s) {             return maximum_token();         }     } else {         auto left_part = n >> (64 - sharding_ignore_msb_bits);         left_part += spans - unsigned(shard > s);         if (left_part >= (1u << sharding_ignore_msb_bits)) {             return maximum_token();         }         left_part <<= (64 - sharding_ignore_msb_bits);         auto right_part = shard_start[shard];         n = left_part | right_part;     }     return bias(n); } int64_t token::to_int64(token t) {     return long_token(t); } dht::token token::from_int64(int64_t i) {     return {kind::key, i}; } static dht::token find_first_token_for_shard_in_not_wrap_around_range(const dht::sharder& sharder, dht::token start, dht::token end, size_t shard_idx) {     // Invariant start < end
    // It is guaranteed that start is not MAX_INT64 because end is greater
    auto t = dht::token::from_int64(dht::token::to_int64(start) + 1);     if (sharder.shard_of(t) != shard_idx) {         t = sharder.token_for_next_shard(t, shard_idx);     }     return std::min(t, end); }    }
 // namespace dht
namespace dht { // Note: Cassandra has a special case where for an empty key it returns
// minimum_token() instead of 0 (the naturally-calculated hash function for
// an empty string). Their thinking was that empty partition keys are not
// allowed anyway. However, they *are* allowed in materialized views, so the
// empty-key partition should get a real token, not an invalid token, so
// we dropped this special case. Since we don't support migrating sstables of
// materialized-views from Cassandra, this Cassandra-Scylla incompatiblity
// will not cause problems in practice.
// Note that get_token(const schema& s, partition_key_view key) below must
// use exactly the same algorithm as this function.
token murmur3_partitioner::get_token(bytes_view key) const {     std::array<uint64_t, 2> hash;     utils::murmur_hash::hash3_x64_128(key, 0, hash);     return get_token(hash[0]); } token murmur3_partitioner::get_token(uint64_t value) const {     return token(token::kind::key, value); } token murmur3_partitioner::get_token(const sstables::key_view& key) const {     return key.with_linearized([&] (bytes_view v) {         return get_token(v);     }); } token murmur3_partitioner::get_token(const schema& s, partition_key_view key) const {     std::array<uint64_t, 2> hash;     auto&& legacy = key.legacy_form(s);     utils::murmur_hash::hash3_x64_128(legacy.begin(), legacy.size(), 0, hash);     return get_token(hash[0]); } using registry = class_registrator<i_partitioner, murmur3_partitioner>; static registry registrator("org.apache.cassandra.dht.Murmur3Partitioner"); static registry registrator_short_name("Murmur3Partitioner"); }
 static logging::logger blogger("boot_strapper");
 namespace dht { future<> boot_strapper::bootstrap(streaming::stream_reason reason, gms::gossiper& gossiper, inet_address replace_address) {     blogger.debug("Beginning bootstrap process: sorted_tokens={}", get_token_metadata().sorted_tokens());     sstring description;     if (reason == streaming::stream_reason::bootstrap) {         description = "Bootstrap";     } else if (reason == streaming::stream_reason::replace) {         description = "Replace";     } else {         throw std::runtime_error("Wrong stream_reason provided: it can only be replace or bootstrap");     }     try {         auto streamer = make_lw_shared<range_streamer>(_db, _stream_manager, _token_metadata_ptr, _abort_source, _tokens, _address, _dr, description, reason);         auto nodes_to_filter = gossiper.get_unreachable_members();         if (reason == streaming::stream_reason::replace) {             nodes_to_filter.insert(std::move(replace_address));         }         blogger.debug("nodes_to_filter={}", nodes_to_filter);         streamer->add_source_filter(std::make_unique<range_streamer::failure_detector_source_filter>(nodes_to_filter));         auto ks_erms = _db.local().get_non_local_strategy_keyspaces_erms();         for (const auto& [keyspace_name, erm] : ks_erms) {             auto& strategy = erm->get_replication_strategy();             // We took a strategy ptr to keep it alive during the `co_await`.
            // The keyspace may be dropped in the meantime.
            dht::token_range_vector ranges = co_await strategy.get_pending_address_ranges(_token_metadata_ptr, _tokens, _address, _dr);             blogger.debug("Will stream keyspace={}, ranges={}", keyspace_name, ranges);             co_await streamer->add_ranges(keyspace_name, erm, std::move(ranges), gossiper, reason == streaming::stream_reason::replace);         }         _abort_source.check();         co_await streamer->stream_async();     } catch (...) {         blogger.warn("Error during bootstrap: {}", std::current_exception());         throw;     } } std::unordered_set<token> boot_strapper::get_random_bootstrap_tokens(const token_metadata_ptr tmptr, size_t num_tokens, dht::check_token_endpoint check) {     if (num_tokens < 1) {         throw std::runtime_error("num_tokens must be >= 1");     }     if (num_tokens == 1) {         blogger.warn("Picking random token for a single vnode.  You should probably add more vnodes; failing that, you should probably specify the token manually");     }     auto tokens = get_random_tokens(std::move(tmptr), num_tokens);     blogger.info("Get random bootstrap_tokens={}", tokens);     return tokens; } std::unordered_set<token> boot_strapper::get_bootstrap_tokens(const token_metadata_ptr tmptr, const db::config& cfg, dht::check_token_endpoint check) {     std::unordered_set<sstring> initial_tokens;     sstring tokens_string = cfg.initial_token();     try {         boost::split(initial_tokens, tokens_string, boost::is_any_of(sstring(", ")));     } catch (...) {         throw std::runtime_error(format("Unable to parse initial_token={}", tokens_string));     }     initial_tokens.erase("");     // if user specified tokens, use those
    if (initial_tokens.size() > 0) {         blogger.debug("tokens manually specified as {}", initial_tokens);         std::unordered_set<token> tokens;         for (auto& token_string : initial_tokens) {             auto token = dht::token::from_sstring(token_string);             if (check && tmptr->get_endpoint(token)) {                 throw std::runtime_error(format("Bootstrapping to existing token {} is not allowed (decommission/removenode the old node first).", token_string));             }             tokens.insert(token);         }         blogger.info("Get manually specified bootstrap_tokens={}", tokens);         return tokens;     }     return get_random_bootstrap_tokens(tmptr, cfg.num_tokens(), check); } std::unordered_set<token> boot_strapper::get_random_tokens(const token_metadata_ptr tmptr, size_t num_tokens) {     std::unordered_set<token> tokens;     while (tokens.size() < num_tokens) {         auto token = dht::token::get_random_token();         auto ep = tmptr->get_endpoint(token);         if (!ep) {             tokens.emplace(token);         }     }     return tokens; } }
 // namespace dht
namespace dht { using inet_address = gms::inet_address; std::unordered_map<inet_address, dht::token_range_vector> range_streamer::get_range_fetch_map(const std::unordered_map<dht::token_range, std::vector<inet_address>>& ranges_with_sources,                                     const std::unordered_set<std::unique_ptr<i_source_filter>>& source_filters,                                     const sstring& keyspace) {     std::unordered_map<inet_address, dht::token_range_vector> range_fetch_map_map;     for (const auto& x : ranges_with_sources) {         const dht::token_range& range_ = x.first;         const std::vector<inet_address>& addresses = x.second;         bool found_source = false;         for (const auto& address : addresses) {             if (address == utils::fb_utilities::get_broadcast_address()) {                 // If localhost is a source, we have found one, but we don't add it to the map to avoid streaming locally
                found_source = true;                 continue;             }             auto filtered = false;             for (const auto& filter : source_filters) {                 if (!filter->should_include(get_token_metadata().get_topology(), address)) {                     filtered = true;                     break;                 }             }             if (filtered) {                 logger.debug("In get_range_fetch_map, keyspace = {}, endpoint= {} is filtered", keyspace, address);                 continue;             }             range_fetch_map_map[address].push_back(range_);             found_source = true;             break; // ensure we only stream from one other node for each range
        }         if (!found_source) {             auto& ks = _db.local().find_keyspace(keyspace);             auto rf = ks.get_effective_replication_map()->get_replication_factor();             // When a replacing node replaces a dead node with keyspace of RF
            // 1, it is expected that replacing node could not find a peer node
            // that contains data to stream from.
            if (_reason == streaming::stream_reason::replace && rf == 1) {                 logger.warn("Unable to find sufficient sources to stream range {} for keyspace {} with RF = 1 for replace operation", range_, keyspace);             } else {                 throw std::runtime_error(format("unable to find sufficient sources for streaming range {} in keyspace {}", range_, keyspace));             }         }     }     return range_fetch_map_map; } // Must be called from a seastar thread
std::unordered_map<dht::token_range, std::vector<inet_address>> range_streamer::get_all_ranges_with_sources_for(const sstring& keyspace_name, locator::vnode_effective_replication_map_ptr erm, dht::token_range_vector desired_ranges) {     logger.debug("{} ks={}", __func__, keyspace_name);     auto range_addresses = erm->get_range_addresses().get0();     logger.debug("keyspace={}, desired_ranges.size={}, range_addresses.size={}", keyspace_name, desired_ranges.size(), range_addresses.size());     std::unordered_map<dht::token_range, std::vector<inet_address>> range_sources;     for (auto& desired_range : desired_ranges) {         auto found = false;         for (auto& x : range_addresses) {             if (need_preempt()) {                 seastar::thread::yield();             }             const range<token>& src_range = x.first;             if (src_range.contains(desired_range, dht::operator<=>)) {                 inet_address_vector_replica_set preferred(x.second.begin(), x.second.end());                 get_token_metadata().get_topology().sort_by_proximity(_address, preferred);                 for (inet_address& p : preferred) {                     range_sources[desired_range].push_back(p);                 }                 found = true;             }         }         if (!found) {             throw std::runtime_error(format("No sources found for {}", desired_range));         }     }     return range_sources; } // Must be called from a seastar thread
std::unordered_map<dht::token_range, std::vector<inet_address>> range_streamer::get_all_ranges_with_strict_sources_for(const sstring& keyspace_name, locator::vnode_effective_replication_map_ptr erm, dht::token_range_vector desired_ranges, gms::gossiper& gossiper) {     logger.debug("{} ks={}", __func__, keyspace_name);     assert (_tokens.empty() == false);     auto& strat = erm->get_replication_strategy();     //Active ranges
    auto metadata_clone = get_token_metadata().clone_only_token_map().get0();     auto range_addresses = strat.get_range_addresses(metadata_clone).get0();     //Pending ranges
    metadata_clone.update_topology(_address, _dr);     metadata_clone.update_normal_tokens(_tokens, _address).get();     auto pending_range_addresses  = strat.get_range_addresses(metadata_clone).get0();     metadata_clone.clear_gently().get();     //Collects the source that will have its range moved to the new node
    std::unordered_map<dht::token_range, std::vector<inet_address>> range_sources;     logger.debug("keyspace={}, desired_ranges.size={}, range_addresses.size={}", keyspace_name, desired_ranges.size(), range_addresses.size());     for (auto& desired_range : desired_ranges) {         for (auto& x : range_addresses) {             const range<token>& src_range = x.first;             if (need_preempt()) {                 seastar::thread::yield();             }             if (src_range.contains(desired_range, dht::operator<=>)) {                 std::vector<inet_address> old_endpoints(x.second.begin(), x.second.end());                 auto it = pending_range_addresses.find(desired_range);                 if (it == pending_range_addresses.end()) {                     throw std::runtime_error(format("Can not find desired_range = {} in pending_range_addresses", desired_range));                 }                 std::unordered_set<inet_address> new_endpoints(it->second.begin(), it->second.end());                 //Due to CASSANDRA-5953 we can have a higher RF then we have endpoints.
                //So we need to be careful to only be strict when endpoints == RF
                if (old_endpoints.size() == erm->get_replication_factor()) {                     std::erase_if(old_endpoints,                         [&new_endpoints] (inet_address ep) { return new_endpoints.contains(ep); });                     if (old_endpoints.size() != 1) {                         throw std::runtime_error(format("Expected 1 endpoint but found {:d}", old_endpoints.size()));                     }                 }                 range_sources[desired_range].push_back(old_endpoints.front());             }         }         //Validate
        auto it = range_sources.find(desired_range);         if (it == range_sources.end()) {             throw std::runtime_error(format("No sources found for {}", desired_range));         }         if (it->second.size() != 1) {             throw std::runtime_error(format("Multiple endpoints found for {}", desired_range));         }         inet_address source_ip = it->second.front();         if (gossiper.is_enabled() && !gossiper.is_alive(source_ip)) {             throw std::runtime_error(format("A node required to move the data consistently is down ({}).  If you wish to move the data from a potentially inconsistent replica, restart the node with consistent_rangemovement=false", source_ip));         }     }     return range_sources; } bool range_streamer::use_strict_sources_for_ranges(const sstring& keyspace_name, const locator::vnode_effective_replication_map_ptr& erm) {     auto rf = erm->get_replication_factor();     auto nr_nodes_in_ring = get_token_metadata().get_all_endpoints().size();     bool everywhere_topology = erm->get_replication_strategy().get_type() == locator::replication_strategy_type::everywhere_topology;     // Use strict when number of nodes in the ring is equal or more than RF
    auto strict = _db.local().get_config().consistent_rangemovement()            && !_tokens.empty()            && !everywhere_topology            && nr_nodes_in_ring >= rf;     logger.debug("use_strict_sources_for_ranges: ks={}, nr_nodes_in_ring={}, rf={}, strict={}",             keyspace_name, nr_nodes_in_ring, rf, strict);     return strict; } void range_streamer::add_tx_ranges(const sstring& keyspace_name, std::unordered_map<inet_address, dht::token_range_vector> ranges_per_endpoint) {     if (_nr_rx_added) {         throw std::runtime_error("Mixed sending and receiving is not supported");     }     _nr_tx_added++;     _to_stream.emplace(keyspace_name, std::move(ranges_per_endpoint)); } void range_streamer::add_rx_ranges(const sstring& keyspace_name, std::unordered_map<inet_address, dht::token_range_vector> ranges_per_endpoint) {     if (_nr_tx_added) {         throw std::runtime_error("Mixed sending and receiving is not supported");     }     _nr_rx_added++;     _to_stream.emplace(keyspace_name, std::move(ranges_per_endpoint)); } // TODO: This is the legacy range_streamer interface, it is add_rx_ranges which adds rx ranges.
future<> range_streamer::add_ranges(const sstring& keyspace_name, locator::vnode_effective_replication_map_ptr erm, dht::token_range_vector ranges, gms::gossiper& gossiper, bool is_replacing) {   return seastar::async([this, keyspace_name, erm = std::move(erm), ranges= std::move(ranges), &gossiper, is_replacing] () mutable {     if (_nr_tx_added) {         throw std::runtime_error("Mixed sending and receiving is not supported");     }     _nr_rx_added++;     auto ranges_for_keyspace = !is_replacing && use_strict_sources_for_ranges(keyspace_name, erm)         ? get_all_ranges_with_strict_sources_for(keyspace_name, erm, std::move(ranges), gossiper)         : get_all_ranges_with_sources_for(keyspace_name, erm, std::move(ranges));     if (logger.is_enabled(logging::log_level::debug)) {         for (auto& x : ranges_for_keyspace) {             logger.debug("{} : keyspace {} range {} exists on {}", _description, keyspace_name, x.first, x.second);         }     }     std::unordered_map<inet_address, dht::token_range_vector> range_fetch_map = get_range_fetch_map(ranges_for_keyspace, _source_filters, keyspace_name);     utils::clear_gently(ranges_for_keyspace).get();     if (logger.is_enabled(logging::log_level::debug)) {         for (auto& x : range_fetch_map) {             logger.debug("{} : keyspace={}, ranges={} from source={}, range_size={}", _description, keyspace_name, x.second, x.first, x.second.size());         }     }     _to_stream.emplace(keyspace_name, std::move(range_fetch_map));   }); } future<> range_streamer::stream_async() {     _nr_ranges_remaining = nr_ranges_to_stream();     _nr_total_ranges = _nr_ranges_remaining;     logger.info("{} starts, nr_ranges_remaining={}", _description, _nr_ranges_remaining);     auto start = lowres_clock::now();     return do_for_each(_to_stream, [this, description = _description] (auto& stream) {         const auto& keyspace = stream.first;         auto& ip_range_vec = stream.second;         auto ips = boost::copy_range<std::list<inet_address>>(ip_range_vec | boost::adaptors::map_keys);         // Fetch from or send to peer node in parallel
        logger.info("{} with {} for keyspace={} started, nodes_to_stream={}", description, ips, keyspace, ip_range_vec.size());         return parallel_for_each(ip_range_vec, [this, description, keyspace] (auto& ip_range) {           auto& source = ip_range.first;           auto& range_vec = ip_range.second;           return seastar::with_semaphore(_limiter, 1, [this, description, keyspace, source, &range_vec] () mutable {             return seastar::async([this, description, keyspace, source, &range_vec] () mutable {                 // TODO: It is better to use fiber instead of thread here because
                // creating a thread per peer can be some memory in a large cluster.
                auto start_time = lowres_clock::now();                 unsigned sp_index = 0;                 unsigned nr_ranges_streamed = 0;                 size_t nr_ranges_total = range_vec.size();                 size_t nr_ranges_per_stream_plan = nr_ranges_total / 10;                 auto do_streaming = [&] (dht::token_range_vector&& ranges_to_stream) {                     auto sp = stream_plan(_stream_manager.local(), format("{}-{}-index-{:d}", description, keyspace, sp_index++), _reason);                     auto abort_listener = _abort_source.subscribe([&] () noexcept { sp.abort(); });                     _abort_source.check();                     logger.info("{} with {} for keyspace={}, streaming [{}, {}) out of {} ranges",                             description, source, keyspace,                             nr_ranges_streamed, nr_ranges_streamed + ranges_to_stream.size(), nr_ranges_total);                     auto ranges_streamed = ranges_to_stream.size();                     if (_nr_rx_added) {                         sp.request_ranges(source, keyspace, std::move(ranges_to_stream));                     } else if (_nr_tx_added) {                         sp.transfer_ranges(source, keyspace, std::move(ranges_to_stream));                     }                     sp.execute().discard_result().get();                     // Update finished percentage
                    nr_ranges_streamed += ranges_streamed;                     _nr_ranges_remaining -= ranges_streamed;                     float percentage = _nr_total_ranges == 0 ? 1 : (_nr_total_ranges - _nr_ranges_remaining) / (float)_nr_total_ranges;                     _stream_manager.local().update_finished_percentage(_reason, percentage);                     logger.info("Finished {} out of {} ranges for {}, finished percentage={}",                             _nr_total_ranges - _nr_ranges_remaining, _nr_total_ranges, _reason, percentage);                 };                 dht::token_range_vector ranges_to_stream;                 try {                     for (auto it = range_vec.begin(); it < range_vec.end();) {                         ranges_to_stream.push_back(*it);                         ++it;                         if (ranges_to_stream.size() < nr_ranges_per_stream_plan) {                             continue;                         } else {                             do_streaming(std::exchange(ranges_to_stream, {}));                             it = range_vec.erase(range_vec.begin(), it);                         }                     }                     if (ranges_to_stream.size() > 0) {                         do_streaming(std::exchange(ranges_to_stream, {}));                         range_vec.clear();                     }                 } catch (...) {                     auto t = std::chrono::duration_cast<std::chrono::duration<float>>(lowres_clock::now() - start_time).count();                     logger.warn("{} with {} for keyspace={} failed, took {} seconds: {}", description, source, keyspace, t, std::current_exception());                     throw;                 }                 auto t = std::chrono::duration_cast<std::chrono::duration<float>>(lowres_clock::now() - start_time).count();                 logger.info("{} with {} for keyspace={} succeeded, took {} seconds", description, source, keyspace, t);               });           });         });     }).finally([this, start] {         auto t = std::chrono::duration_cast<std::chrono::seconds>(lowres_clock::now() - start).count();         auto nr_ranges_remaining = nr_ranges_to_stream();         if (nr_ranges_remaining) {             logger.warn("{} failed, took {} seconds, nr_ranges_remaining={}", _description, t, nr_ranges_remaining);         } else {             logger.info("{} succeeded, took {} seconds, nr_ranges_remaining={}", _description, t, nr_ranges_remaining);         }     }); } size_t range_streamer::nr_ranges_to_stream() {     size_t nr_ranges_remaining = 0;     for (auto& fetch : _to_stream) {         const auto& keyspace = fetch.first;         auto& ip_range_vec = fetch.second;         for (auto& ip_range : ip_range_vec) {             auto& source = ip_range.first;             auto& range_vec = ip_range.second;             nr_ranges_remaining += range_vec.size();             logger.debug("Remaining: keyspace={}, source={}, ranges={}", keyspace, source, range_vec);         }     }     return nr_ranges_remaining; } }
 // dht
namespace unimplemented { static thread_local std::unordered_map<cause, bool> _warnings; static logging::logger ulogger("unimplemented"); std::ostream& operator<<(std::ostream& out, cause c) {     switch (c) {         case cause::INDEXES: return out << "INDEXES";         case cause::LWT: return out << "LWT";         case cause::PAGING: return out << "PAGING";         case cause::AUTH: return out << "AUTH";         case cause::PERMISSIONS: return out << "PERMISSIONS";         case cause::TRIGGERS: return out << "TRIGGERS";         case cause::COUNTERS: return out << "COUNTERS";         case cause::METRICS: return out << "METRICS";         case cause::MIGRATIONS: return out << "MIGRATIONS";         case cause::GOSSIP: return out << "GOSSIP";         case cause::TOKEN_RESTRICTION: return out << "TOKEN_RESTRICTION";         case cause::LEGACY_COMPOSITE_KEYS: return out << "LEGACY_COMPOSITE_KEYS";         case cause::COLLECTION_RANGE_TOMBSTONES: return out << "COLLECTION_RANGE_TOMBSTONES";         case cause::RANGE_DELETES: return out << "RANGE_DELETES";         case cause::THRIFT: return out << "THRIFT";         case cause::VALIDATION: return out << "VALIDATION";         case cause::REVERSED: return out << "REVERSED";         case cause::COMPRESSION: return out << "COMPRESSION";         case cause::NONATOMIC: return out << "NONATOMIC";         case cause::CONSISTENCY: return out << "CONSISTENCY";         case cause::HINT: return out << "HINT";         case cause::SUPER: return out << "SUPER";         case cause::WRAP_AROUND: return out << "WRAP_AROUND";         case cause::STORAGE_SERVICE: return out << "STORAGE_SERVICE";         case cause::API: return out << "API";         case cause::SCHEMA_CHANGE: return out << "SCHEMA_CHANGE";         case cause::MIXED_CF: return out << "MIXED_CF";         case cause::SSTABLE_FORMAT_M: return out << "SSTABLE_FORMAT_M";     }     abort(); }  void fail(cause c) {     throw std::runtime_error(format("Not implemented: {}", c)); } }
 namespace query { static logging::logger qlogger("query"); constexpr size_t result_memory_limiter::minimum_result_size; constexpr size_t result_memory_limiter::maximum_result_size; constexpr size_t result_memory_limiter::unlimited_result_size; thread_local semaphore result_memory_tracker::_dummy { 0 }; const dht::partition_range full_partition_range = dht::partition_range::make_open_ended_both_sides(); const clustering_range full_clustering_range = clustering_range::make_open_ended_both_sides();                partition_slice::partition_slice(clustering_row_ranges row_ranges,     query::column_id_vector static_columns,     query::column_id_vector regular_columns,     option_set options,     std::unique_ptr<specific_ranges> specific_ranges,     cql_serialization_format cql_format,     uint32_t partition_row_limit_low_bits,     uint32_t partition_row_limit_high_bits)     : _row_ranges(std::move(row_ranges))     , static_columns(std::move(static_columns))     , regular_columns(std::move(regular_columns))     , options(options)     , _specific_ranges(std::move(specific_ranges))     , _partition_row_limit_low_bits(partition_row_limit_low_bits)     , _partition_row_limit_high_bits(partition_row_limit_high_bits) {     cql_format.ensure_supported(); } partition_slice::partition_slice(clustering_row_ranges row_ranges,     query::column_id_vector static_columns,     query::column_id_vector regular_columns,     option_set options,     std::unique_ptr<specific_ranges> specific_ranges,     uint64_t partition_row_limit)     : partition_slice(std::move(row_ranges), std::move(static_columns), std::move(regular_columns), options,             std::move(specific_ranges), cql_serialization_format::latest(), static_cast<uint32_t>(partition_row_limit),             static_cast<uint32_t>(partition_row_limit >> 32)) {} partition_slice::partition_slice(clustering_row_ranges ranges, const schema& s, const column_set& columns, option_set options)     : partition_slice(ranges, query::column_id_vector{}, query::column_id_vector{}, options) {     regular_columns.reserve(columns.count());     for (ordinal_column_id id = columns.find_first(); id != column_set::npos; id = columns.find_next(id)) {         const column_definition& def = s.column_at(id);         if (def.is_static()) {             static_columns.push_back(def.id);         } else if (def.is_regular()) {             regular_columns.push_back(def.id);         } // else clustering or partition key column - skip, these are controlled by options
    } } partition_slice::partition_slice(partition_slice&&) = default; partition_slice& partition_slice::operator=(partition_slice&& other) noexcept = default; // Only needed because selection_statement::execute does copies of its read_command
// in the map-reduce op.
partition_slice::partition_slice(const partition_slice& s)     : _row_ranges(s._row_ranges)     , static_columns(s.static_columns)     , regular_columns(s.regular_columns)     , options(s.options)     , _specific_ranges(s._specific_ranges ? std::make_unique<specific_ranges>(*s._specific_ranges) : nullptr)     , _partition_row_limit_low_bits(s._partition_row_limit_low_bits)     , _partition_row_limit_high_bits(s._partition_row_limit_high_bits) {} partition_slice::~partition_slice() {} const clustering_row_ranges& partition_slice::row_ranges(const schema& s, const partition_key& k) const {     auto* r = _specific_ranges ? _specific_ranges->range_for(s, k) : nullptr;     return r ? *r : _row_ranges; } void partition_slice::set_range(const schema& s, const partition_key& k, clustering_row_ranges range) {     if (!_specific_ranges) {         _specific_ranges = std::make_unique<specific_ranges>(k, std::move(range));     } else {         _specific_ranges->add(s, k, std::move(range));     } } void partition_slice::clear_range(const schema& s, const partition_key& k) {     if (_specific_ranges && _specific_ranges->contains(s, k)) {         // just in case someone changes the impl above,
        // we should do actual remove if specific_ranges suddenly
        // becomes an actual map
        assert(_specific_ranges->size() == 1);         _specific_ranges = nullptr;     } } clustering_row_ranges partition_slice::get_all_ranges() const {     auto all_ranges = default_row_ranges();     const auto& specific_ranges = get_specific_ranges();     if (specific_ranges) {         all_ranges.insert(all_ranges.end(), specific_ranges->ranges().begin(), specific_ranges->ranges().end());     }     return all_ranges; } sstring result::pretty_print(schema_ptr s, const query::partition_slice& slice) const {     std::ostringstream out;     out << "{ result: " << result_set::from_raw_result(s, slice, *this);     out << " digest: ";     if (_digest) {         out << std::hex << std::setw(2);         for (auto&& c : _digest->get()) {             out << unsigned(c) << " ";         }     } else {         out << "{}";     }     out << ", short_read=" << is_short_read() << " }";     return out.str(); } query::result::printer result::pretty_printer(schema_ptr s, const query::partition_slice& slice) const {     return query::result::printer{s, slice, *this}; }  void result::ensure_counts() {     if (!_partition_count || !row_count()) {         uint64_t row_count;         std::tie(_partition_count, row_count) = result_view::do_with(*this, [] (auto&& view) {             return view.count_partitions_and_rows();         });         set_row_count(row_count);     } } full_position result::get_or_calculate_last_position() const {     if (_last_position) {         return *_last_position;     }     return result_view::do_with(*this, [] (const result_view& v) {         return v.calculate_last_position();     }); } result::result()     : result([] {         bytes_ostream out;         ser::writer_of_query_result<bytes_ostream>(out).skip_partitions().end_query_result();         return out;     }(), short_read::no, 0, 0, {}) { } static void write_partial_partition(ser::writer_of_qr_partition<bytes_ostream>&& pw, const ser::qr_partition_view& pv, uint64_t rows_to_include) {     auto key = pv.key();     auto static_cells_wr = (key ? std::move(pw).write_key(*key) : std::move(pw).skip_key())             .start_static_row()             .start_cells();     for (auto&& cell : pv.static_row().cells()) {         static_cells_wr.add(cell);     }     auto rows_wr = std::move(static_cells_wr)             .end_cells()             .end_static_row()             .start_rows();     auto rows = pv.rows();     // rows.size() can be 0 is there's a single static row
    auto it = rows.begin();     for (uint64_t i = 0; i < std::min(rows.size(), rows_to_include); ++i) {         rows_wr.add(*it++);     }     std::move(rows_wr).end_rows().end_qr_partition(); } foreign_ptr<lw_shared_ptr<query::result>> result_merger::get() {     if (_partial.size() == 1) {         return std::move(_partial[0]);     }     bytes_ostream w;     auto partitions = ser::writer_of_query_result<bytes_ostream>(w).start_partitions();     uint64_t row_count = 0;     short_read is_short_read;     uint32_t partition_count = 0;     std::optional<full_position> last_position;     for (auto&& r : _partial) {         result_view::do_with(*r, [&] (result_view rv) {             last_position.reset();             for (auto&& pv : rv._v.partitions()) {                 auto rows = pv.rows();                 // If rows.empty(), then there's a static row, or there wouldn't be a partition
                const uint64_t rows_in_partition = rows.size() ? : 1;                 const uint64_t rows_to_include = std::min(_max_rows - row_count, rows_in_partition);                 row_count += rows_to_include;                 if (rows_to_include >= rows_in_partition) {                     partitions.add(pv);                     if (++partition_count >= _max_partitions) {                         return;                     }                 } else if (rows_to_include > 0) {                     ++partition_count;                     write_partial_partition(partitions.add(), pv, rows_to_include);                     return;                 } else {                     return;                 }             }             last_position = r->last_position();         });         if (r->is_short_read()) {             is_short_read = short_read::yes;             break;         }         if (row_count >= _max_rows || partition_count >= _max_partitions) {             break;         }     }     std::move(partitions).end_partitions().end_query_result();     return make_foreign(make_lw_shared<query::result>(std::move(w), is_short_read, row_count, partition_count, std::move(last_position))); } std::ostream& operator<<(std::ostream& out, const query::forward_result::printer& p) {     if (p.functions.size() != p.res.query_results.size()) {         return out << "[malformed forward_result (" << p.res.query_results.size()             << " results, " << p.functions.size() << " aggregates)]";     }     out << "[";     for (size_t i = 0; i < p.functions.size(); i++) {         auto& return_type = p.functions[i]->return_type();         out << return_type->to_string(bytes_view(*p.res.query_results[i]));         if (i + 1 < p.functions.size()) {             out << ", ";         }     }     return out << "]"; } }
  std::atomic<int64_t> clocks_offset;
  partition_slice_builder::partition_slice_builder(const schema& schema, query::partition_slice slice)     : _regular_columns(std::move(slice.regular_columns))     , _static_columns(std::move(slice.static_columns))     , _row_ranges(std::move(slice._row_ranges))     , _specific_ranges(std::move(slice._specific_ranges))     , _schema(schema)     , _options(std::move(slice.options)) { }
 partition_slice_builder::partition_slice_builder(const schema& schema)     : _schema(schema) {     _options.set<query::partition_slice::option::send_partition_key>();     _options.set<query::partition_slice::option::send_clustering_key>();     _options.set<query::partition_slice::option::send_timestamp>();     _options.set<query::partition_slice::option::send_expiry>(); }
 query::partition_slice partition_slice_builder::build() {     std::vector<query::clustering_range> ranges;     if (_row_ranges) {         ranges = std::move(*_row_ranges);     } else {         ranges.emplace_back(query::clustering_range::make_open_ended_both_sides());     }     query::column_id_vector static_columns;     if (_static_columns) {         static_columns = std::move(*_static_columns);     } else {         boost::range::push_back(static_columns,             _schema.static_columns() | boost::adaptors::transformed(std::mem_fn(&column_definition::id)));     }     query::column_id_vector regular_columns;     if (_regular_columns) {         regular_columns = std::move(*_regular_columns);     } else {         boost::range::push_back(regular_columns,             _schema.regular_columns() | boost::adaptors::transformed(std::mem_fn(&column_definition::id)));     }     return {         std::move(ranges),         std::move(static_columns),         std::move(regular_columns),         std::move(_options),         std::move(_specific_ranges),         _partition_row_limit,     }; }
 partition_slice_builder& partition_slice_builder::with_range(query::clustering_range range) {     if (!_row_ranges) {         _row_ranges = std::vector<query::clustering_range>();     }     _row_ranges->emplace_back(std::move(range));     return *this; }
 partition_slice_builder& partition_slice_builder::with_ranges(std::vector<query::clustering_range> ranges) {     if (!_row_ranges) {         _row_ranges = std::move(ranges);     } else {         for (auto&& r : ranges) {             with_range(std::move(r));         }     }     return *this; }
 partition_slice_builder& partition_slice_builder::mutate_ranges(std::function<void(std::vector<query::clustering_range>&)> func) {     if (_row_ranges) {         func(*_row_ranges);     }     return *this; }
 partition_slice_builder& partition_slice_builder::mutate_specific_ranges(std::function<void(query::specific_ranges&)> func) {     if (_specific_ranges) {         func(*_specific_ranges);     }     return *this; }
 partition_slice_builder& partition_slice_builder::with_no_regular_columns() {     _regular_columns = query::column_id_vector();     return *this; }
 partition_slice_builder& partition_slice_builder::with_regular_column(bytes name) {     if (!_regular_columns) {         _regular_columns = query::column_id_vector();     }     const column_definition* def = _schema.get_column_definition(name);     if (!def) {         throw std::runtime_error(format("No such column: {}", _schema.regular_column_name_type()->to_string(name)));     }     if (!def->is_regular()) {         throw std::runtime_error(format("Column is not regular: {}", _schema.column_name_type(*def)->to_string(name)));     }     _regular_columns->push_back(def->id);     return *this; }
 partition_slice_builder& partition_slice_builder::with_no_static_columns() {     _static_columns = query::column_id_vector();     return *this; }
 partition_slice_builder& partition_slice_builder::with_static_column(bytes name) {     if (!_static_columns) {         _static_columns = query::column_id_vector();     }     const column_definition* def = _schema.get_column_definition(name);     if (!def) {         throw std::runtime_error(format("No such column: {}", utf8_type->to_string(name)));     }     if (!def->is_static()) {         throw std::runtime_error(format("Column is not static: {}", utf8_type->to_string(name)));     }     _static_columns->push_back(def->id);     return *this; }
 partition_slice_builder& partition_slice_builder::reversed() {     _options.set<query::partition_slice::option::reversed>();     return *this; }
 partition_slice_builder& partition_slice_builder::without_partition_key_columns() {     _options.remove<query::partition_slice::option::send_partition_key>();     return *this; }
 partition_slice_builder& partition_slice_builder::without_clustering_key_columns() {     _options.remove<query::partition_slice::option::send_clustering_key>();     return *this; }
 partition_slice_builder& partition_slice_builder::with_partition_row_limit(uint64_t limit) {     _partition_row_limit = limit;     return *this; }
 thread_local disk_error_signal_type commit_error;
 thread_local disk_error_signal_type general_disk_error;
 thread_local disk_error_signal_type sstable_write_error;
 thread_local io_error_handler commit_error_handler = default_io_error_handler(commit_error);
 thread_local io_error_handler general_disk_error_handler = default_io_error_handler(general_disk_error);
 thread_local io_error_handler sstable_write_error_handler = default_io_error_handler(sstable_write_error);
 io_error_handler default_io_error_handler(disk_error_signal_type& signal) {     return [&signal] (std::exception_ptr eptr) {         try {             std::rethrow_exception(eptr);         } catch(std::system_error& e) {             if (should_stop_on_system_error(e)) {                 signal();                 throw storage_io_error(e);             }         }     }; }
  static_assert(Hasher<hasher>);
 static_assert(HasherReturningBytes<md5_hasher>);
 static_assert(HasherReturningBytes<sha256_hasher>);
 static_assert(HasherReturningBytes<xx_hasher>);
 static_assert(SimpleHasher<simple_xx_hasher>);
 template <typename T> struct hasher_traits;
 template <> struct hasher_traits<md5_hasher> { using impl_type = CryptoPP::Weak::MD5; };
 template <> struct hasher_traits<sha256_hasher> { using impl_type = CryptoPP::SHA256; };
 template<typename H> concept HashUpdater =     requires(typename hasher_traits<H>::impl_type& h, const CryptoPP::byte* ptr, size_t size) {         // We need Update() not to throw, but it isn't marked noexcept
        // in CryptoPP source. We'll just hope it doesn't throw.
        { h.Update(ptr, size) } -> std::same_as<void>;     };
 template <typename T, size_t size> struct cryptopp_hasher<T, size>::impl {     static_assert(HashUpdater<T>);     using impl_type = typename hasher_traits<T>::impl_type;     impl_type hash{};     void update(const char* ptr, size_t length) noexcept {         using namespace CryptoPP;         static_assert(sizeof(char) == sizeof(byte), "Assuming lengths will be the same");         hash.Update(reinterpret_cast<const byte*>(ptr), length * sizeof(byte));     }     bytes finalize() {         bytes digest{bytes::initialized_later(), size};         hash.Final(reinterpret_cast<unsigned char*>(digest.begin()));         return digest;     }     std::array<uint8_t, size> finalize_array() {         std::array<uint8_t, size> array;         hash.Final(reinterpret_cast<unsigned char*>(array.data()));         return array;     } };
 template <typename T, size_t size> cryptopp_hasher<T, size>::cryptopp_hasher() : _impl(std::make_unique<impl>()) {}
 template <typename T, size_t size> cryptopp_hasher<T, size>::~cryptopp_hasher() = default;
 template <typename T, size_t size> cryptopp_hasher<T, size>::cryptopp_hasher(cryptopp_hasher&& o) noexcept = default;
 template <typename T, size_t size> cryptopp_hasher<T, size>::cryptopp_hasher(const cryptopp_hasher& o) : _impl(std::make_unique<cryptopp_hasher<T, size>::impl>(*o._impl)) {}
 template <typename T, size_t size> cryptopp_hasher<T, size>& cryptopp_hasher<T, size>::operator=(cryptopp_hasher&& o) noexcept = default;
 template <typename T, size_t size> cryptopp_hasher<T, size>& cryptopp_hasher<T, size>::operator=(const cryptopp_hasher& o) {     _impl = std::make_unique<cryptopp_hasher<T, size>::impl>(*o._impl);     return *this; }
 template <typename T, size_t size> bytes cryptopp_hasher<T, size>::finalize() { return _impl->finalize(); }
 template <typename T, size_t size> std::array<uint8_t, size> cryptopp_hasher<T, size>::finalize_array() {     return _impl->finalize_array(); }
 template <typename T, size_t size> void cryptopp_hasher<T, size>::update(const char* ptr, size_t length) noexcept { _impl->update(ptr, length); }
 template <typename T, size_t size> bytes cryptopp_hasher<T, size>::calculate(const std::string_view& s) {     typename cryptopp_hasher<T, size>::impl::impl_type hash;     unsigned char digest[size];     hash.CalculateDigest(digest, reinterpret_cast<const unsigned char*>(s.data()), s.size());     return bytes{reinterpret_cast<const int8_t*>(digest), size}; }
 template class cryptopp_hasher<md5_hasher, 16>;
 template class cryptopp_hasher<sha256_hasher, 32>;
 using namespace std::chrono_literals;
 namespace utils { namespace aws { static hmac_sha256_digest hmac_sha256(std::string_view key, std::string_view msg) {     hmac_sha256_digest digest;     int ret = gnutls_hmac_fast(GNUTLS_MAC_SHA256, key.data(), key.size(), msg.data(), msg.size(), digest.data());     if (ret) {         throw std::runtime_error(fmt::format("Computing HMAC failed ({}): {}", ret, gnutls_strerror(ret)));     }     return digest; } static hmac_sha256_digest get_signature_key(std::string_view key, std::string_view date_stamp, std::string_view region_name, std::string_view service_name) {     auto date = hmac_sha256("AWS4" + std::string(key), date_stamp);     auto region = hmac_sha256(std::string_view(date.data(), date.size()), region_name);     auto service = hmac_sha256(std::string_view(region.data(), region.size()), service_name);     auto signing = hmac_sha256(std::string_view(service.data(), service.size()), "aws4_request");     return signing; } static std::string apply_sha256(std::string_view msg) {     sha256_hasher hasher;     hasher.update(msg.data(), msg.size());     return to_hex(hasher.finalize()); } static std::string apply_sha256(const std::vector<temporary_buffer<char>>& msg) {     sha256_hasher hasher;     for (const temporary_buffer<char>& buf : msg) {         hasher.update(buf.get(), buf.size());     }     return to_hex(hasher.finalize()); }  void check_expiry(std::string_view signature_date) {     //FIXME: The default 15min can be changed with X-Amz-Expires header - we should honor it
    std::string expiration_str = format_time_point(db_clock::now() - 15min);     std::string validity_str = format_time_point(db_clock::now() + 15min);     if (signature_date < expiration_str) {         throw std::runtime_error(                 fmt::format("Signature expired: {} is now earlier than {} (current time - 15 min.)",                 signature_date, expiration_str));     }     if (signature_date > validity_str) {         throw std::runtime_error(                 fmt::format("Signature not yet current: {} is still later than {} (current time + 15 min.)",                 signature_date, validity_str));     } } std::string get_signature(std::string_view access_key_id, std::string_view secret_access_key,         std::string_view host, std::string_view canonical_uri, std::string_view method,         std::optional<std::string_view> orig_datestamp, std::string_view signed_headers_str, const std::map<std::string_view, std::string_view>& signed_headers_map,         const std::vector<temporary_buffer<char>>* body_content, std::string_view region, std::string_view service, std::string_view query_string) {     auto amz_date_it = signed_headers_map.find("x-amz-date");     if (amz_date_it == signed_headers_map.end()) {         throw std::runtime_error("X-Amz-Date header is mandatory for signature verification");     }     std::string_view amz_date = amz_date_it->second;     std::string_view datestamp = amz_date.substr(0, 8);     if (orig_datestamp) {         check_expiry(amz_date);         if (datestamp != *orig_datestamp) {             throw std::runtime_error(                     format("X-Amz-Date date does not match the provided datestamp. Expected {}, got {}",                             *orig_datestamp, datestamp));         }     }     std::stringstream canonical_headers;     for (const auto& header : signed_headers_map) {         canonical_headers << fmt::format("{}:{}", header.first, header.second) << '\n';     }     std::string payload_hash = body_content != nullptr ? apply_sha256(*body_content) : "UNSIGNED-PAYLOAD";     std::string canonical_request = fmt::format("{}\n{}\n{}\n{}\n{}\n{}", method, canonical_uri, query_string, canonical_headers.str(), signed_headers_str, payload_hash);     std::string_view algorithm = "AWS4-HMAC-SHA256";     std::string credential_scope = fmt::format("{}/{}/{}/aws4_request", datestamp, region, service);     std::string string_to_sign = fmt::format("{}\n{}\n{}\n{}", algorithm, amz_date, credential_scope,  apply_sha256(canonical_request));     hmac_sha256_digest signing_key = get_signature_key(secret_access_key, datestamp, region, service);     hmac_sha256_digest signature = hmac_sha256(std::string_view(signing_key.data(), signing_key.size()), string_to_sign);     return to_hex(bytes_view(reinterpret_cast<const int8_t*>(signature.data()), signature.size())); } } // aws namespace
}
 // utils namespace
namespace { //
// Helper for retrieving the counter based on knowing its type.
//
template<class Counter> constexpr typename Counter::value_type& counter_ref(cql_duration &) noexcept; template<> constexpr months_counter::value_type& counter_ref<months_counter>(cql_duration &d) noexcept {     return d.months; } template<> constexpr days_counter::value_type& counter_ref<days_counter>(cql_duration &d) noexcept {     return d.days; } template<> constexpr nanoseconds_counter::value_type& counter_ref<nanoseconds_counter>(cql_duration &d) noexcept {     return d.nanoseconds; } // Unit for a component of a duration. For example, years.
class duration_unit { public:     using index_type = uint8_t;     using common_counter_type = cql_duration::common_counter_type;     virtual ~duration_unit() = default;     // Units with larger indicies are greater. For example, "months" have a greater index than "days".
    virtual index_type index() const noexcept = 0;     virtual const char* short_name() const noexcept = 0;     virtual const char* long_name() const noexcept = 0;     // Increment the appropriate counter in the duration instance based on a count of this unit.
    virtual void increment_count(cql_duration&, common_counter_type) const noexcept = 0;     // The remaining capacity (in terms of this unit) of the appropriate counter in the duration instance.
    virtual common_counter_type available_count(const cql_duration&) const noexcept = 0; }; // `_index` is the assigned index of this unit.
// `Counter` is the counter type in the `cql_duration` instance that is used to store this unit.
// `_factor` is the conversion factor of one count of this unit to the corresponding count in `Counter`.
template <uint8_t _index, class Counter, cql_duration::common_counter_type _factor> class duration_unit_impl : public duration_unit { public:     static constexpr auto factor = _factor;          index_type index() const noexcept override {         return _index;     }     void increment_count(cql_duration &d, common_counter_type c) const noexcept override {         counter_ref<Counter>(d) += (c * factor);     }     common_counter_type available_count(const cql_duration& d) const noexcept override {         const auto limit = std::numeric_limits<typename Counter::value_type>::max();         return {(limit - counter_ref<Counter>(const_cast<cql_duration&>(d))) / factor};     } }; struct nanosecond_unit final : public duration_unit_impl<0, nanoseconds_counter , 1> {     const char* short_name() const noexcept override { return "ns"; }     const char* long_name() const noexcept override { return "nanoseconds"; } } const nanosecond{}; struct microsecond_unit final : public duration_unit_impl<1, nanoseconds_counter, 1000> {     const char* short_name() const noexcept override { return "us"; }     const char* long_name() const noexcept override { return "microseconds"; } } const microsecond{}; struct millisecond_unit final : public duration_unit_impl<2, nanoseconds_counter, microsecond_unit::factor * 1000> {     const char* short_name() const noexcept override { return "ms"; }     const char* long_name() const noexcept override { return "milliseconds"; } } const millisecond{}; struct second_unit final : public duration_unit_impl<3, nanoseconds_counter, millisecond_unit::factor * 1000> {     const char* short_name() const noexcept override { return "s"; }     const char* long_name() const noexcept override { return "seconds"; } } const second{}; struct minute_unit final : public duration_unit_impl<4, nanoseconds_counter, second_unit::factor * 60> {     const char* short_name() const noexcept override { return "m"; }     const char* long_name() const noexcept override { return "minutes"; } } const minute{}; struct hour_unit final : public duration_unit_impl<5, nanoseconds_counter, minute_unit::factor * 60> {     const char* short_name() const noexcept override { return "h"; }     const char* long_name() const noexcept override { return "hours"; } } const hour{}; struct day_unit final : public duration_unit_impl<6, days_counter, 1> {     const char* short_name() const noexcept override { return "d"; }     const char* long_name() const noexcept override { return "days"; } } const day{}; struct week_unit final : public duration_unit_impl<7, days_counter, 7> {     const char* short_name() const noexcept override { return "w"; }     const char* long_name() const noexcept override { return "weeks"; } } const week{}; struct month_unit final : public duration_unit_impl<8, months_counter, 1> {     const char* short_name() const noexcept override { return "mo"; }     const char* long_name() const noexcept override { return "months"; } } const month{}; struct year_unit final : public duration_unit_impl<9, months_counter, 12> {     const char* short_name() const noexcept override { return "y"; }     const char* long_name() const noexcept override { return "years"; } } const year{}; const auto unit_table = std::unordered_map<std::string_view, std::reference_wrapper<const duration_unit>>{         {year.short_name(), year},         {month.short_name(), month},         {week.short_name(), week},         {day.short_name(), day},         {hour.short_name(), hour},         {minute.short_name(), minute},         {second.short_name(), second},         {millisecond.short_name(), millisecond},         {microsecond.short_name(), microsecond}, {"µs", microsecond},         {nanosecond.short_name(), nanosecond} }; //
// Convenient helper to parse the indexed sub-expression from a match group as a duration counter.
//
// Throws `std::out_of_range` if a counter is out of range.
//
template <class Match, class Index = typename Match::size_type> cql_duration::common_counter_type parse_count(const Match& m, Index group_index) {     try {         return boost::lexical_cast<cql_duration::common_counter_type>(m[group_index].str());     } catch (const boost::bad_lexical_cast&) {         throw std::out_of_range("duration counter");     } } //
// Build up a duration unit-by-unit.
//
// We support overflow detection on construction for convenience and compatibility with Cassandra.
//
// We maintain some additional state over a `cql_duration` in order to track the order in which components are added when
// parsing the standard format.
//
class duration_builder final { public:     duration_builder& add(cql_duration::common_counter_type count, const duration_unit& unit) {         validate_addition(count, unit);         validate_and_update_order(unit);         unit.increment_count(_duration, count);         return *this;     }     template <class Match, class Index = typename Match::size_type>     duration_builder& add_parsed_count(const Match& m, Index group_index, const duration_unit& unit) {         cql_duration::common_counter_type count;         try {             count = parse_count(m, group_index);         } catch (const std::out_of_range&) {             throw cql_duration_error(seastar::format("Invalid duration. The count for the {} is out of range", unit.long_name()));         }         return add(count, unit);     }     cql_duration build() const noexcept {         return _duration;     } private:     const duration_unit* _current_unit{nullptr};     cql_duration _duration{};     //
    // Throws `cql_duration_error` if the addition of a quantity of the designated unit would overflow one of the
    // counters.
    //
    void validate_addition(typename cql_duration::common_counter_type count, const duration_unit& unit) const {         const auto available = unit.available_count(_duration);         if (count > available) {             throw cql_duration_error(                     seastar::format("Invalid duration. The number of {} must be less than or equal to {}",                            unit.long_name(),                            available));         }     }     //
    // Validate that an addition of a quantity of the designated unit is not out of order. We require that units are
    // added in decreasing size.
    //
    // This function also updates the last-observed unit for the next invocation.
    //
    // Throws `cql_duration_error` for order violations.
    //
    void validate_and_update_order(const duration_unit& unit) {         const auto index = unit.index();         if (_current_unit != nullptr) {             if (index == _current_unit->index()) {                 throw cql_duration_error(seastar::format("Invalid duration. The {} are specified multiple times", unit.long_name()));             } else if (index > _current_unit->index()) {                 throw cql_duration_error(                         seastar::format("Invalid duration. The {} should be after {}",                                _current_unit->long_name(),                                unit.long_name()));             }         }         _current_unit = &unit;     } }; //
// These functions assume no sign information ('-). That is left to the `cql_duration` constructor.
//
std::optional<cql_duration> parse_duration_standard_format(std::string_view s) {     //
    // We parse one component (pair of a count and unit) at a time in order to give more precise error messages when
    // units are specified multiple times or out of order rather than just "parse error".
    //
    // The other formats are more strict and complain less helpfully.
    //
    static const auto pattern =             boost::regex("(\\d+)(y|Y|mo|MO|mO|Mo|w|W|d|D|h|H|s|S|ms|MS|mS|Ms|us|US|uS|Us|µs|µS|ns|NS|nS|Ns|m|M)");     auto iter = s.cbegin();     boost::cmatch match;     duration_builder b;     // `match_continuous` ensures that the entire string must be included in a match.
    while (boost::regex_search(iter, s.end(), match, pattern, boost::regex_constants::match_continuous)) {         iter += match.length();         auto symbol = match[2].str();         // Special case for mu.
        {             auto view = std::string_view(symbol);             view.remove_suffix(1);             if (view == reinterpret_cast<const char*>(u8"µ")) {                 b.add_parsed_count(match, 1, microsecond);                 continue;             }         }         // Otherwise, we can just convert to lower-case for look-up.
        std::transform(symbol.begin(), symbol.end(), symbol.begin(), [](char ch) { return std::tolower(ch); });         b.add_parsed_count(match, 1, unit_table.at(symbol));     }     if (iter != s.cend()) {         // There is unconsumed input.
        return {};     }     return b.build(); } std::optional<cql_duration> parse_duration_iso8601_format(std::string_view s) {     static const auto pattern = boost::regex("P((\\d+)Y)?((\\d+)M)?((\\d+)D)?(T((\\d+)H)?((\\d+)M)?((\\d+)S)?)?");     boost::cmatch match;     if (!boost::regex_match(s.data(), match, pattern)) {         return {};     }     duration_builder b;     if (match[1].matched) {         b.add_parsed_count(match, 2, year);     }     if (match[3].matched) {         b.add_parsed_count(match, 4, month);     }     if (match[5].matched) {         b.add_parsed_count(match, 6, day);     }     // Optional, more granular, information.
    if (match[7].matched) {         if (match[8].matched) {             b.add_parsed_count(match, 9, hour);         }         if (match[10].matched) {             b.add_parsed_count(match, 11, minute);         }         if (match[12].matched) {             b.add_parsed_count(match, 13, second);         }     }     return b.build(); } std::optional<cql_duration> parse_duration_iso8601_alternative_format(std::string_view s) {     static const auto pattern = boost::regex("P(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})");     boost::cmatch match;     if (!boost::regex_match(s.data(), match, pattern)) {         return {};     }     return duration_builder()             .add_parsed_count(match, 1, year)             .add_parsed_count(match, 2, month)             .add_parsed_count(match, 3, day)             .add_parsed_count(match, 4, hour)             .add_parsed_count(match, 5, minute)             .add_parsed_count(match, 6, second)             .build(); } std::optional<cql_duration> parse_duration_iso8601_week_format(std::string_view s) {     static const auto pattern = boost::regex("P(\\d+)W");     boost::cmatch match;     if (!boost::regex_match(s.data(), match, pattern)) {         return {};     }     return duration_builder()             .add_parsed_count(match, 1, week)             .build(); } // Parse a duration string without sign information assuming one of the supported formats.
std::optional<cql_duration> parse_duration(std::string_view s) {     if (s.length() == 0u) {         return {};     }     if (s.front() == 'P') {         if (s.back() == 'W') {             return parse_duration_iso8601_week_format(s);         }         if (s.find('-') != s.npos) {             return parse_duration_iso8601_alternative_format(s);         }         return parse_duration_iso8601_format(s);     }     return parse_duration_standard_format(s); } }
 cql_duration::cql_duration(std::string_view s) {     const bool is_negative = (s.length() != 0) && (s[0] == '-');     // Without any sign indicator ('-').
    const auto ps = (is_negative ? s.cbegin() + 1 : s.cbegin());     const auto d = parse_duration(ps);     if (!d) {         throw cql_duration_error(seastar::format("Unable to convert '{}' to a duration", s));     }     *this = *d;     if (is_negative) {         months = -months;         days = -days;         nanoseconds = -nanoseconds;     } }
 std::ostream& operator<<(std::ostream& os, const cql_duration& d) {     if ((d.months < 0) || (d.days < 0) || (d.nanoseconds < 0)) {         os << '-';     }     // If a non-zero integral component of the count can be expressed in `unit`, then append it to the stream with its
    // unit.
    //
    // Returns the remaining count.
    const auto append = [&os](cql_duration::common_counter_type count, auto&& unit) {         const auto divider = unit.factor;         if ((count == 0) || (count < divider)) {             return count;         }         os << (count / divider) << unit.short_name();         return count % divider;     };     const auto month_remainder = append(std::abs(d.months), year);     append(month_remainder, month);     append(std::abs(d.days), day);     auto nanosecond_remainder = append(std::abs(d.nanoseconds), hour);     nanosecond_remainder = append(nanosecond_remainder, minute);     nanosecond_remainder = append(nanosecond_remainder, second);     nanosecond_remainder = append(nanosecond_remainder, millisecond);     nanosecond_remainder = append(nanosecond_remainder, microsecond);     append(nanosecond_remainder, nanosecond);     return os; }
  static_assert(-1 == ~0, "Not a twos-complement architecture");
 // Accounts for the case that all bits are zero.
static vint_size_type count_leading_zero_bits(uint64_t n) noexcept {     if (n == 0) {         return vint_size_type(std::numeric_limits<uint64_t>::digits);     }     return vint_size_type(count_leading_zeros(n)); }
 static constexpr uint64_t encode_zigzag(int64_t n) noexcept {     // The right shift has to be arithmetic and not logical.
    return (static_cast<uint64_t>(n) << 1) ^ static_cast<uint64_t>(n >> 63); }
 static constexpr int64_t decode_zigzag(uint64_t n) noexcept {     return static_cast<int64_t>((n >> 1) ^ -(n & 1)); }
 // Mask for extracting from the first byte the part that is not used for indicating the total number of bytes.
static uint64_t first_byte_value_mask(vint_size_type extra_bytes_size) {     // Include the sentinel zero bit in the mask.
    return uint64_t(0xff) >> extra_bytes_size; }
 vint_size_type signed_vint::serialize(int64_t value, bytes::iterator out) {     return unsigned_vint::serialize(encode_zigzag(value), out); }
 vint_size_type signed_vint::serialized_size(int64_t value) noexcept {     return unsigned_vint::serialized_size(encode_zigzag(value)); }
 int64_t signed_vint::deserialize(bytes_view v) {     const auto un = unsigned_vint::deserialize(v);     return decode_zigzag(un); }
 vint_size_type signed_vint::serialized_size_from_first_byte(bytes::value_type first_byte) {     return unsigned_vint::serialized_size_from_first_byte(first_byte); }
 // The number of additional bytes that we need to read.
static vint_size_type count_extra_bytes(int8_t first_byte) {     // Sign extension.
    const int64_t v(first_byte);     return count_leading_zero_bits(static_cast<uint64_t>(~v)) - vint_size_type(64 - 8); }
 static void encode(uint64_t value, vint_size_type size, bytes::iterator out) {     std::array<int8_t, 9> buffer({});     // `size` is always in the range [1, 9].
    const auto extra_bytes_size = size - 1;     for (vint_size_type i = 0; i <= extra_bytes_size; ++i) {         buffer[extra_bytes_size - i] = static_cast<int8_t>(value & 0xff);         value >>= 8;     }     buffer[0] |= ~first_byte_value_mask(extra_bytes_size);     std::copy_n(buffer.cbegin(), size, out); }
 vint_size_type unsigned_vint::serialize(uint64_t value, bytes::iterator out) {     const auto size = serialized_size(value);     if (size == 1) {         *out = static_cast<int8_t>(value & 0xff);         return 1;     }     encode(value, size, out);     return size; }
 vint_size_type unsigned_vint::serialized_size(uint64_t value) noexcept {     // No need for the overhead of checking that all bits are zero.
    //
    // A signed quantity, to allow the case of `magnitude == 0` to result in a value of 9 below.
    const auto magnitude = static_cast<int64_t>(count_leading_zeros(value | uint64_t(1)));     return vint_size_type(9) - vint_size_type((magnitude - 1) / 7); }
 uint64_t unsigned_vint::deserialize(bytes_view v) {     auto src = v.data();     auto len = v.size();     const int8_t first_byte = *src;     // No additional bytes, since the most significant bit is not set.
    if (first_byte >= 0) {         return uint64_t(first_byte);     }     const auto extra_bytes_size = count_extra_bytes(first_byte);     // Extract the bits not used for counting bytes.
    auto result = uint64_t(first_byte) & first_byte_value_mask(extra_bytes_size);     for (vint_size_type index = 0; index < extra_bytes_size; ++index) {         result <<= 8;         result |= (uint64_t(v[index + 1]) & uint64_t(0xff));     }     return result; }
 vint_size_type unsigned_vint::serialized_size_from_first_byte(bytes::value_type first_byte) {     int8_t first_byte_casted = first_byte;     return 1 + (first_byte_casted >= 0 ? 0 : count_extra_bytes(first_byte_casted)); }
 namespace utils { namespace utf8 { using namespace internal; struct codepoint_status {     size_t bytes_validated;     bool error;     uint8_t more_bytes_needed; }; static codepoint_status inline evaluate_codepoint(const uint8_t* data, size_t len) {     const uint8_t byte1 = data[0];     static const uint8_t len_from_first_nibble[16] = { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 4 };     auto codepoint_len = len_from_first_nibble[byte1 >> 4];     if (codepoint_len > len) {         return codepoint_status{.more_bytes_needed = uint8_t(codepoint_len - len)};     } else {         if (byte1 <= 0x7F) {             // 00..7F
            return codepoint_status{.bytes_validated = codepoint_len};         } else if (len >= 2 && byte1 >= 0xC2 && byte1 <= 0xDF &&                 (int8_t)data[1] <= (int8_t)0xBF) {             // C2..DF, 80..BF
            return codepoint_status{.bytes_validated = codepoint_len};         } else if (len >= 3) {             const uint8_t byte2 = data[1];             // Is byte2, byte3 between 0x80 ~ 0xBF
            const int byte2_ok = (int8_t)byte2 <= (int8_t)0xBF;             const int byte3_ok = (int8_t)data[2] <= (int8_t)0xBF;             if (byte2_ok && byte3_ok &&                      // E0, A0..BF, 80..BF
                    ((byte1 == 0xE0 && byte2 >= 0xA0) ||                      // E1..EC, 80..BF, 80..BF
                     (byte1 >= 0xE1 && byte1 <= 0xEC) ||                      // ED, 80..9F, 80..BF
                     (byte1 == 0xED && byte2 <= 0x9F) ||                      // EE..EF, 80..BF, 80..BF
                     (byte1 >= 0xEE && byte1 <= 0xEF))) {                 return codepoint_status{.bytes_validated = codepoint_len};             } else if (len >= 4) {                 // Is byte4 between 0x80 ~ 0xBF
                const int byte4_ok = (int8_t)data[3] <= (int8_t)0xBF;                 if (byte2_ok && byte3_ok && byte4_ok &&                          // F0, 90..BF, 80..BF, 80..BF
                        ((byte1 == 0xF0 && byte2 >= 0x90) ||                          // F1..F3, 80..BF, 80..BF, 80..BF
                         (byte1 >= 0xF1 && byte1 <= 0xF3) ||                          // F4, 80..8F, 80..BF, 80..BF
                         (byte1 == 0xF4 && byte2 <= 0x8F))) {                     return codepoint_status{.bytes_validated = codepoint_len};                 } else {                     return codepoint_status{.error = true};                 }             } else {                 return codepoint_status{.error = true};             }         } else {             return codepoint_status{.error = true};         }     } } // 3x faster than boost utf_to_utf
 static partial_validation_results validate_partial_naive(const uint8_t *data, size_t len) {     while (len) {         auto cs = evaluate_codepoint(data, len);         data += cs.bytes_validated;         len -= cs.bytes_validated;         if (cs.error) {             return partial_validation_results{.error = true};         }         if (cs.more_bytes_needed) {             return partial_validation_results{.unvalidated_tail = len, .bytes_needed_for_tail = cs.more_bytes_needed};         }     }     return partial_validation_results{}; } } // namespace utf8
}
 // namespace utils
namespace utils { namespace utf8 { // Map high nibble of "First Byte" to legal character length minus 1
// 0x00 ~ 0xBF --> 0
// 0xC0 ~ 0xDF --> 1
// 0xE0 ~ 0xEF --> 2
// 0xF0 ~ 0xFF --> 3
alignas(16) static const int8_t s_first_len_tbl[] = {     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, }; // Map "First Byte" to 8-th item of range table (0xC2 ~ 0xF4)
alignas(16) static const int8_t s_first_range_tbl[] = {     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, }; // Range table, map range index to min and max values
// Index 0    : 00 ~ 7F (First Byte, ascii)
// Index 1,2,3: 80 ~ BF (Second, Third, Fourth Byte)
// Index 4    : A0 ~ BF (Second Byte after E0)
// Index 5    : 80 ~ 9F (Second Byte after ED)
// Index 6    : 90 ~ BF (Second Byte after F0)
// Index 7    : 80 ~ 8F (Second Byte after F4)
// Index 8    : C2 ~ F4 (First Byte, non ascii)
// Index 9~15 : illegal: i >= 127 && i <= -128
alignas(16) static const int8_t s_range_min_tbl[] = {     '\x00', '\x80', '\x80', '\x80', '\xA0', '\x80', '\x90', '\x80',     '\xC2', '\x7F', '\x7F', '\x7F', '\x7F', '\x7F', '\x7F', '\x7F', }; alignas(16) static const int8_t s_range_max_tbl[] = {     '\x7F', '\xBF', '\xBF', '\xBF', '\xBF', '\x9F', '\xBF', '\x8F',     '\xF4', '\x80', '\x80', '\x80', '\x80', '\x80', '\x80', '\x80', }; // Tables for fast handling of four special First Bytes(E0,ED,F0,F4), after
// which the Second Byte are not 80~BF. It contains "range index adjustment".
// +------------+---------------+------------------+----------------+
// | First Byte | original range| range adjustment | adjusted range |
// +------------+---------------+------------------+----------------+
// | E0         | 2             | 2                | 4              |
// +------------+---------------+------------------+----------------+
// | ED         | 2             | 3                | 5              |
// +------------+---------------+------------------+----------------+
// | F0         | 3             | 3                | 6              |
// +------------+---------------+------------------+----------------+
// | F4         | 4             | 4                | 8              |
// +------------+---------------+------------------+----------------+
// index1 -> E0, index14 -> ED
alignas(16) static const int8_t s_df_ee_tbl[] = {     0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, }; // index1 -> F0, index5 -> F4
alignas(16) static const int8_t s_ef_fe_tbl[] = {     0, 3, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, }; // 5x faster than naive method
partial_validation_results internal::validate_partial(const uint8_t *data, size_t len) {     if (len >= 16) {         __m128i prev_input = _mm_set1_epi8(0);         __m128i prev_first_len = _mm_set1_epi8(0);         // Cached tables
        const __m128i first_len_tbl = _mm_load_si128((const __m128i *)s_first_len_tbl);         const __m128i first_range_tbl = _mm_load_si128((const __m128i *)s_first_range_tbl);         const __m128i range_min_tbl = _mm_load_si128((const __m128i *)s_range_min_tbl);         const __m128i range_max_tbl = _mm_load_si128((const __m128i *)s_range_max_tbl);         const __m128i df_ee_tbl = _mm_load_si128((const __m128i *)s_df_ee_tbl);         const __m128i ef_fe_tbl = _mm_load_si128((const __m128i *)s_ef_fe_tbl);         __m128i error = _mm_set1_epi8(0);         while (len >= 16) {             const __m128i input = _mm_lddqu_si128((const __m128i *)data);             // high_nibbles = input >> 4
            const __m128i high_nibbles =                 _mm_and_si128(_mm_srli_epi16(input, 4), _mm_set1_epi8(0x0F));             // first_len = legal character length minus 1
            // 0 for 00~7F, 1 for C0~DF, 2 for E0~EF, 3 for F0~FF
            // first_len = first_len_tbl[high_nibbles]
            __m128i first_len = _mm_shuffle_epi8(first_len_tbl, high_nibbles);             // First Byte: set range index to 8 for bytes within 0xC0 ~ 0xFF
            // range = first_range_tbl[high_nibbles]
            __m128i range = _mm_shuffle_epi8(first_range_tbl, high_nibbles);             // Second Byte: set range index to first_len
            // 0 for 00~7F, 1 for C0~DF, 2 for E0~EF, 3 for F0~FF
            // range |= (first_len, prev_first_len) << 1 byte
            range = _mm_or_si128(                     range, _mm_alignr_epi8(first_len, prev_first_len, 15));             // Third Byte: set range index to saturate_sub(first_len, 1)
            // 0 for 00~7F, 0 for C0~DF, 1 for E0~EF, 2 for F0~FF
            __m128i tmp1, tmp2;             // tmp1 = saturate_sub(first_len, 1)
            tmp1 = _mm_subs_epu8(first_len, _mm_set1_epi8(1));             // tmp2 = saturate_sub(prev_first_len, 1)
            tmp2 = _mm_subs_epu8(prev_first_len, _mm_set1_epi8(1));             // range |= (tmp1, tmp2) << 2 bytes
            range = _mm_or_si128(range, _mm_alignr_epi8(tmp1, tmp2, 14));             // Fourth Byte: set range index to saturate_sub(first_len, 2)
            // 0 for 00~7F, 0 for C0~DF, 0 for E0~EF, 1 for F0~FF
            // tmp1 = saturate_sub(first_len, 2)
            tmp1 = _mm_subs_epu8(first_len, _mm_set1_epi8(2));             // tmp2 = saturate_sub(prev_first_len, 2)
            tmp2 = _mm_subs_epu8(prev_first_len, _mm_set1_epi8(2));             // range |= (tmp1, tmp2) << 3 bytes
            range = _mm_or_si128(range, _mm_alignr_epi8(tmp1, tmp2, 13));             // Now we have below range indices caluclated
            // Correct cases:
            // - 8 for C0~FF
            // - 3 for 1st byte after F0~FF
            // - 2 for 1st byte after E0~EF or 2nd byte after F0~FF
            // - 1 for 1st byte after C0~DF or 2nd byte after E0~EF or
            //         3rd byte after F0~FF
            // - 0 for others
            // Error cases:
            //   9,10,11 if non ascii First Byte overlaps
            //   E.g., F1 80 C2 90 --> 8 3 10 2, where 10 indicates error
            // Adjust Second Byte range for special First Bytes(E0,ED,F0,F4)
            // Overlaps lead to index 9~15, which are illegal in range table
            __m128i shift1, pos, range2;             // shift1 = (input, prev_input) << 1 byte
            shift1 = _mm_alignr_epi8(input, prev_input, 15);             pos = _mm_sub_epi8(shift1, _mm_set1_epi8(0xEF));             // shift1:  | EF  F0 ... FE | FF  00  ... ...  DE | DF  E0 ... EE |
            // pos:     | 0   1      15 | 16  17           239| 240 241    255|
            // pos-240: | 0   0      0  | 0   0            0  | 0   1      15 |
            // pos+112: | 112 113    127|       >= 128        |     >= 128    |
            tmp1 = _mm_subs_epu8(pos, _mm_set1_epi8(char(240)));             range2 = _mm_shuffle_epi8(df_ee_tbl, tmp1);             tmp2 = _mm_adds_epu8(pos, _mm_set1_epi8(112));             range2 = _mm_add_epi8(range2, _mm_shuffle_epi8(ef_fe_tbl, tmp2));             range = _mm_add_epi8(range, range2);             // Load min and max values per calculated range index
            __m128i minv = _mm_shuffle_epi8(range_min_tbl, range);             __m128i maxv = _mm_shuffle_epi8(range_max_tbl, range);             // Check value range
            error = _mm_or_si128(error, _mm_cmplt_epi8(input, minv));             error = _mm_or_si128(error, _mm_cmpgt_epi8(input, maxv));             prev_input = input;             prev_first_len = first_len;             data += 16;             len -= 16;         }         // Reduce error vector, error_reduced = 0xFFFF if error == 0
        int error_reduced =             _mm_movemask_epi8(_mm_cmpeq_epi8(error, _mm_set1_epi8(0)));         if (error_reduced != 0xFFFF) {             return partial_validation_results{.error = true};         }         // Find previous token (not 80~BF)
        int32_t token4 = _mm_extract_epi32(prev_input, 3);         const int8_t *token = (const int8_t *)&token4;         int lookahead = 0;         if (token[3] > (int8_t)0xBF) {             lookahead = 1;         } else if (token[2] > (int8_t)0xBF) {             lookahead = 2;         } else if (token[1] > (int8_t)0xBF) {             lookahead = 3;         }         data -= lookahead;         len += lookahead;     }     // Continue with remaining bytes with naive method
    return validate_partial_naive(data, len); } } // namespace utf8
}
 // namespace utils
namespace utils { namespace utf8 {   } // namespace utf8
}
 // namespace utils
namespace utils { namespace ascii {  } // namespace ascii
}
 // namespace utils
namespace { using std::wstring; /// Processes a new pattern character, extending re with the equivalent regex pattern.
void process_char(wchar_t c, wstring& re, bool& escaping) {     if (c == L'\\' && !escaping) {         escaping = true;         return;     }     switch (c) {     case L'.':     case L'[':     case L'\\':     case L'*':     case L'^':     case L'$':         // These are meant to match verbatim in LIKE, but they'd be special characters in regex --
        // must escape them.
        re.push_back(L'\\');         re.push_back(c);         break;     case L'_':     case L'%':         if (escaping) {             re.push_back(c);         } else { // LIKE wildcard.
            re.push_back(L'.');             if (c == L'%') {                 re.push_back(L'*');             }         }         break;     default:         re.push_back(c);         break;     }     escaping = false; } /// Returns a regex string matching the given LIKE pattern.
wstring regex_from_pattern(bytes_view pattern) {     if (pattern.empty()) {         return L"^$"; // Like SQL, empty pattern matches only empty text.
    }     using namespace boost::locale::conv;     wstring wpattern = utf_to_utf<wchar_t>(pattern.begin(), pattern.end(), stop);     if (wpattern.back() == L'\\') {         // Add an extra backslash, in case that last character is unescaped.  (If it is escaped, the
        // extra backslash will be ignored.)
        wpattern += L'\\';     }     wstring re;     re.reserve(wpattern.size() * 2); // Worst case: every element is a special character and must be escaped.
    bool escaping = false;     for (const wchar_t c : wpattern) {         process_char(c, re, escaping);     }     return re; } }
 // anonymous namespace
class like_matcher::impl {     bytes _pattern;     boost::u32regex _re; // Performs pattern matching.
  public:     explicit impl(bytes_view pattern);     bool operator()(bytes_view text) const;     void reset(bytes_view pattern);   private:     void init_re() {         _re = boost::make_u32regex(regex_from_pattern(_pattern), boost::u32regex::basic | boost::u32regex::optimize);     } };
 like_matcher::impl::impl(bytes_view pattern) : _pattern(pattern) {     init_re(); }
 bool like_matcher::impl::operator()(bytes_view text) const {     return boost::u32regex_match(text.begin(), text.end(), _re); }
 void like_matcher::impl::reset(bytes_view pattern) {     if (pattern != _pattern) {         _pattern = bytes(pattern);         init_re();     } }
 like_matcher::like_matcher(bytes_view pattern)         : _impl(std::make_unique<impl>(pattern)) { }
 like_matcher::~like_matcher() = default;
 like_matcher::like_matcher(like_matcher&& that) noexcept = default;
 bool like_matcher::operator()(bytes_view text) const {     return _impl->operator()(text); }
 void like_matcher::reset(bytes_view pattern) {     return _impl->reset(pattern); }
 namespace utils { logging::logger errinj_logger("debug_error_injection"); thread_local error_injection<false> error_injection<false>::_local; }
 // namespace utils
using namespace seastar;
 static const Elf64_Nhdr* get_nt_build_id(dl_phdr_info* info) {     auto base = info->dlpi_addr;     const auto* h = info->dlpi_phdr;     auto num_headers = info->dlpi_phnum;     for (int i = 0; i != num_headers; ++i, ++h) {         if (h->p_type != PT_NOTE) {             continue;         }         auto* p = reinterpret_cast<const char*>(base + h->p_vaddr);         auto* e = p + h->p_memsz;         while (p != e) {             const auto* n = reinterpret_cast<const Elf64_Nhdr*>(p);             if (n->n_type == NT_GNU_BUILD_ID) {                 return n;             }             p += sizeof(Elf64_Nhdr);             p += n->n_namesz;             p = align_up(p, 4);             p += n->n_descsz;             p = align_up(p, 4);         }     }     assert(0 && "no NT_GNU_BUILD_ID note"); }
    namespace ser { logging::logger serlog("serializer"); }
 // namespace ser
namespace utils { managed_bytes_view buffer_view_to_managed_bytes_view(ser::buffer_view<bytes_ostream::fragment_iterator> bv) {     auto impl = bv.extract_implementation();     return build_managed_bytes_view_from_internals(             impl.current,             impl.next.extract_implementation().current_chunk,             impl.size     ); } managed_bytes_view_opt buffer_view_to_managed_bytes_view(std::optional<ser::buffer_view<bytes_ostream::fragment_iterator>> bvo) {     if (!bvo) {         return std::nullopt;     }     return buffer_view_to_managed_bytes_view(*bvo); } }
 // namespace utils
namespace std {    }
 // namespace std
seastar::logger testlog("testlog");
 namespace tests { namespace {  }     }
    bool row_assertion::matches(const query::result_set_row& row) const {     for (auto&& column_and_value : _expected_values) {         auto&& name = column_and_value.first;         auto&& value = column_and_value.second;         // FIXME: result_set_row works on sstring column names instead of more general "bytes".
        auto ss_name = to_sstring(name);         const data_value* val = row.get_data_value(ss_name);         if (val == nullptr) {             if (!value.is_null()) {                 return false;             }         } else {             if (*val != value) {                 return false;             }         }     }     if (_only_that) {         for (auto&& e : row.cells()) {             auto name = to_bytes(e.first);             if (!_expected_values.contains(name)) {                 return false;             }         }     }     return true; }
 sstring row_assertion::describe(schema_ptr schema) const {     return format("{{{}}}", fmt::join(_expected_values | boost::adaptors::transformed([&schema] (auto&& e) {         auto&& name = e.first;         auto&& value = e.second;         const column_definition* def = schema->get_column_definition(name);         if (!def) {             BOOST_FAIL(format("Schema is missing column definition for '{}'", name));         }         if (value.is_null()) {             return format("{}=null", to_sstring(name));         } else {             return format("{}=\"{}\"", to_sstring(name), def->type->to_string(def->type->decompose(value)));         }     }), ", ")); }
 const result_set_assertions& result_set_assertions::has(const row_assertion& ra) const {     for (auto&& row : _rs.rows()) {         if (ra.matches(row)) {             return *this;         }     }     BOOST_FAIL(format("Row {} not found in {}", ra.describe(_rs.schema()), _rs));     return *this; }
 const result_set_assertions& result_set_assertions::has_only(const row_assertion& ra) const {     BOOST_REQUIRE(_rs.rows().size() == 1);     auto& row = _rs.rows()[0];     if (!ra.matches(row)) {         BOOST_FAIL(format("Expected {} but got {}", ra.describe(_rs.schema()), row));     }     return *this; }
 const result_set_assertions& result_set_assertions::is_empty() const {     BOOST_REQUIRE_EQUAL(_rs.rows().size(), 0);     return *this; }
 const result_set_assertions& result_set_assertions::has_size(int row_count) const {     BOOST_REQUIRE_EQUAL(_rs.rows().size(), row_count);     return *this; }
 // partitions must be sorted by decorated key
  namespace { // Helper class for testing mutation_reader::fast_forward_to(dht::partition_range).
class partition_range_walker {     std::vector<dht::partition_range> _ranges;     size_t _current_position = 0; private:      public:                }; }
    // Reproduces https://github.com/scylladb/scylla/issues/2733
                     struct mutation_sets {     std::vector<std::vector<mutation>> equal;     std::vector<std::vector<mutation>> unequal;      };
   static const mutation_sets& get_mutation_sets() ;
 void for_each_mutation_pair(std::function<void(const mutation&, const mutation&, are_equal)> callback) {     auto&& ms = get_mutation_sets();     for (auto&& mutations : ms.equal) {         auto i = mutations.begin();         assert(i != mutations.end());         const mutation& first = *i++;         while (i != mutations.end()) {             callback(first, *i, are_equal::yes);             ++i;         }     }     for (auto&& mutations : ms.unequal) {         auto i = mutations.begin();         assert(i != mutations.end());         const mutation& first = *i++;         while (i != mutations.end()) {             callback(first, *i, are_equal::no);             ++i;         }     } }
 void for_each_mutation(std::function<void(const mutation&)> callback) {     auto&& ms = get_mutation_sets();     for (auto&& mutations : ms.equal) {         for (auto&& m : mutations) {             callback(m);         }     }     for (auto&& mutations : ms.unequal) {         for (auto&& m : mutations) {             callback(m);         }     } }
 bytes make_blob(size_t blob_size) {     return tests::random::get_bytes(blob_size); };
 class random_mutation_generator::impl {     enum class timestamp_level {         partition_tombstone = 0,         range_tombstone = 1,         row_shadowable_tombstone = 2,         row_tombstone = 3,         row_marker_tombstone = 4,         collection_tombstone = 5,         cell_tombstone = 6,         data = 7,     }; private:     // Set to true in order to produce mutations which are easier to work with during debugging.
    static const bool debuggable = false;     // The "333" prefix is so that it's easily distinguishable from other numbers in the printout.
    static const api::timestamp_type min_timestamp = debuggable ? 3330000 : ::api::min_timestamp;     friend class random_mutation_generator;     generate_counters _generate_counters;     local_shard_only _local_shard_only;     generate_uncompactable _uncompactable;     const size_t _external_blob_size = debuggable ? 4 : 128; // Should be enough to force use of external bytes storage
    const size_t n_blobs = debuggable ? 32 : 1024;     const column_id column_count = debuggable ? 3 : 64;     std::mt19937 _gen;     schema_ptr _schema;     std::vector<bytes> _blobs;     std::uniform_int_distribution<size_t> _ck_index_dist{0, n_blobs - 1};     std::uniform_int_distribution<int> _bool_dist{0, 1};     std::uniform_int_distribution<int> _not_dummy_dist{0, 19};     std::uniform_int_distribution<int> _range_tombstone_dist{0, 29};     std::uniform_int_distribution<api::timestamp_type> _timestamp_dist{min_timestamp, min_timestamp + 2};     // Sequence number for mutation elements.
    // Intended to be put as "deletion time".
    // The "777" prefix is so that it's easily distinguishable from other numbers in the printout.
    // Also makes it easy to grep for a particular element.
    uint64_t _seq = 777000000;     template <typename Generator>     static gc_clock::time_point expiry_dist(Generator& gen) {         static thread_local std::uniform_int_distribution<int> dist(0, 2);         return gc_clock::time_point() + std::chrono::seconds(dist(gen));     }     schema_ptr do_make_schema(data_type type, const char* ks_name, const char* cf_name) {         auto builder = schema_builder(ks_name, cf_name)                 .with_column("pk", bytes_type, column_kind::partition_key)                 .with_column("ck1", bytes_type, column_kind::clustering_key)                 .with_column("ck2", bytes_type, column_kind::clustering_key);         auto add_column = [&] (const sstring& column_name, column_kind kind) {             auto col_type = type == counter_type || _bool_dist(_gen) ? type : list_type_impl::get_instance(type, true);             builder.with_column(to_bytes(column_name), col_type, kind);         };         for (column_id i = 0; i < column_count; ++i) {             add_column(format("v{:d}", i), column_kind::regular_column);             add_column(format("s{:d}", i), column_kind::static_column);         }         return builder.build();     }     schema_ptr make_schema(const char* ks_name, const char* cf_name) {         return _generate_counters ? do_make_schema(counter_type, ks_name, cf_name)                                   : do_make_schema(bytes_type, ks_name, cf_name);     }     api::timestamp_type gen_timestamp(timestamp_level l) {         auto ts = _timestamp_dist(_gen);         if (_uncompactable) {             // Offset the timestamp such that no higher level tombstones
            // covers any lower level tombstone, and no tombstone covers data.
            return ts + static_cast<std::underlying_type_t<timestamp_level>>(l) * 10;         }         return ts;     }     gc_clock::time_point new_expiry() {         return debuggable ? gc_clock::time_point(gc_clock::time_point::duration(_seq++))                           : expiry_dist(_gen);     }     tombstone random_tombstone(timestamp_level l) {         return tombstone(gen_timestamp(l), new_expiry());     } public:     explicit impl(generate_counters counters, local_shard_only lso = local_shard_only::yes,             generate_uncompactable uc = generate_uncompactable::no, std::optional<uint32_t> seed_opt = std::nullopt, const char* ks_name="ks", const char* cf_name="cf") : _generate_counters(counters), _local_shard_only(lso), _uncompactable(uc) {         // In case of errors, reproduce using the --random-seed command line option with the test_runner seed.
        auto seed = seed_opt.value_or(tests::random::get_int<uint32_t>());         std::cout << "random_mutation_generator seed: " << seed << "\n";         _gen = std::mt19937(seed);         _schema = make_schema(ks_name, cf_name);         // The pre-existing assumption here is that the type of all the primary key components is blob.
        // So we generate partition keys and take the single blob component and save it as a random blob value.
        auto keys = tests::generate_partition_keys(n_blobs, _schema, _local_shard_only, tests::key_size{_external_blob_size, _external_blob_size});         _blobs =  boost::copy_range<std::vector<bytes>>(keys | boost::adaptors::transformed([] (const dht::decorated_key& dk) { return dk.key().explode().front(); }));     }     void set_key_cardinality(size_t n_keys) {         assert(n_keys <= n_blobs);         _ck_index_dist = std::uniform_int_distribution<size_t>{0, n_keys - 1};     }     bytes random_blob() {         return _blobs[std::min(_blobs.size() - 1, std::max<size_t>(0, _ck_index_dist(_gen)))];     }     clustering_key make_random_key() {         return clustering_key::from_exploded(*_schema, { random_blob(), random_blob() });     }     clustering_key_prefix make_random_prefix(std::optional<size_t> max_components_opt = std::nullopt) {         std::vector<bytes> components = { random_blob() };         auto max_components = max_components_opt.value_or(_schema->clustering_key_size());         for (size_t i = 1; i < max_components; i++) {             if (_bool_dist(_gen)) {                 components.push_back(random_blob());             }         }         return clustering_key_prefix::from_exploded(*_schema, std::move(components));     }     std::vector<query::clustering_range> make_random_ranges(unsigned n_ranges) {         std::vector<query::clustering_range> ranges;         if (n_ranges == 0) {             return ranges;         }         auto keys = std::set<clustering_key, clustering_key::less_compare>{clustering_key::less_compare(*_schema)};         while (keys.size() < n_ranges * 2) {             keys.insert(make_random_key());         }         auto i = keys.begin();         bool open_start = _bool_dist(_gen);         bool open_end = _bool_dist(_gen);         if (open_start && open_end && n_ranges == 1) {             ranges.push_back(query::clustering_range::make_open_ended_both_sides());             return ranges;         }         if (open_start) {             ranges.push_back(query::clustering_range(                 { }, { query::clustering_range::bound(*i++, _bool_dist(_gen)) }             ));         }         n_ranges -= unsigned(open_start);         n_ranges -= unsigned(open_end);         while (n_ranges--) {             auto start_key = *i++;             auto end_key = *i++;             ranges.push_back(query::clustering_range(                 { query::clustering_range::bound(start_key, _bool_dist(_gen)) },                 { query::clustering_range::bound(end_key, _bool_dist(_gen)) }             ));         }         if (open_end) {             ranges.push_back(query::clustering_range(                 { query::clustering_range::bound(*i++, _bool_dist(_gen)) }, { }             ));         }         return ranges;     }     range_tombstone make_random_range_tombstone() {         auto t = random_tombstone(timestamp_level::range_tombstone);         switch (_range_tombstone_dist(_gen)) {         case 0: {             // singular prefix
            auto prefix = make_random_prefix(_schema->clustering_key_size()-1);    // make sure the prefix is partial
            auto start = bound_view(prefix, bound_kind::incl_start);             auto end = bound_view(prefix, bound_kind::incl_end);             return range_tombstone(std::move(start), std::move(end), std::move(t));         }         case 1: {             // unbound start
            auto prefix = make_random_prefix();             auto start = bound_view::bottom();             auto end = bound_view(prefix, _bool_dist(_gen) ? bound_kind::incl_end : bound_kind::excl_end);             return range_tombstone(std::move(start), std::move(end), std::move(t));         }         case 2: {             // unbound end
            auto prefix = make_random_prefix();             auto start = bound_view(prefix, _bool_dist(_gen) ? bound_kind::incl_start : bound_kind::excl_start);             auto end = bound_view::top();             return range_tombstone(std::move(start), std::move(end), std::move(t));         }         default:             // fully bounded
            auto start_prefix = make_random_prefix();             auto end_prefix = make_random_prefix();             clustering_key_prefix::tri_compare cmp(*_schema);             auto d = cmp(end_prefix, start_prefix);             while (d == 0) {                 end_prefix = make_random_prefix();                 d = cmp(end_prefix, start_prefix);             }             if (d < 0) {                 std::swap(end_prefix, start_prefix);             }             auto start = bound_view(std::move(start_prefix), _bool_dist(_gen) ? bound_kind::incl_start : bound_kind::excl_start);             auto end = bound_view(std::move(end_prefix), _bool_dist(_gen) ? bound_kind::incl_end : bound_kind::excl_end);             return range_tombstone(std::move(start), std::move(end), std::move(t));         }     }     mutation operator()() {         std::uniform_int_distribution<column_id> column_count_dist(1, column_count);         std::uniform_int_distribution<column_id> column_id_dist(0, column_count - 1);         std::uniform_int_distribution<size_t> value_blob_index_dist(0, 2);         auto pkey = partition_key::from_single_value(*_schema, _blobs[0]);         mutation m(_schema, pkey);         std::map<counter_id, std::set<int64_t>> counter_used_clock_values;         std::vector<counter_id> counter_ids;         std::generate_n(std::back_inserter(counter_ids), 8, counter_id::create_random_id);         auto random_counter_cell = [&] {             std::uniform_int_distribution<size_t> shard_count_dist(1, counter_ids.size());             std::uniform_int_distribution<int64_t> value_dist(-100, 100);             std::uniform_int_distribution<int64_t> clock_dist(0, 20000);             auto shard_count = shard_count_dist(_gen);             std::set<counter_id> shards;             for (auto i = 0u; i < shard_count; i++) {                 shards.emplace(counter_ids[shard_count_dist(_gen) - 1]);             }             counter_cell_builder ccb;             for (auto&& id : shards) {                 // Make sure we don't get shards with the same id and clock
                // but different value.
                int64_t clock = clock_dist(_gen);                 while (counter_used_clock_values[id].contains(clock)) {                     clock = clock_dist(_gen);                 }                 counter_used_clock_values[id].emplace(clock);                 ccb.add_shard(counter_shard(id, value_dist(_gen), clock));             }             return ccb.build(gen_timestamp(timestamp_level::data));         };         auto set_random_cells = [&] (row& r, column_kind kind) {             auto columns_to_set = column_count_dist(_gen);             for (column_id i = 0; i < columns_to_set; ++i) {                 auto cid = column_id_dist(_gen);                 auto& col = _schema->column_at(kind, cid);                 auto get_live_cell = [&] () -> atomic_cell_or_collection {                     if (_generate_counters) {                         return random_counter_cell();                     }                     if (col.is_atomic()) {                         return atomic_cell::make_live(*col.type, gen_timestamp(timestamp_level::data), _blobs[value_blob_index_dist(_gen)]);                     }                     static thread_local std::uniform_int_distribution<int> element_dist{1, 13};                     static thread_local std::uniform_int_distribution<int64_t> uuid_ts_dist{-12219292800000L, -12219292800000L + 1000};                     collection_mutation_description m;                     auto num_cells = element_dist(_gen);                     m.cells.reserve(num_cells);                     std::unordered_set<bytes> unique_cells;                     unique_cells.reserve(num_cells);                     auto ctype = static_pointer_cast<const collection_type_impl>(col.type);                     for (auto i = 0; i < num_cells; ++i) {                         auto uuid = utils::UUID_gen::min_time_UUID(std::chrono::milliseconds{uuid_ts_dist(_gen)}).serialize();                         if (unique_cells.emplace(uuid).second) {                             m.cells.emplace_back(                                 bytes(reinterpret_cast<const int8_t*>(uuid.data()), uuid.size()),                                 atomic_cell::make_live(*ctype->value_comparator(), gen_timestamp(timestamp_level::data), _blobs[value_blob_index_dist(_gen)],                                     atomic_cell::collection_member::yes));                         }                     }                     std::sort(m.cells.begin(), m.cells.end(), [] (auto&& c1, auto&& c2) {                             return timeuuid_type->as_less_comparator()(c1.first, c2.first);                     });                     return m.serialize(*ctype);                 };                 auto get_dead_cell = [&] () -> atomic_cell_or_collection{                     if (col.is_atomic() || col.is_counter()) {                         return atomic_cell::make_dead(gen_timestamp(timestamp_level::cell_tombstone), new_expiry());                     }                     collection_mutation_description m;                     m.tomb = tombstone(gen_timestamp(timestamp_level::collection_tombstone), new_expiry());                     return m.serialize(*col.type);                 };                 // FIXME: generate expiring cells
                auto cell = _bool_dist(_gen) ? get_live_cell() : get_dead_cell();                 r.apply(_schema->column_at(kind, cid), std::move(cell));             }         };         auto random_row_marker = [&] {             static thread_local std::uniform_int_distribution<int> dist(0, 3);             switch (dist(_gen)) {                 case 0: return row_marker();                 case 1: return row_marker(random_tombstone(timestamp_level::row_marker_tombstone));                 case 2: return row_marker(gen_timestamp(timestamp_level::data));                 case 3: return row_marker(gen_timestamp(timestamp_level::data), std::chrono::seconds(1), new_expiry());                 default: assert(0);             }             abort();         };         if (tests::random::with_probability(0.11)) {             m.partition().apply(random_tombstone(timestamp_level::partition_tombstone));         }         m.partition().set_static_row_continuous(_bool_dist(_gen));         set_random_cells(m.partition().static_row().maybe_create(), column_kind::static_column);         auto row_count_dist = [&] (auto& gen) {             static thread_local std::normal_distribution<> dist(32, 1.5);             return static_cast<size_t>(std::min(100.0, std::max(0.0, dist(gen))));         };         size_t row_count = row_count_dist(_gen);         std::unordered_set<clustering_key, clustering_key::hashing, clustering_key::equality> keys(                 0, clustering_key::hashing(*_schema), clustering_key::equality(*_schema));         while (keys.size() < row_count) {             keys.emplace(make_random_key());         }         for (auto&& ckey : keys) {             is_continuous continuous = is_continuous(_bool_dist(_gen));             if (_not_dummy_dist(_gen)) {                 deletable_row& row = m.partition().clustered_row(*_schema, ckey, is_dummy::no, continuous);                 row.apply(random_row_marker());                 if (!row.marker().is_missing() && !row.marker().is_live()) {                     // Mutations are not associative if dead marker is not matched with a dead row
                    // due to shadowable tombstone merging rules. See #11307.
                    row.apply(tombstone(row.marker().timestamp(), row.marker().deletion_time()));                 }                 if (_bool_dist(_gen)) {                     set_random_cells(row.cells(), column_kind::regular_column);                 } else {                     bool is_regular = _bool_dist(_gen);                     if (is_regular) {                         row.apply(random_tombstone(timestamp_level::row_tombstone));                     } else {                         row.apply(shadowable_tombstone{random_tombstone(timestamp_level::row_shadowable_tombstone)});                     }                     bool second_tombstone = _bool_dist(_gen);                     if (second_tombstone) {                         // Need to add the opposite of what has been just added
                        if (is_regular) {                             row.apply(shadowable_tombstone{random_tombstone(timestamp_level::row_shadowable_tombstone)});                         } else {                             row.apply(random_tombstone(timestamp_level::row_tombstone));                         }                     }                 }             } else {                 m.partition().clustered_row(*_schema, position_in_partition::after_key(*_schema, ckey), is_dummy::yes, continuous);             }         }         size_t range_tombstone_count = row_count_dist(_gen);         for (size_t i = 0; i < range_tombstone_count; ++i) {             m.partition().apply_row_tombstone(*_schema, make_random_range_tombstone());         }         if (_bool_dist(_gen)) {             m.partition().ensure_last_dummy(*_schema);             m.partition().clustered_rows().rbegin()->set_continuous(is_continuous(_bool_dist(_gen)));         }         return m;     }     std::vector<dht::decorated_key> make_partition_keys(size_t n) {         return tests::generate_partition_keys(n, _schema, _local_shard_only);     }     std::vector<mutation> operator()(size_t n) {         auto keys = make_partition_keys(n);         std::vector<mutation> mutations;         for (auto&& dkey : keys) {             auto m = operator()();             mutations.emplace_back(_schema, std::move(dkey), std::move(m.partition()));         }         return mutations;     } };
 random_mutation_generator::~random_mutation_generator() {}
 random_mutation_generator::random_mutation_generator(generate_counters counters, local_shard_only lso, generate_uncompactable uc, std::optional<uint32_t> seed_opt, const char* ks_name, const char* cf_name)     : _impl(std::make_unique<random_mutation_generator::impl>(counters, lso, uc, seed_opt,  ks_name, cf_name)) { }
 mutation random_mutation_generator::operator()() {     return (*_impl)(); }
 std::vector<mutation> random_mutation_generator::operator()(size_t n) {     return (*_impl)(n); }
 std::vector<dht::decorated_key> random_mutation_generator::make_partition_keys(size_t n) {     return _impl->make_partition_keys(n); }
 schema_ptr random_mutation_generator::schema() const {     return _impl->_schema; }
 range_tombstone random_mutation_generator::make_random_range_tombstone() {     return _impl->make_random_range_tombstone(); }
 clustering_key random_mutation_generator::make_random_key() {     return _impl->make_random_key(); }
 std::vector<query::clustering_range> random_mutation_generator::make_random_ranges(unsigned n_ranges) {     return _impl->make_random_ranges(n_ranges); }
 void random_mutation_generator::set_key_cardinality(size_t n_keys) {     _impl->set_key_cardinality(n_keys); }
 void for_each_schema_change(std::function<void(schema_ptr, const std::vector<mutation>&,                                                schema_ptr, const std::vector<mutation>&)> fn) {     auto map_of_int_to_int = map_type_impl::get_instance(int32_type, int32_type, true);     auto map_of_int_to_bytes = map_type_impl::get_instance(int32_type, bytes_type, true);     auto frozen_map_of_int_to_int = map_type_impl::get_instance(int32_type, int32_type, false);     auto frozen_map_of_int_to_bytes = map_type_impl::get_instance(int32_type, bytes_type, false);     auto tuple_of_int_long = tuple_type_impl::get_instance({ int32_type, long_type });     auto tuple_of_bytes_long = tuple_type_impl::get_instance( { bytes_type, long_type });     auto tuple_of_bytes_bytes = tuple_type_impl::get_instance( { bytes_type, bytes_type });     auto set_of_text = set_type_impl::get_instance(utf8_type, true);     auto set_of_bytes = set_type_impl::get_instance(bytes_type, true);     auto udt_int_text = user_type_impl::get_instance("ks", "udt",         { utf8_type->decompose("f1"), utf8_type->decompose("f2"), },         { int32_type, utf8_type }, true);     auto udt_int_blob_long = user_type_impl::get_instance("ks", "udt",         { utf8_type->decompose("v1"), utf8_type->decompose("v2"), utf8_type->decompose("v3"), },         { int32_type, bytes_type, long_type }, true);     auto frozen_udt_int_text = user_type_impl::get_instance("ks", "udt",         { utf8_type->decompose("f1"), utf8_type->decompose("f2"), },         { int32_type, utf8_type }, false);     auto frozen_udt_int_blob_long = user_type_impl::get_instance("ks", "udt",         { utf8_type->decompose("v1"), utf8_type->decompose("v2"), utf8_type->decompose("v3"), },         { int32_type, bytes_type, long_type }, false);     auto random_int32_value = [] {         return int32_type->decompose(tests::random::get_int<int32_t>());     };     auto random_text_value = [] {         return utf8_type->decompose(tests::random::get_sstring());     };     int32_t key_id = 0;     auto random_partition_key = [&] () -> tests::data_model::mutation_description::key {         return { random_int32_value(), random_int32_value(), int32_type->decompose(key_id++), };     };     auto random_clustering_key = [&] () -> tests::data_model::mutation_description::key {         return {             utf8_type->decompose(tests::random::get_sstring()),             utf8_type->decompose(tests::random::get_sstring()),             utf8_type->decompose(format("{}", key_id++)),         };     };     auto random_map = [&] () -> tests::data_model::mutation_description::collection {         return {             { int32_type->decompose(1), random_int32_value() },             { int32_type->decompose(2), random_int32_value() },             { int32_type->decompose(3), random_int32_value() },         };     };     auto random_frozen_map = [&] {         return map_of_int_to_int->decompose(make_map_value(map_of_int_to_int, map_type_impl::native_type({             { 1, tests::random::get_int<int32_t>() },             { 2, tests::random::get_int<int32_t>() },             { 3, tests::random::get_int<int32_t>() },         })));     };     auto random_tuple = [&] {         return tuple_of_int_long->decompose(make_tuple_value(tuple_of_int_long, tuple_type_impl::native_type{             tests::random::get_int<int32_t>(), tests::random::get_int<int64_t>(),         }));     };     auto random_set = [&] () -> tests::data_model::mutation_description::collection {         return {             { utf8_type->decompose("a"), bytes() },             { utf8_type->decompose("b"), bytes() },             { utf8_type->decompose("c"), bytes() },         };     };     auto random_udt = [&] () -> tests::data_model::mutation_description::collection {         return {             { serialize_field_index(0), random_int32_value() },             { serialize_field_index(1), random_text_value() },         };     };     auto random_frozen_udt = [&] {         return frozen_udt_int_text->decompose(make_user_value(udt_int_text, user_type_impl::native_type{             tests::random::get_int<int32_t>(),             tests::random::get_sstring(),         }));     };     struct column_description {         int id;         data_type type;         std::vector<data_type> alter_to;         std::vector<std::function<tests::data_model::mutation_description::value()>> data_generators;         data_type old_type;     };     auto columns = std::vector<column_description> {         { 100, int32_type, { varint_type, bytes_type }, { [&] { return random_int32_value(); }, [&] { return bytes(); } }, uuid_type },         { 200, map_of_int_to_int, { map_of_int_to_bytes }, { [&] { return random_map(); } }, empty_type },         { 300, int32_type, { varint_type, bytes_type }, { [&] { return random_int32_value(); }, [&] { return bytes(); } }, empty_type },         { 400, frozen_map_of_int_to_int, { frozen_map_of_int_to_bytes }, { [&] { return random_frozen_map(); } }, empty_type },         { 500, tuple_of_int_long, { tuple_of_bytes_long, tuple_of_bytes_bytes }, { [&] { return random_tuple(); } }, empty_type },         { 600, set_of_text, { set_of_bytes }, { [&] { return random_set(); } }, empty_type },         { 700, udt_int_text, { udt_int_blob_long }, { [&] { return random_udt(); } }, empty_type },         { 800, frozen_udt_int_text, { frozen_udt_int_blob_long }, { [&] { return random_frozen_udt(); } }, empty_type },     };     auto static_columns = columns;     auto regular_columns = columns;     // Base schema
    auto s = tests::data_model::table_description({ { "pk1", int32_type }, { "pk2", int32_type }, { "pk3", int32_type }, },                                                   { { "ck1", utf8_type }, { "ck2", utf8_type }, { "ck3", utf8_type }, });     for (auto& sc : static_columns) {         auto name = format("s{}", sc.id);         s.add_static_column(name, sc.type);         if (sc.old_type != empty_type) {             s.add_old_static_column(name, sc.old_type);         }     }     for (auto& rc : regular_columns) {         auto name = format("r{}", rc.id);         s.add_regular_column(name, rc.type);         if (rc.old_type != empty_type) {             s.add_old_regular_column(name, rc.old_type);         }     }     auto max_generator_count = std::max(         // boost::max_elements wants the iterators to be copy-assignable. The ones we get
        // from boost::adaptors::transformed aren't.
        boost::accumulate(static_columns | boost::adaptors::transformed([] (const column_description& c) {             return c.data_generators.size();         }), 0u, [] (size_t a, size_t b) { return std::max(a, b); }),         boost::accumulate(regular_columns | boost::adaptors::transformed([] (const column_description& c) {             return c.data_generators.size();         }), 0u, [] (size_t a, size_t b) { return std::max(a, b); })     );     // Base data
    // Single column in a static row, nothing else
    for (auto& [id, type, alter_to, data_generators, old_type] : static_columns) {         auto name = format("s{}", id);         for (auto& dg : data_generators) {             auto m = tests::data_model::mutation_description(random_partition_key());             m.add_static_cell(name, dg());             s.unordered_mutations().emplace_back(std::move(m));         }     }     // Partition with rows each having a single column
    auto m = tests::data_model::mutation_description(random_partition_key());     for (auto& [id, type, alter_to, data_generators, old_type] : regular_columns) {         auto name = format("r{}", id);         for (auto& dg : data_generators) {             m.add_clustered_cell(random_clustering_key(), name, dg());         }     }     s.unordered_mutations().emplace_back(std::move(m));     // Absolutely everything
    for (auto i = 0u; i < max_generator_count; i++) {         auto m = tests::data_model::mutation_description(random_partition_key());         for (auto& [id, type, alter_to, data_generators, old_type] : static_columns) {             auto name = format("s{}", id);             m.add_static_cell(name, data_generators[std::min<size_t>(i, data_generators.size() - 1)]());         }         for (auto& [id, type, alter_to, data_generators, old_type] : regular_columns) {             auto name = format("r{}", id);             m.add_clustered_cell(random_clustering_key(), name, data_generators[std::min<size_t>(i, data_generators.size() - 1)]());         }         m.add_range_tombstone(random_clustering_key(), random_clustering_key());         m.add_range_tombstone(random_clustering_key(), random_clustering_key());         m.add_range_tombstone(random_clustering_key(), random_clustering_key());         s.unordered_mutations().emplace_back(std::move(m));     }     // Transformations
    auto base = s.build();     std::vector<tests::data_model::table_description::table> schemas;     schemas.emplace_back(base);     auto test_mutated_schemas = [&] {         auto& [ base_change_log, base_schema, base_mutations ] = base;         for (auto&& [ mutated_change_log, mutated_schema, mutated_mutations ] : schemas) {             testlog.info("\nSchema change from:\n\n{}\n\nto:\n\n{}\n", base_change_log, mutated_change_log);             fn(base_schema, base_mutations, mutated_schema, mutated_mutations);         }         for (auto i = 2u; i < schemas.size(); i++) {             auto& [ base_change_log, base_schema, base_mutations ] = schemas[i - 1];             auto& [ mutated_change_log, mutated_schema, mutated_mutations ] = schemas[i];             testlog.info("\nSchema change from:\n\n{}\n\nto:\n\n{}\n", base_change_log, mutated_change_log);             fn(base_schema, base_mutations, mutated_schema, mutated_mutations);         }         schemas.clear();         schemas.emplace_back(base);     };     auto original_s = s;      // Remove and add back all static columns
    for (auto& sc : static_columns) {         s.remove_static_column(format("s{}", sc.id));         schemas.emplace_back(s.build());     }     for (auto& sc : static_columns) {         s.add_static_column(format("s{}", sc.id), uuid_type);         auto mutated = s.build();         schemas.emplace_back(s.build());     }     test_mutated_schemas();     s = original_s;     // Remove and add back all regular columns
    for (auto& rc : regular_columns) {         s.remove_regular_column(format("r{}", rc.id));         schemas.emplace_back(s.build());     }     auto temp_s = s;     auto temp_schemas = schemas;     for (auto& rc : regular_columns) {         s.add_regular_column(format("r{}", rc.id), uuid_type);         schemas.emplace_back(s.build());     }     test_mutated_schemas();     s = temp_s;     schemas = temp_schemas;     // Add back all regular columns as collections
    for (auto& rc : regular_columns) {         s.add_regular_column(format("r{}", rc.id), map_of_int_to_bytes);         schemas.emplace_back(s.build());     }     test_mutated_schemas();     s = temp_s;     schemas = temp_schemas;     // Add back all regular columns as frozen collections
    for (auto& rc : regular_columns) {         s.add_regular_column(format("r{}", rc.id), frozen_map_of_int_to_int);         schemas.emplace_back(s.build());     }     test_mutated_schemas();     s = original_s;     // Add more static columns
    for (auto& sc : static_columns) {         s.add_static_column(format("s{}", sc.id + 1), uuid_type);         schemas.emplace_back(s.build());     }     test_mutated_schemas();     s = original_s;     // Add more regular columns
    for (auto& rc : regular_columns) {         s.add_regular_column(format("r{}", rc.id + 1), uuid_type);         schemas.emplace_back(s.build());     }     test_mutated_schemas();     s = original_s;     // Alter column types
    for (auto& sc : static_columns) {         for (auto& target : sc.alter_to) {             s.alter_static_column_type(format("s{}", sc.id), target);             schemas.emplace_back(s.build());         }     }     for (auto& rc : regular_columns) {         for (auto& target : rc.alter_to) {             s.alter_regular_column_type(format("r{}", rc.id), target);             schemas.emplace_back(s.build());         }     }     for (auto i = 1; i <= 3; i++) {         s.alter_clustering_column_type(format("ck{}", i), bytes_type);         schemas.emplace_back(s.build());     }     for (auto i = 1; i <= 3; i++) {         s.alter_partition_column_type(format("pk{}", i), bytes_type);         schemas.emplace_back(s.build());     }     test_mutated_schemas();     s = original_s;     // Rename clustering key
    for (auto i = 1; i <= 3; i++) {         s.rename_clustering_column(format("ck{}", i), format("ck{}", 100 - i));         schemas.emplace_back(s.build());     }     test_mutated_schemas();     s = original_s;     // Rename partition key
    for (auto i = 1; i <= 3; i++) {         s.rename_partition_column(format("pk{}", i), format("pk{}", 100 - i));         schemas.emplace_back(s.build());     }     test_mutated_schemas(); }
 static bool compare_readers(const schema& s, flat_mutation_reader_v2& authority, flat_reader_assertions_v2& tested) {     bool empty = true;     while (auto expected = authority().get()) {         tested.produces(s, *expected);         empty = false;     }     tested.produces_end_of_stream();     return !empty; }
 void compare_readers(const schema& s, flat_mutation_reader_v2 authority, flat_mutation_reader_v2 tested) {     auto close_authority = deferred_close(authority);     auto assertions = assert_that(std::move(tested));     compare_readers(s, authority, assertions); }
 // Assumes that the readers return fragments from (at most) a single (and the same) partition.
void compare_readers(const schema& s, flat_mutation_reader_v2 authority, flat_mutation_reader_v2 tested, const std::vector<position_range>& fwd_ranges) {     auto close_authority = deferred_close(authority);     auto assertions = assert_that(std::move(tested));     if (compare_readers(s, authority, assertions)) {         for (auto& r: fwd_ranges) {             authority.fast_forward_to(r).get();             assertions.fast_forward_to(r);             compare_readers(s, authority, assertions);         }     } }
 mutation forwardable_reader_to_mutation(flat_mutation_reader_v2 r, const std::vector<position_range>& fwd_ranges) {     auto close_reader = deferred_close(r);     struct consumer {         schema_ptr _s;         std::optional<mutation_rebuilder_v2>& _builder;         consumer(schema_ptr s, std::optional<mutation_rebuilder_v2>& builder)             : _s(std::move(s))             , _builder(builder) { }         void consume_new_partition(const dht::decorated_key& dk) {             assert(!_builder);             _builder = mutation_rebuilder_v2(std::move(_s));             _builder->consume_new_partition(dk);         }         stop_iteration consume(tombstone t) {             assert(_builder);             return _builder->consume(t);         }         stop_iteration consume(range_tombstone_change&& rt) {             assert(_builder);             return _builder->consume(std::move(rt));         }         stop_iteration consume(static_row&& sr) {             assert(_builder);             return _builder->consume(std::move(sr));         }         stop_iteration consume(clustering_row&& cr) {             assert(_builder);             return _builder->consume(std::move(cr));         }         stop_iteration consume_end_of_partition() {             assert(_builder);             return stop_iteration::yes;         }         void consume_end_of_stream() { }     };     std::optional<mutation_rebuilder_v2> builder{};     r.consume(consumer(r.schema(), builder)).get();     BOOST_REQUIRE(builder);     for (auto& range : fwd_ranges) {         testlog.trace("forwardable_reader_to_mutation: forwarding to {}", range);         r.fast_forward_to(range).get();         r.consume(consumer(r.schema(), builder)).get();     }     auto m = builder->consume_end_of_stream();     BOOST_REQUIRE(m);     return std::move(*m); }
 std::vector<mutation> squash_mutations(std::vector<mutation> mutations) {     if (mutations.empty()) {         return {};     }     std::map<dht::decorated_key, mutation, dht::ring_position_less_comparator> merged_muts{             dht::ring_position_less_comparator{*mutations.front().schema()}};     for (const auto& mut : mutations) {         auto [it, inserted] = merged_muts.try_emplace(mut.decorated_key(), mut);         if (!inserted) {             it->second.apply(mut);         }     }     return boost::copy_range<std::vector<mutation>>(merged_muts | boost::adaptors::map_values); }
 namespace tests::data_model { mutation_description::atomic_value::atomic_value(bytes value, api::timestamp_type timestamp) : value(std::move(value)), timestamp(timestamp) { } mutation_description::atomic_value::atomic_value(bytes value, api::timestamp_type timestamp, gc_clock::duration ttl, gc_clock::time_point expiry_point)     : value(std::move(value)), timestamp(timestamp), expiring(expiry_info{ttl, expiry_point}) { } mutation_description::collection::collection(std::initializer_list<collection_element> elements) : elements(elements) { } mutation_description::collection::collection(std::vector<collection_element> elements) : elements(std::move(elements)) { } mutation_description::row_marker::row_marker(api::timestamp_type timestamp) : timestamp(timestamp) { } mutation_description::row_marker::row_marker(api::timestamp_type timestamp, gc_clock::duration ttl, gc_clock::time_point expiry_point)     : timestamp(timestamp), expiring(expiry_info{ttl, expiry_point}) { } void mutation_description::remove_column(row& r, const sstring& name) {     auto it = boost::range::find_if(r, [&] (const cell& c) {         return c.column_name == name;     });     if (it != r.end()) {         r.erase(it);     } } mutation_description::mutation_description(key partition_key)     : _partition_key(std::move(partition_key)) { } void mutation_description::set_partition_tombstone(tombstone partition_tombstone) {     _partition_tombstone = partition_tombstone; } void mutation_description::add_static_cell(const sstring& column, value v) {     _static_row.emplace_back(cell { column, std::move(v) }); } void mutation_description::add_clustered_cell(const key& ck, const sstring& column, value v) {     _clustered_rows[ck].cells.emplace_back(cell { column, std::move(v) }); } void mutation_description::add_clustered_row_marker(const key& ck, row_marker marker) {     _clustered_rows[ck].marker = marker; } void mutation_description::add_clustered_row_tombstone(const key& ck, row_tombstone tomb) {     _clustered_rows[ck].tomb = tomb; } void mutation_description::remove_static_column(const sstring& name) {     remove_column(_static_row, name); } void mutation_description::remove_regular_column(const sstring& name) {     for (auto& [ ckey, cr ] : _clustered_rows) {         (void)ckey;         remove_column(cr.cells, name);     } } void mutation_description::add_range_tombstone(const key& start, const key& end, tombstone tomb) {     add_range_tombstone(nonwrapping_range<key>::make(start, end), tomb); } void mutation_description::add_range_tombstone(nonwrapping_range<key> range, tombstone tomb) {     _range_tombstones.emplace_back(range_tombstone { std::move(range), tomb }); } mutation mutation_description::build(schema_ptr s) const {     auto m = mutation(s, partition_key::from_exploded(*s, _partition_key));     m.partition().apply(_partition_tombstone);     for (auto& [ column, value_or_collection ] : _static_row) {         auto cdef = s->get_column_definition(utf8_type->decompose(column));         assert(cdef);         std::visit(make_visitor(             [&] (const atomic_value& v) {                 assert(cdef->is_atomic());                 if (!v.expiring) {                     m.set_static_cell(*cdef, atomic_cell::make_live(*cdef->type, v.timestamp, v.value));                 } else {                     m.set_static_cell(*cdef, atomic_cell::make_live(*cdef->type, v.timestamp, v.value,                                                                     v.expiring->expiry_point, v.expiring->ttl));                 }             },             [&] (const collection& c) {                 assert(!cdef->is_atomic());                 auto get_value_type = visit(*cdef->type, make_visitor(                     [] (const collection_type_impl& ctype) -> std::function<const abstract_type&(bytes_view)> {                         return [&] (bytes_view) -> const abstract_type& { return *ctype.value_comparator(); };                     },                     [] (const user_type_impl& utype) -> std::function<const abstract_type&(bytes_view)> {                         return [&] (bytes_view key) -> const abstract_type& { return *utype.type(deserialize_field_index(key)); };                     },                     [] (const abstract_type& o) -> std::function<const abstract_type&(bytes_view)> {                         assert(false);                     }                 ));                 collection_mutation_description mut;                 mut.tomb = c.tomb;                 for (auto& [ key, value ] : c.elements) {                     if (!value.expiring) {                         mut.cells.emplace_back(key, atomic_cell::make_live(get_value_type(key), value.timestamp,                                                                             value.value, atomic_cell::collection_member::yes));                     } else {                         mut.cells.emplace_back(key, atomic_cell::make_live(get_value_type(key),                                                                            value.timestamp,                                                                            value.value,                                                                            value.expiring->expiry_point,                                                                            value.expiring->ttl,                                                                            atomic_cell::collection_member::yes));                     }                 }                 m.set_static_cell(*cdef, mut.serialize(*cdef->type));             }         ), value_or_collection);     }     for (auto& [ ckey, cr ] : _clustered_rows) {         auto& [ marker, tomb, cells ] = cr;         auto ck = clustering_key::from_exploded(*s, ckey);         for (auto& [ column, value_or_collection ] : cells) {             auto cdef = s->get_column_definition(utf8_type->decompose(column));             assert(cdef);             std::visit(make_visitor(             [&] (const atomic_value& v) {                     assert(cdef->is_atomic());                     if (!v.expiring) {                         m.set_clustered_cell(ck, *cdef, atomic_cell::make_live(*cdef->type, v.timestamp, v.value));                     } else {                         m.set_clustered_cell(ck, *cdef, atomic_cell::make_live(*cdef->type, v.timestamp, v.value,                                                                                v.expiring->expiry_point, v.expiring->ttl));                     }                 },             [&] (const collection& c) {                     assert(!cdef->is_atomic());                     auto get_value_type = visit(*cdef->type, make_visitor(                         [] (const collection_type_impl& ctype) -> std::function<const abstract_type&(bytes_view)> {                             return [&] (bytes_view) -> const abstract_type& { return *ctype.value_comparator(); };                         },                         [] (const user_type_impl& utype) -> std::function<const abstract_type&(bytes_view)> {                             return [&] (bytes_view key) -> const abstract_type& { return *utype.type(deserialize_field_index(key)); };                         },                         [] (const abstract_type& o) -> std::function<const abstract_type&(bytes_view)> {                             assert(false);                         }                     ));                     collection_mutation_description mut;                     mut.tomb = c.tomb;                     for (auto& [ key, value ] : c.elements) {                         if (!value.expiring) {                             mut.cells.emplace_back(key, atomic_cell::make_live(get_value_type(key), value.timestamp,                                                                             value.value, atomic_cell::collection_member::yes));                         } else {                             mut.cells.emplace_back(key, atomic_cell::make_live(get_value_type(key),                                                                                value.timestamp,                                                                                value.value,                                                                                value.expiring->expiry_point,                                                                                value.expiring->ttl,                                                                                atomic_cell::collection_member::yes));                         }                     }                     m.set_clustered_cell(ck, *cdef, mut.serialize(*cdef->type));                 }             ), value_or_collection);         }         if (marker.timestamp != api::missing_timestamp) {             if (marker.expiring) {                 m.partition().clustered_row(*s, ckey).apply(::row_marker(marker.timestamp, marker.expiring->ttl, marker.expiring->expiry_point));             } else {                 m.partition().clustered_row(*s, ckey).apply(::row_marker(marker.timestamp));             }         }         if (tomb) {             m.partition().clustered_row(*s, ckey).apply(tomb);         }     }     clustering_key::less_compare cmp(*s);     for (auto& [ range, tomb ] : _range_tombstones) {         auto clustering_range = range.transform([&s = *s] (const key& k) {             return clustering_key::from_exploded(s, k);         });         if (!clustering_range.is_singular()) {             auto start = clustering_range.start();             auto end = clustering_range.end();             if (start && end && cmp(end->value(), start->value())) {                 clustering_range = nonwrapping_range<clustering_key>(std::move(end), std::move(start));             }         }         auto rt = ::range_tombstone(                 bound_view::from_range_start(clustering_range),                 bound_view::from_range_end(clustering_range),                 tomb);         m.partition().apply_delete(*s, std::move(rt));     }     return m; } std::vector<table_description::column>::iterator table_description::find_column(std::vector<column>& columns, const sstring& name) {     return boost::range::find_if(columns, [&] (const column& c) {         return std::get<sstring>(c) == name;     }); } void table_description::add_column(std::vector<column>& columns, const sstring& name, data_type type) {     assert(find_column(columns, name) == columns.end());     columns.emplace_back(name, type); } void table_description::add_old_column(const sstring& name, data_type type) {     _removed_columns.emplace_back(removed_column { name, type, previously_removed_column_timestamp }); } void table_description::remove_column(std::vector<column>& columns, const sstring& name) {     auto it = find_column(columns, name);     assert(it != columns.end());     _removed_columns.emplace_back(removed_column { name, std::get<data_type>(*it), column_removal_timestamp });     columns.erase(it); } void table_description::alter_column_type(std::vector<column>& columns, const sstring& name, data_type new_type) {     auto it = find_column(columns, name);     assert(it != columns.end());     std::get<data_type>(*it) = new_type; } schema_ptr table_description::build_schema() const {     auto sb = schema_builder("ks", "cf");     for (auto&& [ name, type ] : _partition_key) {         sb.with_column(utf8_type->decompose(name), type, column_kind::partition_key);     }     for (auto&& [ name, type ] : _clustering_key) {         sb.with_column(utf8_type->decompose(name), type, column_kind::clustering_key);     }     for (auto&& [ name, type ] : _static_columns) {         sb.with_column(utf8_type->decompose(name), type, column_kind::static_column);     }     for (auto&& [ name, type ] : _regular_columns) {         sb.with_column(utf8_type->decompose(name), type);     }     for (auto&& [ name, type, timestamp ] : _removed_columns) {         sb.without_column(name, type, timestamp);     }     return sb.build(); } std::vector<mutation> table_description::build_mutations(schema_ptr s) const {     auto ms = boost::copy_range<std::vector<mutation>>(         _mutations | boost::adaptors::transformed([&] (const mutation_description& md) {             return md.build(s);         })     );     boost::sort(ms, mutation_decorated_key_less_comparator());     return ms; } table_description::table_description(std::vector<column> partition_key, std::vector<column> clustering_key)     : _partition_key(std::move(partition_key))     , _clustering_key(std::move(clustering_key)) { } void table_description::add_static_column(const sstring& name, data_type type) {     _change_log.emplace_back(format("added static column \'{}\' of type \'{}\'", name, type->as_cql3_type().to_string()));     add_column(_static_columns, name, type); } void table_description::add_regular_column(const sstring& name, data_type type) {     _change_log.emplace_back(format("added regular column \'{}\' of type \'{}\'", name, type->as_cql3_type().to_string()));     add_column(_regular_columns, name, type); } void table_description::add_old_static_column(const sstring& name, data_type type) {     add_old_column(name, type); } void table_description::add_old_regular_column(const sstring& name, data_type type) {     add_old_column(name, type); } void table_description::remove_static_column(const sstring& name) {     _change_log.emplace_back(format("removed static column \'{}\'", name));     remove_column(_static_columns, name);     for (auto& m : _mutations) {         m.remove_static_column(name);     } } void table_description::remove_regular_column(const sstring& name) {     _change_log.emplace_back(format("removed regular column \'{}\'", name));     remove_column(_regular_columns, name);     for (auto& m : _mutations) {         m.remove_regular_column(name);     } } void table_description::alter_partition_column_type(const sstring& name, data_type new_type) {     _change_log.emplace_back(format("altered partition column \'{}\' type to \'{}\'", name, new_type->as_cql3_type().to_string()));     alter_column_type(_partition_key, name, new_type); } void table_description::alter_clustering_column_type(const sstring& name, data_type new_type) {     _change_log.emplace_back(format("altered clustering column \'{}\' type to \'{}\'", name, new_type->as_cql3_type().to_string()));     alter_column_type(_clustering_key, name, new_type); } void table_description::alter_static_column_type(const sstring& name, data_type new_type) {     _change_log.emplace_back(format("altered static column \'{}\' type to \'{}\'", name, new_type->as_cql3_type().to_string()));     alter_column_type(_static_columns, name, new_type); } void table_description::alter_regular_column_type(const sstring& name, data_type new_type) {     _change_log.emplace_back(format("altered regular column \'{}\' type to \'{}\'", name, new_type->as_cql3_type().to_string()));     alter_column_type(_regular_columns, name, new_type); } void table_description::rename_partition_column(const sstring& from, const sstring& to) {     _change_log.emplace_back(format("renamed partition column \'{}\' to \'{}\'", from, to));     auto it = find_column(_partition_key, from);     assert(it != _partition_key.end());     std::get<sstring>(*it) = to; } void table_description::rename_clustering_column(const sstring& from, const sstring& to) {     _change_log.emplace_back(format("renamed clustering column \'{}\' to \'{}\'", from, to));     auto it = find_column(_clustering_key, from);     assert(it != _clustering_key.end());     std::get<sstring>(*it) = to; } table_description::table table_description::build() const {     auto s = build_schema();     return { boost::algorithm::join(_change_log, "\n"), s, build_mutations(s) }; } }
 std::function<bool(const std::exception&)> exception_predicate::make(         std::function<bool(const std::exception&)> check,         std::function<sstring(const std::exception&)> err) {     return [check = std::move(check), err = std::move(err)] (const std::exception& e) {                const bool status = check(e);                BOOST_CHECK_MESSAGE(status, err(e));                return status;            }; }
 std::function<bool(const std::exception&)> exception_predicate::message_contains(         const sstring& fragment,         const std::source_location& loc) {     return make([=] (const std::exception& e) { return sstring(e.what()).find(fragment) != sstring::npos; },                 [=] (const std::exception& e) {                     return fmt::format("Message '{}' doesn't contain '{}'\n{}:{}: invoked here",                                        e.what(), fragment, loc.file_name(), loc.line());                 }); }
 std::function<bool(const std::exception&)> exception_predicate::message_equals(         const sstring& text,         const std::source_location& loc) {     return make([=] (const std::exception& e) { return text == e.what(); },                 [=] (const std::exception& e) {                     return fmt::format("Message '{}' doesn't equal '{}'\n{}:{}: invoked here",                                        e.what(), text, loc.file_name(), loc.line());                 }); }
 std::function<bool(const std::exception&)> exception_predicate::message_matches(         const std::string& regex,         const std::source_location& loc) {     // Use boost::regex since std::regex (with libstdc++ 12) uses too much stack
    return make([=] (const std::exception& e) { return boost::regex_search(e.what(), boost::regex(regex)); },                 [=] (const std::exception& e) {                     return fmt::format("Message '{}' doesn't match '{}'\n{}:{}: invoked here",                                        e.what(), regex, loc.file_name(), loc.line());                 }); }
 namespace tests { type_generator::type_generator(random_schema_specification& spec) : _spec(spec) {     struct simple_type_generator {         data_type type;         data_type operator()(std::mt19937&, is_multi_cell) { return type; }     };     _generators = {         simple_type_generator{byte_type},         simple_type_generator{short_type},         simple_type_generator{int32_type},         simple_type_generator{long_type},         simple_type_generator{ascii_type},         simple_type_generator{bytes_type},         simple_type_generator{utf8_type},         simple_type_generator{boolean_type},         simple_type_generator{date_type},         simple_type_generator{timeuuid_type},         simple_type_generator{timestamp_type},         simple_type_generator{simple_date_type},         simple_type_generator{time_type},         simple_type_generator{uuid_type},         simple_type_generator{inet_addr_type},         simple_type_generator{float_type},         simple_type_generator{double_type},         simple_type_generator{varint_type},         simple_type_generator{decimal_type},         simple_type_generator{duration_type}};     // tuple
    _generators.emplace_back(         [this] (std::mt19937& engine, is_multi_cell) {             std::uniform_int_distribution<size_t> count_dist{2, 4};             const auto count = count_dist(engine);             std::vector<data_type> data_types;             for (size_t i = 0; i < count; ++i) {                 data_types.emplace_back((*this)(engine, type_generator::is_multi_cell::no));             }             return tuple_type_impl::get_instance(std::move(data_types));         });     // user
    _generators.emplace_back(         [this] (std::mt19937& engine, is_multi_cell multi_cell) mutable {             std::uniform_int_distribution<size_t> count_dist{2, 4};             const auto count = count_dist(engine);             std::vector<bytes> field_names;             std::vector<data_type> field_types;             for (size_t i = 0; i < count; ++i) {                 field_names.emplace_back(to_bytes(format("f{}", i)));                 field_types.emplace_back((*this)(engine, type_generator::is_multi_cell::no));             }             return user_type_impl::get_instance(_spec.keyspace_name(), to_bytes(_spec.udt_name(engine)), std::move(field_names),                     std::move(field_types), bool(multi_cell));         });     // list
    _generators.emplace_back(         [this] (std::mt19937& engine, is_multi_cell multi_cell) {             auto element_type = (*this)(engine, type_generator::is_multi_cell::no);             return list_type_impl::get_instance(std::move(element_type), bool(multi_cell));         });     // set
    _generators.emplace_back(         [this] (std::mt19937& engine, is_multi_cell multi_cell) {             auto element_type = (*this)(engine, type_generator::is_multi_cell::no);             return set_type_impl::get_instance(std::move(element_type), bool(multi_cell));         });     // map
    _generators.emplace_back(         [this] (std::mt19937& engine, is_multi_cell multi_cell) {             auto key_type = (*this)(engine, type_generator::is_multi_cell::no);             auto value_type = (*this)(engine, type_generator::is_multi_cell::no);             return map_type_impl::get_instance(std::move(key_type), std::move(value_type), bool(multi_cell));         }); } data_type type_generator::operator()(std::mt19937& engine, is_multi_cell multi_cell) {     auto dist = std::uniform_int_distribution<size_t>(0, _generators.size() - 1);     auto type = _generators.at(dist(engine))(engine, multi_cell);     // duration type is not allowed in:
    // * primary key components
    // * as member types of collections
    //
    // To cover all this, we simply disallow it altogether when multi_cell is
    // no, which will be the case in all the above cases.
    while (!multi_cell && type == duration_type) {         type = (*this)(engine, multi_cell);     }     return type; } namespace { class default_random_schema_specification : public random_schema_specification {     std::unordered_set<unsigned> _used_table_ids;     std::unordered_set<unsigned> _used_udt_ids;     std::uniform_int_distribution<size_t> _partition_column_count_dist;     std::uniform_int_distribution<size_t> _clustering_column_count_dist;     std::uniform_int_distribution<size_t> _regular_column_count_dist;     std::uniform_int_distribution<size_t> _static_column_count_dist;     type_generator _type_generator; private:     static unsigned generate_unique_id(std::mt19937& engine, std::unordered_set<unsigned>& used_ids) {         std::uniform_int_distribution<unsigned> id_dist(0, 1024);         unsigned id;         do {             id = id_dist(engine);         } while (used_ids.contains(id));         used_ids.insert(id);         return id;     }     std::vector<data_type> generate_types(std::mt19937& engine, std::uniform_int_distribution<size_t>& count_dist,             type_generator::is_multi_cell multi_cell, bool allow_reversed = false) {         std::uniform_int_distribution<uint8_t> reversed_dist{0, uint8_t(allow_reversed)};         std::uniform_int_distribution<uint8_t> multi_cell_dist{0, uint8_t(bool(multi_cell))};         std::vector<data_type> types;         const auto count = count_dist(engine);         for (size_t c = 0; c < count; ++c) {             auto type = _type_generator(engine, type_generator::is_multi_cell(bool(multi_cell_dist(engine))));             if (reversed_dist(engine)) {                 types.emplace_back(make_shared<reversed_type_impl>(std::move(type)));             } else {                 types.emplace_back(std::move(type));             }         }         return types;     } public:     default_random_schema_specification(             sstring keyspace_name,             std::uniform_int_distribution<size_t> partition_column_count_dist,             std::uniform_int_distribution<size_t> clustering_column_count_dist,             std::uniform_int_distribution<size_t> regular_column_count_dist,             std::uniform_int_distribution<size_t> static_column_count_dist)         : random_schema_specification(std::move(keyspace_name))         , _partition_column_count_dist(partition_column_count_dist)         , _clustering_column_count_dist(clustering_column_count_dist)         , _regular_column_count_dist(regular_column_count_dist)         , _static_column_count_dist(static_column_count_dist)         , _type_generator(*this) {         assert(_partition_column_count_dist.a() > 0);         assert(_regular_column_count_dist.a() > 0);     }     virtual sstring table_name(std::mt19937& engine) override {         return format("table{}", generate_unique_id(engine, _used_table_ids));     }     virtual sstring udt_name(std::mt19937& engine) override {         return format("udt{}", generate_unique_id(engine, _used_udt_ids));     }     virtual std::vector<data_type> partition_key_columns(std::mt19937& engine) override {         return generate_types(engine, _partition_column_count_dist, type_generator::is_multi_cell::no, false);     }     virtual std::vector<data_type> clustering_key_columns(std::mt19937& engine) override {         return generate_types(engine, _clustering_column_count_dist, type_generator::is_multi_cell::no, true);     }     virtual std::vector<data_type> regular_columns(std::mt19937& engine) override {         return generate_types(engine, _regular_column_count_dist, type_generator::is_multi_cell::yes, false);     }     virtual std::vector<data_type> static_columns(std::mt19937& engine) override {         return generate_types(engine, _static_column_count_dist, type_generator::is_multi_cell::yes, false);     } }; } // anonymous namespace
std::unique_ptr<random_schema_specification> make_random_schema_specification(         sstring keyspace_name,         std::uniform_int_distribution<size_t> partition_column_count_dist,         std::uniform_int_distribution<size_t> clustering_column_count_dist,         std::uniform_int_distribution<size_t> regular_column_count_dist,         std::uniform_int_distribution<size_t> static_column_count_dist) {     return std::make_unique<default_random_schema_specification>(std::move(keyspace_name), partition_column_count_dist, clustering_column_count_dist,             regular_column_count_dist, static_column_count_dist); } namespace { utils::multiprecision_int generate_multiprecision_integer_value(std::mt19937& engine, size_t min_size_in_bytes, size_t max_size_in_bytes) {     using utils::multiprecision_int;     const auto max_bytes = std::min(size_t(16), std::max(size_t(2), max_size_in_bytes) - 1);     const auto generate_int = [] (std::mt19937& engine, size_t max_bytes) {         if (max_bytes == 8) {             return multiprecision_int(random::get_int<uint64_t>(engine));         } else { // max_bytes < 8
            return multiprecision_int(random::get_int<uint64_t>(0, (uint64_t(1) << (max_bytes * 8)) - uint64_t(1), engine));         }     };     if (max_bytes <= 8) {         return generate_int(engine, max_bytes);     } else { // max_bytes > 8
        auto ls = multiprecision_int(generate_int(engine, 8));         auto ms = multiprecision_int(generate_int(engine, max_bytes - 8));         return multiprecision_int(ls) + (multiprecision_int(ms) << 64);     } } template <typename String> String generate_string_value(std::mt19937& engine, typename String::value_type min, typename String::value_type max,         size_t min_size_in_bytes, size_t max_size_in_bytes) {     auto size_dist = random::stepped_int_distribution<size_t>{{         {95.0, {   0,   31}},         { 4.5, {  32,   99}},         { 0.4, { 100,  999}},         { 0.1, {1000, 9999}}}};     auto char_dist = std::uniform_int_distribution<typename String::value_type>(min, max);     const auto size = std::clamp(             size_dist(engine),             min_size_in_bytes / sizeof(typename String::value_type),             max_size_in_bytes / sizeof(typename String::value_type));     String str(size, '\0');     for (size_t i = 0; i < size; ++i) {         str[i] = char_dist(engine);     }     return str; } std::vector<data_value> generate_frozen_tuple_values(std::mt19937& engine, value_generator& val_gen, const std::vector<data_type>& member_types,         size_t min_size_in_bytes, size_t max_size_in_bytes) {     std::vector<data_value> values;     values.reserve(member_types.size());     const auto member_min_size_in_bytes = min_size_in_bytes / member_types.size();     const auto member_max_size_in_bytes = max_size_in_bytes / member_types.size();     for (auto member_type : member_types) {         values.push_back(val_gen.generate_atomic_value(engine, *member_type, member_min_size_in_bytes, member_max_size_in_bytes));     }     return values; } data_model::mutation_description::collection generate_user_value(std::mt19937& engine, const user_type_impl& type,         value_generator& val_gen) {     using md = data_model::mutation_description;     // Non-null fields.
    auto fields_num = std::uniform_int_distribution<size_t>(1, type.size())(engine);     auto field_idxs = random::random_subset<unsigned>(type.size(), fields_num, engine);     std::sort(field_idxs.begin(), field_idxs.end());     md::collection collection;     for (auto i: field_idxs) {         collection.elements.push_back({serialize_field_index(i),                 val_gen.generate_atomic_value(engine, *type.type(i), value_generator::no_size_in_bytes_limit).serialize_nonnull()});     }     return collection; } data_model::mutation_description::collection generate_collection(std::mt19937& engine, const abstract_type& key_type,         const abstract_type& value_type, value_generator& val_gen) {     using md = data_model::mutation_description;     auto key_generator = val_gen.get_atomic_value_generator(key_type);     auto value_generator = val_gen.get_atomic_value_generator(value_type);     auto size_dist = std::uniform_int_distribution<size_t>(0, 16);     const auto size = size_dist(engine);     std::map<bytes, md::atomic_value, serialized_compare> collection{key_type.as_less_comparator()};     for (size_t i = 0; i < size; ++i) {         collection.emplace(key_generator(engine, 0, value_generator::no_size_in_bytes_limit).serialize_nonnull(),                 value_generator(engine, 0, value_generator::no_size_in_bytes_limit).serialize().value_or(""));     }     md::collection flat_collection;     flat_collection.elements.reserve(collection.size());     for (auto&& [key, value] : collection) {         flat_collection.elements.emplace_back(md::collection_element{key, value});     }     return flat_collection; } std::vector<data_value> generate_frozen_list(std::mt19937& engine, const abstract_type& value_type, value_generator& val_gen,         size_t min_size_in_bytes, size_t max_size_in_bytes) {     auto value_generator = val_gen.get_atomic_value_generator(value_type);     auto size_dist = std::uniform_int_distribution<size_t>(0, 4);     const auto size = std::min(size_dist(engine), max_size_in_bytes / std::max(val_gen.min_size(value_type), size_t(1)));     std::vector<data_value> collection;     if (!size) {         return collection;     }     const auto value_min_size_in_bytes = min_size_in_bytes / size;     const auto value_max_size_in_bytes = max_size_in_bytes / size;     for (size_t i = 0; i < size; ++i) {         collection.emplace_back(value_generator(engine, value_min_size_in_bytes, value_max_size_in_bytes));     }     return collection; } std::vector<data_value> generate_frozen_set(std::mt19937& engine, const abstract_type& key_type, value_generator& val_gen,         size_t min_size_in_bytes, size_t max_size_in_bytes) {     auto key_generator = val_gen.get_atomic_value_generator(key_type);     auto size_dist = std::uniform_int_distribution<size_t>(0, 4);     const auto size = std::min(size_dist(engine), max_size_in_bytes / std::max(val_gen.min_size(key_type), size_t(1)));     std::map<bytes, data_value, serialized_compare> collection{key_type.as_less_comparator()};     std::vector<data_value> flat_collection;     if (!size) {         return flat_collection;     }     const auto value_max_size_in_bytes = max_size_in_bytes / size;     const auto value_min_size_in_bytes = min_size_in_bytes / size;     for (size_t i = 0; i < size; ++i) {         auto val = key_generator(engine, value_min_size_in_bytes, value_max_size_in_bytes);         auto serialized_key = val.serialize_nonnull();         collection.emplace(std::move(serialized_key), std::move(val));     }     flat_collection.reserve(collection.size());     for (auto&& element : collection) {         flat_collection.emplace_back(std::move(element.second));     }     return flat_collection; } std::vector<std::pair<data_value, data_value>> generate_frozen_map(std::mt19937& engine, const abstract_type& key_type,         const abstract_type& value_type, value_generator& val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes) {     auto key_generator = val_gen.get_atomic_value_generator(key_type);     auto value_generator = val_gen.get_atomic_value_generator(value_type);     auto size_dist = std::uniform_int_distribution<size_t>(0, 4);     const auto min_item_size_in_bytes = val_gen.min_size(key_type) + val_gen.min_size(value_type);     const auto size = std::min(size_dist(engine), max_size_in_bytes / std::max(min_item_size_in_bytes, size_t(1)));     std::map<bytes, std::pair<data_value, data_value>, serialized_compare> collection(key_type.as_less_comparator());     std::vector<std::pair<data_value, data_value>> flat_collection;     if (!size) {         return flat_collection;     }     const auto item_max_size_in_bytes = max_size_in_bytes / size;     const auto key_max_size_in_bytes = item_max_size_in_bytes / 2;     const auto value_max_size_in_bytes = item_max_size_in_bytes / 2;     const auto item_min_size_in_bytes = min_size_in_bytes / size;     const auto key_min_size_in_bytes = item_min_size_in_bytes / 2;     const auto value_min_size_in_bytes = item_min_size_in_bytes / 2;     for (size_t i = 0; i < size; ++i) {         auto key = key_generator(engine, key_min_size_in_bytes, key_max_size_in_bytes);         auto serialized_key = key.serialize_nonnull();         auto value = value_generator(engine, value_min_size_in_bytes, value_max_size_in_bytes);         collection.emplace(std::move(serialized_key), std::pair(std::move(key), std::move(value)));     }     flat_collection.reserve(collection.size());     for (auto&& element : collection) {         flat_collection.emplace_back(std::move(element.second));     }     return flat_collection; } data_value generate_empty_value(std::mt19937&, size_t, size_t) {     return data_value::make_null(empty_type); } data_value generate_byte_value(std::mt19937& engine, size_t, size_t) {     return data_value(random::get_int<int8_t>(engine)); } data_value generate_short_value(std::mt19937& engine, size_t, size_t) {     return data_value(random::get_int<int16_t>(engine)); } data_value generate_int32_value(std::mt19937& engine, size_t, size_t) {     return data_value(random::get_int<int32_t>(engine)); } data_value generate_long_value(std::mt19937& engine, size_t, size_t) {     return data_value(random::get_int<int64_t>(engine)); } data_value generate_ascii_value(std::mt19937& engine, size_t min_size_in_bytes, size_t max_size_in_bytes) {     return data_value(ascii_native_type{generate_string_value<sstring>(engine, 0, 127, min_size_in_bytes, max_size_in_bytes)}); } data_value generate_bytes_value(std::mt19937& engine, size_t min_size_in_bytes, size_t max_size_in_bytes) {     return data_value(generate_string_value<bytes>(engine, std::numeric_limits<bytes::value_type>::min(),             std::numeric_limits<bytes::value_type>::max(), min_size_in_bytes, max_size_in_bytes)); } data_value generate_utf8_value(std::mt19937& engine, size_t min_size_in_bytes, size_t max_size_in_bytes) {     auto wstr = generate_string_value<std::wstring>(engine, 0, 0x0FFF, min_size_in_bytes, max_size_in_bytes);     std::locale locale("en_US.utf8");     using codec = std::codecvt<wchar_t, char, std::mbstate_t>;     auto& f = std::use_facet<codec>(locale);     sstring utf8_str(wstr.size() * f.max_length(), '\0');     const wchar_t* from_next;     char* to_next;     std::mbstate_t mb{};     auto res = f.out(mb, &wstr[0], &wstr[wstr.size()], from_next, &utf8_str[0], &utf8_str[utf8_str.size()], to_next);     assert(res == codec::ok);     utf8_str.resize(to_next - &utf8_str[0]);     return data_value(std::move(utf8_str)); } data_value generate_boolean_value(std::mt19937& engine, size_t, size_t) {     auto dist = std::uniform_int_distribution<int8_t>(0, 1);     return data_value(bool(dist(engine))); } data_value generate_date_value(std::mt19937& engine, size_t, size_t) {     return data_value(date_type_native_type{db_clock::time_point(db_clock::duration(random::get_int<std::make_unsigned_t<db_clock::rep>>(engine)))}); } data_value generate_timeuuid_value(std::mt19937&, size_t, size_t) {     return data_value(timeuuid_native_type{utils::UUID_gen::get_time_UUID()}); } data_value generate_timestamp_value(std::mt19937& engine, size_t, size_t) {     using pt = db_clock::time_point;     return data_value(pt(pt::duration(random::get_int<pt::rep>(engine)))); } data_value generate_simple_date_value(std::mt19937& engine, size_t, size_t) {     return data_value(simple_date_native_type{random::get_int<simple_date_native_type::primary_type>(engine)}); } data_value generate_time_value(std::mt19937& engine, size_t, size_t) {     return data_value(time_native_type{random::get_int<time_native_type::primary_type>(engine)}); } data_value generate_uuid_value(std::mt19937& engine, size_t, size_t) {     return data_value(utils::make_random_uuid()); } data_value generate_inet_addr_value(std::mt19937& engine, size_t, size_t) {     return data_value(net::ipv4_address(random::get_int<int32_t>(engine))); } data_value generate_float_value(std::mt19937& engine, size_t, size_t) {     return data_value(random::get_real<float>(engine)); } data_value generate_double_value(std::mt19937& engine, size_t, size_t) {     return data_value(random::get_real<double>(engine)); } data_value generate_varint_value(std::mt19937& engine, size_t min_size_in_bytes, size_t max_size_in_bytes) {     return data_value(generate_multiprecision_integer_value(engine, min_size_in_bytes, max_size_in_bytes)); } data_value generate_decimal_value(std::mt19937& engine, size_t min_size_in_bytes, size_t max_size_in_bytes) {     auto scale_dist = std::uniform_int_distribution<int32_t>(-8, 8);     return data_value(big_decimal(scale_dist(engine), generate_multiprecision_integer_value(engine,                     min_size_in_bytes - sizeof(int32_t), max_size_in_bytes - sizeof(int32_t)))); } data_value generate_duration_value(std::mt19937& engine, size_t, size_t) {     auto months = months_counter(random::get_int<months_counter::value_type>(engine));     auto days = days_counter(random::get_int<days_counter::value_type>(0, 31, engine));     auto nanoseconds = nanoseconds_counter(random::get_int<nanoseconds_counter::value_type>(86400000000000, engine));     return data_value(cql_duration{months, days, nanoseconds}); } data_value generate_frozen_tuple_value(std::mt19937& engine, const tuple_type_impl& type, value_generator& val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes) {     assert(!type.is_multi_cell());     return make_tuple_value(type.shared_from_this(), generate_frozen_tuple_values(engine, val_gen, type.all_types(), min_size_in_bytes, max_size_in_bytes)); } data_value generate_frozen_user_value(std::mt19937& engine, const user_type_impl& type, value_generator& val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes) {     assert(!type.is_multi_cell());     return make_user_value(type.shared_from_this(), generate_frozen_tuple_values(engine, val_gen, type.all_types(), min_size_in_bytes, max_size_in_bytes)); } data_model::mutation_description::collection generate_list_value(std::mt19937& engine, const list_type_impl& type, value_generator& val_gen) {     assert(type.is_multi_cell());     return generate_collection(engine, *type.name_comparator(), *type.value_comparator(), val_gen); } data_value generate_frozen_list_value(std::mt19937& engine, const list_type_impl& type, value_generator& val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes) {     assert(!type.is_multi_cell());     return make_list_value(type.shared_from_this(),             generate_frozen_list(engine, *type.get_elements_type(), val_gen, min_size_in_bytes, max_size_in_bytes)); } data_model::mutation_description::collection generate_set_value(std::mt19937& engine, const set_type_impl& type, value_generator& val_gen) {     assert(type.is_multi_cell());     return generate_collection(engine, *type.name_comparator(), *type.value_comparator(), val_gen); } data_value generate_frozen_set_value(std::mt19937& engine, const set_type_impl& type, value_generator& val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes) {     assert(!type.is_multi_cell());     return make_set_value(type.shared_from_this(),             generate_frozen_set(engine, *type.get_elements_type(), val_gen, min_size_in_bytes, max_size_in_bytes)); } data_model::mutation_description::collection generate_map_value(std::mt19937& engine, const map_type_impl& type, value_generator& val_gen) {     assert(type.is_multi_cell());     return generate_collection(engine, *type.name_comparator(), *type.value_comparator(), val_gen); } data_value generate_frozen_map_value(std::mt19937& engine, const map_type_impl& type, value_generator& val_gen, size_t min_size_in_bytes, size_t max_size_in_bytes) {     assert(!type.is_multi_cell());     return make_map_value(type.shared_from_this(),             generate_frozen_map(engine, *type.get_keys_type(), *type.get_values_type(), val_gen, min_size_in_bytes, max_size_in_bytes)); } } // anonymous namespace
data_value value_generator::generate_atomic_value(std::mt19937& engine, const abstract_type& type, size_t max_size_in_bytes) {     return generate_atomic_value(engine, type, 0, max_size_in_bytes); } data_value value_generator::generate_atomic_value(std::mt19937& engine, const abstract_type& type, size_t min_size_in_bytes, size_t max_size_in_bytes) {     assert(!type.is_multi_cell());     return get_atomic_value_generator(type)(engine, min_size_in_bytes, max_size_in_bytes); } value_generator::value_generator()     : _regular_value_generators{             {empty_type.get(), &generate_empty_value},             {byte_type.get(), &generate_byte_value},             {short_type.get(), &generate_short_value},             {int32_type.get(), &generate_int32_value},             {long_type.get(), &generate_long_value},             {ascii_type.get(), &generate_ascii_value},             {bytes_type.get(), &generate_bytes_value},             {utf8_type.get(), &generate_utf8_value},             {boolean_type.get(), &generate_boolean_value},             {date_type.get(), &generate_date_value},             {timeuuid_type.get(), &generate_timeuuid_value},             {timestamp_type.get(), &generate_timestamp_value},             {simple_date_type.get(), &generate_simple_date_value},             {time_type.get(), &generate_time_value},             {uuid_type.get(), &generate_uuid_value},             {inet_addr_type.get(), &generate_inet_addr_value},             {float_type.get(), &generate_float_value},             {double_type.get(), &generate_double_value},             {varint_type.get(), &generate_varint_value},             {decimal_type.get(), &generate_decimal_value},             {duration_type.get(), &generate_duration_value}} {     std::mt19937 engine;     for (const auto& [regular_type, regular_value_gen] : _regular_value_generators) {         _regular_value_min_sizes.emplace(regular_type, regular_value_gen(engine, size_t{}, size_t{}).serialized_size());     } } size_t value_generator::min_size(const abstract_type& type) {     assert(!type.is_multi_cell());     auto it = _regular_value_min_sizes.find(&type);     if (it != _regular_value_min_sizes.end()) {         return it->second;     }     std::mt19937 engine;     if (auto maybe_user_type = dynamic_cast<const user_type_impl*>(&type)) {         return generate_frozen_user_value(engine, *maybe_user_type, *this, size_t{}, size_t{}).serialized_size();     }     if (auto maybe_tuple_type = dynamic_cast<const tuple_type_impl*>(&type)) {         return generate_frozen_tuple_value(engine, *maybe_tuple_type, *this, size_t{}, size_t{}).serialized_size();     }     if (auto maybe_list_type = dynamic_cast<const list_type_impl*>(&type)) {         return generate_frozen_list_value(engine, *maybe_list_type, *this, size_t{}, size_t{}).serialized_size();     }     if (auto maybe_set_type = dynamic_cast<const set_type_impl*>(&type)) {         return generate_frozen_set_value(engine, *maybe_set_type, *this, size_t{}, size_t{}).serialized_size();     }     if (auto maybe_map_type = dynamic_cast<const map_type_impl*>(&type)) {         return generate_frozen_map_value(engine, *maybe_map_type, *this, size_t{}, size_t{}).serialized_size();     }     if (auto maybe_reversed_type = dynamic_cast<const reversed_type_impl*>(&type)) {         return min_size(*maybe_reversed_type->underlying_type());     }     throw std::runtime_error(fmt::format("Don't know how to calculate min size for unknown type {}", type.name())); } value_generator::atomic_value_generator value_generator::get_atomic_value_generator(const abstract_type& type) {     assert(!type.is_multi_cell());     auto it = _regular_value_generators.find(&type);     if (it != _regular_value_generators.end()) {         return it->second;     }     if (auto maybe_user_type = dynamic_cast<const user_type_impl*>(&type)) {         return [this, maybe_user_type] (std::mt19937& engine, size_t min_size_in_bytes, size_t max_size_in_bytes) {             return generate_frozen_user_value(engine, *maybe_user_type, *this, min_size_in_bytes, max_size_in_bytes);         };     }     if (auto maybe_tuple_type = dynamic_cast<const tuple_type_impl*>(&type)) {         return [this, maybe_tuple_type] (std::mt19937& engine, size_t min_size_in_bytes, size_t max_size_in_bytes) {             return generate_frozen_tuple_value(engine, *maybe_tuple_type, *this, min_size_in_bytes, max_size_in_bytes);         };     }     if (auto maybe_list_type = dynamic_cast<const list_type_impl*>(&type)) {         return [this, maybe_list_type] (std::mt19937& engine, size_t min_size_in_bytes, size_t max_size_in_bytes) {             return generate_frozen_list_value(engine, *maybe_list_type, *this, min_size_in_bytes, max_size_in_bytes);         };     }     if (auto maybe_set_type = dynamic_cast<const set_type_impl*>(&type)) {         return [this, maybe_set_type] (std::mt19937& engine, size_t min_size_in_bytes, size_t max_size_in_bytes) {             return generate_frozen_set_value(engine, *maybe_set_type, *this, min_size_in_bytes, max_size_in_bytes);         };     }     if (auto maybe_map_type = dynamic_cast<const map_type_impl*>(&type)) {         return [this, maybe_map_type] (std::mt19937& engine, size_t min_size_in_bytes, size_t max_size_in_bytes) {             return generate_frozen_map_value(engine, *maybe_map_type, *this, min_size_in_bytes, max_size_in_bytes);         };     }     if (auto maybe_reversed_type = dynamic_cast<const reversed_type_impl*>(&type)) {         return get_atomic_value_generator(*maybe_reversed_type->underlying_type());     }     throw std::runtime_error(fmt::format("Don't know how to generate value for unknown type {}", type.name())); } value_generator::generator value_generator::get_generator(const abstract_type& type) {     auto it = _regular_value_generators.find(&type);     if (it != _regular_value_generators.end()) {         return [gen = it->second] (std::mt19937& engine) -> data_model::mutation_description::value {             return gen(engine, 0, no_size_in_bytes_limit).serialize_nonnull();         };     }     if (auto maybe_user_type = dynamic_cast<const user_type_impl*>(&type)) {         if (maybe_user_type->is_multi_cell()) {             return [this, maybe_user_type] (std::mt19937& engine) -> data_model::mutation_description::value {                 return generate_user_value(engine, *maybe_user_type, *this);             };         } else {             return [this, maybe_user_type] (std::mt19937& engine) -> data_model::mutation_description::value {                 return generate_frozen_user_value(engine, *maybe_user_type, *this, 0, no_size_in_bytes_limit).serialize_nonnull();             };         }     }     if (auto maybe_tuple_type = dynamic_cast<const tuple_type_impl*>(&type)) {         return [this, maybe_tuple_type] (std::mt19937& engine) -> data_model::mutation_description::value {             return generate_frozen_tuple_value(engine, *maybe_tuple_type, *this, 0, no_size_in_bytes_limit).serialize_nonnull();         };     }     if (auto maybe_list_type = dynamic_cast<const list_type_impl*>(&type)) {         if (maybe_list_type->is_multi_cell()) {             return [this, maybe_list_type] (std::mt19937& engine) -> data_model::mutation_description::value {                 return generate_list_value(engine, *maybe_list_type, *this);             };         } else {             return [this, maybe_list_type] (std::mt19937& engine) -> data_model::mutation_description::value {                 return generate_frozen_list_value(engine, *maybe_list_type, *this, 0, no_size_in_bytes_limit).serialize_nonnull();             };         }     }     if (auto maybe_set_type = dynamic_cast<const set_type_impl*>(&type)) {         if (maybe_set_type->is_multi_cell()) {             return [this, maybe_set_type] (std::mt19937& engine) -> data_model::mutation_description::value {                 return generate_set_value(engine, *maybe_set_type, *this);             };         } else {             return [this, maybe_set_type] (std::mt19937& engine) -> data_model::mutation_description::value {                 return generate_frozen_set_value(engine, *maybe_set_type, *this, 0, no_size_in_bytes_limit).serialize_nonnull();             };         }     }     if (auto maybe_map_type = dynamic_cast<const map_type_impl*>(&type)) {         if (maybe_map_type->is_multi_cell()) {             return [this, maybe_map_type] (std::mt19937& engine) -> data_model::mutation_description::value {                 return generate_map_value(engine, *maybe_map_type, *this);             };         } else {             return [this, maybe_map_type] (std::mt19937& engine) -> data_model::mutation_description::value {                 return generate_frozen_map_value(engine, *maybe_map_type, *this, 0, no_size_in_bytes_limit).serialize_nonnull();             };         }     }     if (auto maybe_reversed_type = dynamic_cast<const reversed_type_impl*>(&type)) {         return get_generator(*maybe_reversed_type->underlying_type());     }     throw std::runtime_error(fmt::format("Don't know how to generate value for unknown type {}", type.name())); } data_model::mutation_description::value value_generator::generate_value(std::mt19937& engine, const abstract_type& type) {     return get_generator(type)(engine); } timestamp_generator default_timestamp_generator() {     return [] (std::mt19937& engine, timestamp_destination, api::timestamp_type min_timestamp) {         auto ts_dist = std::uniform_int_distribution<api::timestamp_type>(min_timestamp, api::max_timestamp);         return ts_dist(engine);     }; } expiry_generator no_expiry_expiry_generator() {     return [] (std::mt19937& engine, timestamp_destination destination) -> std::optional<expiry_info> {         return std::nullopt;     }; } namespace { schema_ptr build_random_schema(uint32_t seed, random_schema_specification& spec) {     auto engine = std::mt19937{seed};     auto builder = schema_builder(spec.keyspace_name(), spec.table_name(engine));     auto pk_columns = spec.partition_key_columns(engine);     assert(!pk_columns.empty()); // Let's not pull in boost::test here
    for (size_t pk = 0; pk < pk_columns.size(); ++pk) {         builder.with_column(to_bytes(format("pk{}", pk)), std::move(pk_columns[pk]), column_kind::partition_key);     }     const auto ck_columns = spec.clustering_key_columns(engine);     for (size_t ck = 0; ck < ck_columns.size(); ++ck) {         builder.with_column(to_bytes(format("ck{}", ck)), std::move(ck_columns[ck]), column_kind::clustering_key);     }     if (!ck_columns.empty()) {         const auto static_columns = spec.static_columns(engine);         for (size_t s = 0; s < static_columns.size(); ++s) {             builder.with_column(to_bytes(format("s{}", s)), std::move(static_columns[s]), column_kind::static_column);         }     }     const auto regular_columns = spec.regular_columns(engine);     assert(!regular_columns.empty()); // Let's not pull in boost::test here
    for (size_t r = 0; r < regular_columns.size(); ++r) {         builder.with_column(to_bytes(format("v{}", r)), std::move(regular_columns[r]), column_kind::regular_column);     }     return builder.build(); } sstring udt_to_str(const user_type_impl& udt) {     std::stringstream ss;     udt.describe(ss);     return ss.str(); } struct udt_list {     std::vector<const user_type_impl*> vector;     void insert(const user_type_impl* udt) {         auto it = std::find(vector.begin(), vector.end(), udt);         if (it == vector.end()) {             vector.push_back(udt);         }     }     void merge(udt_list other) {         for (auto& udt : other.vector) {             insert(udt);         }     } }; udt_list dump_udts(const std::vector<data_type>& types) {     udt_list udts;     for (const auto& dt : types) {         const auto* const type = dt.get();         if (auto maybe_user_type = dynamic_cast<const user_type_impl*>(type)) {             udts.merge(dump_udts(maybe_user_type->field_types()));             udts.insert(maybe_user_type);         } else if (auto maybe_tuple_type = dynamic_cast<const tuple_type_impl*>(type)) {             udts.merge(dump_udts(maybe_tuple_type->all_types()));         } else if (auto maybe_list_type = dynamic_cast<const list_type_impl*>(type)) {             udts.merge(dump_udts({maybe_list_type->get_elements_type()}));         } else if (auto maybe_set_type = dynamic_cast<const set_type_impl*>(type)) {             udts.merge(dump_udts({maybe_set_type->get_elements_type()}));         } else if (auto maybe_map_type = dynamic_cast<const map_type_impl*>(type)) {             udts.merge(dump_udts({maybe_map_type->get_keys_type(), maybe_map_type->get_values_type()}));         } else if (auto maybe_reversed_type = dynamic_cast<const reversed_type_impl*>(type)) {             udts.merge(dump_udts({maybe_reversed_type->underlying_type()}));         }     }     return udts; } std::vector<const user_type_impl*> dump_udts(const schema& schema) {     udt_list udts;     const auto cdefs_to_types = [] (const schema::const_iterator_range_type& cdefs) -> std::vector<data_type> {         return boost::copy_range<std::vector<data_type>>(cdefs |                 boost::adaptors::transformed([] (const column_definition& cdef) { return cdef.type; }));     };     udts.merge(dump_udts(cdefs_to_types(schema.partition_key_columns())));     udts.merge(dump_udts(cdefs_to_types(schema.clustering_key_columns())));     udts.merge(dump_udts(cdefs_to_types(schema.regular_columns())));     udts.merge(dump_udts(cdefs_to_types(schema.static_columns())));     return udts.vector; } std::vector<sstring> columns_specs(schema_ptr schema, column_kind kind) {     const auto count = schema->columns_count(kind);     if (!count) {         return {};     }     std::vector<sstring> col_specs;     for (column_count_type c = 0; c < count; ++c) {         const auto& cdef = schema->column_at(kind, c);         col_specs.emplace_back(format("{} {}{}", cdef.name_as_cql_string(), cdef.type->as_cql3_type().to_string(),                 kind == column_kind::static_column ? " static" : ""));     }     return col_specs; } std::vector<sstring> column_names(schema_ptr schema, column_kind kind) {     const auto count = schema->columns_count(kind);     if (!count) {         return {};     }     std::vector<sstring> col_names;     for (column_count_type c = 0; c < count; ++c) {         const auto& cdef = schema->column_at(kind, c);         col_names.emplace_back(cdef.name_as_cql_string());     }     return col_names; } } void decorate_with_timestamps(const schema& schema, std::mt19937& engine, timestamp_generator& ts_gen, expiry_generator exp_gen,         data_model::mutation_description::value& value) {     std::visit(             make_visitor(                     [&] (data_model::mutation_description::atomic_value& v) {                         v.timestamp = ts_gen(engine, timestamp_destination::cell_timestamp, api::min_timestamp);                         if (auto expiry_opt = exp_gen(engine, timestamp_destination::cell_timestamp)) {                             v.expiring = data_model::mutation_description::expiry_info{expiry_opt->ttl, expiry_opt->expiry_point};                         }                     },                     [&] (data_model::mutation_description::collection& c) {                         if (auto ts = ts_gen(engine, timestamp_destination::collection_tombstone, api::min_timestamp);                                 ts != api::missing_timestamp) {                             if (ts == api::max_timestamp) {                                 // Caveat: leave some headroom for the cells
                                // having a timestamp larger than the
                                // tombstone's.
                                ts--;                             }                             auto expiry_opt = exp_gen(engine, timestamp_destination::collection_tombstone);                             const auto deletion_time = expiry_opt ? expiry_opt->expiry_point : gc_clock::now();                             c.tomb = tombstone(ts, deletion_time);                         }                         for (auto& [ key, value ] : c.elements) {                             value.timestamp = ts_gen(engine, timestamp_destination::collection_cell_timestamp, c.tomb.timestamp);                             assert(!c.tomb || value.timestamp > c.tomb.timestamp);                             if (auto expiry_opt = exp_gen(engine, timestamp_destination::collection_cell_timestamp)) {                                 value.expiring = data_model::mutation_description::expiry_info{expiry_opt->ttl, expiry_opt->expiry_point};                             }                         }                     }),             value); } data_model::mutation_description::key random_schema::make_key(uint32_t n, value_generator& gen, schema::const_iterator_range_type columns,         size_t max_size_in_bytes) {     std::mt19937 engine(n);     const size_t max_component_size = max_size_in_bytes / std::distance(columns.begin(), columns.end());     std::vector<bytes> key;     for (const auto& cdef : columns) {         key.emplace_back(gen.generate_atomic_value(engine, *cdef.type, max_component_size).serialize_nonnull());     }     return key; } data_model::mutation_description::key random_schema::make_partition_key(uint32_t n, value_generator& gen) const {     return make_key(n, gen, _schema->partition_key_columns(), std::numeric_limits<partition_key::compound::element_type::size_type>::max()); } data_model::mutation_description::key random_schema::make_clustering_key(uint32_t n, value_generator& gen) const {     assert(_schema->clustering_key_size() > 0);     return make_key(n, gen, _schema->clustering_key_columns(), std::numeric_limits<clustering_key::compound::element_type::size_type>::max()); } random_schema::random_schema(uint32_t seed, random_schema_specification& spec)     : _schema(build_random_schema(seed, spec)) { } sstring random_schema::cql() const {     auto udts = dump_udts(*_schema);     sstring udts_str;     if (!udts.empty()) {         udts_str = boost::algorithm::join(udts |                 boost::adaptors::transformed([] (const user_type_impl* const udt) { return udt_to_str(*udt); }), "\n");     }     std::vector<sstring> col_specs;     for (auto kind : {column_kind::partition_key, column_kind::clustering_key, column_kind::regular_column, column_kind::static_column}) {         auto cols = columns_specs(_schema, kind);         std::move(cols.begin(), cols.end(), std::back_inserter(col_specs));     }     sstring primary_key;     auto partition_column_names = column_names(_schema, column_kind::partition_key);     auto clustering_key_names = column_names(_schema, column_kind::clustering_key);     if (!clustering_key_names.empty()) {         primary_key = format("({}), {}", boost::algorithm::join(partition_column_names, ", "), boost::algorithm::join(clustering_key_names, ", "));     } else {         primary_key = format("{}", boost::algorithm::join(partition_column_names, ", "));     }     // FIXME include the clustering column orderings
    return format(             "{}\nCREATE TABLE {}.{} (\n\t{}\n\tPRIMARY KEY ({}))",             udts_str,             _schema->ks_name(),             _schema->cf_name(),             boost::algorithm::join(col_specs, ",\n\t"),             primary_key); } data_model::mutation_description::key random_schema::make_pkey(uint32_t n) {     value_generator g;     return make_partition_key(n, g); } std::vector<data_model::mutation_description::key> random_schema::make_pkeys(size_t n) {     std::set<dht::decorated_key, dht::ring_position_less_comparator> keys{dht::ring_position_less_comparator{*_schema}};     value_generator val_gen;     uint32_t i{0};     while (keys.size() < n) {         keys.emplace(dht::decorate_key(*_schema, partition_key::from_exploded(make_partition_key(i, val_gen))));         ++i;     }     return boost::copy_range<std::vector<data_model::mutation_description::key>>(keys |             boost::adaptors::transformed([] (const dht::decorated_key& dkey) { return dkey.key().explode(); })); } data_model::mutation_description::key random_schema::make_ckey(uint32_t n) {     value_generator g;     return make_clustering_key(n, g); } std::vector<data_model::mutation_description::key> random_schema::make_ckeys(size_t n) {     std::set<clustering_key, clustering_key::less_compare> keys{clustering_key::less_compare{*_schema}};     value_generator val_gen;     for (uint32_t i = 0; i < n; i++) {         keys.emplace(clustering_key::from_exploded(make_clustering_key(i, val_gen)));     }     return boost::copy_range<std::vector<data_model::mutation_description::key>>(keys |             boost::adaptors::transformed([] (const clustering_key& ckey) { return ckey.explode(); })); } data_model::mutation_description random_schema::new_mutation(data_model::mutation_description::key pkey) {     return data_model::mutation_description(std::move(pkey)); } data_model::mutation_description random_schema::new_mutation(uint32_t n) {     return new_mutation(make_pkey(n)); } void random_schema::set_partition_tombstone(std::mt19937& engine, data_model::mutation_description& md, timestamp_generator ts_gen,         expiry_generator exp_gen) {     if (const auto ts = ts_gen(engine, timestamp_destination::partition_tombstone, api::min_timestamp); ts != api::missing_timestamp) {         auto expiry_opt = exp_gen(engine, timestamp_destination::partition_tombstone);         const auto deletion_time = expiry_opt ? expiry_opt->expiry_point : gc_clock::now();         md.set_partition_tombstone(tombstone(ts, deletion_time));     } } void random_schema::add_row(std::mt19937& engine, data_model::mutation_description& md, uint32_t n, timestamp_generator ts_gen,         expiry_generator exp_gen) {     add_row(engine, md, make_ckey(n), std::move(ts_gen), std::move(exp_gen)); } void random_schema::add_static_row(std::mt19937& engine, data_model::mutation_description& md, timestamp_generator ts_gen, expiry_generator exp_gen) {     value_generator gen;     for (const auto& cdef : _schema->static_columns()) {         auto value = gen.generate_value(engine, *cdef.type);         decorate_with_timestamps(*_schema, engine, ts_gen, exp_gen, value);         md.add_static_cell(cdef.name_as_text(), std::move(value));     } } void random_schema::delete_range(         std::mt19937& engine,         data_model::mutation_description& md,         nonwrapping_range<data_model::mutation_description::key> range,         timestamp_generator ts_gen,         expiry_generator exp_gen) {     auto expiry_opt = exp_gen(engine, timestamp_destination::range_tombstone);     const auto deletion_time = expiry_opt ? expiry_opt->expiry_point : gc_clock::now();     md.add_range_tombstone(std::move(range), tombstone{ts_gen(engine, timestamp_destination::range_tombstone, api::min_timestamp), deletion_time}); } future<> random_schema::create_with_cql(cql_test_env& env) {     return async([this, &env] {         const auto ks_name = _schema->ks_name();         const auto tbl_name = _schema->cf_name();         for (const auto& udt : dump_udts(*_schema)) {             env.execute_cql(udt_to_str(*udt)).get();             eventually_true([&] () mutable {                 return env.db().map_reduce0([&] (replica::database& db) {                     return db.user_types().get(ks_name).has_type(udt->get_name());                 }, true, std::logical_and<bool>{}).get();             });         }         auto& db = env.local_db();         std::stringstream ss;         _schema->describe(db, ss, false);         env.execute_cql(ss.str()).get();         env.require_table_exists(ks_name, tbl_name).get();         auto& tbl = db.find_column_family(ks_name, tbl_name);         _schema = tbl.schema();     }); } future<std::vector<mutation>> generate_random_mutations(         uint32_t seed,         tests::random_schema& random_schema,         timestamp_generator ts_gen,         expiry_generator exp_gen,         std::uniform_int_distribution<size_t> partition_count_dist,         std::uniform_int_distribution<size_t> clustering_row_count_dist,         std::uniform_int_distribution<size_t> range_tombstone_count_dist) {     auto engine = std::mt19937(seed);     const auto schema_has_clustering_columns = random_schema.schema()->clustering_key_size() > 0;     const auto partition_count = partition_count_dist(engine);     std::vector<mutation> muts;     muts.reserve(partition_count);     for (size_t pk = 0; pk != partition_count; ++pk) {         auto mut = random_schema.new_mutation(pk);         random_schema.set_partition_tombstone(engine, mut, ts_gen, exp_gen);         random_schema.add_static_row(engine, mut, ts_gen, exp_gen);         if (!schema_has_clustering_columns) {             muts.emplace_back(mut.build(random_schema.schema()));             continue;         }         const auto clustering_row_count = clustering_row_count_dist(engine);         const auto range_tombstone_count = range_tombstone_count_dist(engine);         auto ckeys = random_schema.make_ckeys(std::max(clustering_row_count, range_tombstone_count));         for (uint32_t ck = 0; ck < ckeys.size(); ++ck) {             random_schema.add_row(engine, mut, ckeys[ck], ts_gen, exp_gen);             co_await coroutine::maybe_yield();         }         for (size_t i = 0; i < range_tombstone_count; ++i) {             const auto a = tests::random::get_int<size_t>(0, ckeys.size() - 1, engine);             const auto b = tests::random::get_int<size_t>(0, ckeys.size() - 1, engine);             random_schema.delete_range(                     engine,                     mut,                     nonwrapping_range<tests::data_model::mutation_description::key>::make(ckeys.at(std::min(a, b)), ckeys.at(std::max(a, b))),                     ts_gen,                     exp_gen);             co_await coroutine::maybe_yield();         }         muts.emplace_back(mut.build(random_schema.schema()));     }     boost::sort(muts, [s = random_schema.schema()] (const mutation& a, const mutation& b) {             return a.decorated_key().less_compare(*s, b.decorated_key());             });     auto range = boost::unique(muts, [s = random_schema.schema()] (const mutation& a, const mutation& b) {             return a.decorated_key().equal(*s, b.decorated_key());             });     muts.erase(range.end(), muts.end());     co_return std::move(muts); } future<std::vector<mutation>> generate_random_mutations(         tests::random_schema& random_schema,         timestamp_generator ts_gen,         expiry_generator exp_gen,         std::uniform_int_distribution<size_t> partition_count_dist,         std::uniform_int_distribution<size_t> clustering_row_count_dist,         std::uniform_int_distribution<size_t> range_tombstone_count_dist) {     return generate_random_mutations(tests::random::get_int<uint32_t>(), random_schema, std::move(ts_gen), std::move(exp_gen), partition_count_dist,             clustering_row_count_dist, range_tombstone_count_dist); } future<std::vector<mutation>> generate_random_mutations(tests::random_schema& random_schema, size_t partition_count) {     return generate_random_mutations(             random_schema,             default_timestamp_generator(),             no_expiry_expiry_generator(),             std::uniform_int_distribution<size_t>(partition_count, partition_count)); } }
 // namespace tests
namespace tests { namespace { template<typename RawKey, typename DecoratedKey, typename Comparator> std::vector<DecoratedKey> generate_keys(         size_t n,         schema_ptr s,         Comparator cmp,         const std::vector<data_type>& types,         std::function<std::optional<DecoratedKey>(const RawKey&)> decorate_fun,         bool allow_prefixes,         std::optional<key_size> size) {     auto keys = std::set<DecoratedKey, Comparator>(cmp);     const auto effective_size = size.value_or(tests::key_size{1, 128});     std::mt19937 engine(tests::random::get_int<uint32_t>());     std::uniform_int_distribution<size_t> component_count_dist(1, types.size());     tests::value_generator value_gen;     std::vector<data_value> components;     components.reserve(types.size());     while (keys.size() != n) {         components.clear();         auto component_count = allow_prefixes ? component_count_dist(engine) : types.size();         for (size_t i = 0; i < component_count; ++i) {             components.emplace_back(value_gen.generate_atomic_value(engine, *types.at(i), effective_size.min, effective_size.max));         }         auto raw_key = RawKey::from_deeply_exploded(*s, components);         // discard empty keys on the off chance that we generate one
        if (raw_key.is_empty() || (types.size() == 1 && raw_key.begin(*s)->empty())) {             continue;         }         if constexpr (std::is_same_v<RawKey, DecoratedKey>) {             keys.emplace(std::move(raw_key));         } else if (auto decorated_key_opt = decorate_fun(raw_key); decorated_key_opt) {             keys.emplace(std::move(*decorated_key_opt));         }     }     return std::vector<DecoratedKey>(keys.begin(), keys.end()); } } std::vector<dht::decorated_key> generate_partition_keys(size_t n, schema_ptr s, std::optional<shard_id> shard, std::optional<key_size> size) {     return generate_keys<partition_key, dht::decorated_key, dht::decorated_key::less_comparator>(             n,             s,             dht::decorated_key::less_comparator(s),             s->partition_key_type()->types(),             [s, shard, tokens = std::set<dht::token>()] (const partition_key& pkey) mutable -> std::optional<dht::decorated_key> {                 auto dkey = dht::decorate_key(*s, pkey);                 if (shard && *shard != dht::shard_of(*s, dkey.token())) {                     return {};                 }                 if (!tokens.insert(dkey.token()).second) {                     return {};                 }                 return dkey;             },             false,             size); } std::vector<dht::decorated_key> generate_partition_keys(size_t n, schema_ptr s, local_shard_only lso, std::optional<key_size> size) {     return generate_partition_keys(n, std::move(s), lso == local_shard_only::yes ? std::optional(this_shard_id()) : std::nullopt, size); } dht::decorated_key generate_partition_key(schema_ptr s, std::optional<shard_id> shard, std::optional<key_size> size) {     auto&& keys = generate_partition_keys(1, std::move(s), shard, size);     return std::move(keys.front()); } dht::decorated_key generate_partition_key(schema_ptr s, local_shard_only lso, std::optional<key_size> size) {     return generate_partition_key(std::move(s), lso == local_shard_only::yes ? std::optional(this_shard_id()) : std::nullopt, size); } std::vector<clustering_key> generate_clustering_keys(size_t n, schema_ptr s, bool allow_prefixes, std::optional<key_size> size) {     return generate_keys<clustering_key, clustering_key, clustering_key::less_compare>(             n,             s,             clustering_key::less_compare(*s),             s->clustering_key_type()->types(),             {},             allow_prefixes,             size); } clustering_key generate_clustering_key(schema_ptr s, bool allow_prefix, std::optional<key_size> size) {     auto&& keys = generate_clustering_keys(1, std::move(s), allow_prefix, size);     return std::move(keys.front()); } }
 // namespace tests
